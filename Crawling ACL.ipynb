{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a235f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aef1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url=\"https://dblp.org/db/conf/acl/index.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conference List Each Year\n",
    "links_response = requests.get(base_url)\n",
    "links=[]\n",
    "if links_response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(links_response.text, 'html.parser')\n",
    "    for link in soup.select('ul > li > cite > a.toc-link'):\n",
    "        links.append(link.get('href'))\n",
    "    soup.decompose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdea6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawling Papers in Each Conference\n",
    "paper_links=[]\n",
    "for link in links:\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        temp_list=soup.select('ul > li.inproceedings > nav > ul > li > div > ul > li.ee > a')\n",
    "        temp_list_len=len(temp_list)\n",
    "        for i in range(0,temp_list_len):\n",
    "            if temp_list[i].get('href').startswith('https://doi.org/'):\n",
    "                paper_links.append(temp_list[i].get('href'))\n",
    "        soup.decompose()\n",
    "    elif link_response.status_code == 404:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"THERE IS AN ERROR!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33262997",
   "metadata": {},
   "source": [
    "## Use Semanticscholar API to get Paper informations\n",
    "### https://github.com/danielnsilva/semanticscholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semanticscholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper doi만 추출\n",
    "paper_DOI=[]\n",
    "for paper_link in paper_links:\n",
    "    paper_DOI.append(paper_link.split('https://doi.org/')[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "aedda86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dblp_links, paper_links,paper_DOI\n",
    "with open('./dblp_links.json','w') as f:\n",
    "    json.dump(links,f)\n",
    "with open('./paper_links.json','w') as f:\n",
    "    json.dump(paper_links,f)\n",
    "with open('./paper_doi.json','w') as f:\n",
    "    json.dump(paper_DOI,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75696a6",
   "metadata": {},
   "source": [
    "## API Only accepts 500 papers in one call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c45eec",
   "metadata": {},
   "source": [
    "## 약 10000개 paper 500개씩 Chunking & API CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88698cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the list of paper doi\n",
    "listOfPaper=[]\n",
    "before=0\n",
    "for i in range(500,len(paper_links),500):\n",
    "    listOfPaper.append(paper_DOI[before:i])\n",
    "    before=i\n",
    "listOfPaper.append(paper_DOI[10000:len(paper_links)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "bf769062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "sch = SemanticScholar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc268c",
   "metadata": {},
   "source": [
    "## 안에 있는 500개 중에 ERROR가 있어서 되는 chunk 먼저 API CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There is an error with API, so it needs to be modified a little bit\n",
    "## 7~20 index first\n",
    "temp7to9=[]\n",
    "for i in range(7,10):\n",
    "    print(i)\n",
    "    paper=sch.get_papers(listOfPaper[i])\n",
    "    temp7to9+=list(paper)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10,11,12,13,14 is ERROR...............................ㅠㅠㅠㅠ\n",
    "temp15to17=[]  ## typo!! 16까지임\n",
    "for i in range(15,17):\n",
    "    print(i)\n",
    "    paper=sch.get_papers(listOfPaper[i])\n",
    "    temp15to17+=list(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b7adc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17,18,19 ERROR\n",
    "temp20=[]\n",
    "paper=sch.get_papers(listOfPaper[20])\n",
    "temp20+=list(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "52f916c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listOfPapers\n",
    "with open('listOfPaper.json','w') as f:\n",
    "    json.dump(listOfPaper,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439419b1",
   "metadata": {},
   "source": [
    "## SO many errors in API.....*(짧은 시간에 너무 많은 API call로 Ban당하는 중)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "dcf144e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # for time sleep\n",
    "from tqdm import tqdm\n",
    "paperIdList=[]# paperid 저장용 list size(15,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "884e2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_FIELDS = [\n",
    "        'abstract',\n",
    "        'authors',\n",
    "        'citationCount',\n",
    "        'corpusId',\n",
    "        'externalIds',\n",
    "        'fieldsOfStudy',\n",
    "        'influentialCitationCount',\n",
    "        'isOpenAccess',\n",
    "        'journal',\n",
    "        'openAccessPdf',\n",
    "        'paperId',\n",
    "        'publicationDate',\n",
    "        'publicationTypes',\n",
    "        'publicationVenue',\n",
    "        'referenceCount',\n",
    "        's2FieldsOfStudy',\n",
    "        'title',\n",
    "        'url',\n",
    "        'venue',\n",
    "        'year'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c9d785",
   "metadata": {},
   "source": [
    "## 중간에 소문자로 인해 인식 안 되는 중 -> 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074412d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list11to13=[] #11~13번 chunk\n",
    "for i in [11,12,13]:\n",
    "    ls=[]\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        doi=listOfPaper[i][j].split(\"10.18653/v1/\")[1].upper()\n",
    "        ls.append(\"10.18653/v1/\"+doi)\n",
    "    list11to13.append(ls)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca2392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp11to13=[]\n",
    "for i in range(0,3):\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(list11to13[i][j],SEARCH_FIELDS)\n",
    "            temp11to13.append(paper)\n",
    "        except:\n",
    "            print(i,j,list11to13[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d2d06",
   "metadata": {},
   "source": [
    "## 10.18653 이 아니라 10.3115인 구간으로 인해 (14,473)부터 다시 request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b19a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list14=[] #14번 청크\n",
    "for j in tqdm(range(0,473),desc=str(14)):\n",
    "        doi=listOfPaper[14][j].split(\"10.18653/v1/\")[1].upper()\n",
    "        list14.append(\"10.18653/v1/\"+doi)\n",
    "for j in tqdm(range(473,500),desc=str(14)):\n",
    "    doi=listOfPaper[14][j].split(\"10.3115/v1/\")[1].upper()\n",
    "    list14.append(\"10.3115/v1/\"+doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "38c0755c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  26%|████████████████████▋                                                            | 128/500 [04:53<04:38,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 10.18653/v1/P16-1146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  45%|████████████████████████████████████▎                                            | 224/500 [08:35<10:51,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223 10.18653/v1/P16-2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  71%|█████████████████████████████████████████████████████████▏                       | 353/500 [13:24<07:59,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 10.18653/v1/P16-3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2: 100%|█████████████████████████████████████████████████████████████████████████████████| 500/500 [18:47<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "temp14=[]\n",
    "for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(list14[j],SEARCH_FIELDS)\n",
    "            temp14.append(paper)\n",
    "        except:\n",
    "            print(j,list14[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163c883",
   "metadata": {},
   "source": [
    "## 17,18,19 chunk 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list17to19=[]\n",
    "for i in [17,18]:\n",
    "    ls=[]\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        doi=listOfPaper[i][j].split(\"10.3115/\")[1].upper()\n",
    "        ls.append(\"10.3115/\"+doi)\n",
    "    list17to19.append(ls) \n",
    "    \n",
    "ls=[]\n",
    "for j in tqdm(range(0,260),desc=str(19)):\n",
    "    doi=listOfPaper[i][j].split(\"10.3115/\")[1].upper()\n",
    "    ls.append(\"10.3115/\"+doi)\n",
    "k=23\n",
    "for i in range(260,282):\n",
    "    ls.append(\"10.1007/3-540-55801-2_\"+str(k))\n",
    "    k+=1\n",
    "for j in tqdm(range(282,500),desc=str(19)):\n",
    "    doi=listOfPaper[19][j].split(\"10.3115/\")[1].upper()\n",
    "    ls.append(\"10.3115/\"+doi)\n",
    "list17to19.append(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16accc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  34%|███████████████████████████▍                                                     | 169/500 [06:19<03:09,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 168 10.3115/1218955.1219038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  60%|████████████████████████████████████████████████▎                                | 298/500 [11:10<02:27,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 297 10.3115/1073083.1073091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  65%|████████████████████████████████████████████████████▊                            | 326/500 [12:26<12:13,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 325 10.3115/1073083.1073126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  73%|███████████████████████████████████████████████████████████▍                     | 367/500 [13:57<02:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 365 10.3115/1118149.1118157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "0:  74%|███████████████████████████████████████████████████████████▌                     | 368/500 [13:57<01:30,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 367 10.3115/1118149.1118159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  75%|████████████████████████████████████████████████████████████▍                    | 373/500 [14:19<06:53,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 372 10.3115/1118656.1118659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  75%|████████████████████████████████████████████████████████████▊                    | 375/500 [14:19<03:35,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 374 10.3115/1118656.1118661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  76%|█████████████████████████████████████████████████████████████▌                   | 380/500 [14:20<00:55,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 378 10.3115/1118656.1118665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  77%|██████████████████████████████████████████████████████████████▎                  | 385/500 [14:41<03:14,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 383 10.3115/1118656.1118670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 100%|█████████████████████████████████████████████████████████████████████████████████| 500/500 [18:51<00:00,  2.26s/it]\n",
      "1:  44%|███████████████████████████████████▊                                             | 221/500 [08:48<43:39,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 220 10.3115/980845.980951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1: 100%|█████████████████████████████████████████████████████████████████████████████████| 500/500 [18:56<00:00,  2.27s/it]\n",
      "2:  53%|██████████████████████████████████████████▉                                      | 265/500 [10:09<06:36,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 263 10.1007/3-540-55801-2_26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  64%|███████████████████████████████████████████████████▋                             | 319/500 [12:07<04:53,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 318 10.3115/981823.981860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  90%|████████████████████████████████████████████████████████████████████████▉        | 450/500 [17:02<00:24,  2.08it/s]"
     ]
    }
   ],
   "source": [
    "temp17to19=[]\n",
    "for i in range(0,3):\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(list17to19[i][j],SEARCH_FIELDS)\n",
    "            temp17to19.append(paper)\n",
    "        except:\n",
    "            print(i,j,list17to19[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in tqdm(range(385,500),desc=str(10)):\n",
    "        doi=listOfPaper[10][j].split(\"10.18653/v1/\")[1].upper() ## 대문자로 변경해야 됨\n",
    "        listOfPaper[10][j]=\"10.18653/v1/\"+doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "53bd1f2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:   4%|██▌                                                                   | 18/500 [00:54<29:49,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 17 10.18653/v1/2020.acl-main.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:  74%|██████████████████████████████████████████████████▉                  | 369/500 [14:03<01:15,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 367 10.18653/v1/2020.acl-tutorials.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:  92%|███████████████████████████████████████████████████████████████▌     | 461/500 [17:46<04:09,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 460 10.18653/v1/P19-1076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [18:55<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "temp10=[]\n",
    "\n",
    "for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(listOfPaper[10][j],SEARCH_FIELDS)\n",
    "            temp10.append(paper)\n",
    "        except:\n",
    "            print(i,j,listOfPaper[10][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93352245",
   "metadata": {},
   "source": [
    "## 0~6 chunk는 중간에 계속 에러 나는 경우가 많아서 어쩔 수 없이 paperId필요\n",
    "## 크롤링 사이트의 형식이 이상해서 가끔 제목추출이 이상하게 되는 경우가 있음. \n",
    "## 따라서 논문 제목을 위해 Crossref Unified Resource API 사용....\n",
    "https://api.crossref.org/swagger-ui/index.html#/Works/get_works__doi_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://api.crossref.org/works/\"+listOfPaper[0][0]\n",
    "paper_response=requests.get(url)\n",
    "paper=paper_response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddef34f",
   "metadata": {},
   "source": [
    "### DOI 중에 arkiv로 되어있는 DOI로 찾아야하는 경우가 많음. 따라서 semantic scholar에서 논문 제목을 통해 paperId 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "cdd5ebe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  53%|████████████████████████████████████▌                                | 265/500 [33:14<20:42,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Ordinal Latent Variable Model of Conflict Intensity 10.18653/v1/2023.acl-long.265 0 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  53%|████████████████████████████████████▊                                | 267/500 [33:25<22:33,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training to Learn in Context 10.18653/v1/2023.acl-long.267 0 266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  81%|███████████████████████████████████████████████████████▊             | 404/500 [46:59<21:03, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peeking inside the black box: A Commonsense-aware Generative Framework for Explainable Complaint Detection 10.18653/v1/2023.acl-long.404 0 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  83%|█████████████████████████████████████████████████████████▏           | 414/500 [48:37<17:17, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic representations for fewer-shot relation extraction across domains 10.18653/v1/2023.acl-long.414 0 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  89%|█████████████████████████████████████████████████████████████▌       | 446/500 [50:11<04:14,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Document-Level Event Argument Extraction 10.18653/v1/2023.acl-long.446 0 445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [53:29<00:00,  6.42s/it]\n",
      "1:  13%|█████████▏                                                            | 66/500 [03:51<34:48,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just Like a Human Would, Direct Access to Sarcasm Augmented with Potential Result and Reaction 10.18653/v1/2023.acl-long.566 1 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [52:57<00:00,  6.36s/it]\n",
      "2: 100%|███████████████████████████████████████████████████████████████████| 500/500 [1:17:52<00:00,  9.35s/it]\n",
      "3:  70%|██████████████████████████████████████████████▋                    | 348/500 [1:56:05<20:33,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models 10.18653/v1/2023.findings-acl.741 3 347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  87%|████████████████████████████████████████████████████████▌        | 435/500 [2:16:58<1:35:38, 88.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffuSum: Generation Enhanced Extractive Summarization with Diffusion 10.18653/v1/2023.findings-acl.828 3 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  89%|█████████████████████████████████████████████████████████▌       | 443/500 [2:26:58<1:34:42, 99.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract then Play: A Skill-centric Reinforcement Learning Framework for Text-based Games 10.18653/v1/2023.findings-acl.836 3 442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  90%|█████████████████████████████████████████████████████████▌      | 450/500 [2:43:10<2:02:32, 147.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don’t Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness 10.18653/v1/2023.findings-acl.843 3 449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  91%|██████████████████████████████████████████████████████████▎     | 456/500 [2:50:10<1:22:32, 112.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiaASQ: A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis 10.18653/v1/2023.findings-acl.849 3 455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  93%|███████████████████████████████████████████████████████████▌    | 465/500 [3:02:13<1:15:59, 130.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring the Effectiveness of Prompt Engineering for Legal Reasoning Tasks 10.18653/v1/2023.findings-acl.858 3 464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  97%|████████████████████████████████████████████████████████████████▊  | 484/500 [3:21:16<24:50, 93.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection and Mitigation of the Negative Impact of Dataset Extractivity on Abstractive Summarization 10.18653/v1/2023.findings-acl.877 3 483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3: 100%|███████████████████████████████████████████████████████████████████| 500/500 [3:29:20<00:00, 25.12s/it]\n",
      "4:  78%|█████████████████████████████████████████████████████▉               | 391/500 [20:04<13:13,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating Failures of Automatic Translationin the Case of Unambiguous Gender 10.18653/v1/2022.acl-long.243 4 390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:  92%|███████████████████████████████████████████████████████████████▍     | 460/500 [23:07<00:36,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models 10.18653/v1/2022.acl-long.312 4 459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [24:57<00:00,  2.99s/it]\n",
      "5:  31%|█████████████████████▌                                               | 156/500 [07:39<26:38,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpness-Aware Minimization Improves Language Model Generalization 10.18653/v1/2022.acl-long.508 5 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [26:35<00:00,  3.19s/it]\n",
      "6: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [36:58<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "paperIdList=[]\n",
    "for i in [0,1,2,3,4,5,6]:\n",
    "    ls=[]\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "            if j%10==0 and not (i==0 and j==0):\n",
    "                time.sleep(20)\n",
    "            url=\"https://api.crossref.org/works/\"+listOfPaper[i][j]\n",
    "            paper_response=requests.get(url)\n",
    "            if paper_response.status_code==200:\n",
    "                    paper=paper_response.json()\n",
    "                    title=paper['message']['title'][0]\n",
    "                    try:\n",
    "                        temp=sch.search_paper(title)\n",
    "                        if len(temp[0].paperId)==0:\n",
    "                            ls.append(listOfPaper[i][j])\n",
    "                            print(title,listOfPaper[i][j],i,j)\n",
    "                        else:\n",
    "                            ls.append(temp[0].paperId)\n",
    "                            \n",
    "                    except:\n",
    "                        ls.append(listOfPaper[i][j])\n",
    "                        print(title,listOfPaper[i][j],i,j)\n",
    "            else:\n",
    "                print(\"no\")\n",
    "    paperIdList.append(ls)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e74f1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  39%|███████████████████████████▏                                         | 197/500 [08:06<20:48,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 196 ade39c39048d66465c7288e4a7f8258a1bce9e60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  79%|██████████████████████████████████████████████████████▊              | 397/500 [16:00<02:17,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 396 0232715f9089e3a2fc002cff6737bb9939805b8d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  93%|███████████████████████████████████████████████████████████████▉     | 463/500 [18:53<03:54,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 462 52f3b181f6361cd85914798273e497264b2bc32d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [20:01<00:00,  2.40s/it]\n",
      "1: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [18:55<00:00,  2.27s/it]\n",
      "2:  29%|████████████████████▎                                                | 147/500 [05:44<09:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 146 ef5f7cd21b5d34797636239a7b9c8ba6af440aab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [19:09<00:00,  2.30s/it]\n",
      "6:   4%|██▊                                                                   | 20/500 [00:45<04:07,  1.94it/s]"
     ]
    }
   ],
   "source": [
    "temp0to6=[]\n",
    "for i in range(0,7):\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(paperIdList[i][j],SEARCH_FIELDS)\n",
    "            temp0to6.append(paper)\n",
    "        except:\n",
    "            print(i,j,paperIdList[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in [temp0to6,temp7to9,temp10,temp11to13,temp14,temp15to17,temp17to19,temp20]:\n",
    "    print(len(temp))\n",
    "    for i in range(0,len(temp)):\n",
    "        temp[i]=dict(temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "057149c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_papers=[]\n",
    "\n",
    "acl_papers+=temp0to6\n",
    "\n",
    "acl_papers+=temp7to9 #7~9 chunk\n",
    "\n",
    "acl_papers+=temp10\n",
    "\n",
    "acl_papers+=temp11to13\n",
    "\n",
    "acl_papers+=temp14\n",
    "\n",
    "acl_papers+=temp15to17 #15,16, '17' is typo!!!!\n",
    "\n",
    "acl_papers+=temp17to19\n",
    "\n",
    "acl_papers+=temp20 # 20 chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "id": "39d68c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=\"./ACL_PAPERS.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datapath,'w') as f:\n",
    "    json.dump(acl_papers,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4954e60",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa8b41",
   "metadata": {},
   "source": [
    "## 300개 missing data 중 55개 제외 살림 (abstract data 없는게 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "id": "d6467bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9684 none\n",
      "9760 none\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "absls=[]\n",
    "for i in range(0,len(acl_papers)):\n",
    "        if acl_papers[i]['abstract']==None:\n",
    "            print(i, 'none')\n",
    "            absls.append(i)\n",
    "            c+=1\n",
    "        elif len(acl_papers[i]['abstract'])<2:\n",
    "            print(i,'dot')\n",
    "            absls.append(i)\n",
    "            c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "id": "f552f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4028 dd\n"
     ]
    }
   ],
   "source": [
    "for i in absls:\n",
    "    try:\n",
    "        doi=acl_papers[i]['externalIds']['DOI'].split(\"10.18653/v1/\")[1]\n",
    "        link=\"https://aclanthology.org/\"+doi\n",
    "        link_response = requests.get(link)\n",
    "    except:\n",
    "        print(i,\"no site\")\n",
    "        break\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        abstract=soup.select('#main >div>div> div >div> span')\n",
    "        try:\n",
    "            if len(abstract[0].text)!=0:\n",
    "                acl_papers[i]['abstract']=abstract[0].text\n",
    "        except:\n",
    "            print(i,\"no abstract\")\n",
    "            break\n",
    "        soup.decompose()\n",
    "    elif link_response.status_code == 404:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e2fca",
   "metadata": {},
   "source": [
    "# 약 79~99 - ACM library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "id": "a4a98dd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9760\n",
      "9761\n",
      "9762\n",
      "9764\n",
      "9765\n",
      "9766\n",
      "9767\n",
      "9768\n",
      "9769\n",
      "9770\n",
      "9771\n",
      "9772\n",
      "9773\n",
      "9774\n",
      "9775\n",
      "9776\n",
      "9777\n",
      "9778\n",
      "9779\n",
      "9780\n",
      "9781\n"
     ]
    }
   ],
   "source": [
    "for i in absls:\n",
    "    try:\n",
    "        doi=acl_papers[i]['externalIds']['DOI']\n",
    "        link=\"https://dl.acm.org/doi/abs/\"+doi\n",
    "        link_response = requests.get(link)\n",
    "    except:\n",
    "        print(i,\"no site\")\n",
    "        break\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        abstract=soup.select('div.abstractSection > p')\n",
    "        try:\n",
    "            if len(abstract[0].text)!=0:\n",
    "                acl_papers[i]['abstract']=abstract[0].text\n",
    "        except:\n",
    "            print(i,\"no abstract\")\n",
    "            break\n",
    "        soup.decompose()\n",
    "    elif link_response.status_code == 404:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(\"!!\",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59978d91",
   "metadata": {},
   "source": [
    "# 9684,9760 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475e56a",
   "metadata": {},
   "source": [
    "# 문제 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a69771",
   "metadata": {},
   "source": [
    "{2023: 1739,\n",
    " 2022: 1266,\n",
    " 2021: 1345,\n",
    " 2020: 937,\n",
    " 2019: 821,\n",
    " 2018: 474,\n",
    " 2017: 388,\n",
    " 2016: 489,\n",
    " 2015: 397,\n",
    " 2014: 484,\n",
    " 2013: 2,\n",
    " 2006: 165,\n",
    " 2005: 79,\n",
    " 2004: 87,\n",
    " 2002: 96,\n",
    " 2003: 119,\n",
    " 2001: 70,\n",
    " 2000: 79,\n",
    " 1999: 83,\n",
    " 1998: 245,\n",
    " 1997: 73,\n",
    " 1996: 57,\n",
    " 1995: 56,\n",
    " 1994: 52,\n",
    " 1993: 47,\n",
    " 1992: 54,\n",
    " 1991: 78,\n",
    " 1990: 39,\n",
    " 1989: 34,\n",
    " 1988: 35,\n",
    " 1987: 34,\n",
    " 1986: 40,\n",
    " 1985: 41,\n",
    " 1984: 116,\n",
    " 1983: 25,\n",
    " 1982: 39,\n",
    " 1981: 36,\n",
    " 1980: 44,\n",
    " 1979: 28}\n",
    " \n",
    " 없는 년도가 있다...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b2dd8",
   "metadata": {},
   "source": [
    "## 당연히 DOI가 모든 paper에 있을 줄 알고 doi.org/~ 된 것만 추가함..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "id": "bd13561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████▊              | 135/168 [05:52<01:26,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THERE IS AN ERROR!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Crawling Papers in Each Conference\n",
    "missed_links=[]\n",
    "for link in tqdm(links):\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        temp_list=soup.select('ul > li.inproceedings > nav > ul > li > div > ul > li.ee > a')\n",
    "        temp_list_len=len(temp_list)\n",
    "        alreadyIn=0\n",
    "        for i in range(0,temp_list_len):\n",
    "            if temp_list[i].get('href').startswith('https://doi.org/'):\n",
    "                alreadyIn=1\n",
    "                break\n",
    "        if alreadyIn==0:\n",
    "            for i in range(0,temp_list_len):\n",
    "                if temp_list[i].get('href').startswith('https://aclanthology.org/'):\n",
    "                    missed_links.append(temp_list[i].get('href'))\n",
    "        soup.decompose()\n",
    "    else:\n",
    "        print(link)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc492d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in tqdm(links[136:]):\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        temp_list=soup.select('ul > li.inproceedings > nav > ul > li > div > ul > li.ee > a')\n",
    "        temp_list_len=len(temp_list)\n",
    "        alreadyIn=0\n",
    "        for i in range(0,temp_list_len):\n",
    "            if temp_list[i].get('href').startswith('https://doi.org/'):\n",
    "                alreadyIn=1\n",
    "                break\n",
    "        if alreadyIn==0:\n",
    "            for i in range(0,temp_list_len):\n",
    "                if temp_list[i].get('href').startswith('https://aclanthology.org/'):\n",
    "                    missed_links.append(temp_list[i].get('href'))\n",
    "        soup.decompose()\n",
    "    else:\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_response = requests.get(\"https://aclanthology.org/W97-1005/\")\n",
    "if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        title=soup.select('#title > a')[0].text\n",
    "        temp=soup.select('p.lead > a')\n",
    "        authors=[{'name':i.text} for i in temp]\n",
    "        year=soup.select('div.order-2 > dl > dd')[3].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "id": "e4e5046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for abstraction\n",
    "def paperId(title):\n",
    "    temp=sch.search_paper(title)\n",
    "    if temp[0]['title']==title:\n",
    "        return temp[0]\n",
    "    else :\n",
    "        print(title)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "id": "4768ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa85b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in tqdm(missed_links[2438:]):\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        title=soup.select('#title > a')[0].text\n",
    "#         temp=soup.select('p.lead > a')\n",
    "#         authors=[{'name':i.text} for i in temp]\n",
    "#         year=soup.select('div.order-2 > dl > dd')[3].text\n",
    "        paper_info=paperId(title)\n",
    "        if paper_info is None:\n",
    "            continue\n",
    "        else:\n",
    "            missed_papers.append(paper_info)\n",
    "            x=random.randint(10, 15)\n",
    "            time.sleep(x)\n",
    "        soup.decompose()\n",
    "    else:\n",
    "        print(1,link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "id": "1099a77b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2177 none\n",
      "2347 none\n",
      "2391 none\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "mls=[]\n",
    "for i in range(0,len(missed_papers)):\n",
    "        if missed_papers[i]['abstract']==None:\n",
    "            print(i, 'none')\n",
    "            mls.append(i)\n",
    "            c+=1\n",
    "        elif len(missed_papers[i]['abstract'])<2:\n",
    "            print(i,'dot')\n",
    "            mls.append(i)\n",
    "            c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1438,
   "id": "6aee7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paperId': 'b8b961044536ca4e95b025d8683b6f3a096d256a', 'externalIds': {'DBLP': 'conf/acl/Alm11', 'MAG': '2251892406', 'CorpusId': 16940041}, 'corpusId': 16940041, 'publicationVenue': {'id': '1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44', 'name': 'Annual Meeting of the Association for Computational Linguistics', 'type': 'conference', 'alternate_names': ['Annu Meet Assoc Comput Linguistics', 'Meeting of the Association for Computational Linguistics', 'ACL', 'Meet Assoc Comput Linguistics'], 'url': 'https://www.aclweb.org/anthology/venues/acl/'}, 'url': 'https://www.semanticscholar.org/paper/b8b961044536ca4e95b025d8683b6f3a096d256a', 'title': 'Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications', 'abstract': 'This opinion paper discusses subjective natural language problems in terms of their motivations, applications, characterizations, and implications. It argues that such problems deserve increased attention because of their potential to challenge the status of theoretical understanding, problem-solving methods, and evaluation techniques in computational linguistics. The author supports a more holistic approach to such problems; a view that extends beyond opinion mining or sentiment analysis.', 'venue': 'Annual Meeting of the Association for Computational Linguistics', 'year': 2011, 'referenceCount': 32, 'citationCount': 56, 'influentialCitationCount': 1, 'isOpenAccess': False, 'openAccessPdf': None, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle', 'Conference'], 'publicationDate': '2011-06-19', 'journal': {'pages': '107-112'}, 'authors': [{'authorId': '144648940', 'name': 'Cecilia Ovesdotter Alm'}]}\n",
      "{'paperId': '02940f157db0b2415f1cbc50576a607d40b2bdc4', 'externalIds': {'CorpusId': 5131462}, 'corpusId': 5131462, 'publicationVenue': None, 'url': 'https://www.semanticscholar.org/paper/02940f157db0b2415f1cbc50576a607d40b2bdc4', 'title': 'From Bilingual Dictionaries to Interlingual Document Representations', 'abstract': 'Mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation. Since the candidate alignments are noisy, we develop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain.', 'venue': '', 'year': 2011, 'referenceCount': 25, 'citationCount': 6, 'influentialCitationCount': 0, 'isOpenAccess': False, 'openAccessPdf': None, 'fieldsOfStudy': None, 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}], 'publicationTypes': None, 'publicationDate': None, 'journal': None, 'authors': [{'authorId': '1728602', 'name': 'S. Dumais'}]}\n",
      "{'paperId': 'abd398e97c591326bae6410e1df35dad95f46d3c', 'externalIds': {'DBLP': 'conf/acl/Seaghdha10', 'MAG': '2147298557', 'CorpusId': 272315}, 'corpusId': 272315, 'publicationVenue': {'id': '1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44', 'name': 'Annual Meeting of the Association for Computational Linguistics', 'type': 'conference', 'alternate_names': ['Annu Meet Assoc Comput Linguistics', 'Meeting of the Association for Computational Linguistics', 'ACL', 'Meet Assoc Comput Linguistics'], 'url': 'https://www.aclweb.org/anthology/venues/acl/'}, 'url': 'https://www.semanticscholar.org/paper/abd398e97c591326bae6410e1df35dad95f46d3c', 'title': 'Latent Variable Models of Selectional Preference', 'abstract': 'This paper describes the application of so-called topic models to selectional preference induction. Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word cooccurrences, are presented and evaluated on datasets of human plausibility judgements. Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data.', 'venue': 'Annual Meeting of the Association for Computational Linguistics', 'year': 2010, 'referenceCount': 33, 'citationCount': 119, 'influentialCitationCount': 7, 'isOpenAccess': False, 'openAccessPdf': None, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle', 'Conference'], 'publicationDate': '2010-07-11', 'journal': {'pages': '435-444'}, 'authors': [{'authorId': '8311581', 'name': 'Diarmuid Ó Séaghdha'}]}\n",
      "{'paperId': '443a10f3bd061cd43485b1aa1ca7e3f6be09068d', 'externalIds': {'MAG': '2133685497', 'DBLP': 'conf/acl/Kaljahi10', 'CorpusId': 15662214}, 'corpusId': 15662214, 'publicationVenue': {'id': '1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44', 'name': 'Annual Meeting of the Association for Computational Linguistics', 'type': 'conference', 'alternate_names': ['Annu Meet Assoc Comput Linguistics', 'Meeting of the Association for Computational Linguistics', 'ACL', 'Meet Assoc Comput Linguistics'], 'url': 'https://www.aclweb.org/anthology/venues/acl/'}, 'url': 'https://www.semanticscholar.org/paper/443a10f3bd061cd43485b1aa1ca7e3f6be09068d', 'title': 'Adapting Self-Training for Semantic Role Labeling', 'abstract': 'Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods.', 'venue': 'Annual Meeting of the Association for Computational Linguistics', 'year': 2010, 'referenceCount': 15, 'citationCount': 15, 'influentialCitationCount': 1, 'isOpenAccess': False, 'openAccessPdf': None, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle', 'Conference'], 'publicationDate': '2010-07-13', 'journal': {'pages': '91-96'}, 'authors': [{'authorId': '2095950', 'name': 'R. Kaljahi'}]}\n",
      "{'paperId': '83b2f3f334036ff792cdf1f8d1dff183813fae1e', 'externalIds': {'MAG': '2114874863', 'DBLP': 'conf/acl/NguyenP09', 'DOI': '10.3115/1667583.1667639', 'CorpusId': 3171223}, 'corpusId': 3171223, 'publicationVenue': None, 'url': 'https://www.semanticscholar.org/paper/83b2f3f334036ff792cdf1f8d1dff183813fae1e', 'title': 'An Ontology-Based Approach for Key Phrase Extraction', 'abstract': 'Automatic key phrase extraction is fundamental to the success of many recent digital library applications and semantic information retrieval techniques and a difficult and essential problem in Vietnamese natural language processing (NLP). In this work, we propose a novel method for key phrase extracting of Vietnamese text that exploits the Vietnamese Wikipedia as an ontology and exploits specific characteristics of the Vietnamese language for the key phrase selection stage. We also explore NLP techniques that we propose for the analysis of Vietnamese texts, focusing on the advanced candidate phrases recognition phase as well as part-of-speech (POS) tagging. Finally, we review the results of several experiments that have examined the impacts of strategies chosen for Vietnamese key phrase extracting.', 'venue': 'ACL/IJCNLP', 'year': 2009, 'referenceCount': 7, 'citationCount': 23, 'influentialCitationCount': 1, 'isOpenAccess': True, 'openAccessPdf': None, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle', 'Review'], 'publicationDate': '2009-08-04', 'journal': {'pages': '181-184'}, 'authors': [{'authorId': '1815400', 'name': 'Chau Q. Nguyen'}, {'authorId': '2843932', 'name': 'T. Phan'}]}\n",
      "{'paperId': '12c34b0f5b4420a8bd2551394d23527504a4d774', 'externalIds': {'DBLP': 'conf/acl/Daume09', 'MAG': '2121812871', 'DOI': '10.3115/1667583.1667673', 'CorpusId': 10368968}, 'corpusId': 10368968, 'publicationVenue': None, 'url': 'https://www.semanticscholar.org/paper/12c34b0f5b4420a8bd2551394d23527504a4d774', 'title': 'Markov Random Topic Fields', 'abstract': 'Most approaches to topic modeling assume an independence between documents that is frequently violated. We present an topic model that makes use of one or more user-specified graphs describing relationships between documents. These graph are encoded in the form of a Markov random field over topics and serve to encourage related documents to have similar topic structures. Experiments on show upwards of a 10% improvement in modeling performance.', 'venue': 'ACL/IJCNLP', 'year': 2009, 'referenceCount': 7, 'citationCount': 29, 'influentialCitationCount': 1, 'isOpenAccess': True, 'openAccessPdf': {'url': 'http://dl.acm.org/ft_gateway.cfm?id=1667673&type=pdf', 'status': 'BRONZE'}, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Mathematics', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle'], 'publicationDate': '2009-08-04', 'journal': {'pages': '293-296'}, 'authors': [{'authorId': '1722360', 'name': 'Hal Daumé'}]}\n",
      "{'paperId': '4171c85cc6ed4accca639421bc9deda322c22f9e', 'externalIds': {'DBLP': 'conf/sigmorphon/Jones08', 'MAG': '2050952146', 'DOI': '10.3115/1626324.1626331', 'CorpusId': 17336620}, 'corpusId': 17336620, 'publicationVenue': {'id': '56b19d19-ddf9-4e56-ba1e-37ef38ef5943', 'name': 'Special Interest Group on Computational Morphology and Phonology Workshop', 'type': 'conference', 'alternate_names': ['SIGMORPHON', 'Spéc Interest Group Comput Morphol Phonol Workshop'], 'url': 'http://sigmorphon.org/'}, 'url': 'https://www.semanticscholar.org/paper/4171c85cc6ed4accca639421bc9deda322c22f9e', 'title': 'Phonotactic Probability and the Maori Passive: A Computational Approach', 'abstract': 'Two analyses of Maori passives and gerunds have been debated in the literature. Both assume that the thematic consonants in these forms are unpredictable. This paper reports on three computational experiments designed to test whether this assumption is sound. The results suggest that thematic consonants are predictable from the phonotactic probabilities of their active counterparts. This study has potential implications for allomorphy in other Polynesian languages. It also exemplifies the benefits of using computational methods in linguistic analyses.', 'venue': 'Special Interest Group on Computational Morphology and Phonology Workshop', 'year': 2008, 'referenceCount': 29, 'citationCount': 5, 'influentialCitationCount': 0, 'isOpenAccess': True, 'openAccessPdf': {'url': 'http://dl.acm.org/ft_gateway.cfm?id=1626331&type=pdf', 'status': 'BRONZE'}, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle'], 'publicationDate': '2008-06-19', 'journal': {'pages': '39-48'}, 'authors': [{'authorId': '3834060', 'name': 'Oiwi Parker Jones'}]}\n",
      "{'paperId': 'e8ce3ae449642877c0ecf91cd216969dd19a998f', 'externalIds': {'MAG': '2100413690', 'DBLP': 'conf/acl/Seaghdha07', 'DOI': '10.3115/1557835.1557852', 'CorpusId': 1717722}, 'corpusId': 1717722, 'publicationVenue': {'id': '1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44', 'name': 'Annual Meeting of the Association for Computational Linguistics', 'type': 'conference', 'alternate_names': ['Annu Meet Assoc Comput Linguistics', 'Meeting of the Association for Computational Linguistics', 'ACL', 'Meet Assoc Comput Linguistics'], 'url': 'https://www.aclweb.org/anthology/venues/acl/'}, 'url': 'https://www.semanticscholar.org/paper/e8ce3ae449642877c0ecf91cd216969dd19a998f', 'title': 'Annotating and Learning Compound Noun Semantics', 'abstract': 'There is little consensus on a standard experimental design for the compound interpretation task. This paper introduces well-motivated general desiderata for semantic annotation schemes, and describes such a scheme for in-context compound annotation accompanied by detailed publicly available guidelines. Classification experiments on an open-text dataset compare favourably with previously reported results and provide a solid baseline for future research.', 'venue': 'Annual Meeting of the Association for Computational Linguistics', 'year': 2007, 'referenceCount': 20, 'citationCount': 35, 'influentialCitationCount': 6, 'isOpenAccess': True, 'openAccessPdf': {'url': 'https://dl.acm.org/doi/pdf/10.5555/1557835.1557852', 'status': 'BRONZE'}, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle', 'Conference'], 'publicationDate': '2007-06-25', 'journal': {'name': '', 'pages': '73-78', 'volume': ''}, 'authors': [{'authorId': '8311581', 'name': 'Diarmuid Ó Séaghdha'}]}\n",
      "{'paperId': 'f4d1a5da9c66da653b88adb7a63c0271b2de8a85', 'externalIds': {'MAG': '2950429596', 'DBLP': 'conf/acl/MurataUMI01', 'ArXiv': 'cs/0112003', 'DOI': '10.3115/1118037.1118052', 'CorpusId': 15818617}, 'corpusId': 15818617, 'publicationVenue': None, 'url': 'https://www.semanticscholar.org/paper/f4d1a5da9c66da653b88adb7a63c0271b2de8a85', 'title': 'Using a Support-Vector Machine for Japanese-to-English Translation of Tense, Aspect, and Modality', 'abstract': 'This paper describes experiments carried out using a variety of machine-learning methods, including the k-nearest neighborhood method that was used in a previous study, for the translation of tense, aspect, and modality. It was found that the support-vector machine method was the most precise of all the methods tested.', 'venue': 'DDMMT@ACL', 'year': 2001, 'referenceCount': 12, 'citationCount': 15, 'influentialCitationCount': 0, 'isOpenAccess': True, 'openAccessPdf': {'url': 'https://dl.acm.org/doi/pdf/10.3115/1118037.1118052', 'status': 'BRONZE'}, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle'], 'publicationDate': '2001-07-07', 'journal': {'name': 'ArXiv', 'volume': 'cs.CL/0112003'}, 'authors': [{'authorId': '1697381', 'name': 'M. Murata'}, {'authorId': '1783611', 'name': 'Kiyotaka Uchimoto'}, {'authorId': '145853732', 'name': 'Qing Ma'}, {'authorId': '1714134', 'name': 'H. Isahara'}]}\n",
      "{'paperId': '7cd7ea196c8dbf915030d078ca853b550e43fa2e', 'externalIds': {'CorpusId': 11415196}, 'corpusId': 11415196, 'publicationVenue': None, 'url': 'https://www.semanticscholar.org/paper/7cd7ea196c8dbf915030d078ca853b550e43fa2e', 'title': 'Distortion Model Considering Rich Context for Statistical Machine Translation', 'abstract': 'This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models.', 'venue': '', 'year': 2013, 'referenceCount': 19, 'citationCount': 11, 'influentialCitationCount': 1, 'isOpenAccess': False, 'openAccessPdf': None, 'fieldsOfStudy': None, 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}], 'publicationTypes': None, 'publicationDate': None, 'journal': None, 'authors': [{'authorId': '2205832', 'name': 'Isao Goto'}, {'authorId': '1802277', 'name': 'M. Utiyama'}, {'authorId': '1698363', 'name': 'E. Sumita'}, {'authorId': '2263274512', 'name': 'Akihiro Tamura'}, {'authorId': '1795664', 'name': 'S. Kurohashi'}]}\n",
      "{'paperId': 'd24b7c8bfcb321aaa3e76ed3877ab79c20935b81', 'externalIds': {'CorpusId': 8214629}, 'corpusId': 8214629, 'publicationVenue': None, 'url': 'https://www.semanticscholar.org/paper/d24b7c8bfcb321aaa3e76ed3877ab79c20935b81', 'title': 'Using Derivation Trees for Treebank Error Detection', 'abstract': 'This work introduces a new approach to checking treebank consistency. Derivation trees based on a variant of Tree Adjoining Grammar are used to compare the annotation of word sequences based on their structural similarity. This overcomes the problems of earlier approaches based on using strings of words rather than tree structure to identify the appropriate contexts for comparison. We report on the result of applying this approach to the Penn Arabic Treebank and how this approach leads to high precision of error detection.', 'venue': '', 'year': 2011, 'referenceCount': 11, 'citationCount': 9, 'influentialCitationCount': 0, 'isOpenAccess': False, 'openAccessPdf': None, 'fieldsOfStudy': None, 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}], 'publicationTypes': None, 'publicationDate': None, 'journal': None, 'authors': [{'authorId': '2695965', 'name': 'S. Kulick'}, {'authorId': '3212973', 'name': 'Ann Bies'}, {'authorId': '47517717', 'name': 'Justin Mott'}]}\n"
     ]
    }
   ],
   "source": [
    "for i in missed_papers:\n",
    "    try:\n",
    "        if i['externalIds']['ACL']==None:\n",
    "            print(i)\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1430,
   "id": "84b20125",
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_papers+=missed_papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
