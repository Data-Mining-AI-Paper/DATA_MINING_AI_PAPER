[{"paperId": "a293a01ddd639b25360cf4f23e2df8dd0d1caa8e", "externalIds": {"ArXiv": "2105.07464", "DBLP": "journals/corr/abs-2105-07464", "ACL": "2021.acl-long.248", "DOI": "10.18653/v1/2021.acl-long.248", "CorpusId": 234742165}, "corpusId": 234742165, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a293a01ddd639b25360cf4f23e2df8dd0d1caa8e", "title": "Few-NERD: A Few-shot Named Entity Recognition Dataset", "abstract": "Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present Few-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of the two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research. The Few-NERD dataset and the baselines will be publicly available to facilitate the research on this problem.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 122, "influentialCitationCount": 44, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.248.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-16", "journal": {"name": "ArXiv", "volume": "abs/2105.07464"}, "authors": [{"authorId": "46649145", "name": "Ning Ding"}, {"authorId": "2149131512", "name": "Guangwei Xu"}, {"authorId": "2135835258", "name": "Yulin Chen"}, {"authorId": "2108114693", "name": "Xiaobin Wang"}, {"authorId": "48506411", "name": "Xu Han"}, {"authorId": "35930962", "name": "Pengjun Xie"}, {"authorId": "16215052", "name": "Haitao Zheng"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}]}, {"paperId": "b4985722eeb0f0a60b8545b770ea94c7e6adfccf", "externalIds": {"ACL": "2021.acl-long.249", "DBLP": "conf/acl/ZhangZZ0L20", "DOI": "10.18653/v1/2021.acl-long.249", "CorpusId": 236460179}, "corpusId": 236460179, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b4985722eeb0f0a60b8545b770ea94c7e6adfccf", "title": "MultiMET: A Multimodal Dataset for Metaphor Understanding", "abstract": "Metaphor involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought, which makes understanding it challenging. As a means of cognition, metaphor is rendered by more than texts alone, and multimodal information in which vision/audio content is integrated with the text can play an important role in expressing and understanding metaphor. However, previous metaphor processing and understanding has focused on texts, partly due to the unavailability of large-scale datasets with ground truth labels of multimodal metaphor. In this paper, we introduce MultiMET, a novel multimodal metaphor dataset to facilitate understanding metaphorical information from multimodal text and image. It contains 10,437 text-image pairs from a range of sources with multimodal annotations of the occurrence of metaphors, domain relations, sentiments metaphors convey, and author intents. MultiMET opens the door to automatic metaphor understanding by investigating multimodal cues and their interplay. Moreover, we propose a range of strong baselines and show the importance of combining multimodal cues for metaphor understanding. MultiMET will be released publicly for research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 67, "citationCount": 15, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.249.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3214-3225"}, "authors": [{"authorId": "2109567247", "name": "Dongyu Zhang"}, {"authorId": "2112152933", "name": "Minghao Zhang"}, {"authorId": "1390919608", "name": "Heting Zhang"}, {"authorId": "2143920912", "name": "Liang Yang"}, {"authorId": "37553559", "name": "Hongfei Lin"}]}, {"paperId": "efdb6e2691decd4de9b58d3967b5a592a6b0a809", "externalIds": {"ArXiv": "2107.08720", "DBLP": "journals/corr/abs-2107-08720", "ACL": "2021.acl-long.250", "DOI": "10.18653/v1/2021.acl-long.250", "CorpusId": 236087808}, "corpusId": 236087808, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/efdb6e2691decd4de9b58d3967b5a592a6b0a809", "title": "Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech", "abstract": "Undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities. Thus, some NLP studies have started addressing the task of counter narrative generation. Although such studies have made an effort to build hate speech / counter narrative (HS/CN) datasets for neural generation, they fall short in reaching either high-quality and/or high-quantity. In this paper, we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and/or post-edit. Our experiments comprised several loops including diverse dynamic variations. Results show that the methodology is scalable and facilitates diverse, novel, and cost-effective data collection. To our knowledge, the resulting dataset is the only expert-based multi-target HS/CN dataset available to the community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 46, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.250.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-07-19", "journal": {"pages": "3226-3240"}, "authors": [{"authorId": "23752383", "name": "Margherita Fanton"}, {"authorId": "2120042648", "name": "Helena Bonaldi"}, {"authorId": "2034636", "name": "Serra Sinem Tekiro\u011flu"}, {"authorId": "1912357", "name": "Marco Guerini"}]}, {"paperId": "f62acd332fd7a6f35b117ed4ffaf93b19483dcf7", "externalIds": {"ACL": "2021.acl-long.251", "ArXiv": "2106.01561", "DBLP": "journals/corr/abs-2106-01561", "DOI": "10.18653/v1/2021.acl-long.251", "CorpusId": 235313524}, "corpusId": 235313524, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f62acd332fd7a6f35b117ed4ffaf93b19483dcf7", "title": "Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?", "abstract": "Recent work has investigated the interesting question using pre-trained language models (PLMs) as knowledge bases for answering open questions. However, existing work is limited in using small benchmarks with high test-train overlaps. We construct a new dataset of closed-book QA using SQuAD, and investigate the performance of BART. Experiments show that it is challenging for BART to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant knowledge is retained. Some promising directions are found, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 52, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.251.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01561"}, "authors": [{"authorId": "35504092", "name": "Cunxiang Wang"}, {"authorId": "2108139697", "name": "Pai Liu"}, {"authorId": "1591125925", "name": "Yue Zhang"}]}, {"paperId": "af4453d721cffea3d393572f3803d5f3a1864e7c", "externalIds": {"DBLP": "conf/acl/ZhangVM20", "ArXiv": "2107.04217", "ACL": "2021.acl-long.252", "DOI": "10.18653/v1/2021.acl-long.252", "CorpusId": 235790533}, "corpusId": 235790533, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/af4453d721cffea3d393572f3803d5f3a1864e7c", "title": "Joint Models for Answer Verification in Question Answering Systems", "abstract": "This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (AS2) modules, which are core components of retrieval-based Question Answering (QA) systems. Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers. For this purpose, we build a three-way multi-classifier, which decides if an answer supports, refutes, or is neutral with respect to another one. More specifically, our neural architecture integrates a state-of-the-art AS2 module with the multi-classifier, and a joint layer connecting all components. We tested our models on WikiQA, TREC-QA, and a real-world dataset. The results show that our models obtain the new state of the art in AS2.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 33, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.252.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-09", "journal": {"name": "ArXiv", "volume": "abs/2107.04217"}, "authors": [{"authorId": "74723434", "name": "Zeyu Zhang"}, {"authorId": "145223120", "name": "Thuy Vu"}, {"authorId": "1719404", "name": "Alessandro Moschitti"}]}, {"paperId": "17ccd81f77d8cf798f12608ade1d838ffb96b66c", "externalIds": {"ACL": "2021.acl-long.253", "DBLP": "journals/corr/abs-2011-13137", "MAG": "3109846333", "ArXiv": "2011.13137", "DOI": "10.18653/v1/2021.acl-long.253", "CorpusId": 227208738}, "corpusId": 227208738, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/17ccd81f77d8cf798f12608ade1d838ffb96b66c", "title": "Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction", "abstract": "In open-domain question answering, questions are highly likely to be ambiguous because users may not know the scope of relevant topics when formulating them. Therefore, a system needs to find possible interpretations of the question, and predict one or multiple plausible answers. When multiple plausible answers are found, the system should rewrite the question for each answer to resolve the ambiguity. In this paper, we present a model that aggregates and combines evidence from multiple passages to adaptively predict a single answer or a set of question-answer pairs for ambiguous questions. In addition, we propose a novel round-trip prediction approach to iteratively generate additional interpretations that our model fails to find in the first pass, and then verify and filter out the incorrect question-answer pairs to arrive at the final disambiguated output. Our model, named Refuel, achieves a new state-of-the-art performance on the AmbigQA dataset, and shows competitive performance on NQ-Open and TriviaQA. The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves our Refuel as well as several baseline models. We release source code for our models and experiments at https://github.com/amzn/refuel-open-domain-qa.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 13, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.253.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-26", "journal": {"pages": "3263-3276"}, "authors": [{"authorId": "1921742", "name": "Yifan Gao"}, {"authorId": "1787682", "name": "Henghui Zhu"}, {"authorId": "145878390", "name": "Patrick Ng"}, {"authorId": "1790831", "name": "C. D. Santos"}, {"authorId": "40296541", "name": "Zhiguo Wang"}, {"authorId": "144647318", "name": "Feng Nan"}, {"authorId": "2358258", "name": "Dejiao Zhang"}, {"authorId": "1701451", "name": "Ramesh Nallapati"}, {"authorId": "2112031035", "name": "Andrew O. Arnold"}, {"authorId": "144028698", "name": "Bing Xiang"}]}, {"paperId": "b3213c84a6ff7a2f11099de783c93166e4fc02a4", "externalIds": {"ACL": "2021.acl-long.254", "DBLP": "conf/acl/ZhuLHWZLFC20", "ArXiv": "2105.07624", "DOI": "10.18653/v1/2021.acl-long.254", "CorpusId": 234741852}, "corpusId": 234741852, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b3213c84a6ff7a2f11099de783c93166e4fc02a4", "title": "TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance", "abstract": "Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and the compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. TAGOP achieves 58.0% inF1, which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind performance of expert human, i.e.90.8% in F1. It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 97, "influentialCitationCount": 23, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.254.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-17", "journal": {"name": "ArXiv", "volume": "abs/2105.07624"}, "authors": [{"authorId": "31734386", "name": "Fengbin Zhu"}, {"authorId": "39165620", "name": "Wenqiang Lei"}, {"authorId": "2156083891", "name": "Youcheng Huang"}, {"authorId": "2144448019", "name": "Chao Wang"}, {"authorId": "2108032328", "name": "Shuo Zhang"}, {"authorId": "2075420316", "name": "Jiancheng Lv"}, {"authorId": "2163400298", "name": "Fuli Feng"}, {"authorId": "143779329", "name": "Tat-seng Chua"}]}, {"paperId": "ce4ed8a9a21e19997f3b371db988aae8cf2684a6", "externalIds": {"ACL": "2021.acl-long.255", "DBLP": "conf/acl/Lan020", "DOI": "10.18653/v1/2021.acl-long.255", "CorpusId": 236459867}, "corpusId": 236459867, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ce4ed8a9a21e19997f3b371db988aae8cf2684a6", "title": "Modeling Transitions of Focal Entities for Conversational Knowledge Base Question Answering", "abstract": "Conversational KBQA is about answering a sequence of questions related to a KB. Follow-up questions in conversational KBQA often have missing information referring to entities from the conversation history. In this paper, we propose to model these implied entities, which we refer to as the focal entities of the conversation. We propose a novel graph-based model to capture the transitions of focal entities and apply a graph neural network to derive a probability distribution of focal entities for each question, which is then combined with a standard KBQA module to perform answer ranking. Our experiments on two datasets demonstrate the effectiveness of our proposed method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 31, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.255.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3288-3297"}, "authors": [{"authorId": "3458560", "name": "Yunshi Lan"}, {"authorId": null, "name": "Jing Jiang"}]}, {"paperId": "03cb8234036dedd356901f574c1771a88e3578d8", "externalIds": {"ArXiv": "2012.15788", "DBLP": "journals/corr/abs-2106-01072", "ACL": "2021.acl-long.256", "DOI": "10.18653/v1/2021.acl-long.256", "CorpusId": 235294035}, "corpusId": 235294035, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/03cb8234036dedd356901f574c1771a88e3578d8", "title": "Evidence-based Factual Error Correction", "abstract": "This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 26, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.256.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"pages": "3298-3309"}, "authors": [{"authorId": "2053211210", "name": "James Thorne"}, {"authorId": "2064056928", "name": "Andreas Vlachos"}]}, {"paperId": "5382d681722a3b4a79b8ef638cd251f07f6316d3", "externalIds": {"ArXiv": "2106.06002", "DBLP": "journals/corr/abs-2106-06002", "ACL": "2021.acl-long.257", "DOI": "10.18653/v1/2021.acl-long.257", "CorpusId": 235417150}, "corpusId": 235417150, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5382d681722a3b4a79b8ef638cd251f07f6316d3", "title": "Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of AMR Alignments", "abstract": "We present algorithms for aligning components of Abstract Meaning Representation (AMR) graphs to spans in English sentences. We leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous AMR aligners. Our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse. Our approach covers a wider variety of AMR substructures than previously considered, achieves higher coverage of nodes and edges, and does so with higher accuracy. We will release our LEAMR datasets and aligner for use in research on AMR parsing, generation, and evaluation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 8, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.257.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-10", "journal": {"pages": "3310-3321"}, "authors": [{"authorId": "145079952", "name": "Austin Blodgett"}, {"authorId": "145254207", "name": "Nathan Schneider"}]}, {"paperId": "70a136547d81290b9f4dbc1fac49d31bc010bd3c", "externalIds": {"DBLP": "conf/acl/ConklinWST20", "ACL": "2021.acl-long.258", "ArXiv": "2106.04252", "DOI": "10.18653/v1/2021.acl-long.258", "CorpusId": 235367710}, "corpusId": 235367710, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/70a136547d81290b9f4dbc1fac49d31bc010bd3c", "title": "Meta-Learning to Compositionally Generalize", "abstract": "Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts. This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience. Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve). Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution. We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization. We construct pairs of tasks for meta-learning by sub-sampling existing training data. Each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input. Experimental results on the COGS and SCAN datasets show that our similarity-driven meta-learning can improve generalization performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 48, "citationCount": 56, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.258.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"name": "ArXiv", "volume": "abs/2106.04252"}, "authors": [{"authorId": "2107677996", "name": "Henry Conklin"}, {"authorId": "2118640406", "name": "Bailin Wang"}, {"authorId": "3054578", "name": "Kenny Smith"}, {"authorId": "144889265", "name": "Ivan Titov"}]}, {"paperId": "959927c3bdc254d0c02583d47f35654d53c4d6ef", "externalIds": {"ACL": "2021.acl-long.259", "DBLP": "conf/acl/DiaoXSJSZ20", "DOI": "10.18653/v1/2021.acl-long.259", "CorpusId": 236459863}, "corpusId": 236459863, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/959927c3bdc254d0c02583d47f35654d53c4d6ef", "title": "Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation", "abstract": "Large pre-trained models such as BERT are known to improve different downstream NLP tasks, even when such a model is trained on a generic domain. Moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks. However, this practice requires significant domain-specific data and computational resources which may not always be available. In this paper, we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data. We demonstrate that by explicitly incorporating multi-granularity information of unseen and domain-specific words via the adaptation of (word based) n-grams, the performance of a generic pretrained model can be greatly improved. Specifically, we introduce a Transformer-based Domain-aware N-gram Adaptor, T-DNA, to effectively learn and incorporate the semantic representation of different combinations of words in the new domain. Experimental results illustrate the effectiveness of T-DNA on eight low-resource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities. Our code is available at https://github.com/shizhediao/T-DNA.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 46, "citationCount": 25, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.259.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3336-3349"}, "authors": [{"authorId": "50826757", "name": "Shizhe Diao"}, {"authorId": "2192053707", "name": "Ruijia Xu"}, {"authorId": "2152173042", "name": "Hongjin Su"}, {"authorId": "48752037", "name": "Yilei Jiang"}, {"authorId": "1922182598", "name": "Yan Song"}, {"authorId": "2146324423", "name": "Tong Zhang"}]}, {"paperId": "25c3b294b9ed2786c4476a25e8b36ebf49fd5b4b", "externalIds": {"DBLP": "journals/corr/abs-2012-15022", "ACL": "2021.acl-long.260", "ArXiv": "2012.15022", "DOI": "10.18653/v1/2021.acl-long.260", "CorpusId": 229923565}, "corpusId": 229923565, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/25c3b294b9ed2786c4476a25e8b36ebf49fd5b4b", "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning", "abstract": "Pre-trained Language Models (PLMs) have shown superior performance on various downstream Natural Language Processing (NLP) tasks. However, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding. To address this issue, we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text. Specifically, we define two novel pre-training tasks to better understand entities and relations: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; (2) the relation discrimination task to distinguish whether two relations are close or not semantically, which involves complex relational reasoning. Experimental results demonstrate that ERICA can improve typical PLMs (BERT and RoBERTa) on several language understanding tasks, including relation extraction, entity typing and question answering, especially under low-resource settings.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 64, "citationCount": 86, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.260.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-30", "journal": {"name": "ArXiv", "volume": "abs/2012.15022"}, "authors": [{"authorId": "50625437", "name": "Yujia Qin"}, {"authorId": "2427350", "name": "Yankai Lin"}, {"authorId": "51055574", "name": "Ryuichi Takanobu"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "50492525", "name": "Peng Li"}, {"authorId": "2113323573", "name": "Heng Ji"}, {"authorId": "1730108", "name": "Minlie Huang"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "49178343", "name": "Jie Zhou"}]}, {"paperId": "02ea66066e0f257aee147766a0707e352639e5c0", "externalIds": {"ArXiv": "2106.03518", "MAG": "3173276074", "DBLP": "conf/acl/Yan0PH20", "ACL": "2021.acl-long.261", "DOI": "10.18653/v1/2021.acl-long.261", "CorpusId": 235359000}, "corpusId": 235359000, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/02ea66066e0f257aee147766a0707e352639e5c0", "title": "Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction", "abstract": "The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing models for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing models on such adversarial examples and observe a significant performance drop. To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 30, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.261.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "journal": {"pages": "3364-3375"}, "authors": [{"authorId": "1830443015", "name": "Hanqi Yan"}, {"authorId": "145096580", "name": "Lin Gui"}, {"authorId": "46922295", "name": "Gabriele Pergola"}, {"authorId": "1390509967", "name": "Yulan He"}]}, {"paperId": "86cceb149bc903724fa4eafbfc6e532ae18850d0", "externalIds": {"DBLP": "conf/acl/Bar-HaimEKFS20", "ACL": "2021.acl-long.262", "ArXiv": "2106.06758", "DOI": "10.18653/v1/2021.acl-long.262", "CorpusId": 235421642}, "corpusId": 235421642, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/86cceb149bc903724fa4eafbfc6e532ae18850d0", "title": "Every Bite Is an Experience: Key Point Analysis of Business Reviews", "abstract": "Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary. These approaches provide only a partial view of the data: aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views. Recently, Key Point Analysis (KPA) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data. We adapt KPA to review data by introducing Collective Key Point Mining for better key point extraction; integrating sentiment analysis into KPA; identifying good key point candidates for review summaries; and leveraging the massive amount of available reviews and their metadata. We show empirically that these novel extensions of KPA substantially improve its performance. We demonstrate that promising results can be achieved without any domain-specific annotation, while human supervision can lead to further improvement.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.262.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-06-12", "journal": {"name": "ArXiv", "volume": "abs/2106.06758"}, "authors": [{"authorId": "1693525", "name": "Roy Bar-Haim"}, {"authorId": "1668029117", "name": "Lilach Eden"}, {"authorId": "2965962", "name": "Yoav Kantor"}, {"authorId": "1404506446", "name": "Roni Friedman"}, {"authorId": "1766595", "name": "N. Slonim"}]}, {"paperId": "fd288ade5654d3e485e0102386d290a7e6fcae95", "externalIds": {"DBLP": "journals/corr/abs-2105-14504", "ArXiv": "2105.14504", "ACL": "2021.acl-long.263", "DOI": "10.18653/v1/2021.acl-long.263", "CorpusId": 235254574}, "corpusId": 235254574, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fd288ade5654d3e485e0102386d290a7e6fcae95", "title": "Structured Sentiment Analysis as Dependency Graph Parsing", "abstract": "Structured sentiment analysis attempts to extract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classification. We argue that this division has become counterproductive and propose a new unified framework to remedy the situation. We cast the structured sentiment problem as dependency graph parsing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs are the relations between them. We perform experiments on five datasets in four languages (English, Norwegian, Basque, and Catalan) and show that this approach leads to strong improvements over state-of-the-art baselines. Our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 56, "citationCount": 46, "influentialCitationCount": 17, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.263.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-30", "journal": {"name": "ArXiv", "volume": "abs/2105.14504"}, "authors": [{"authorId": "144435436", "name": "Jeremy Barnes"}, {"authorId": "34657387", "name": "Robin Kurtz"}, {"authorId": "2949607", "name": "S. Oepen"}, {"authorId": "1432235931", "name": "Lilja Ovrelid"}, {"authorId": "2027091", "name": "Erik Velldal"}]}, {"paperId": "1da75c7a8915c0794bc9857f93c6cf31aac17ad2", "externalIds": {"ArXiv": "2106.08226", "DBLP": "conf/acl/Zheng0HWCSC0SW20", "ACL": "2021.acl-long.264", "DOI": "10.18653/v1/2021.acl-long.264", "CorpusId": 235436224}, "corpusId": 235436224, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1da75c7a8915c0794bc9857f93c6cf31aac17ad2", "title": "Consistency Regularization for Cross-Lingual Fine-Tuning", "abstract": "Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 42, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.264.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-15", "journal": {"name": "ArXiv", "volume": "abs/2106.08226"}, "authors": [{"authorId": "2112670329", "name": "Bo Zheng"}, {"authorId": "145307652", "name": "Li Dong"}, {"authorId": "3110003", "name": "Shaohan Huang"}, {"authorId": "51456429", "name": "Wenhui Wang"}, {"authorId": "46221722", "name": "Zewen Chi"}, {"authorId": "14839052", "name": "Saksham Singhal"}, {"authorId": "2256319", "name": "Wanxiang Che"}, {"authorId": "40282288", "name": "Ting Liu"}, {"authorId": "50706785", "name": "Xia Song"}, {"authorId": "49807919", "name": "Furu Wei"}]}, {"paperId": "5539127e3907a492d97181c9e62e43f91d7cf19e", "externalIds": {"ArXiv": "2106.06381", "ACL": "2021.acl-long.265", "DBLP": "journals/corr/abs-2106-06381", "DOI": "10.18653/v1/2021.acl-long.265", "CorpusId": 235417503}, "corpusId": 235417503, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5539127e3907a492d97181c9e62e43f91d7cf19e", "title": "Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment", "abstract": "The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-label word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two steps in an expectation-maximization manner. Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rate on the alignment benchmarks. The code and pretrained parameters are available at github.com/CZWin32768/XLM-Align.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 54, "citationCount": 39, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.265.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-11", "journal": {"name": "ArXiv", "volume": "abs/2106.06381"}, "authors": [{"authorId": "46221722", "name": "Zewen Chi"}, {"authorId": "145307652", "name": "Li Dong"}, {"authorId": "145646411", "name": "Bo Zheng"}, {"authorId": "3110003", "name": "Shaohan Huang"}, {"authorId": "134880677", "name": "Xian-Ling Mao"}, {"authorId": "4590286", "name": "Heyan Huang"}, {"authorId": "49807919", "name": "Furu Wei"}]}, {"paperId": "d2197b6b3453d9bcb460588e077853020e2e0cdc", "externalIds": {"ACL": "2021.acl-long.266", "ArXiv": "2106.00903", "DBLP": "journals/corr/abs-2106-00903", "DOI": "10.18653/v1/2021.acl-long.266", "CorpusId": 235294012}, "corpusId": 235294012, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d2197b6b3453d9bcb460588e077853020e2e0cdc", "title": "Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation", "abstract": "Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words. To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining. By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source. Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words. To make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting NAT performance. We conduct experiments on five translation benchmarks over two advanced architectures. Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, respectively. Our code, data, and trained models are available at https://github.com/longyuewangdcu/RLFW-NAT.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 47, "citationCount": 35, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.266.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.00903"}, "authors": [{"authorId": "46573238", "name": "Liang Ding"}, {"authorId": "2111542852", "name": "Longyue Wang"}, {"authorId": "2151060023", "name": "Xuebo Liu"}, {"authorId": "1758353", "name": "Derek F. Wong"}, {"authorId": "143719920", "name": "D. Tao"}, {"authorId": "2909321", "name": "Zhaopeng Tu"}]}, {"paperId": "b0de1d5fe394226cec0a59d783ab739eb52da76f", "externalIds": {"ACL": "2021.acl-long.267", "DBLP": "journals/corr/abs-2105-14761", "MAG": "3169932073", "ArXiv": "2105.14761", "DOI": "10.18653/v1/2021.acl-long.267", "CorpusId": 235254048}, "corpusId": 235254048, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b0de1d5fe394226cec0a59d783ab739eb52da76f", "title": "G-Transformer for Document-Level Machine Translation", "abstract": "Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 47, "citationCount": 39, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.267.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"name": "ArXiv", "volume": "abs/2105.14761"}, "authors": [{"authorId": "1993226927", "name": "Guangsheng Bao"}, {"authorId": "1591125925", "name": "Yue Zhang"}, {"authorId": "2272668", "name": "Zhiyang Teng"}, {"authorId": "2152687324", "name": "Boxing Chen"}, {"authorId": "1935569", "name": "Weihua Luo"}]}, {"paperId": "82b57e0ed286fd8bc591c77b5301c1414055244c", "externalIds": {"DBLP": "journals/corr/abs-2105-11098", "ACL": "2021.acl-long.268", "ArXiv": "2105.11098", "DOI": "10.18653/v1/2021.acl-long.268", "CorpusId": 235166394}, "corpusId": 235166394, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/82b57e0ed286fd8bc591c77b5301c1414055244c", "title": "Prevent the Language Model from being Overconfident in Neural Machine Translation", "abstract": "The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token. The Margin is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentence-level Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 46, "citationCount": 18, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.268.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-24", "journal": {"pages": "3456-3468"}, "authors": [{"authorId": "2113705866", "name": "Mengqi Miao"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "2108060455", "name": "Yijin Liu"}, {"authorId": "2109126706", "name": "Xiao Zhou"}, {"authorId": "48128428", "name": "Jie Zhou"}]}, {"paperId": "96e7f77ed0101ac3c7c4dc41601563ce0bc8889b", "externalIds": {"DBLP": "journals/corr/abs-2106-01144", "ArXiv": "2106.01144", "ACL": "2021.acl-long.269", "DOI": "10.18653/v1/2021.acl-long.269", "CorpusId": 235294326}, "corpusId": 235294326, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/96e7f77ed0101ac3c7c4dc41601563ce0bc8889b", "title": "Towards Emotional Support Dialog Systems", "abstract": "Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 91, "influentialCitationCount": 26, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.269.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01144"}, "authors": [{"authorId": "50152447", "name": "Siyang Liu"}, {"authorId": "146452866", "name": "Chujie Zheng"}, {"authorId": "3403238", "name": "O. Demasi"}, {"authorId": "2106627931", "name": "Sahand Sabour"}, {"authorId": "40058381", "name": "Yu Li"}, {"authorId": "1564034697", "name": "Zhou Yu"}, {"authorId": "2118476056", "name": "Yong Jiang"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "60edc2750936fcea191f0bb835119743b11b76d2", "externalIds": {"ACL": "2021.acl-long.270", "DBLP": "journals/corr/abs-2105-14313", "ArXiv": "2105.14313", "DOI": "10.18653/v1/2021.acl-long.270", "CorpusId": 235254552}, "corpusId": 235254552, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/60edc2750936fcea191f0bb835119743b11b76d2", "title": "Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in the Task-Oriented Dialogue System", "abstract": "Existing slot filling models can only recognize pre-defined in-domain slot types from a limited slot set. In the practical application, a reliable dialogue system should know what it does not know. In this paper, we introduce a new task, Novel Slot Detection (NSD), in the task-oriented dialogue system. NSD aims to discover unknown or out-of-domain slot types to strengthen the capability of a dialogue system based on in-domain training data. Besides, we construct two public NSD datasets, propose several strong NSD baselines, and establish a benchmark for future work. Finally, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future directions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.270.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-29", "journal": {"pages": "3484-3494"}, "authors": [{"authorId": "50117922", "name": "Yanan Wu"}, {"authorId": "2101321888", "name": "Zhiyuan Zeng"}, {"authorId": "2058349088", "name": "Keqing He"}, {"authorId": "2146234760", "name": "Hong Xu"}, {"authorId": "1500528818", "name": "Yuanmeng Yan"}, {"authorId": "2309680", "name": "Huixing Jiang"}, {"authorId": "1753096", "name": "Weiran Xu"}]}, {"paperId": "b7ca0b0fa14f8a01596f027ec0eb91e2635bd15c", "externalIds": {"DBLP": "conf/acl/ShenMZFZ20", "ACL": "2021.acl-long.271", "ArXiv": "2106.03635", "DOI": "10.18653/v1/2021.acl-long.271", "CorpusId": 235358586}, "corpusId": 235358586, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b7ca0b0fa14f8a01596f027ec0eb91e2635bd15c", "title": "GTM: A Generative Triple-wise Model for Conversational Question Generation", "abstract": "Generating some appealing questions in open-domain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. To avoid dull or deviated questions, some researchers tried to utilize answer, the \u201cfuture\u201d information, to guide question generation. However, they separate a post-question-answer (PQA) triple into two parts: post-question (PQ) and question-answer (QA) pairs, which may hurt the overall coherence. Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations. To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG). Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs. Experimental results on a large-scale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 49, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.271.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Physics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "journal": {"pages": "3495-3506"}, "authors": [{"authorId": null, "name": "Lei Shen"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "2108970018", "name": "Jinchao Zhang"}, {"authorId": "49771779", "name": "Yang Feng"}, {"authorId": "49178343", "name": "Jie Zhou"}]}, {"paperId": "94772377a9c08ad81e506240f844534b6669b8e9", "externalIds": {"DBLP": "journals/corr/abs-2105-14556", "ArXiv": "2105.14556", "ACL": "2021.acl-long.272", "DOI": "10.18653/v1/2021.acl-long.272", "CorpusId": 235254714}, "corpusId": 235254714, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/94772377a9c08ad81e506240f844534b6669b8e9", "title": "Diversifying Dialog Generation via Adaptive Label Smoothing", "abstract": "Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. Our model can be trained in an endto-end manner. Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 49, "citationCount": 26, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.272.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-30", "journal": {"name": "ArXiv", "volume": "abs/2105.14556"}, "authors": [{"authorId": "2143476152", "name": "Yida Wang"}, {"authorId": "51456789", "name": "Yinhe Zheng"}, {"authorId": "2118476056", "name": "Yong Jiang"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "68c506d3d7e830df15226c020638af2e5fdd7a98", "externalIds": {"DBLP": "conf/acl/ZhanLLFWL20", "ACL": "2021.acl-long.273", "ArXiv": "2106.08616", "DOI": "10.18653/v1/2021.acl-long.273", "CorpusId": 235446895}, "corpusId": 235446895, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/68c506d3d7e830df15226c020638af2e5fdd7a98", "title": "Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training", "abstract": "Out-of-scope intent detection is of practical importance in task-oriented dialogue systems. Since the distribution of outlier utterances is arbitrary and unknown in the training stage, existing methods commonly rely on strong assumptions on data distribution such as mixture of Gaussians to make inference, resulting in either complex multi-step training procedures or hand-crafted rules such as confidence threshold selection for outlier detection. In this paper, we propose a simple yet effective method to train an out-of-scope intent classifier in a fully end-to-end manner by simulating the test scenario in training, which requires no assumption on data distribution and no additional post-processing or threshold setting. Specifically, we construct a set of pseudo outliers in the training stage, by generating synthetic outliers using inliner features via self-supervision and sampling out-of-scope sentences from easily available open-domain datasets. The pseudo outliers are used to train a discriminative classifier that can be directly applied to and generalize well on the test task. We evaluate our method extensively on four benchmark dialogue datasets and observe significant improvements over state-of-the-art approaches. Our code has been released at https://github.com/liam0949/DCLOOS.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 43, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.273.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-16", "journal": {"pages": "3521-3532"}, "authors": [{"authorId": "2061239456", "name": "Li-Ming Zhan"}, {"authorId": "2152874181", "name": "Haowen Liang"}, {"authorId": "73548014", "name": "Bo Liu"}, {"authorId": "2147259441", "name": "Lu Fan"}, {"authorId": "19195265", "name": "Xiao-Ming Wu"}, {"authorId": "1902169", "name": "Albert Y. S. Lam"}]}, {"paperId": "67615300b2bd2aa0f0566ee1e3a5e7d5bd6450b3", "externalIds": {"DBLP": "conf/acl/XuLLC20", "ArXiv": "2105.14924", "ACL": "2021.acl-long.274", "DOI": "10.18653/v1/2021.acl-long.274", "CorpusId": 235253912}, "corpusId": 235253912, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/67615300b2bd2aa0f0566ee1e3a5e7d5bd6450b3", "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker", "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 48, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.274.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "3533-3546"}, "authors": [{"authorId": "1748844142", "name": "Runxin Xu"}, {"authorId": "1500520681", "name": "Tianyu Liu"}, {"authorId": "143900005", "name": "Lei Li"}, {"authorId": "7267809", "name": "Baobao Chang"}]}, {"paperId": "2a964485a195bb2f65e129dec6976d26ef2803e3", "externalIds": {"ACL": "2021.acl-long.275", "DBLP": "conf/acl/WangS0W20", "DOI": "10.18653/v1/2021.acl-long.275", "CorpusId": 236460185}, "corpusId": 236460185, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2a964485a195bb2f65e129dec6976d26ef2803e3", "title": "Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path", "abstract": "This paper presents a novel method for nested named entity recognition. As a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path. Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.275.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3547-3557"}, "authors": [{"authorId": "2107920627", "name": "Yiran Wang"}, {"authorId": "73467110", "name": "Hiroyuki Shindo"}, {"authorId": "2087152269", "name": "Yuji Matsumoto"}, {"authorId": "2110694221", "name": "Taro Watanabe"}]}, {"paperId": "1351a827a5039586a3b3e27865ab8ceda342a235", "externalIds": {"ArXiv": "2106.01649", "DBLP": "conf/acl/ZuoC000PC20", "ACL": "2021.acl-long.276", "DOI": "10.18653/v1/2021.acl-long.276", "CorpusId": 235313431}, "corpusId": 235313431, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1351a827a5039586a3b3e27865ab8ceda342a235", "title": "LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification", "abstract": "Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 61, "citationCount": 29, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.276.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"pages": "3558-3571"}, "authors": [{"authorId": "87696608", "name": "Xinyu Zuo"}, {"authorId": "49776272", "name": "Pengfei Cao"}, {"authorId": "152829071", "name": "Yubo Chen"}, {"authorId": "77397868", "name": "Kang Liu"}, {"authorId": "11447228", "name": "Jun Zhao"}, {"authorId": "49161576", "name": "Weihua Peng"}, {"authorId": "2145264600", "name": "Yuguang Chen"}]}, {"paperId": "ddbab45dd9dbd5672730042ebb592be9cd6b19fb", "externalIds": {"DBLP": "journals/corr/abs-2105-10158", "ACL": "2021.acl-long.277", "ArXiv": "2105.10158", "DOI": "10.18653/v1/2021.acl-long.277", "CorpusId": 235125924}, "corpusId": 235125924, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ddbab45dd9dbd5672730042ebb592be9cd6b19fb", "title": "Revisiting the Negative Data of Distantly Supervised Relation Extraction", "abstract": "Distantly supervision automatically generates plenty of training samples for relation extraction. However, it also incurs two major problems: noisy labels and imbalanced training data. Previous works focus more on reducing wrongly labeled relations (false positives) while few explore the missing relations that are caused by incompleteness of knowledge base (false negatives). Furthermore, the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations. In this paper, we first provide a thorough analysis of the above challenges caused by negative data. Next, we formulate the problem of relation extraction into as a positive unlabeled learning task to alleviate false negative problem. Thirdly, we propose a pipeline approach, dubbed ReRe, that first performs sentence classification with relational labels and then extracts the subjects/objects. Experimental results show that the proposed method consistently outperforms existing approaches and remains excellent performance even learned with a large quantity of false positive samples. Source code is available online at https://github.com/redreamality/RERE-relation-extraction.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.277.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-21", "journal": {"pages": "3572-3581"}, "authors": [{"authorId": "2064754256", "name": "Chenhao Xie"}, {"authorId": "3366523", "name": "Jiaqing Liang"}, {"authorId": "2108343759", "name": "Jingping Liu"}, {"authorId": "31937655", "name": "Chengsong Huang"}, {"authorId": "2158103505", "name": "Wenhao Huang"}, {"authorId": "3011950", "name": "Yanghua Xiao"}]}, {"paperId": "30a677b3e4d1a52fffe84463f832929c48e27856", "externalIds": {"ACL": "2021.acl-long.278", "DBLP": "journals/corr/abs-2106-02248", "ArXiv": "2106.02248", "DOI": "10.18653/v1/2021.acl-long.278", "CorpusId": 235352501}, "corpusId": 235352501, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/30a677b3e4d1a52fffe84463f832929c48e27856", "title": "Knowing the No-match: Entity Alignment with Dangling Cases", "abstract": "This paper studies a new problem setting of entity alignment for knowledge graphs (KGs). Since KGs possess different sets of entities, there could be entities that cannot find alignment across them, leading to the problem of dangling entities. As the first attempt to this problem, we construct a new dataset and design a multi-task learning framework for both entity alignment and dangling entity detection. The framework can opt to abstain from predicting alignment for the detected dangling entities. We propose three techniques for dangling entity detection that are based on the distribution of nearest-neighbor distances, i.e., nearest neighbor classification, marginal ranking and background ranking. After detecting and removing dangling entities, an incorporated entity alignment model in our framework can provide more robust alignment for remaining entities. Comprehensive experiments and analyses demonstrate the effectiveness of our framework. We further discover that the dangling entity detection module can, in turn, improve alignment learning and the final performance. The contributed resource is publicly available to foster further research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 56, "citationCount": 24, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.278.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02248"}, "authors": [{"authorId": "2109745316", "name": "Zequn Sun"}, {"authorId": "1998918", "name": "Muhao Chen"}, {"authorId": "145066190", "name": "Wei Hu"}]}, {"paperId": "fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224", "externalIds": {"ACL": "2021.acl-long.279", "DBLP": "conf/acl/HofmannPS20a", "ArXiv": "2101.00403", "DOI": "10.18653/v1/2021.acl-long.279", "CorpusId": 233880587}, "corpusId": 233880587, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224", "title": "Superbizarre Is Not Superb: Derivational Morphology Improves BERT\u2019s Interpretation of Complex Words", "abstract": "How does the input segmentation of pretrained language models (PLMs) affect their interpretations of complex words? We present the first study investigating this question, taking BERT as the example PLM and focusing on its semantic representations of English derivatives. We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words. This hypothesis is confirmed by a series of semantic probing tasks on which DelBERT (Derivation leveraging BERT), a model with derivational input segmentation, substantially outperforms BERT with WordPiece segmentation. Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 105, "citationCount": 28, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.279.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-02", "journal": {"pages": "3594-3608"}, "authors": [{"authorId": "1667898858", "name": "Valentin Hofmann"}, {"authorId": "1970864", "name": "J. Pierrehumbert"}, {"authorId": "144418438", "name": "Hinrich Sch\u00fctze"}]}, {"paperId": "465491b0507e107f248a8277ac17248c2ff8f915", "externalIds": {"DBLP": "journals/corr/abs-2105-04949", "ArXiv": "2105.04949", "ACL": "2021.acl-long.280", "DOI": "10.18653/v1/2021.acl-long.280", "CorpusId": 234357758}, "corpusId": 234357758, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/465491b0507e107f248a8277ac17248c2ff8f915", "title": "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?", "abstract": "Analogies play a central role in human commonsense reasoning. The ability to recognize analogies such as \u201ceye is to seeing what ear is to hearing\u201d, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language. Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 77, "citationCount": 56, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.280.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-11", "journal": {"name": "ArXiv", "volume": "abs/2105.04949"}, "authors": [{"authorId": "27044733", "name": "Asahi Ushio"}, {"authorId": "2254466", "name": "Luis Espinosa Anke"}, {"authorId": "2265382", "name": "S. Schockaert"}, {"authorId": "1387447871", "name": "Jos\u00e9 Camacho-Collados"}]}, {"paperId": "4ea7c7e911b0d96838bd57a3f5e79028e0f9a1b4", "externalIds": {"DBLP": "journals/corr/abs-2106-13553", "ArXiv": "2106.13553", "ACL": "2021.acl-long.281", "DOI": "10.18653/v1/2021.acl-long.281", "CorpusId": 235652017}, "corpusId": 235652017, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4ea7c7e911b0d96838bd57a3f5e79028e0f9a1b4", "title": "Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy", "abstract": "This paper presents a multilingual study of word meaning representations in context. We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy. To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context. However, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences. Experiments are performed in Galician, Portuguese, English, and Spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 69, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.281.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-25", "journal": {"name": "ArXiv", "volume": "abs/2106.13553"}, "authors": [{"authorId": "144918133", "name": "Marcos Garcia"}]}, {"paperId": "c09d3857e732c7c44980115b4c0a0d9e4129d584", "externalIds": {"DBLP": "conf/acl/HuangCXH20", "ArXiv": "2105.13255", "ACL": "2021.acl-long.282", "DOI": "10.18653/v1/2021.acl-long.282", "CorpusId": 235212026}, "corpusId": 235212026, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c09d3857e732c7c44980115b4c0a0d9e4129d584", "title": "Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach", "abstract": "We propose to measure fine-grained domain relevance\u2013 the degree that a term is relevant to a broad (e.g., computer science) or narrow (e.g., deep learning) domain. Such measurement is crucial for many downstream tasks in natural language processing. To handle long-tail terms, we build a core-anchored semantic graph, which uses core terms with rich description information to bridge the vast remaining fringe terms semantically. To support a fine-grained domain without relying on a matching corpus for supervision, we develop hierarchical core-fringe learning, which learns core and fringe terms jointly in a semi-supervised manner contextualized in the hierarchy of the domain. To reduce expensive human efforts, we employ automatic annotation and hierarchical positive-unlabeled learning. Our approach applies to big or small domains, covers head or tail terms, and requires little human effort. Extensive experiments demonstrate that our methods outperform strong baselines and even surpass professional human performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.282.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-27", "journal": {"name": "ArXiv", "volume": "abs/2105.13255"}, "authors": [{"authorId": "1490651934", "name": "Jie Huang"}, {"authorId": "143922493", "name": "K. Chang"}, {"authorId": "145042856", "name": "Jinjun Xiong"}, {"authorId": "143668320", "name": "Wen-mei W. Hwu"}]}, {"paperId": "e6a7f3344685cde0acfc2a6be13242c0c441eb05", "externalIds": {"DBLP": "journals/corr/abs-2106-00162", "ArXiv": "2106.00162", "ACL": "2021.acl-long.283", "DOI": "10.18653/v1/2021.acl-long.283", "CorpusId": 235266157}, "corpusId": 235266157, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e6a7f3344685cde0acfc2a6be13242c0c441eb05", "title": "HERALD: An Annotation Efficient Method to Detect User Disengagement in Social Conversations", "abstract": "Open-domain dialog systems have a user-centric goal: to provide humans with an engaging conversation experience. User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. Existing work on detecting user disengagement typically requires hand-labeling many dialog samples. We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a denoising problem. Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically. We then denoise the weakly labeled data using the Shapley algorithm. Finally, we use the denoised data to train a user engagement detector. Our experiments show that HERALD improves annotation efficiency significantly and achieves 86% user disengagement detection accuracy in two dialog corpora.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 55, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.283.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"name": "ArXiv", "volume": "abs/2106.00162"}, "authors": [{"authorId": "151253861", "name": "Weixin Liang"}, {"authorId": "2068640838", "name": "Kai-Hui Liang"}, {"authorId": "1564034697", "name": "Zhou Yu"}]}, {"paperId": "1a272eb83fbf3082ca6d6009963186bb23254b03", "externalIds": {"ACL": "2021.acl-long.284", "DBLP": "conf/acl/PlataniosPRZKGT20", "DOI": "10.18653/v1/2021.acl-long.284", "CorpusId": 236460195}, "corpusId": 236460195, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1a272eb83fbf3082ca6d6009963186bb23254b03", "title": "Value-Agnostic Conversational Semantic Parsing", "abstract": "Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a model that abstracts over values to focus prediction on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency. Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. Trained on the SMCalFlow and TreeDST datasets, our model outperforms prior work by 7.3% and 10.6% respectively in terms of absolute accuracy. Trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4% and 6.4%. These results indicate that simple representations are key to effective generalization in conversational semantic parsing.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 49, "citationCount": 18, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3666-3681"}, "authors": [{"authorId": "144888672", "name": "Emmanouil Antonios Platanios"}, {"authorId": "13461242", "name": "Adam Pauls"}, {"authorId": "2109961337", "name": "Subhro Roy"}, {"authorId": null, "name": "Yuchen Zhang"}, {"authorId": "1396506715", "name": "Alexander Kyte"}, {"authorId": "2057587844", "name": "Alan Guo"}, {"authorId": "2140419664", "name": "Sam Thomson"}, {"authorId": "2517825", "name": "Jayant Krishnamurthy"}, {"authorId": "38760084", "name": "J. Wolfe"}, {"authorId": "2112400", "name": "Jacob Andreas"}, {"authorId": "38666915", "name": "D. Klein"}]}, {"paperId": "51b9f8aef39de4b6db820b5c4b5bca14fc32aa4d", "externalIds": {"DBLP": "conf/acl/GuTLXGJ20", "ArXiv": "2106.01541", "ACL": "2021.acl-long.285", "DOI": "10.18653/v1/2021.acl-long.285", "CorpusId": 235313361}, "corpusId": 235313361, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/51b9f8aef39de4b6db820b5c4b5bca14fc32aa4d", "title": "MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding", "abstract": "Recently, various neural models for multi-party conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. To this end, we present MPC-BERT, a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks. Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection. We evaluate MPC-BERT on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 30, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.285.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"pages": "3682-3692"}, "authors": [{"authorId": "3028818", "name": "Jia-Chen Gu"}, {"authorId": "8801869", "name": "Chongyang Tao"}, {"authorId": "1749989", "name": "Zhenhua Ling"}, {"authorId": "2110091832", "name": "Can Xu"}, {"authorId": "2442662", "name": "Xiubo Geng"}, {"authorId": "71790825", "name": "Daxin Jiang"}]}, {"paperId": "6c7b337f535eb32d47c651c5464c5a52e9a453cb", "externalIds": {"ACL": "2021.acl-long.286", "DBLP": "conf/acl/RohanianH20", "DOI": "10.18653/v1/2021.acl-long.286", "CorpusId": 236460290}, "corpusId": 236460290, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6c7b337f535eb32d47c651c5464c5a52e9a453cb", "title": "Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental", "abstract": "While Transformer-based text classifiers pre-trained on large volumes of text have yielded significant improvements on a wide range of computational linguistics tasks, their implementations have been unsuitable for live incremental processing thus far, operating only on the level of complete sentence inputs. We address the challenge of introducing methods for word-by-word left-to-right incremental processing to Transformers such as BERT, models without an intrinsic sense of linear order. We modify the training method and live decoding of non-incremental models to detect speech disfluencies with minimum latency and without pre-segmentation of dialogue acts. We experiment with several decoding methods to predict the rightward context of the word currently being processed using a GPT-2 language model and apply a BERT-based disfluency detector to sequences, including predicted words. We show our method of incrementalising Transformers maintains most of their high non-incremental performance while operating strictly incrementally. We also evaluate our models\u2019 incremental performance to establish the trade-off between incremental performance and final performance, using different prediction strategies. We apply our system to incremental speech recognition results as they arrive into a live system and achieve state-of-the-art results in this setting.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3693-3703"}, "authors": [{"authorId": "66301207", "name": "Morteza Rohanian"}, {"authorId": "144397346", "name": "J. Hough"}]}, {"paperId": "7ccd4960b0dd1d72f87b12a739442bf4c87e70dc", "externalIds": {"DBLP": "conf/acl/KimCL20", "ArXiv": "2105.14454", "ACL": "2021.acl-long.287", "DOI": "10.18653/v1/2021.acl-long.287", "CorpusId": 235254478}, "corpusId": 235254478, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7ccd4960b0dd1d72f87b12a739442bf4c87e70dc", "title": "NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation", "abstract": "We propose NeuralWOZ, a novel dialogue collection framework that uses model-based dialogue simulation. NeuralWOZ has two pipelined models, Collector and Labeler. Collector generates dialogues from (1) user\u2019s goal instructions, which are the user context and task constraints in natural language, and (2) system\u2019s API call results, which is a list of possible query responses for user requests from the given knowledge base. Labeler annotates the generated dialogue by formulating the annotation as a multiple-choice problem, in which the candidate labels are extracted from goal instructions and API call results. We demonstrate the effectiveness of the proposed method in the zero-shot domain transfer learning for dialogue state tracking. In the evaluation, the synthetic dialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with improvements of 4.4% point joint goal accuracy on average across domains, and improvements of 5.7% point of zero-shot coverage against the MultiWOZ 2.1 dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.287.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-30", "journal": {"name": "ArXiv", "volume": "abs/2105.14454"}, "authors": [{"authorId": "2829848", "name": "Sungdong Kim"}, {"authorId": "10741225", "name": "Minsuk Chang"}, {"authorId": "3226948", "name": "Sang-Woo Lee"}]}, {"paperId": "030865ddd9d8615d5e9b96e293f02a1a62fd95f6", "externalIds": {"DBLP": "conf/acl/Shain20", "ACL": "2021.acl-long.288", "DOI": "10.18653/v1/2021.acl-long.288", "CorpusId": 236460166}, "corpusId": 236460166, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/030865ddd9d8615d5e9b96e293f02a1a62fd95f6", "title": "CDRNN: Discovering Complex Dynamics in Human Language Processing", "abstract": "The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes. This study proposes the continuous-time deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (Shain & Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time). Despite this flexibility, CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study. Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines, supporting a potential role for CDRNN in studying human language processing.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 92, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3718-3734"}, "authors": [{"authorId": "40372491", "name": "Cory Shain"}]}, {"paperId": "f75fd3288a60233bbc766037941f5b370fae9168", "externalIds": {"DBLP": "conf/acl/QianNLA20", "ArXiv": "2108.00104", "ACL": "2021.acl-long.289", "DOI": "10.18653/v1/2021.acl-long.289", "CorpusId": 236459941}, "corpusId": 236459941, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f75fd3288a60233bbc766037941f5b370fae9168", "title": "Structural Guidance for Transformer Language Models", "abstract": "Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The \u201cGenerative Parsing\u201d idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The \u201cStructural Scaffold\u201d idea guides the language model\u2019s representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models\u2019 syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 22, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.289.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-30", "journal": {"pages": "3735-3745"}, "authors": [{"authorId": "1483502658", "name": "Peng Qian"}, {"authorId": "2138053379", "name": "Tahira Naseem"}, {"authorId": "50007746", "name": "R. Levy"}, {"authorId": "3394760", "name": "Ram\u00f3n Fern\u00e1ndez Astudillo"}]}, {"paperId": "72c007a987ea26bec00ba150b725e5eb54cd6c8c", "externalIds": {"DBLP": "conf/acl/OhCS20", "ACL": "2021.acl-long.290", "DOI": "10.18653/v1/2021.acl-long.290", "CorpusId": 236460083}, "corpusId": 236460083, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/72c007a987ea26bec00ba150b725e5eb54cd6c8c", "title": "Surprisal Estimators for Human Reading Times Need Character Models", "abstract": "While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling. This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3746-3757"}, "authors": [{"authorId": "71829539", "name": "Byung-Doh Oh"}, {"authorId": "2150039966", "name": "Christian Clark"}, {"authorId": "1747648", "name": "William Schuler"}]}, {"paperId": "ce0feb3e46f82e491bf2178ee6d86a284d9c3e79", "externalIds": {"DBLP": "journals/corr/abs-2106-05544", "ArXiv": "2106.05544", "ACL": "2021.acl-long.291", "DOI": "10.18653/v1/2021.acl-long.291", "CorpusId": 235390612}, "corpusId": 235390612, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ce0feb3e46f82e491bf2178ee6d86a284d9c3e79", "title": "CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals", "abstract": "Most previous studies integrate cognitive language processing signals (e.g., eye-tracking or EEG data) into neural models of natural language processing (NLP) just by directly concatenating word embeddings with cognitive features, ignoring the gap between the two modalities (i.e., textual vs. cognitive) and noise in cognitive features. In this paper, we propose a CogAlign approach to these issues, which learns to align textual neural representations to cognitive features. In CogAlign, we use a shared encoder equipped with a modality discriminator to alternatively encode textual and cognitive inputs to capture their differences and commonalities. Additionally, a text-aware attention mechanism is proposed to detect task-related information and to avoid using noise in cognitive features. Experimental results on three NLP tasks, namely named entity recognition, sentiment analysis and relation extraction, show that CogAlign achieves significant improvements with multiple cognitive features over state-of-the-art models on public datasets. Moreover, our model is able to transfer cognitive information to other datasets that do not have any cognitive processing signals.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.291.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-10", "journal": {"pages": "3758-3769"}, "authors": [{"authorId": "152321611", "name": "Yuqi Ren"}, {"authorId": "2694222", "name": "Deyi Xiong"}]}, {"paperId": "b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea", "externalIds": {"DBLP": "conf/acl/YaoPPN20", "ACL": "2021.acl-long.292", "ArXiv": "2105.11115", "DOI": "10.18653/v1/2021.acl-long.292", "CorpusId": 235166395}, "corpusId": 235166395, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b2186dd1ccc4b7adcf70c0cf7649c2c118e4ceea", "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages", "abstract": "Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with D+1 layers and O(log k) memory size (per token per layer) that recognizes Dyck-(k, D), and a soft-attention network with two layers and O(log k) memory size that generates Dyck-(k, D). Experiments show that self-attention networks trained on Dyck-(k, D) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 57, "citationCount": 38, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.292.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-24", "journal": {"pages": "3770-3785"}, "authors": [{"authorId": "47188964", "name": "Shunyu Yao"}, {"authorId": "30098894", "name": "Binghui Peng"}, {"authorId": "144102674", "name": "C. Papadimitriou"}, {"authorId": "144958935", "name": "Karthik Narasimhan"}]}, {"paperId": "d31b5b60e1b3af84cd977da8db0ed4faeb79e7f7", "externalIds": {"DBLP": "conf/acl/RileyCGKUP20", "ACL": "2021.acl-long.293", "ArXiv": "2010.03802", "DOI": "10.18653/v1/2021.acl-long.293", "CorpusId": 235621503}, "corpusId": 235621503, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d31b5b60e1b3af84cd977da8db0ed4faeb79e7f7", "title": "TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling", "abstract": "We present a novel approach to the problem of text style transfer. Unlike previous approaches requiring style-labeled training data, our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences, and uses labeled data only at inference time. We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to extract a style vector from text and use it to condition the decoder to perform style transfer. As our label-free training results in a style vector space encoding many facets of style, we recast transfers as \u201ctargeted restyling\u201d vector operations that adjust specific attributes of the input while preserving others. We demonstrate that training on unlabeled Amazon reviews data results in a model that is competitive on sentiment transfer, even compared to models trained fully on labeled data. Furthermore, applying our novel method to a diverse corpus of unlabeled web text results in a single model capable of transferring along multiple dimensions of style (dialect, emotiveness, formality, politeness, sentiment) despite no additional training and using only a handful of exemplars at inference time.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 38, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.293.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-10-08", "journal": {"pages": "3786-3800"}, "authors": [{"authorId": "47718053", "name": "Parker Riley"}, {"authorId": "40832517", "name": "Noah Constant"}, {"authorId": "51150315", "name": "Mandy Guo"}, {"authorId": null, "name": "Girish Kumar"}, {"authorId": "3046959", "name": "David C. Uthus"}, {"authorId": "27456119", "name": "Zarana Parekh"}]}, {"paperId": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6", "externalIds": {"ArXiv": "2107.11906", "ACL": "2021.acl-long.294", "DBLP": "conf/acl/ZhuS20", "DOI": "10.18653/v1/2021.acl-long.294", "CorpusId": 236428421}, "corpusId": 236428421, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6", "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences", "abstract": "We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 23, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.294.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-25", "journal": {"pages": "3801-3815"}, "authors": [{"authorId": "2062703", "name": "Zhenhai Zhu"}, {"authorId": "1737285", "name": "Radu Soricut"}]}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "externalIds": {"DBLP": "conf/acl/GaoFC20", "ACL": "2021.acl-long.295", "ArXiv": "2012.15723", "DOI": "10.18653/v1/2021.acl-long.295", "CorpusId": 229923710}, "corpusId": 229923710, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners", "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF\u2014better few-shot fine-tuning of language models\u2014a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 61, "citationCount": 1121, "influentialCitationCount": 196, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.295.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-01", "journal": {"name": "ArXiv", "volume": "abs/2012.15723"}, "authors": [{"authorId": "4800645", "name": "Tianyu Gao"}, {"authorId": "2064150446", "name": "Adam Fisch"}, {"authorId": "50536468", "name": "Danqi Chen"}]}, {"paperId": "c94529aff09763b607b7594197f1bbf01c006759", "externalIds": {"ACL": "2021.acl-long.296", "DBLP": "conf/acl/LeP020", "ArXiv": "2011.10492", "DOI": "10.18653/v1/2021.acl-long.296", "CorpusId": 234353603}, "corpusId": 234353603, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c94529aff09763b607b7594197f1bbf01c006759", "title": "A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger\u2019s Adversarial Attacks", "abstract": "The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the \u201choneypot\u201d concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger. DARCY greedily searches and injects multiple trapdoors into an NN model to \u201cbait and catch\u201d potential attacks. Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger\u2019s adversarial attacks with up to 99% TPR and less than 2% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1% margin. We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers\u2019 varying levels of knowledge and skills. We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 19, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.296.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-20", "journal": {"pages": "3831-3844"}, "authors": [{"authorId": "145535348", "name": "Thai Le"}, {"authorId": "5166698", "name": "Noseong Park"}, {"authorId": "145948198", "name": "Dongwon Lee"}]}, {"paperId": "10825c7414dd839be051682a5ae653aa7ba8e08c", "externalIds": {"ACL": "2021.acl-long.297", "DBLP": "journals/corr/abs-2107-11934", "ArXiv": "2107.11934", "DOI": "10.18653/v1/2021.acl-long.297", "CorpusId": 236428961}, "corpusId": 236428961, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/10825c7414dd839be051682a5ae653aa7ba8e08c", "title": "Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection", "abstract": "Detecting rumors on social media is a very critical task with significant implications to the economy, public health, etc. Previous works generally capture effective features from texts and the propagation structure. However, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data. Most approaches neglect it and may seriously limit the learning of features. Towards this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection. Specifically, we propose a novel Edge-enhanced Bayesian Graph Convolutional Network (EBGCN) to capture robust structural features. The model adaptively rethinks the reliability of latent relations by adopting a Bayesian approach. Besides, we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations. Experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 36, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.297.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-26", "journal": {"pages": "3845-3854"}, "authors": [{"authorId": "12660091", "name": "Lingwei Wei"}, {"authorId": "120427046", "name": "Dou Hu"}, {"authorId": "101829183", "name": "Wei Zhou"}, {"authorId": "38713444", "name": "Zhaojuan Yue"}, {"authorId": "40845069", "name": "Songlin Hu"}]}, {"paperId": "efb570104b0a148315a1eedd8e9a5dc106143514", "externalIds": {"ACL": "2021.acl-long.298", "DBLP": "conf/acl/MaYZH20", "DOI": "10.18653/v1/2021.acl-long.298", "CorpusId": 236460275}, "corpusId": 236460275, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/efb570104b0a148315a1eedd8e9a5dc106143514", "title": "Label-Specific Dual Graph Neural Network for Multi-Label Text Classification", "abstract": "Multi-label text classification is one of the fundamental tasks in natural language processing. Previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels, that is they do not explicitly extract label-specific semantic components from documents. Moreover, they do not fully explore the high-order interactions among these semantic components, which is very helpful to predict tail labels. In this paper, we propose a novel label-specific dual graph neural network (LDGN), which incorporates category information to learn label-specific components from documents, and employs dual Graph Convolution Network (GCN) to model complete and adaptive interactions among these components based on the statistical label co-occurrence and dynamic reconstruction graph in a joint way. Experimental results on three benchmark datasets demonstrate that LDGN significantly outperforms the state-of-the-art models, and also achieves better performance with respect to tail labels.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 20, "citationCount": 29, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.298.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3855-3864"}, "authors": [{"authorId": "1500398963", "name": "Qianwen Ma"}, {"authorId": "153585587", "name": "Chunyuan Yuan"}, {"authorId": "101829183", "name": "Wei Zhou"}, {"authorId": "40845069", "name": "Songlin Hu"}]}, {"paperId": "68768d4dca5b71e88094d63d546da9574b09c7b2", "externalIds": {"MAG": "3106926072", "DBLP": "journals/corr/abs-2012-01524", "ArXiv": "2012.01524", "ACL": "2021.acl-long.299", "DOI": "10.18653/v1/2021.acl-long.299", "CorpusId": 227253730}, "corpusId": 227253730, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/68768d4dca5b71e88094d63d546da9574b09c7b2", "title": "TAN-NTM: Topic Attention Networks for Neural Topic Modeling", "abstract": "Topic models have been widely used to learn text representations and gain insight into document corpora. To perform topic discovery, most existing neural models either take document bag-of-words (BoW) or sequence of tokens as input followed by variational inference and BoW reconstruction to learn topic-word distribution. However, leveraging topic-word distribution for learning better features during document encoding has not been explored much. To this end, we develop a framework TAN-NTM, which processes document as a sequence of tokens through a LSTM whose contextual outputs are attended in a topic-aware manner. We propose a novel attention mechanism which factors in topic-word distribution to enable the model to attend on relevant words that convey topic related cues. The output of topic attention module is then used to carry out variational inference. We perform extensive ablations and experiments resulting in ~9-15 percentage improvement over score of existing SOTA topic models in NPMI coherence on several benchmark datasets - 20Newsgroups, Yelp Review Polarity and AGNews. Further, we show that our method learns better latent document-topic features compared to existing topic models through improvement on two downstream tasks: document classification and topic guided keyphrase generation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 66, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.299.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-12-02", "journal": {"pages": "3865-3880"}, "authors": [{"authorId": "2030973547", "name": "Madhur Panwar"}, {"authorId": "2030976071", "name": "Shashank Shailabh"}, {"authorId": "6657914", "name": "Milan Aggarwal"}, {"authorId": "145846953", "name": "Balaji Krishnamurthy"}]}, {"paperId": "3df7970d24ac31744b772455307feb71d3d092b0", "externalIds": {"DBLP": "conf/acl/ChenKNGZOM20", "ACL": "2021.acl-long.300", "ArXiv": "2106.02293", "DOI": "10.18653/v1/2021.acl-long.300", "CorpusId": 235353004}, "corpusId": 235353004, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3df7970d24ac31744b772455307feb71d3d092b0", "title": "Cross-language Sentence Selection via Data Augmentation and Rationale Training", "abstract": "This paper proposes an approach to cross-language sentence selection in a low-resource setting. It uses data augmentation and negative sampling techniques on noisy parallel sentence data to directly learn a cross-lingual embedding-based query relevance model. Results show that this approach performs as well as or better than multiple state-of-the-art machine translation + monolingual retrieval systems trained on the same parallel data. Moreover, when a rationale training secondary objective is applied to encourage the model to match word alignment hints from a phrase-based statistical machine translation model, consistent improvements are seen across three language pairs (English-Somali, English-Swahili and English-Tagalog) over a variety of state-of-the-art baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 54, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.300.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"pages": "3881-3895"}, "authors": [{"authorId": "2109268730", "name": "Yanda Chen"}, {"authorId": "98736546", "name": "C. Kedzie"}, {"authorId": "143913642", "name": "Suraj Nair"}, {"authorId": "2107033402", "name": "Petra Galuvsvc'akov'a"}, {"authorId": "144142360", "name": "Rui Zhang"}, {"authorId": "1737250", "name": "Douglas W. Oard"}, {"authorId": "145590324", "name": "K. McKeown"}]}, {"paperId": "068eb6b3797d5807b744d9326e7ebb50769e4b4d", "externalIds": {"DBLP": "conf/acl/PappasA20", "ArXiv": "2106.08908", "ACL": "2021.acl-long.301", "DOI": "10.18653/v1/2021.acl-long.301", "CorpusId": 235446913}, "corpusId": 235446913, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/068eb6b3797d5807b744d9326e7ebb50769e4b4d", "title": "A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections", "abstract": "Question answering (QA) systems for large document collections typically use pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them, (iii) rank paragraphs or other snippets of the top-ranked documents, and (iv) select spans of the top-ranked snippets as exact answers. Pipelines are conceptually simple, but errors propagate from one component to the next, without later components being able to revise earlier decisions. We present an architecture for joint document and snippet ranking, the two middle stages, which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. The architecture is general and can be used with any neural text relevance ranker. We experiment with two main instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a BERT-based ranker. Experiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval, the main goal for QA, with fewer trainable parameters, also remaining competitive in document retrieval. Furthermore, our joint PDRMM-based model is competitive with BERT-based models, despite using orders of magnitude fewer parameters. These claims are also supported by human evaluation on two test batches of BIOASQ. To test our key findings on another dataset, we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval. Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset, even though it performs worse than the pipeline in document retrieval. We make our code and the modified Natural Questions dataset publicly available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 60, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.301.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-16", "journal": {"name": "ArXiv", "volume": "abs/2106.08908"}, "authors": [{"authorId": "36753496", "name": "Dimitris Pappas"}, {"authorId": "1752430", "name": "Ion Androutsopoulos"}]}, {"paperId": "04821f01347bd58be330d5dc12e510f586b23286", "externalIds": {"ArXiv": "2106.02658", "ACL": "2021.acl-long.302", "DBLP": "journals/corr/abs-2106-02658", "DOI": "10.18653/v1/2021.acl-long.302", "CorpusId": 235359132}, "corpusId": 235359132, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/04821f01347bd58be330d5dc12e510f586b23286", "title": "W-RST: Towards a Weighted RST-style Discourse Framework", "abstract": "Aiming for a better integration of data-driven and linguistically-inspired approaches, we explore whether RST Nuclearity, assigning a binary assessment of importance between text segments, can be replaced by automatically generated, real-valued scores, in what we call a Weighted-RST framework. In particular, we find that weighted discourse trees from auxiliary tasks can benefit key NLP downstream applications, compared to nuclearity-centered approaches. We further show that real-valued importance distributions partially and interestingly align with the assessment and uncertainty of human annotators.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 1, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.302.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"pages": "3908-3918"}, "authors": [{"authorId": "2024784636", "name": "Patrick Huber"}, {"authorId": "49617120", "name": "Wen Xiao"}, {"authorId": "1825424", "name": "G. Carenini"}]}, {"paperId": "862b9f75e4b249ea20b3598ea3a4375b3dfff368", "externalIds": {"ArXiv": "2106.12027", "ACL": "2021.acl-long.303", "DBLP": "conf/acl/GaoHP20", "DOI": "10.18653/v1/2021.acl-long.303", "CorpusId": 235606132}, "corpusId": 235606132, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/862b9f75e4b249ea20b3598ea3a4375b3dfff368", "title": "ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences", "abstract": "Atomic clauses are fundamental text units for understanding complex sentences. Identifying the atomic sentences within complex sentences is important for applications such as summarization, argument mining, discourse analysis, discourse parsing, and question answering. Previous work mainly relies on rule-based methods dependent on parsing. We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task. Our neural model learns to Accept, Break, Copy or Drop elements of a graph that combines word adjacency and grammatical dependencies. The full processing pipeline includes modules for graph construction, graph editing, and sentence generation from the output graph. We introduce DeSSE, a new dataset designed to train and evaluate complex sentence decomposition, and MinWiki, a subset of MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on MinWiki. On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.303.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-22", "journal": {"pages": "3919-3931"}, "authors": [{"authorId": "2145972668", "name": "Yanjun Gao"}, {"authorId": "144188081", "name": "Ting-Hao 'Kenneth' Huang"}, {"authorId": "1703046", "name": "R. Passonneau"}]}, {"paperId": "c2aa448fea1d02ede55f0cbfeebe7ba9d08fe16d", "externalIds": {"DBLP": "journals/corr/abs-2101-00391", "ACL": "2021.acl-long.304", "ArXiv": "2101.00391", "DOI": "10.18653/v1/2021.acl-long.304", "CorpusId": 230433881}, "corpusId": 230433881, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c2aa448fea1d02ede55f0cbfeebe7ba9d08fe16d", "title": "Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering", "abstract": "Many Question-Answering (QA) datasets contain unanswerable questions, but their treatment in QA systems remains primitive. Our analysis of the Natural Questions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion of unanswerable questions (~21%) can be explained based on the presence of unverifiable presuppositions. Through a user preference study, we demonstrate that the oracle behavior of our proposed system\u2014which provides responses based on presupposition failure\u2014is preferred over the oracle behavior of existing QA systems. Then, we present a novel framework for implementing such a system in three steps: presupposition generation, presupposition verification, and explanation generation, reporting progress on each. Finally, we show that a simple modification of adding presuppositions and their verifiability to the input of a competitive end-to-end QA system yields modest gains in QA performance and unanswerability detection, demonstrating the promise of our approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 26, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.304.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-02", "journal": {"pages": "3932-3945"}, "authors": [{"authorId": "8756748", "name": "Najoung Kim"}, {"authorId": "2949185", "name": "Ellie Pavlick"}, {"authorId": "143990191", "name": "Burcu Karagol Ayan"}, {"authorId": "143812128", "name": "Deepak Ramachandran"}]}, {"paperId": "75ca384b69494c16ad7e814d069745be854082bb", "externalIds": {"DBLP": "conf/acl/Zhang0Z20", "ACL": "2021.acl-long.305", "DOI": "10.18653/v1/2021.acl-long.305", "CorpusId": 236459897}, "corpusId": 236459897, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/75ca384b69494c16ad7e814d069745be854082bb", "title": "Adversarial Learning for Discourse Rhetorical Structure Parsing", "abstract": "Text-level discourse rhetorical structure (DRS) parsing is known to be challenging due to the notorious lack of training data. Although recent top-down DRS parsers can better leverage global document context and have achieved certain success, the performance is still far from perfect. To our knowledge, all previous DRS parsers make local decisions for either bottom-up node composition or top-down split point ranking at each time step, and largely ignore DRS parsing from the global view point. Obviously, it is not sufficient to build an entire DRS tree only through these local decisions. In this work, we present our insight on evaluating the pros and cons of the entire DRS tree for global optimization. Specifically, based on recent well-performing top-down frameworks, we introduce a novel method to transform both gold standard and predicted constituency trees into tree diagrams with two color channels. After that, we learn an adversarial bot between gold and fake tree diagrams to estimate the generated DRS trees from a global perspective. We perform experiments on both RST-DT and CDTB corpora and use the original Parseval for performance evaluation. The experimental results show that our parser can substantially improve the performance when compared with previous state-of-the-art parsers.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 16, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.305.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3946-3957"}, "authors": [{"authorId": "2108281590", "name": "Longyin Zhang"}, {"authorId": "47425794", "name": "Fang Kong"}, {"authorId": "143740945", "name": "Guodong Zhou"}]}, {"paperId": "d484fb6b39bc8a5ae3ae1039ea551b031c215e20", "externalIds": {"MAG": "3176580085", "DBLP": "journals/corr/abs-2106-00976", "ACL": "2021.acl-long.306", "ArXiv": "2106.00976", "DOI": "10.18653/v1/2021.acl-long.306", "CorpusId": 235294136}, "corpusId": 235294136, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d484fb6b39bc8a5ae3ae1039ea551b031c215e20", "title": "Exploring Discourse Structures for Argument Impact Classification", "abstract": "Discourse relations among arguments reveal logical structures of a debate conversation. However, no prior work has explicitly studied how the sequence of discourse relations influence a claim\u2019s impact. This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument. We further propose DisCOC to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models. Experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classification task defined by Durmus et al. (2019), and discourse structures among the context path of the claim to be classified can further boost the performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.306.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Sociology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.00976"}, "authors": [{"authorId": "2146075375", "name": "Xin Liu"}, {"authorId": "1657712151", "name": "Jiefu Ou"}, {"authorId": "1809614", "name": "Yangqiu Song"}, {"authorId": "2110310493", "name": "Xin Jiang"}]}, {"paperId": "1105f26529bd6ab73e866b13513b36b591470985", "externalIds": {"DBLP": "conf/acl/ZhangZYLSZZZ20", "ACL": "2021.acl-long.307", "DOI": "10.18653/v1/2021.acl-long.307", "CorpusId": 236459912}, "corpusId": 236459912, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1105f26529bd6ab73e866b13513b36b591470985", "title": "Point, Disambiguate and Copy: Incorporating Bilingual Dictionaries for Neural Machine Translation", "abstract": "This paper proposes a sophisticated neural architecture to incorporate bilingual dictionaries into Neural Machine Translation (NMT) models. By introducing three novel components: Pointer, Disambiguator, and Copier, our method PDC achieves the following merits inherently compared with previous efforts: (1) Pointer leverages the semantic information from bilingual dictionaries, for the first time, to better locate source words whose translation in dictionaries can potentially be used; (2) Disambiguator synthesizes contextual information from the source view and the target view, both of which contribute to distinguishing the proper translation of a specific source word from multiple candidates in dictionaries; (3) Copier systematically connects Pointer and Disambiguator based on a hierarchical copy mechanism seamlessly integrated with Transformer, thereby building an end-to-end architecture that could avoid error propagation problems in alternative pipe-line methods. The experimental results on Chinese-English and English-Japanese benchmarks demonstrate the PDC\u2019s overall superiority and effectiveness of each component.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.307.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3970-3979"}, "authors": [{"authorId": "2118942397", "name": "Tong Zhang"}, {"authorId": "2108282134", "name": "Long Zhang"}, {"authorId": "145235143", "name": "Wei Ye"}, {"authorId": "2485552", "name": "Bo Li"}, {"authorId": "2262309", "name": "Jinan Sun"}, {"authorId": "2125169356", "name": "Xiaoyu Zhu"}, {"authorId": "2036264340", "name": "Wenxin Zhao"}, {"authorId": "1705434", "name": "Shikun Zhang"}]}, {"paperId": "cd5a4c3ad315a40ff3d4456a550be818bc5ec7af", "externalIds": {"ACL": "2021.acl-long.308", "DBLP": "conf/acl/Luo0LLBHHS20", "ArXiv": "2010.16046", "DOI": "10.18653/v1/2021.acl-long.308", "CorpusId": 235303732}, "corpusId": 235303732, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cd5a4c3ad315a40ff3d4456a550be818bc5ec7af", "title": "VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation", "abstract": "Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages. It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation. As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1 2 BLEU.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 42, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.308.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-10-30", "journal": {"pages": "3980-3994"}, "authors": [{"authorId": "17866105", "name": "Fuli Luo"}, {"authorId": "38700603", "name": "Wei Wang"}, {"authorId": "2108421184", "name": "Jiahao Liu"}, {"authorId": "2108060432", "name": "Yijia Liu"}, {"authorId": "2555622", "name": "Bin Bi"}, {"authorId": "2410938", "name": "Songfang Huang"}, {"authorId": "2087380523", "name": "Fei Huang"}, {"authorId": "2059080424", "name": "Luo Si"}]}, {"paperId": "b95e1b0b716e36b7a594031192948956fc20fdf6", "externalIds": {"ACL": "2021.acl-long.309", "DBLP": "conf/acl/WicksP20", "DOI": "10.18653/v1/2021.acl-long.309", "CorpusId": 236460330}, "corpusId": 236460330, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b95e1b0b716e36b7a594031192948956fc20fdf6", "title": "A unified approach to sentence segmentation of punctuated text in many languages", "abstract": "The sentence is a fundamental unit of text processing. Yet sentences in the wild are commonly encountered not in isolation, but unsegmented within larger paragraphs and documents. Therefore, the first step in many NLP pipelines is sentence segmentation. Despite its importance, this step is the subject of relatively little research. There are no standard test sets or even methods for evaluation, leaving researchers and engineers without a clear footing for evaluating and selecting models for the task. Existing tools have relatively small language coverage, and efforts to extend them to other languages are often ad hoc. We introduce a modern context-based modeling approach that provides a solution to the problem of segmenting punctuated text in many languages, and show how it can be trained on noisily-annotated data. We also establish a new 23-language multilingual evaluation set. Our approach exceeds high baselines set by existing methods on prior English corpora (WSJ and Brown corpora), and also performs well on average on our new evaluation set. We release our tool, ersatz, as open source.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 19, "citationCount": 16, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.309.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "3995-4007"}, "authors": [{"authorId": "123216314", "name": "R. Wicks"}, {"authorId": "38842528", "name": "Matt Post"}]}, {"paperId": "147974cbbe6fb6a5ab6372210a10ae8bbe139842", "externalIds": {"DBLP": "conf/acl/LinYYLZLHS20", "ACL": "2021.acl-long.310", "ArXiv": "2106.06200", "DOI": "10.18653/v1/2021.acl-long.310", "CorpusId": 235417347}, "corpusId": 235417347, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/147974cbbe6fb6a5ab6372210a10ae8bbe139842", "title": "Towards User-Driven Neural Machine Translation", "abstract": "A good translation should not only translate the original content semantically, but also incarnate personal traits of the original text. For a real-world neural machine translation (NMT) system, these user traits (e.g., topic preference, stylistic characteristics and expression habits) can be preserved in user behavior (e.g., historical inputs). However, current NMT systems marginally consider the user behavior due to: 1) the difficulty of modeling user portraits in zero-shot scenarios, and 2) the lack of user-behavior annotated parallel dataset. To fill this gap, we introduce a novel framework called user-driven NMT. Specifically, a cache-based module and a user-driven contrastive learning method are proposed to offer NMT the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion. Furthermore, we contribute the first Chinese-English parallel corpus annotated with user behavior called UDT-Corpus. Experimental results confirm that the proposed user-driven NMT can generate user-specific translations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.310.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-11", "journal": {"name": "ArXiv", "volume": "abs/2106.06200"}, "authors": [{"authorId": "46933131", "name": "Huan Lin"}, {"authorId": "2106354196", "name": "Liang Yao"}, {"authorId": "21299583", "name": "Baosong Yang"}, {"authorId": "2004587660", "name": "Dayiheng Liu"}, {"authorId": "2111126458", "name": "Haibo Zhang"}, {"authorId": "1935569", "name": "Weihua Luo"}, {"authorId": "2610330", "name": "Degen Huang"}, {"authorId": "34739384", "name": "Jinsong Su"}]}, {"paperId": "9d030241eb35c38bacf689df6e0dfa9dadc789da", "externalIds": {"DBLP": "conf/acl/JonAVB20", "ArXiv": "2106.12398", "ACL": "2021.acl-long.311", "DOI": "10.18653/v1/2021.acl-long.311", "CorpusId": 235606235}, "corpusId": 235606235, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9d030241eb35c38bacf689df6e0dfa9dadc789da", "title": "End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages", "abstract": "Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. Although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output. Our manual analysis shows that 46% of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement. We investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints. In particular, we focus on methods based on training the model with constraints provided as part of the input sequence. Our experiments on English-Czech language pair show that this approach improves translation of constrained terms in both automatic and manual evaluation by reducing errors in agreement. Our approach thus eliminates inflection errors, without introducing new errors or decreasing overall quality of the translation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.311.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-23", "journal": {"pages": "4019-4033"}, "authors": [{"authorId": "1838076488", "name": "Josef Jon"}, {"authorId": "10684139", "name": "Jo\u00e3o Paulo Aires"}, {"authorId": "1999185950", "name": "Duvsan Varivs"}, {"authorId": "151158933", "name": "Ondvrej Bojar"}]}, {"paperId": "300450616c609f97aea9dbf44891f3e5e2fe7dae", "externalIds": {"ACL": "2021.acl-long.312", "DBLP": "conf/acl/AkhbardehAZD20", "DOI": "10.18653/v1/2021.acl-long.312", "CorpusId": 235497555}, "corpusId": 235497555, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/300450616c609f97aea9dbf44891f3e5e2fe7dae", "title": "Handling Extreme Class Imbalance in Technical Logbook Datasets", "abstract": "Technical logbooks are a challenging and under-explored text type in automated event identification. These texts are typically short and written in non-standard yet technical language, posing challenges to off-the-shelf NLP pipelines. The granularity of issue types described in these datasets additionally leads to class imbalance, making it challenging for models to accurately predict which issue each logbook entry describes. In this paper we focus on the problem of technical issue classification by considering logbook datasets from the automotive, aviation, and facilities maintenance domains. We adapt a feedback strategy from computer vision for handling extreme class imbalance, which resamples the training data based on its error in the prediction process. Our experiments show that with statistical significance this feedback strategy provides the best results for four different neural network models trained across a suite of seven different technical logbook datasets from distinct technical domains. The feedback strategy is also generic and could be applied to any learning problem with substantial class imbalances.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.312.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4034-4045"}, "authors": [{"authorId": "2813801", "name": "Farhad Akhbardeh"}, {"authorId": "144648940", "name": "Cecilia Ovesdotter Alm"}, {"authorId": "145130358", "name": "Marcos Zampieri"}, {"authorId": "9552886", "name": "Travis J. Desell"}]}, {"paperId": "61a5ae33fc34efcdbc710004554ec57e607ce75e", "externalIds": {"ACL": "2021.acl-long.313", "DBLP": "journals/corr/abs-2105-13562", "ArXiv": "2105.13562", "DOI": "10.18653/v1/2021.acl-long.313", "CorpusId": 235248187}, "corpusId": 235248187, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/61a5ae33fc34efcdbc710004554ec57e607ce75e", "title": "ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation", "abstract": "An automated system that could assist a judge in predicting the outcome of a case would help expedite the judicial process. For such a system to be practically useful, predictions by the system should be explainable. To promote research in developing such a system, we introduce ILDC (Indian Legal Documents Corpus). ILDC is a large corpus of 35k Indian Supreme Court cases annotated with original court decisions. A portion of the corpus (a separate test set) is annotated with gold standard explanations by legal experts. Based on ILDC, we propose the task of Court Judgment Prediction and Explanation (CJPE). The task requires an automated system to predict an explainable outcome of a case. We experiment with a battery of baseline models for case predictions and propose a hierarchical occlusion based model for explainability. Our best prediction model has an accuracy of 78% versus 94% for human legal experts, pointing towards the complexity of the prediction task. The analysis of explanations by the proposed algorithm reveals a significant difference in the point of view of the algorithm and legal experts for explaining the judgments, pointing towards scope for future research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 56, "citationCount": 59, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.313.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Law", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-28", "journal": {"pages": "4046-4062"}, "authors": [{"authorId": "2047073132", "name": "Vijit Malik"}, {"authorId": "2106387425", "name": "Rishabh Sanjay"}, {"authorId": "1490697659", "name": "S. Nigam"}, {"authorId": "153408379", "name": "Kripabandhu Ghosh"}, {"authorId": "116020578", "name": "S. Guha"}, {"authorId": "145660316", "name": "Arnab Bhattacharya"}, {"authorId": "2477939", "name": "Ashutosh Modi"}]}, {"paperId": "d899c0150ecec3f967030f5cde717c2a0d1ede6d", "externalIds": {"DBLP": "journals/corr/abs-2105-14815", "ACL": "2021.acl-long.314", "ArXiv": "2105.14815", "DOI": "10.18653/v1/2021.acl-long.314", "CorpusId": 235254749}, "corpusId": 235254749, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d899c0150ecec3f967030f5cde717c2a0d1ede6d", "title": "Supporting Cognitive and Emotional Empathic Writing of Students", "abstract": "We present an annotation approach to capturing emotional and cognitive empathy in student-written peer reviews on business models in German. We propose an annotation scheme that allows us to model emotional and cognitive empathy scores based on three types of review components. Also, we conducted an annotation study with three annotators based on 92 student essays to evaluate our annotation scheme. The obtained inter-rater agreement of \u03b1=0.79 for the components and the multi-\u03c0=0.41 for the empathy scores indicate that the proposed annotation scheme successfully guides annotators to a substantial to moderate agreement. Moreover, we trained predictive models to detect the annotated empathy structures and embedded them in an adaptive writing support system for students to receive individual empathy feedback independent of an instructor, time, and location. We evaluated our tool in a peer learning exercise with 58 students and found promising results for perceived empathy skill learning, perceived feedback accuracy, and intention to use. Finally, we present our freely available corpus of 500 empathy-annotated, student-written peer reviews on business models and our annotation guidelines to encourage future research on the design and development of empathy support systems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 62, "citationCount": 20, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.314.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-05-31", "journal": {"pages": "4063-4077"}, "authors": [{"authorId": "94398251", "name": "Thiemo Wambsganss"}, {"authorId": "7193142", "name": "C. Niklaus"}, {"authorId": "145142867", "name": "Matthias S\u00f6llner"}, {"authorId": "1789102", "name": "S. Handschuh"}, {"authorId": "1737216", "name": "J. Leimeister"}]}, {"paperId": "285428daf56a1530b11b6deb515f10c8f1dc5739", "externalIds": {"DBLP": "conf/acl/LiNXZWX20", "ACL": "2021.acl-long.315", "ArXiv": "2108.02866", "DOI": "10.18653/v1/2021.acl-long.315", "CorpusId": 236459854}, "corpusId": 236459854, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/285428daf56a1530b11b6deb515f10c8f1dc5739", "title": "Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering", "abstract": "The current state-of-the-art generative models for open-domain question answering (ODQA) have focused on generating direct answers from unstructured textual information. However, a large amount of world\u2019s knowledge is stored in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In this paper, we propose a hybrid framework that takes both textual and tabular evidences as input and generates either direct answers or SQL queries depending on which form could better answer the question. The generated SQL queries can then be executed on the associated databases to obtain the final answers. To the best of our knowledge, this is the first paper that applies Text2SQL to ODQA tasks. Empirically, we demonstrate that on several ODQA datasets, the hybrid methods consistently outperforms the baseline models that only takes homogeneous input by a large margin. Specifically we achieve the state-of-the-art performance on OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 19, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.315.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-05", "journal": {"pages": "4078-4088"}, "authors": [{"authorId": "2123247219", "name": "A. Li"}, {"authorId": "145878390", "name": "Patrick Ng"}, {"authorId": "104382958", "name": "Peng Xu"}, {"authorId": "1787682", "name": "Henghui Zhu"}, {"authorId": "40296541", "name": "Zhiguo Wang"}, {"authorId": "144028698", "name": "Bing Xiang"}]}, {"paperId": "7d429ad73fc311a0a29ab9d02482b4e6b059d81f", "externalIds": {"DBLP": "conf/acl/MaoHLSG0C20", "MAG": "3087273879", "ArXiv": "2009.08553", "ACL": "2021.acl-long.316", "DOI": "10.18653/v1/2021.acl-long.316", "CorpusId": 221802772}, "corpusId": 221802772, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7d429ad73fc311a0a29ab9d02482b4e6b059d81f", "title": "Generation-Augmented Retrieval for Open-Domain Question Answering", "abstract": "We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision. We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR. We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 43, "citationCount": 114, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.316.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-17", "journal": {"pages": "4089-4100"}, "authors": [{"authorId": "3375249", "name": "Yuning Mao"}, {"authorId": "2107782398", "name": "Pengcheng He"}, {"authorId": "2108860856", "name": "Xiaodong Liu"}, {"authorId": "1752875", "name": "Yelong Shen"}, {"authorId": "1800422", "name": "Jianfeng Gao"}, {"authorId": "153034701", "name": "Jiawei Han"}, {"authorId": "2109136147", "name": "Weizhu Chen"}]}, {"paperId": "b8cb9b5c2964f7f2e0783d328801420ec74e0365", "externalIds": {"ACL": "2021.acl-long.317", "DBLP": "conf/acl/Si0Z0020", "ArXiv": "2106.04605", "DOI": "10.18653/v1/2021.acl-long.317", "CorpusId": 235376797}, "corpusId": 235376797, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b8cb9b5c2964f7f2e0783d328801420ec74e0365", "title": "Check It Again:Progressive Visual Question Answering via Visual Entailment", "abstract": "While sophisticated neural-based models have achieved remarkable success in Visual Question Answering (VQA), these models tend to answer questions only according to superficial correlations between question and answer. Several recent approaches have been developed to address this language priors problem. However, most of them predict the correct answer according to one best output without checking the authenticity of answers. Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers. In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment. Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer. Experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 24, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.317.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"name": "ArXiv", "volume": "abs/2106.04605"}, "authors": [{"authorId": "84109730", "name": "Q. Si"}, {"authorId": "144981131", "name": "Zheng Lin"}, {"authorId": "2108667264", "name": "Mingyu Zheng"}, {"authorId": "2053815125", "name": "Peng Fu"}, {"authorId": "2154491752", "name": "Weiping Wang"}]}, {"paperId": "3fd92feb43d61645429ec4a2c80b1304a3c8bd69", "externalIds": {"DBLP": "conf/acl/ShaoSLH20", "ArXiv": "2106.07174", "ACL": "2021.acl-long.318", "DOI": "10.18653/v1/2021.acl-long.318", "CorpusId": 235421967}, "corpusId": 235421967, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3fd92feb43d61645429ec4a2c80b1304a3c8bd69", "title": "A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering", "abstract": "Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct. Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.318.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-14", "journal": {"pages": "4111-4124"}, "authors": [{"authorId": "144485528", "name": "Zhihong Shao"}, {"authorId": "50812138", "name": "Lifeng Shang"}, {"authorId": "30738758", "name": "Qun Liu"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "c39fc494cb84b260eaa33cceda2b76512f3701a5", "externalIds": {"ACL": "2021.acl-long.319", "DBLP": "conf/acl/RavichanderBNWS20", "DOI": "10.18653/v1/2021.acl-long.319", "CorpusId": 236317505}, "corpusId": 236317505, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c39fc494cb84b260eaa33cceda2b76512f3701a5", "title": "Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?", "abstract": "Privacy plays a crucial role in preserving democratic ideals and personal autonomy. The dominant legal approach to privacy in many jurisdictions is the \u201cNotice and Choice\u201d paradigm, where privacy policies are the primary instrument used to convey information to users. However, privacy policies are long and complex documents that are difficult for users to read and comprehend. We discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 104, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.319.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4125-4140"}, "authors": [{"authorId": "3023068", "name": "Abhilasha Ravichander"}, {"authorId": "144339182", "name": "A. Black"}, {"authorId": "32435017", "name": "Thomas B. Norton"}, {"authorId": "31950200", "name": "Shomir Wilson"}, {"authorId": "2464164", "name": "N. Sadeh"}]}, {"paperId": "02e1d548c243ad4053b6795f009011ac38ed8457", "externalIds": {"ACL": "2021.acl-long.320", "DBLP": "conf/acl/SchroderBANMWBM20", "ArXiv": "2105.05557", "DOI": "10.18653/v1/2021.acl-long.320", "CorpusId": 236460005}, "corpusId": 236460005, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/02e1d548c243ad4053b6795f009011ac38ed8457", "title": "Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning", "abstract": "Open pit mines left many regions worldwide inhospitable or uninhabitable. Many sites are left behind in a hazardous or contaminated state, show remnants of waste, or have other restrictions imposed upon them, e.g., for the protection of human or nature. Such information has to be permanently managed in order to reuse those areas in the future. In this work we present and evaluate an automated workflow for supporting the post-mining management of former lignite open pit mines in the eastern part of Germany, where prior to any planned land reuse, aforementioned information has to be acquired to ensure the safety and validity of such an endeavor. Usually, this information is found in expert reports, either in the form of paper documents, or in the best case as digitized unstructured text\u2014all of them in German language. However, due to the size and complexity of these documents, any inquiry is tedious and time-consuming, thereby slowing down or even obstructing the reuse of related areas. Since no training data is available, we employ active learning in order to perform multi-label sentence classification for two categories of restrictions and seven categories of topics. The final system integrates optical character recognition (OCR), active-learning-based text classification, and geographic information system visualization in order to effectively extract, query, and visualize this information for any area of interest. Active learning and text classification results are twofold: Whereas the restriction categories were reasonably accurate (>0.85 F1), the seven topic-oriented categories seemed to be complex even for human annotators and achieved mediocre evaluation scores (<0.70 F1).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 31, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.320.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-12", "journal": {"pages": "4141-4152"}, "authors": [{"authorId": "2058170882", "name": "Christopher Schr\u00f6der"}, {"authorId": "2121301109", "name": "Kim B\u00fcrgl"}, {"authorId": "2091417825", "name": "Yves Annanias"}, {"authorId": "2121285225", "name": "A. Niekler"}, {"authorId": "2121286581", "name": "Lydia M\u00fcller"}, {"authorId": "51136989", "name": "Daniel Wiegreffe"}, {"authorId": "1753437867", "name": "Christian Bender"}, {"authorId": "2091808685", "name": "Christoph Mengs"}, {"authorId": "1810101", "name": "G. Scheuermann"}, {"authorId": "2013643", "name": "Gerhard Heyer"}]}, {"paperId": "0f71a4fa9736ae916e6aef53045f6be4c901b0ff", "externalIds": {"DBLP": "journals/corr/abs-2105-02590", "ACL": "2021.acl-long.321", "ArXiv": "2105.02590", "DOI": "10.18653/v1/2021.acl-long.321", "CorpusId": 233864598}, "corpusId": 233864598, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0f71a4fa9736ae916e6aef53045f6be4c901b0ff", "title": "Reliability Testing for Natural Language Processing Systems", "abstract": "Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing \u2014 with an emphasis on interdisciplinary collaboration \u2014 will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 129, "citationCount": 21, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.321.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-06", "journal": {"pages": "4153-4169"}, "authors": [{"authorId": "145814654", "name": "Samson Tan"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "48162805", "name": "K. Baxter"}, {"authorId": "3299973", "name": "Araz Taeihagh"}, {"authorId": "32139163", "name": "G. Bennett"}, {"authorId": "37596605", "name": "Min-Yen Kan"}]}, {"paperId": "cc760dd9f909db13a1725e3c1f4975f47be358ef", "externalIds": {"DBLP": "journals/corr/abs-2106-13213", "MAG": "3177017072", "ACL": "2021.acl-long.322", "ArXiv": "2106.13213", "DOI": "10.18653/v1/2021.acl-long.322", "CorpusId": 235624212}, "corpusId": 235624212, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cc760dd9f909db13a1725e3c1f4975f47be358ef", "title": "Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data", "abstract": "Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care. The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders. One promising data source to help monitor human behavior is daily smartphone usage. However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes. In this paper, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors. Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood. However, we find that models trained to predict mood often also capture private user identities in their intermediate representations. To tackle this problem, we evaluate approaches that obfuscate user identity while remaining predictive. By combining multimodal representations with privacy-preserving learning, we are able to push forward the performance-privacy frontier.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 58, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.322.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-24", "journal": {"pages": "4170-4187"}, "authors": [{"authorId": "28130078", "name": "P. Liang"}, {"authorId": "1477124199", "name": "Terrance Liu"}, {"authorId": "47049828", "name": "Anna Cai"}, {"authorId": "37146710", "name": "Michal Muszynski"}, {"authorId": "1736448", "name": "Ryo Ishii"}, {"authorId": "2060747421", "name": "Nicholas Allen"}, {"authorId": "5512630", "name": "R. Auerbach"}, {"authorId": "153497723", "name": "D. Brent"}, {"authorId": "145124475", "name": "R. Salakhutdinov"}, {"authorId": "49933077", "name": "Louis-Philippe Morency"}]}, {"paperId": "9262b3fdd11e1fa73bc96cf4db4de6bfb5f6f6a2", "externalIds": {"DBLP": "conf/acl/LisonPSBO20", "ACL": "2021.acl-long.323", "DOI": "10.18653/v1/2021.acl-long.323", "CorpusId": 236460282}, "corpusId": 236460282, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9262b3fdd11e1fa73bc96cf4db4de6bfb5f6f6a2", "title": "Anonymisation Models for Text Data: State of the art, Challenges and Future Directions", "abstract": "This position paper investigates the problem of automated text anonymisation, which is a prerequisite for secure sharing of documents containing sensitive information about individuals. We summarise the key concepts behind text anonymisation and provide a review of current approaches. Anonymisation methods have so far been developed in two fields with little mutual interaction, namely natural language processing and privacy-preserving data publishing. Based on a case study, we outline the benefits and limitations of these approaches and discuss a number of open challenges, such as (1) how to account for multiple types of semantic inferences, (2) how to strike a balance between disclosure risk and data utility and (3) how to evaluate the quality of the resulting anonymisation. We lay out a case for moving beyond sequence labelling models and incorporate explicit measures of disclosure risk into the text anonymisation process.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 75, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.323.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": null, "journal": {"pages": "4188-4203"}, "authors": [{"authorId": "1746165", "name": "Pierre Lison"}, {"authorId": "2153768", "name": "I. Pil\u00e1n"}, {"authorId": "145604695", "name": "David S\u00e1nchez"}, {"authorId": "144829118", "name": "Montserrat Batet"}, {"authorId": "2732223", "name": "Lilja \u00d8vrelid"}]}, {"paperId": "e8acb757b782ff0fa12656dd4486efd4a49662ed", "externalIds": {"DBLP": "conf/acl/FuSD020", "ACL": "2021.acl-long.324", "DOI": "10.18653/v1/2021.acl-long.324", "CorpusId": 236459820}, "corpusId": 236459820, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e8acb757b782ff0fa12656dd4486efd4a49662ed", "title": "End-to-End AMR Coreference Resolution", "abstract": "Although parsing to Abstract Meaning Representation (AMR) has become very popular and AMR has been shown effective on the many sentence-level downstream tasks, little work has studied how to generate AMRs that can represent multi-sentence information. We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs. Compared with the previous pipeline and rule-based approaches, our model alleviates error propagation and it is more robust for both in-domain and out-domain situations. Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.324.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4204-4214"}, "authors": [{"authorId": "38891296", "name": "Qiankun Fu"}, {"authorId": "50258954", "name": "Linfeng Song"}, {"authorId": "2072591656", "name": "Wenyu Du"}, {"authorId": "1591125925", "name": "Yue Zhang"}]}, {"paperId": "528cd7c546315ae57e532ff9f57a674378a5ad6f", "externalIds": {"DBLP": "conf/acl/LiZT0R20", "ArXiv": "2105.07452", "ACL": "2021.acl-long.325", "DOI": "10.18653/v1/2021.acl-long.325", "CorpusId": 234742522}, "corpusId": 234742522, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/528cd7c546315ae57e532ff9f57a674378a5ad6f", "title": "How is BERT surprised? Layerwise detection of linguistic anomalies", "abstract": "Transformer language models have shown remarkable ability in detecting when a word is anomalous in context, but likelihood scores offer no information about the cause of the anomaly. In this work, we use Gaussian models for density estimation at intermediate layers of three language models (BERT, RoBERTa, and XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark. In lower layers, surprisal is highly correlated to low token frequency, but this correlation diminishes in upper layers. Next, we gather datasets of morphosyntactic, semantic, and commonsense anomalies from psycholinguistic studies; we find that the best performing model RoBERTa exhibits surprisal in earlier layers when the anomaly is morphosyntactic than when it is semantic, while commonsense anomalies do not exhibit surprisal at any intermediate layer. These results suggest that language models employ separate mechanisms to detect different types of linguistic anomalies.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 21, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.325.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-16", "journal": {"name": "ArXiv", "volume": "abs/2105.07452"}, "authors": [{"authorId": "1710717", "name": "Bai Li"}, {"authorId": "8129672", "name": "Zining Zhu"}, {"authorId": "26863450", "name": "Guillaume Thomas"}, {"authorId": "1698958205", "name": "Yang Xu"}, {"authorId": "2479037", "name": "Frank Rudzicz"}]}, {"paperId": "c651019b28c5e4f358d72779487bcc17e5ef726b", "externalIds": {"DBLP": "journals/corr/abs-2106-04963", "ArXiv": "2106.04963", "ACL": "2021.acl-long.326", "DOI": "10.18653/v1/2021.acl-long.326", "CorpusId": 235376785}, "corpusId": 235376785, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c651019b28c5e4f358d72779487bcc17e5ef726b", "title": "Psycholinguistic Tripartite Graph Network for Personality Detection", "abstract": "Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one\u2019s language use and his psychological traits. In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer. The graph network injects structural psycholinguistic knowledge in LIWC, a computerized instrument for psycholinguistic analysis, by constructing a heterogeneous tripartite graph. The initializer is employed to provide initial embeddings for the graph nodes. To reduce the computational cost in graph learning, we further propose a novel flow graph attention network (GAT) that only transmits messages between neighboring parties in the tripartite graph. Benefiting from the tripartite graph, TrigNet can aggregate post information from a psychological perspective, which is a novel way of exploiting domain knowledge. Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art model by 3.47 and 2.10 points in average F1. Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%, respectively, in comparison to the original GAT in our setting.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.326.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-09", "journal": {"name": "ArXiv", "volume": "abs/2106.04963"}, "authors": [{"authorId": "2122901065", "name": "Tao Yang"}, {"authorId": "2108022788", "name": "Feifan Yang"}, {"authorId": "2125735267", "name": "Haolan Ouyang"}, {"authorId": "38472218", "name": "Xiaojun Quan"}]}, {"paperId": "1c181f93490bc2a411b1ff6266e9b4c65ddefdb2", "externalIds": {"DBLP": "conf/acl/SongZFLL20", "ACL": "2021.acl-long.327", "DOI": "10.18653/v1/2021.acl-long.327", "CorpusId": 236459845}, "corpusId": 236459845, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1c181f93490bc2a411b1ff6266e9b4c65ddefdb2", "title": "Verb Metaphor Detection via Contextual Relation Learning", "abstract": "Correct natural language understanding requires computers to distinguish the literal and metaphorical senses of a word. Recent neu- ral models achieve progress on verb metaphor detection by viewing it as sequence labeling. In this paper, we argue that it is appropriate to view this task as relation classification between a verb and its various contexts. We propose the Metaphor-relation BERT (Mr-BERT) model, which explicitly models the relation between a verb and its grammatical, sentential and semantic contexts. We evaluate our method on the VUA, MOH-X and TroFi datasets. Our method gets competitive results compared with state-of-the-art approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 16, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.327.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4240-4251"}, "authors": [{"authorId": "145273601", "name": "Wei Song"}, {"authorId": "1850439845", "name": "Shuhui Zhou"}, {"authorId": "2884436", "name": "Ruiji Fu"}, {"authorId": "2140034750", "name": "Ting Liu"}, {"authorId": "2109489800", "name": "Li-zhen Liu"}]}, {"paperId": "95e838c886346c563005bde16af96b3173b0e016", "externalIds": {"DBLP": "conf/acl/TangPLWG20", "ACL": "2021.acl-long.328", "ArXiv": "2107.05782", "DOI": "10.18653/v1/2021.acl-long.328", "CorpusId": 235828943}, "corpusId": 235828943, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/95e838c886346c563005bde16af96b3173b0e016", "title": "Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task", "abstract": "Pretraining and multitask learning are widely used to improve the speech translation performance. In this study, we are interested in training a speech translation model along with an auxiliary text translation task. We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework. Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules. We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task. The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible, and those layers are critical for the translation quality. Inspired by these findings, we propose three methods to improve translation quality. First, a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks. Second, a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer. Third, an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task. Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-the-art results on the MuST-C English-German, English-French and English-Spanish language pairs.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 50, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.328.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-12", "journal": {"name": "ArXiv", "volume": "abs/2107.05782"}, "authors": [{"authorId": "48066266", "name": "Yun Tang"}, {"authorId": "145503806", "name": "J. Pino"}, {"authorId": "2116235416", "name": "Xian Li"}, {"authorId": "20132361", "name": "Changhan Wang"}, {"authorId": "2870702", "name": "Dmitriy Genzel"}]}, {"paperId": "080df61ee1c15ff3c8e5d0d82d60bfd80e372e38", "externalIds": {"ACL": "2021.acl-long.329", "DBLP": "conf/acl/OusidhoumZFSY20", "DOI": "10.18653/v1/2021.acl-long.329", "CorpusId": 236460108}, "corpusId": 236460108, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/080df61ee1c15ff3c8e5d0d82d60bfd80e372e38", "title": "Probing Toxic Content in Large Pre-Trained Language Models", "abstract": "Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 60, "citationCount": 51, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.329.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4262-4274"}, "authors": [{"authorId": "3056500", "name": "N. Ousidhoum"}, {"authorId": "1500662261", "name": "Xinran Zhao"}, {"authorId": "2044202073", "name": "Tianqing Fang"}, {"authorId": "1809614", "name": "Yangqiu Song"}, {"authorId": "66427434", "name": "Dit-Yan Yeung"}]}, {"paperId": "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a", "externalIds": {"DBLP": "conf/acl/ShengCNP20", "ArXiv": "2105.04054", "ACL": "2021.acl-long.330", "DOI": "10.18653/v1/2021.acl-long.330", "CorpusId": 234337004}, "corpusId": 234337004, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/76a786b1acd6d1aca56e12a8a1db34569fdf9f3a", "title": "Societal Biases in Language Generation: Progress and Challenges", "abstract": "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 105, "citationCount": 112, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.330.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-05-10", "journal": {"pages": "4275-4293"}, "authors": [{"authorId": "23923796", "name": "Emily Sheng"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}, {"authorId": "145603129", "name": "P. Natarajan"}, {"authorId": "3157053", "name": "Nanyun Peng"}]}, {"paperId": "4b30dd65a26e573df9796beb582ba1e1a69f23f7", "externalIds": {"ArXiv": "2012.15045", "DBLP": "conf/acl/ShenBMKAK20", "ACL": "2021.acl-long.331", "MAG": "3176740075", "DOI": "10.18653/v1/2021.acl-long.331", "CorpusId": 235303637}, "corpusId": 235303637, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4b30dd65a26e573df9796beb582ba1e1a69f23f7", "title": "Reservoir Transformers", "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear \u201creservoir\u201d layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 100, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-30", "journal": {"pages": "4294-4309"}, "authors": [{"authorId": "2191455", "name": "Sheng Shen"}, {"authorId": "51428394", "name": "Alexei Baevski"}, {"authorId": "4690624", "name": "Ari S. Morcos"}, {"authorId": "1732330", "name": "K. Keutzer"}, {"authorId": "2325985", "name": "Michael Auli"}, {"authorId": "1743722", "name": "Douwe Kiela"}]}, {"paperId": "45303f4cdc54d27ed0b39753ef77ea3761d94870", "externalIds": {"ACL": "2021.acl-long.332", "DBLP": "conf/acl/RadmardFL20", "DOI": "10.18653/v1/2021.acl-long.332", "CorpusId": 236460226}, "corpusId": 236460226, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/45303f4cdc54d27ed0b39753ef77ea3761d94870", "title": "Subsequence Based Deep Active Learning for Named Entity Recognition", "abstract": "Active Learning (AL) has been successfully applied to Deep Learning in order to drastically reduce the amount of data required to achieve high performance. Previous works have shown that lightweight architectures for Named Entity Recognition (NER) can achieve optimal performance with only 25% of the original training data. However, these methods do not exploit the sequential nature of language and the heterogeneity of uncertainty within each instance, requiring the labelling of whole sentences. Additionally, this standard method requires that the annotator has access to the full sentence when labelling. In this work, we overcome these limitations by allowing the AL algorithm to query subsequences within sentences, and propagate their labels to other sentences. We achieve highly efficient results on OntoNotes 5.0, only requiring 13% of the original training data, and CoNLL 2003, requiring only 27%. This is an improvement of 39% and 37% compared to querying full sentences.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 31, "citationCount": 14, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.332.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4310-4321"}, "authors": [{"authorId": "2121301573", "name": "Puria Radmard"}, {"authorId": "1400414048", "name": "Yassir Fathullah"}, {"authorId": "2121294485", "name": "Aldo Lipani"}]}, {"paperId": "1dbb523a6555d6e0c5727620e2b57daaa5b79dc0", "externalIds": {"DBLP": "journals/corr/abs-2106-05505", "ACL": "2021.acl-long.333", "ArXiv": "2106.05505", "DOI": "10.18653/v1/2021.acl-long.333", "CorpusId": 235390794}, "corpusId": 235390794, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1dbb523a6555d6e0c5727620e2b57daaa5b79dc0", "title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models", "abstract": "In this paper, we detail the relationship between convolutions and self-attention in natural language tasks. We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention. Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework. We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings. To inform future work, we present results comparing lightweight convolutions, dynamic convolutions, and depthwise-separable convolutions in language model pre-training, considering multiple injection points for convolutions in self-attention layers.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.333.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-10", "journal": {"pages": "4322-4333"}, "authors": [{"authorId": "2087001989", "name": "Tyler A. Chang"}, {"authorId": "2125063007", "name": "Yifan Xu"}, {"authorId": "2110546250", "name": "Weijian Xu"}, {"authorId": "144035504", "name": "Z. Tu"}]}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "externalIds": {"DBLP": "journals/corr/abs-2012-15701", "ACL": "2021.acl-long.334", "ArXiv": "2012.15701", "DOI": "10.18653/v1/2021.acl-long.334", "CorpusId": 229923538}, "corpusId": 229923538, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization", "abstract": "The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. Code will be released.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 59, "citationCount": 139, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.334.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"pages": "4334-4348"}, "authors": [{"authorId": "9583912", "name": "Haoli Bai"}, {"authorId": "2155468228", "name": "Wei Zhang"}, {"authorId": "48557122", "name": "Lu Hou"}, {"authorId": "50812138", "name": "Lifeng Shang"}, {"authorId": "2115757711", "name": "Jing Jin"}, {"authorId": "145820291", "name": "Xin Jiang"}, {"authorId": "30738758", "name": "Qun Liu"}, {"authorId": "1785083", "name": "Michael R. Lyu"}, {"authorId": "145310663", "name": "Irwin King"}]}, {"paperId": "2365410a710b421b2295cdca0074946cb50bb1d4", "externalIds": {"ArXiv": "2105.03322", "DBLP": "journals/corr/abs-2105-03322", "ACL": "2021.acl-long.335", "DOI": "10.18653/v1/2021.acl-long.335", "CorpusId": 234094053}, "corpusId": 234094053, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2365410a710b421b2295cdca0074946cb50bb1d4", "title": "Are Pretrained Convolutions Better than Pretrained Transformers?", "abstract": "In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 60, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.335.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-07", "journal": {"pages": "4349-4359"}, "authors": [{"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "3226635", "name": "Mostafa Dehghani"}, {"authorId": "1995602019", "name": "J. Gupta"}, {"authorId": "11774695", "name": "Dara Bahri"}, {"authorId": "1481714624", "name": "V. Aribandi"}, {"authorId": "145144957", "name": "Zhen Qin"}, {"authorId": "1680617", "name": "Donald Metzler"}]}, {"paperId": "1a8a8504722d9a39f17bbaa2968a89acc5cd0c48", "externalIds": {"DBLP": "journals/corr/abs-2011-03798", "ACL": "2021.acl-long.336", "MAG": "3099128825", "ArXiv": "2011.03798", "DOI": "10.18653/v1/2021.acl-long.336", "CorpusId": 226281660}, "corpusId": 226281660, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1a8a8504722d9a39f17bbaa2968a89acc5cd0c48", "title": "PairRE: Knowledge Graph Embeddings via Paired Relation Vectors", "abstract": "Distance based knowledge graph embedding methods show promising results on link prediction task, on which two topics have been widely studied: one is the ability to handle complex relations, such as N-to-1, 1-to-N and N-to-N, the other is to encode various relation patterns, such as symmetry/antisymmetry. However, the existing methods fail to solve these two problems at the same time, which leads to unsatisfactory results. To mitigate this problem, we propose PairRE, a model with paired vectors for each relation representation. The paired vectors enable an adaptive adjustment of the margin in loss function to fit for different complex relations. Besides, PairRE is capable of encoding three important relation patterns, symmetry/antisymmetry, inverse and composition. Given simple constraints on relation representations, PairRE can encode subrelation further. Experiments on link prediction benchmarks demonstrate the proposed key capabilities of PairRE. Moreover, We set a new state-of-the-art on two knowledge graph datasets of the challenging Open Graph Benchmark.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 75, "influentialCitationCount": 20, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.336.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-07", "journal": {"pages": "4360-4369"}, "authors": [{"authorId": "1850313", "name": "Linlin Chao"}, {"authorId": "51235263", "name": "Jianshan He"}, {"authorId": "1799672", "name": "Taifeng Wang"}, {"authorId": "2057047939", "name": "Wei Chu"}]}, {"paperId": "57516ba4a5356154b81a9332010544dce24ee494", "externalIds": {"DBLP": "conf/acl/ChenMLY20", "ACL": "2021.acl-long.337", "DOI": "10.18653/v1/2021.acl-long.337", "CorpusId": 236460056}, "corpusId": 236460056, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/57516ba4a5356154b81a9332010544dce24ee494", "title": "Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification", "abstract": "Hierarchical text classification is an important yet challenging task due to the complex structure of the label hierarchy. Existing methods ignore the semantic relationship between text and labels, so they cannot make full use of the hierarchical information. To this end, we formulate the text-label semantics relationship as a semantic matching problem and thus propose a hierarchy-aware label semantics matching network (HiMatch). First, we project text semantics and label semantics into a joint embedding space. We then introduce a joint embedding loss and a matching learning loss to model the matching relationship between the text semantics and the label semantics. Our model captures the text-label semantics matching relationship among coarse-grained labels and fine-grained labels in a hierarchy-aware manner. The experimental results on various benchmark datasets verify that our model achieves state-of-the-art results.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 18, "citationCount": 31, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.337.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4370-4379"}, "authors": [{"authorId": "2145226532", "name": "Haibin Chen"}, {"authorId": "2087919644", "name": "Qianli Ma"}, {"authorId": "148431025", "name": "Zhenxi Lin"}, {"authorId": "10676335", "name": "Jiangyue Yan"}]}, {"paperId": "291016368158f28829c06c0a037e0ca1a6548cca", "externalIds": {"ArXiv": "2106.00149", "ACL": "2021.acl-long.338", "DBLP": "conf/acl/ChenSCY20", "DOI": "10.18653/v1/2021.acl-long.338", "CorpusId": 235265942}, "corpusId": 235265942, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/291016368158f28829c06c0a037e0ca1a6548cca", "title": "HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability", "abstract": "Fine-tuning large pre-trained models with task-specific data has achieved great success in NLP. However, it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage. This leads to inferior results when generalizing the obtained models to out-of-domain distributions. To this end, we propose a simple yet effective data augmentation technique, HiddenCut, to better regularize the model and encourage it to learn more generalizable features. Specifically, contiguous spans within the hidden space are dynamically and strategically dropped during training. Experiments show that our HiddenCut method outperforms the state-of-the-art augmentation methods on the GLUE benchmark, and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples. We have publicly released our code at https://github.com/GT-SALT/HiddenCut.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 56, "citationCount": 29, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.338.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "4380-4390"}, "authors": [{"authorId": "47739850", "name": "Jiaao Chen"}, {"authorId": "19178763", "name": "Dinghan Shen"}, {"authorId": "2109136147", "name": "Weizhu Chen"}, {"authorId": "2022168", "name": "Diyi Yang"}]}, {"paperId": "8a88313f341ed2f98ead0f0ec03b5aba576c6efe", "externalIds": {"ACL": "2021.acl-long.339", "DBLP": "conf/acl/ZhuZLW20", "DOI": "10.18653/v1/2021.acl-long.339", "CorpusId": 236460126}, "corpusId": 236460126, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8a88313f341ed2f98ead0f0ec03b5aba576c6efe", "title": "Neural Stylistic Response Generation with Disentangled Latent Variables", "abstract": "Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style. Meanwhile, using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance. In this paper, we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations. Combining the desired style representation and a response content representation will then obtain a stylistic response. Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines. Human evaluation results show that our approach significantly improves style intensity and maintains content relevance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.339.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4391-4401"}, "authors": [{"authorId": "2152206489", "name": "Qingfu Zhu"}, {"authorId": "1806419", "name": "Weinan Zhang"}, {"authorId": "2140034831", "name": "Ting Liu"}, {"authorId": "152876475", "name": "W. Wang"}]}, {"paperId": "95d48c41daf7121a4ae815fee21afa375612f57f", "externalIds": {"ArXiv": "2101.00123", "ACL": "2021.acl-long.340", "DBLP": "journals/corr/abs-2101-00123", "DOI": "10.18653/v1/2021.acl-long.340", "CorpusId": 230435779}, "corpusId": 230435779, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/95d48c41daf7121a4ae815fee21afa375612f57f", "title": "Intent Classification and Slot Filling for Privacy Policies", "abstract": "Understanding privacy policies is crucial for users as it empowers them to learn about the information that matters to them. Sentences written in a privacy policy document explain privacy practices, and the constituent text spans convey further specific information about that practice. We refer to predicting the privacy practice explained in a sentence as intent classification and identifying the text spans sharing specific information as slot filling. In this work, we propose PolicyIE, an English corpus consisting of 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of websites and mobile applications. PolicyIE corpus is a challenging real-world benchmark with limited labeled examples reflecting the cost of collecting large-scale annotations from domain experts. We present two alternative neural approaches as baselines, (1) intent classification and slot filling as a joint sequence tagging and (2) modeling them as a sequence-to-sequence (Seq2Seq) learning task. The experiment results show that both approaches perform comparably in intent classification, while the Seq2Seq method outperforms the sequence tagging approach in slot filling by a large margin. We perform a detailed error analysis to reveal the challenges of the proposed corpus.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 63, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.340.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-01", "journal": {"pages": "4402-4417"}, "authors": [{"authorId": "38123220", "name": "Wasi Uddin Ahmad"}, {"authorId": "31357678", "name": "Jianfeng Chi"}, {"authorId": "143977071", "name": "Tu Le"}, {"authorId": "32435017", "name": "Thomas B. Norton"}, {"authorId": "2152947811", "name": "Yuan Tian"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}]}, {"paperId": "e885821da51a4370b4eb8ecd9f62047f6ee604db", "externalIds": {"DBLP": "conf/acl/PengLZZLG20", "ACL": "2021.acl-long.341", "ArXiv": "2012.14666", "DOI": "10.18653/v1/2021.acl-long.341", "CorpusId": 229923179}, "corpusId": 229923179, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e885821da51a4370b4eb8ecd9f62047f6ee604db", "title": "RADDLE: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems", "abstract": "For task-oriented dialog systems to be maximally useful, it must be able to process conversations in a way that is (1) generalizable with a small number of training examples for new task domains, and (2) robust to user input in various styles, modalities, or domains. In pursuit of these goals, we introduce the RADDLE benchmark, a collection of corpora and tools for evaluating the performance of models across a diverse set of domains. By including tasks with limited training data, RADDLE is designed to favor and encourage models with a strong generalization ability. RADDLE also includes a diagnostic checklist that facilitates detailed robustness analysis in aspects such as language variations, speech errors, unseen entities, and out-of-domain utterances. We evaluate recent state-of-the-art systems based on pre-training and fine-tuning, and find that grounded pre-training on heterogeneous dialog corpora performs better than training a separate model per domain. Adversarial training is also proposed to improve model robustness against noisy inputs. Overall, existing models are less than satisfactory in robustness evaluation, which suggests opportunities for future improvement.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 57, "citationCount": 43, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.341.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-29", "journal": {"name": "ArXiv", "volume": "abs/2012.14666"}, "authors": [{"authorId": "1780690", "name": "Baolin Peng"}, {"authorId": null, "name": "Chunyuan Li"}, {"authorId": "2142028984", "name": "Zhu Zhang"}, {"authorId": "8652308", "name": "Chenguang Zhu"}, {"authorId": "2887412", "name": "Jinchao Li"}, {"authorId": "1800422", "name": "Jianfeng Gao"}]}, {"paperId": "1c2499f11b4d061f6a5f0a0a50504a31a7d83090", "externalIds": {"DBLP": "conf/acl/BaiCS020", "ACL": "2021.acl-long.342", "ArXiv": "2105.10188", "DOI": "10.18653/v1/2021.acl-long.342", "CorpusId": 235125766}, "corpusId": 235125766, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1c2499f11b4d061f6a5f0a0a50504a31a7d83090", "title": "Semantic Representation for Dialogue Modeling", "abstract": "Although neural models have achieved competitive results in dialogue systems, they have shown limited ability in representing core semantics, such as ignoring important entities. To this end, we exploit Abstract Meaning Representation (AMR) to help dialogue modeling. Compared with the textual input, AMR explicitly provides core semantic knowledge and reduces data sparsity. We develop an algorithm to construct dialogue-level AMR graphs from sentence-level AMRs and explore two ways to incorporate AMRs into dialogue systems. Experimental results on both dialogue understanding and response generation tasks show the superiority of our model. To our knowledge, we are the first to leverage a formal semantic representation into neural dialogue modeling.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 71, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.342.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-21", "journal": {"name": "ArXiv", "volume": "abs/2105.10188"}, "authors": [{"authorId": "6713131", "name": "Xuefeng Bai"}, {"authorId": "2109404730", "name": "Yulong Chen"}, {"authorId": "50258954", "name": "Linfeng Song"}, {"authorId": "1591125925", "name": "Yue Zhang"}]}, {"paperId": "34a0306bdac2b06b757bbbe2573297171e523616", "externalIds": {"ACL": "2021.acl-long.343", "DBLP": "conf/acl/TaoCFW020", "DOI": "10.18653/v1/2021.acl-long.343", "CorpusId": 236460317}, "corpusId": 236460317, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/34a0306bdac2b06b757bbbe2573297171e523616", "title": "A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations", "abstract": "Recently, many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge (e.g., documents) when conversing with humans. However, it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents, which hinders the effective and adequate training of knowledge selection and response matching. To overcome the challenge, we consider decomposing the training of the knowledge-grounded response selection into three tasks including: 1) query-passage matching task; 2) query-dialogue history matching task; 3) multi-turn response matching task, and joint learning all these tasks in a unified pre-trained language model. The former two tasks could help the model in knowledge selection and comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history). By this means, the model can be learned to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4446-4457"}, "authors": [{"authorId": "8801869", "name": "Chongyang Tao"}, {"authorId": "11699474", "name": "Changyu Chen"}, {"authorId": "147062881", "name": "Jiazhan Feng"}, {"authorId": "153693432", "name": "Ji-rong Wen"}, {"authorId": "2055863231", "name": "Rui Yan"}]}, {"paperId": "d1a99b5e200f535b306995fa119884c95fd4516f", "externalIds": {"DBLP": "conf/acl/TianCSW20", "ACL": "2021.acl-long.344", "DOI": "10.18653/v1/2021.acl-long.344", "CorpusId": 236460104}, "corpusId": 236460104, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d1a99b5e200f535b306995fa119884c95fd4516f", "title": "Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks", "abstract": "Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task. In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN). In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the-shelf dependency parser, to distinguish the importance of different word dependencies. Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling. Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-of-the-art performance on both datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 51, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.344.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4458-4471"}, "authors": [{"authorId": "2152947211", "name": "Yuanhe Tian"}, {"authorId": "2116379168", "name": "Guimin Chen"}, {"authorId": "1922182598", "name": "Yan Song"}, {"authorId": "2005096276", "name": "Xiang Wan"}]}, {"paperId": "4291fe672cf6dc73e237ca0942fa49beb8c98711", "externalIds": {"DBLP": "conf/acl/ChenGLL020", "ArXiv": "2106.06830", "ACL": "2021.acl-long.345", "DOI": "10.18653/v1/2021.acl-long.345", "CorpusId": 235422061}, "corpusId": 235422061, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4291fe672cf6dc73e237ca0942fa49beb8c98711", "title": "Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP", "abstract": "Retrieval is a core component for open-domain NLP tasks. In open-domain tasks, multiple entities can share a name, making disambiguation an inherent yet under-explored problem. We propose an evaluation benchmark for assessing the entity disambiguation capabilities of these retrievers, which we call Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection of entities that share a name along with queries about those entities. By covering the set of entities for polysemous names, AmbER sets act as a challenging test of entity disambiguation. We create AmbER sets for three popular open-domain tasks: fact checking, slot filling, and question answering, and evaluate a diverse set of retrievers. We find that the retrievers exhibit popularity bias, significantly under-performing on rarer entities that share a name, e.g., they are twice as likely to retrieve erroneous documents on queries for the less popular entity under the same name. These experiments on AmbER sets show their utility as an evaluation tool and highlight the weaknesses of popular retrieval systems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 23, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.345.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-12", "journal": {"pages": "4472-4485"}, "authors": [{"authorId": "52151521", "name": "Anthony Chen"}, {"authorId": "9329241", "name": "Pallavi Gudipati"}, {"authorId": "29909347", "name": "S. Longpre"}, {"authorId": "145787377", "name": "Xiao Ling"}, {"authorId": "34650964", "name": "Sameer Singh"}]}, {"paperId": "30f233eecca2239ee1dd754914324092e53f8f19", "externalIds": {"ACL": "2021.acl-long.346", "DBLP": "conf/acl/RodriguezBHLJB20", "DOI": "10.18653/v1/2021.acl-long.346", "CorpusId": 235703772}, "corpusId": 235703772, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/30f233eecca2239ee1dd754914324092e53f8f19", "title": "Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards?", "abstract": "Leaderboards are widely used in NLP and push the field forward. While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models). Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made. Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. Using this model, we analyze the ranking reliability of leaderboards. Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples. We conclude with recommendations for future benchmark tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 110, "citationCount": 45, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.346.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Education", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4486-4503"}, "authors": [{"authorId": "145009056", "name": "Pedro Rodriguez"}, {"authorId": "40080808", "name": "Joe Barrow"}, {"authorId": "49462969", "name": "Alexander Miserlis Hoyle"}, {"authorId": "9051130", "name": "John P. Lalor"}, {"authorId": "3422908", "name": "Robin Jia"}, {"authorId": "1389036863", "name": "Jordan L. Boyd-Graber"}]}, {"paperId": "5775a40e51d79c1ecdec7f7c336667da658914f7", "externalIds": {"ArXiv": "2106.00853", "ACL": "2021.acl-long.347", "DBLP": "conf/acl/KazemiGGH20", "DOI": "10.18653/v1/2021.acl-long.347", "CorpusId": 235294121}, "corpusId": 235294121, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5775a40e51d79c1ecdec7f7c336667da658914f7", "title": "Claim Matching Beyond English to Scale Global Fact-Checking", "abstract": "Manual fact-checking does not scale well to serve the needs of the internet. This issue is further compounded in non-English contexts. In this paper, we discuss claim matching as a possible solution to scale fact-checking. We define claim matching as the task of identifying pairs of textual messages containing claims that can be served with one fact-check. We construct a novel dataset of WhatsApp tipline and public group messages alongside fact-checked claims that are first annotated for containing \u201cclaim-like statements\u201d and then matched with potentially similar items and annotated for claim matching. Our dataset contains content in high-resource (English, Hindi) and lower-resource (Bengali, Malayalam, Tamil) languages. We train our own embedding model using knowledge distillation and a high-quality \u201cteacher\u201d model in order to address the imbalance in embedding quality between the low- and high-resource languages in our dataset. We provide evaluations on the performance of our solution and compare with baselines and existing state-of-the-art multilingual embedding models, namely LASER and LaBSE. We demonstrate that our performance exceeds LASER and LaBSE in all settings. We release our annotated datasets, codebooks, and trained embedding model to allow for further research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 35, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.347.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"pages": "4504-4517"}, "authors": [{"authorId": "1721683964", "name": "Ashkan Kazemi"}, {"authorId": "2005765789", "name": "Kiran Garimella"}, {"authorId": "34366081", "name": "Devin Gaffney"}, {"authorId": "1741886127", "name": "Scott A. Hale"}]}, {"paperId": "1180bf7c1f15d5d472123a7b4aa5969baa5d5e7f", "externalIds": {"DBLP": "conf/acl/RenZ0W0020", "ACL": "2021.acl-long.348", "DOI": "10.18653/v1/2021.acl-long.348", "CorpusId": 236460116}, "corpusId": 236460116, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1180bf7c1f15d5d472123a7b4aa5969baa5d5e7f", "title": "SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation", "abstract": "While pre-training techniques are working very well in natural language processing, how to pre-train a decoder and effectively use it for neural machine translation (NMT) still remains a tricky issue. The main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained, and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs. In this paper, we propose a better pre-training method for NMT by defining a semantic interface (SemFace) between the pre-trained encoder and the pre-trained decoder. Specifically, we propose two types of semantic interfaces, including CL-SemFace which regards cross-lingual embeddings as an interface, and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space. We conduct massive experiments on six supervised translation pairs and three unsupervised pairs. Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.348.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4518-4527"}, "authors": [{"authorId": "50052368", "name": "Shuo Ren"}, {"authorId": "2135918679", "name": "Long Zhou"}, {"authorId": "2107983441", "name": "Shujie Liu"}, {"authorId": "49807919", "name": "Furu Wei"}, {"authorId": "92660691", "name": "Ming Zhou"}, {"authorId": "2152945175", "name": "Shuai Ma"}]}, {"paperId": "42fc352a0db1e742b0248a02b812db4aaf7b2cd3", "externalIds": {"MAG": "3089025530", "DBLP": "conf/acl/BhattacharyyaRN20", "ACL": "2021.acl-long.349", "ArXiv": "2009.13267", "DOI": "10.18653/v1/2021.acl-long.349", "CorpusId": 221970223}, "corpusId": 221970223, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/42fc352a0db1e742b0248a02b812db4aaf7b2cd3", "title": "Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models", "abstract": "The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution \u2013 there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energy-based model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a re-ranking algorithm based on the samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences). Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT: +3.7 BLEU points on IWSLT\u201914 German-English, +3.37 BELU points on Sinhala-English, +1.4 BLEU points on WMT\u201916 English-German tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 42, "citationCount": 41, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.349.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-20", "journal": {"name": "ArXiv", "volume": "abs/2009.13267"}, "authors": [{"authorId": "1805992557", "name": "Subhajit Naskar"}, {"authorId": "2844347", "name": "Pedram Rooshenas"}, {"authorId": "23134878", "name": "Simeng Sun"}, {"authorId": "2136562", "name": "Mohit Iyyer"}, {"authorId": "143753639", "name": "A. McCallum"}]}, {"paperId": "dda0bce7baee7175f7b0e5f0ac81669ed1c13f07", "externalIds": {"ACL": "2021.acl-long.350", "DBLP": "conf/acl/AhmadLCM20", "MAG": "3175001078", "ArXiv": "2106.02134", "DOI": "10.18653/v1/2021.acl-long.350", "CorpusId": 235352715}, "corpusId": 235352715, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dda0bce7baee7175f7b0e5f0ac81669ed1c13f07", "title": "Syntax-augmented Multilingual BERT for Cross-lingual Transfer", "abstract": "In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. However, due to typological differences across languages, the cross-lingual transfer is challenging. Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous works have shown that pre-trained multilingual encoders, such as mBERT (CITATION), capture language syntax, helping cross-lingual transfer. This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer. We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and task-oriented semantic parsing. The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages. In the generalized transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 49, "citationCount": 20, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.350.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.02134"}, "authors": [{"authorId": "38123220", "name": "Wasi Uddin Ahmad"}, {"authorId": "2145539790", "name": "Haoran Li"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}, {"authorId": "2263803", "name": "Yashar Mehdad"}]}, {"paperId": "c89ff2a0416a6d0870e724953a61a798deee238f", "externalIds": {"ACL": "2021.acl-long.351", "ArXiv": "2106.02124", "DBLP": "journals/corr/abs-2106-02124", "DOI": "10.18653/v1/2021.acl-long.351", "CorpusId": 235352637}, "corpusId": 235352637, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c89ff2a0416a6d0870e724953a61a798deee238f", "title": "How to Adapt Your Pretrained Multilingual Model to 1600 Languages", "abstract": "Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining. While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world\u2019s languages. In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for close to 1600 languages: the New Testament. This is challenging for two reasons: (1) the small corpus size, and (2) the narrow domain. While performance drops for all approaches, we surprisingly still see gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 F1 for NER on average over all languages as compared to XLM-R. Another unexpected finding is that continued pretraining, the simplest approach, performs best. Finally, we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 48, "citationCount": 34, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.351.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.02124"}, "authors": [{"authorId": "146057134", "name": "Abteen Ebrahimi"}, {"authorId": "3422953", "name": "Katharina Kann"}]}, {"paperId": "c19d03b7488823050b6f9b208461462648edf1fd", "externalIds": {"ArXiv": "2107.02282", "DBLP": "journals/corr/abs-2107-02282", "ACL": "2021.acl-long.352", "DOI": "10.18653/v1/2021.acl-long.352", "CorpusId": 235743070}, "corpusId": 235743070, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c19d03b7488823050b6f9b208461462648edf1fd", "title": "Weakly Supervised Named Entity Tagging with Learnable Logical Rules", "abstract": "We study the problem of building entity tagging systems by using a few rules as weak supervision. Previous methods mostly focus on disambiguating entity types based on contexts and expert-provided rules, while assuming entity spans are given. In this work, we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner. Specifically, we introduce compound rules that are composed from simple rules to increase the precision of boundary detection and generate more diverse pseudo labels. We further design a dynamic label selection strategy to ensure pseudo label quality and therefore avoid overfitting the neural tagger. Experiments on three datasets demonstrate that our method outperforms other weakly supervised methods and even rivals a state-of-the-art distantly supervised tagger with a lexicon of over 2,000 terms when starting from only 20 simple rules. Our method can serve as a tool for rapidly building taggers in emerging domains and tasks. Case studies show that learned rules can potentially explain the predicted entities.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 26, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.352.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-05", "journal": {"name": "ArXiv", "volume": "abs/2107.02282"}, "authors": [{"authorId": "97483167", "name": "Jiacheng Li"}, {"authorId": "47929135", "name": "Haibo Ding"}, {"authorId": "2884976", "name": "Jingbo Shang"}, {"authorId": "35660011", "name": "Julian McAuley"}, {"authorId": "2113908294", "name": "Zhe Feng"}]}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "externalIds": {"DBLP": "conf/acl/LiL20", "ACL": "2021.acl-long.353", "ArXiv": "2101.00190", "DOI": "10.18653/v1/2021.acl-long.353", "CorpusId": 230433941}, "corpusId": 230433941, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "abstract": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \u201cvirtual tokens\u201d. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 56, "citationCount": 1765, "influentialCitationCount": 235, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.353.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"name": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "volume": "abs/2101.00190"}, "authors": [{"authorId": "32551341", "name": "Xiang Lisa Li"}, {"authorId": "145419642", "name": "Percy Liang"}]}, {"paperId": "e08c6f15565c9f70e5e3375e293d9a5cfe888e74", "externalIds": {"DBLP": "journals/corr/abs-2105-11134", "ArXiv": "2105.11134", "ACL": "2021.acl-long.354", "DOI": "10.18653/v1/2021.acl-long.354", "CorpusId": 235166497}, "corpusId": 235166497, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e08c6f15565c9f70e5e3375e293d9a5cfe888e74", "title": "One2Set: Generating Diverse Keyphrases as a Set", "abstract": "Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training. However, the keyphrases are inherently an unordered set rather than an ordered sequence. Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases. In this work, we propose a new training paradigm One2Set without predefining an order to concatenate the keyphrases. To fit this paradigm, we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel. To solve the problem that there is no correspondence between each prediction and target during training, we propose a K-step label assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the repetition rate of generated keyphrases. The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 44, "influentialCitationCount": 17, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.354.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-24", "journal": {"pages": "4598-4608"}, "authors": [{"authorId": "65846898", "name": "Jiacheng Ye"}, {"authorId": "2067331064", "name": "Tao Gui"}, {"authorId": "97685399", "name": "Yichao Luo"}, {"authorId": "26339093", "name": "Yige Xu"}, {"authorId": "1409702669", "name": "Qi Zhang"}]}, {"paperId": "68c252eaf81d24f5d04eb8317492f6efc664eb3c", "externalIds": {"DBLP": "conf/acl/TangZKB20", "ACL": "2021.acl-long.355", "DOI": "10.18653/v1/2021.acl-long.355", "CorpusId": 236459928}, "corpusId": 236459928, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/68c252eaf81d24f5d04eb8317492f6efc664eb3c", "title": "Continuous Language Generative Flow", "abstract": "Recent years have witnessed various types of generative models for natural language generation (NLG), especially RNNs or transformer based sequence-to-sequence models, as well as variational autoencoder (VAE) and generative adversarial network (GAN) based models. However, flow-based generative models, which achieve strong performance in image generation due to their invertibility and exact density estimation properties, have been less explored for NLG. In this paper, we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings, adapted affine coupling structures, and a novel architecture for autoregressive text generation. We also apply our framework to Sequence-to-Sequence generation, including text- and video-based Question Generation (QG) and Neural Machine Translation (NMT), and data augmentation for Question Answering (QA). We use our language flow model to provide extra input features for QG and NMT, which achieves improvements over the strong QG baselines on SQuAD and TVQA and NMT baseline on WMT16. We also augment QA data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on SQuAD and TVQA.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.355.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4609-4622"}, "authors": [{"authorId": "2125564044", "name": "Zineng Tang"}, {"authorId": "7670835", "name": "Shiyue Zhang"}, {"authorId": "51270689", "name": "Hyounghun Kim"}, {"authorId": "143977268", "name": "Mohit Bansal"}]}, {"paperId": "0ed8c60150106473ed8777b1f4797b1f0b4c613f", "externalIds": {"DBLP": "journals/corr/abs-2106-15135", "ArXiv": "2106.15135", "ACL": "2021.acl-long.356", "DOI": "10.18653/v1/2021.acl-long.356", "CorpusId": 235669622}, "corpusId": 235669622, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0ed8c60150106473ed8777b1f4797b1f0b4c613f", "title": "TWAG: A Topic-Guided Wikipedia Abstract Generator", "abstract": "Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multi-document summarization techniques. However, previous works generally view the abstract as plain text, ignoring the fact that it is a description of a certain entity and can be decomposed into different topics. In this paper, we propose a two-stage model TWAG that guides the abstract generation with topical information. First, we detect the topic of each input paragraph with a classifier trained on existing Wikipedia articles to divide input documents into different topics. Then, we predict the topic distribution of each abstract sentence, and decode the sentence from topic-aware representations with a Pointer-Generator network. We evaluate our model on the WikiCatSum dataset, and the results show that TWAG outperforms various existing baselines and is capable of generating comprehensive abstracts.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.356.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-29", "journal": {"name": "ArXiv", "volume": "abs/2106.15135"}, "authors": [{"authorId": "70585600", "name": "Fangwei Zhu"}, {"authorId": "2116520118", "name": "Shangqing Tu"}, {"authorId": "2048406602", "name": "Jiaxin Shi"}, {"authorId": "8549842", "name": "Juan-Zi Li"}, {"authorId": "2055765060", "name": "Lei Hou"}, {"authorId": "2053996263", "name": "Tong Cui"}]}, {"paperId": "30602e3382df3abedb5f225b55b7efce8580f74d", "externalIds": {"ArXiv": "2005.00792", "MAG": "3088193755", "ACL": "2021.acl-long.357", "DBLP": "conf/acl/JinKKLMGR20", "DOI": "10.18653/v1/2021.acl-long.357", "CorpusId": 221978039}, "corpusId": 221978039, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/30602e3382df3abedb5f225b55b7efce8580f74d", "title": "ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data", "abstract": "Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our task limits accessible information, and thus a model has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERTbased models and find that our best model achieves 61.0% accuracy on the dataset, which still lags behind human performance by about 19%. We hope ForecastQA will support future research efforts in bridging this gap.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 61, "citationCount": 19, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.357.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"pages": "4636-4650"}, "authors": [{"authorId": "8844876", "name": "Woojeong Jin"}, {"authorId": "2109621632", "name": "Suji Kim"}, {"authorId": "1885282", "name": "Rahul Khanna"}, {"authorId": "2115475530", "name": "Dong-Ho Lee"}, {"authorId": "2775559", "name": "Fred Morstatter"}, {"authorId": "143728483", "name": "A. Galstyan"}, {"authorId": "1384550891", "name": "Xiang Ren"}]}, {"paperId": "061c712d9bb32439c70d9d3c01882f4097fc3df3", "externalIds": {"ACL": "2021.acl-long.358", "DBLP": "conf/acl/MriniFN20", "DOI": "10.18653/v1/2021.acl-long.358", "CorpusId": 236459903}, "corpusId": 236459903, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/061c712d9bb32439c70d9d3c01882f4097fc3df3", "title": "Recursive Tree-Structured Self-Attention for Answer Sentence Selection", "abstract": "Syntactic structure is an important component of natural language text. Recent top-performing models in Answer Sentence Selection (AS2) use self-attention and transfer learning, but not syntactic structure. Tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness. We investigate whether tree structures can boost performance in AS2. We introduce the Tree Aggregation Transformer: a novel recursive, tree-structured self-attention model for AS2. The recursive nature of our model is able to represent all levels of syntactic parse trees with only one additional self-attention layer. Without transfer learning, we establish a new state of the art on the popular TrecQA and WikiQA benchmark datasets. Additionally, we evaluate our method on four Community Question Answering datasets, and find that tree-structured representations have limitations with noisy user-generated text. We conduct probing experiments to evaluate how our models leverage tree structures across datasets. Our findings show that the ability of tree-structured models to successfully absorb syntactic information is strongly correlated with a higher performance in AS2.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 52, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.358.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4651-4661"}, "authors": [{"authorId": "2065526768", "name": "K. Mrini"}, {"authorId": "1786321", "name": "E. Farcas"}, {"authorId": "3115592", "name": "Ndapandula Nakashole"}]}, {"paperId": "7f3e27731f96e85de069cdcfd4a9be3de4a6d636", "externalIds": {"ArXiv": "2107.12064", "DBLP": "conf/acl/Hu0HC20", "ACL": "2021.acl-long.359", "DOI": "10.18653/v1/2021.acl-long.359", "CorpusId": 236428380}, "corpusId": 236428380, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7f3e27731f96e85de069cdcfd4a9be3de4a6d636", "title": "How Knowledge Graph and Attention Help? A Qualitative Analysis into Bag-level Relation Extraction", "abstract": "Knowledge Graph (KG) and attention mechanism have been demonstrated effective in introducing and selecting useful information for weakly supervised methods. However, only qualitative analysis and ablation study are provided as evidence. In this paper, we contribute a dataset and propose a paradigm to quantitatively evaluate the effect of attention and KG on bag-level relation extraction (RE). We find that (1) higher attention accuracy may lead to worse performance as it may harm the model\u2019s ability to extract entity mention features; (2) the performance of attention is largely influenced by various noise distribution patterns, which is closely related to real-world datasets; (3) KG-enhanced attention indeed improves RE performance, while not through enhanced attention but by incorporating entity prior; and (4) attention mechanism may exacerbate the issue of insufficient training data. Based on these findings, we show that a straightforward variant of RE model can achieve significant improvements (6% AUC on average) on two real-world datasets as compared with three state-of-the-art baselines. Our codes and datasets are available at https://github.com/zig-kwin-hu/how-KG-ATT-help.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 21, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.359.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-26", "journal": {"name": "ArXiv", "volume": "abs/2107.12064"}, "authors": [{"authorId": "51193721", "name": "Zikun Hu"}, {"authorId": "2112867078", "name": "Yixin Cao"}, {"authorId": "34170717", "name": "Lifu Huang"}, {"authorId": "143779329", "name": "Tat-seng Chua"}]}, {"paperId": "89c32bb4da87815564081be435e54dbd09b4a426", "externalIds": {"ACL": "2021.acl-long.360", "DBLP": "conf/acl/WeiSZZGJ20", "DOI": "10.18653/v1/2021.acl-long.360", "CorpusId": 236460308}, "corpusId": 236460308, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/89c32bb4da87815564081be435e54dbd09b4a426", "title": "Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction", "abstract": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 30, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.360.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4672-4682"}, "authors": [{"authorId": "31407139", "name": "Kaiwen Wei"}, {"authorId": "2946890", "name": "Xian Sun"}, {"authorId": "151473773", "name": "Zequn Zhang"}, {"authorId": "2108123471", "name": "Jingyuan Zhang"}, {"authorId": "2116390646", "name": "Zhi Guo"}, {"authorId": "2152163772", "name": "Li Jin"}]}, {"paperId": "bf2079d4fca3844747a9ffcfe79736a6fa4dba90", "externalIds": {"MAG": "3166217577", "ArXiv": "2106.09558", "DBLP": "journals/corr/abs-2106-09558", "ACL": "2021.acl-long.361", "DOI": "10.18653/v1/2021.acl-long.361", "CorpusId": 235458449}, "corpusId": 235458449, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bf2079d4fca3844747a9ffcfe79736a6fa4dba90", "title": "Element Intervention for Open Relation Extraction", "abstract": "Open relation extraction aims to cluster relation instances referring to the same underlying relation, which is a critical step for general relation extraction. Current OpenRE models are commonly trained on the datasets generated from distant supervision, which often results in instability and makes the model easily collapsed. In this paper, we revisit the procedure of OpenRE from a causal view. By formulating OpenRE using a structural causal model, we identify that the above-mentioned problems stem from the spurious correlations from entities and context to the relation type. To address this issue, we conduct Element Intervention, which intervene on the context and entities respectively to obtain the underlying causal effects of them. We also provide two specific implementations of the interventions based on entity ranking and context contrasting. Experimental results on unsupervised relation extraction datasets show our method to outperform previous state-of-the-art methods and is robust across different datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 13, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.361.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-17", "journal": {"name": "ArXiv", "volume": "abs/2106.09558"}, "authors": [{"authorId": "71211759", "name": "Liu Fangchao"}, {"authorId": "1387839383", "name": "Lingyong Yan"}, {"authorId": "2116455765", "name": "Hongyu Lin"}, {"authorId": "2118233348", "name": "Xianpei Han"}, {"authorId": "2110832778", "name": "Le Sun"}]}, {"paperId": "b2eec2dc3c870b2d3f37564579c45434be128383", "externalIds": {"ArXiv": "2106.02318", "DBLP": "conf/acl/YanZLGRD20", "ACL": "2021.acl-long.362", "DOI": "10.18653/v1/2021.acl-long.362", "CorpusId": 235352757}, "corpusId": 235352757, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b2eec2dc3c870b2d3f37564579c45434be128383", "title": "AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding", "abstract": "Automatic extraction of product attribute values is an important enabling technology in e-Commerce platforms. This task is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction. One line of previous work constructs attribute-specific models, through separate decoders or entirely separate models. However, this approach constrains knowledge sharing across different attributes. Other contributions use a single multi-attribute model, with different techniques to embed attribute information. But sharing the entire network parameters across all attributes can limit the model\u2019s capacity to capture attribute-specific characteristics. In this paper we present AdaTag, which uses adaptive decoding to handle extraction. We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This allows for separate, but semantically correlated, decoders to be generated on the fly for different attributes. This approach facilitates knowledge sharing, while maintaining the specificity of each attribute. Our experiments on a real-world e-Commerce dataset show marked improvements over previous methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 30, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.362.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02318"}, "authors": [{"authorId": "49781448", "name": "Jun Yan"}, {"authorId": "2814303", "name": "Nasser Zalmout"}, {"authorId": "145461906", "name": "Yan Liang"}, {"authorId": "4985351", "name": "Christan Earl Grant"}, {"authorId": "145201124", "name": "Xiang Ren"}, {"authorId": "2143917898", "name": "Xin Dong"}]}, {"paperId": "0c04a256d858f3d03e52f01fb55a5fa2c4c9aa42", "externalIds": {"ACL": "2021.acl-long.363", "DBLP": "journals/corr/abs-2106-00793", "ArXiv": "2106.00793", "DOI": "10.18653/v1/2021.acl-long.363", "CorpusId": 235293851}, "corpusId": 235293851, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0c04a256d858f3d03e52f01fb55a5fa2c4c9aa42", "title": "CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction", "abstract": "Integrating extracted knowledge from the Web to knowledge graphs (KGs) can facilitate tasks like question answering. We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG. To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context. However, the predictions are made independently, which can be mutually inconsistent. We propose a two-stage Collective Relation Integration (CoRI) model, where the first stage independently makes candidate predictions, and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions. We further improve the collective model with augmented data from the portion of the target KG that is otherwise unused. Experiment results on two datasets show that CoRI can significantly outperform the baselines, improving AUC from .677 to .748 and from .716 to .780, respectively.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.363.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"pages": "4706-4716"}, "authors": [{"authorId": "2669515", "name": "Zhengbao Jiang"}, {"authorId": "3024833", "name": "Jialong Han"}, {"authorId": "2582063", "name": "Bunyamin Sisman"}, {"authorId": "2143917898", "name": "Xin Dong"}]}, {"paperId": "973f1092f3eea20254eb290925b41f8b619e1a03", "externalIds": {"DBLP": "conf/acl/LoganM0B20", "ACL": "2021.acl-long.364", "DOI": "10.18653/v1/2021.acl-long.364", "CorpusId": 236460062}, "corpusId": 236460062, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/973f1092f3eea20254eb290925b41f8b619e1a03", "title": "Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference", "abstract": "Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus, it is well-suited for processing streams of data where new entities are frequently introduced. Despite these benefits, this task is currently difficult to study, as existing approaches are either evaluated on datasets that are no longer available, or omit other crucial details needed to ensure fair comparison. In this work, we address this issue by compiling a large benchmark adapted from existing free datasets, and performing a comprehensive evaluation of a number of novel and existing baseline models. We investigate: how to best encode mentions, which clustering algorithms are most effective for grouping mentions, how models transfer to different domains, and how bounding the number of mentions tracked during inference impacts performance. Our results show that the relative performance of neural and feature-based mention encoders varies across different domains, and in most cases the best performance is achieved using a combination of both approaches. We also find that performance is minimally impacted by limiting the number of tracked mentions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.364.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4717-4731"}, "authors": [{"authorId": "30083905", "name": "Robert L Logan IV"}, {"authorId": "2064329879", "name": "Andrew McCallum"}, {"authorId": "34650964", "name": "Sameer Singh"}, {"authorId": "2023469", "name": "D. Bikel"}]}, {"paperId": "a697045644788d9c975bf5ac544ed211f6fc9d5b", "externalIds": {"DBLP": "journals/corr/abs-2106-00327", "ArXiv": "2106.00327", "ACL": "2021.acl-long.365", "DOI": "10.18653/v1/2021.acl-long.365", "CorpusId": 235266233}, "corpusId": 235266233, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a697045644788d9c975bf5ac544ed211f6fc9d5b", "title": "Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs", "abstract": "Temporal Knowledge Graphs (TKGs) have been developed and used in many different areas. Reasoning on TKGs that predicts potential facts (events) in the future brings great challenges to existing models. When facing a prediction task, human beings usually search useful historical information (i.e., clues) in their memories and then reason for future meticulously. Inspired by this mechanism, we propose CluSTeR to predict future facts in a two-stage manner, Clue Searching and Temporal Reasoning, accordingly. Specifically, at the clue searching stage, CluSTeR learns a beam search policy via reinforcement learning (RL) to induce multiple clues from historical facts. At the temporal reasoning stage, it adopts a graph convolution network based sequence method to deduce answers from clues. Experiments on four datasets demonstrate the substantial advantages of CluSTeR compared with the state-of-the-art methods. Moreover, the clues found by CluSTeR further provide interpretability for the results.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 63, "citationCount": 42, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.365.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"name": "ArXiv", "volume": "abs/2106.00327"}, "authors": [{"authorId": "46947005", "name": "Zixuan Li"}, {"authorId": "2149111400", "name": "Xiaolong Jin"}, {"authorId": "24749412", "name": "Saiping Guan"}, {"authorId": "48624966", "name": "Wei Li"}, {"authorId": "70414094", "name": "Jiafeng Guo"}, {"authorId": "2219600", "name": "Yuanzhuo Wang"}, {"authorId": "1717004", "name": "Xueqi Cheng"}]}, {"paperId": "1a5760e164e858ac991d06b7840362a5fc0db4d2", "externalIds": {"DBLP": "conf/acl/KhatibTW0020", "ACL": "2021.acl-long.366", "DOI": "10.18653/v1/2021.acl-long.366", "CorpusId": 236460348}, "corpusId": 236460348, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1a5760e164e858ac991d06b7840362a5fc0db4d2", "title": "Employing Argumentation Knowledge Graphs for Neural Argument Generation", "abstract": "Generating high-quality arguments, while being challenging, may benefit a wide range of downstream applications, such as writing assistants and argument search engines. Motivated by the effectiveness of utilizing knowledge graphs for supporting general text generation tasks, this paper investigates the usage of argumentation-related knowledge graphs to control the generation of arguments. In particular, we construct and populate three knowledge graphs, employing several compositions of them to encode various knowledge into texts of debate portals and relevant paragraphs from Wikipedia. Then, the texts with the encoded knowledge are used to fine-tune a pre-trained text generation model, GPT-2. We evaluate the newly created arguments manually and automatically, based on several dimensions important in argumentative contexts, including argumentativeness and plausibility. The results demonstrate the positive impact of encoding the graphs\u2019 knowledge into debate portal texts for generating arguments with superior quality than those generated without knowledge.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.366.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4744-4754"}, "authors": [{"authorId": "2248209", "name": "Khalid Al Khatib"}, {"authorId": "2121278635", "name": "Lukas Trautner"}, {"authorId": "2626599", "name": "Henning Wachsmuth"}, {"authorId": "39517968", "name": "Yufang Hou"}, {"authorId": "1405867539", "name": "Benno Stein"}]}, {"paperId": "1abaa34bd4cd780f723c23b8fa819e5aa4d6ae08", "externalIds": {"ArXiv": "2107.12214", "DBLP": "journals/corr/abs-2107-12214", "ACL": "2021.acl-long.367", "DOI": "10.18653/v1/2021.acl-long.367", "CorpusId": 236428724}, "corpusId": 236428724, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1abaa34bd4cd780f723c23b8fa819e5aa4d6ae08", "title": "Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA which outputs triplets of an aspect target, its associated sentiment, and the corresponding opinion term. Recent models perform the triplet extraction in an end-to-end manner but heavily rely on the interactions between each target word and opinion word. Thereby, they cannot perform well on targets and opinions which contain multiple words. Our proposed span-level approach explicitly considers the interaction between the whole spans of targets and opinions when predicting their sentiment relation. Thus, it can make predictions with the semantics of whole spans, ensuring better sentiment consistency. To ease the high computational cost caused by span enumeration, we propose a dual-channel span pruning strategy by incorporating supervision from the Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not only improves computational efficiency but also distinguishes the opinion and target spans more properly. Our framework simultaneously achieves strong performance for the ASTE as well as ATE and OTE tasks. In particular, our analysis shows that our span-level approach achieves more significant improvements over the baselines on triplets with multi-word targets or opinions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 51, "citationCount": 81, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.367.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-26", "journal": {"pages": "4755-4766"}, "authors": [{"authorId": "153152460", "name": "Lu Xu"}, {"authorId": "2066312627", "name": "Yew Ken Chia"}, {"authorId": "1996394", "name": "Lidong Bing"}]}, {"paperId": "03ad126cfe495933f7bb769f27c03e5f31caedf8", "externalIds": {"ArXiv": "2105.14802", "DBLP": "conf/acl/LiYCZ20", "ACL": "2021.acl-long.368", "DOI": "10.18653/v1/2021.acl-long.368", "CorpusId": 235254420}, "corpusId": 235254420, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/03ad126cfe495933f7bb769f27c03e5f31caedf8", "title": "On Compositional Generalization of Neural Machine Translation", "abstract": "Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks such as WMT. However, there still exist significant issues such as robustness, domain generalization, etc. In this paper, we study NMT models from the perspective of compositional generalization by building a benchmark dataset, CoGnition, consisting of 216k clean and consistent sentence pairs. We quantitatively analyze effects of various factors using compound translation error rate, then demonstrate that the NMT model fails badly on compositional generalization, although it performs remarkably well under traditional metrics.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 57, "citationCount": 27, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.368.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"name": "ArXiv", "volume": "abs/2105.14802"}, "authors": [{"authorId": "2110450452", "name": "Yafu Li"}, {"authorId": "79701068", "name": "Yongjing Yin"}, {"authorId": "2109404730", "name": "Yulong Chen"}, {"authorId": "1591125925", "name": "Yue Zhang"}]}, {"paperId": "61fb9e3709aa396dd234ed96e4e4333905f69092", "externalIds": {"ACL": "2021.acl-long.369", "DBLP": "journals/corr/abs-2012-07162", "MAG": "3111519275", "ArXiv": "2012.07162", "DOI": "10.18653/v1/2021.acl-long.369", "CorpusId": 229152858}, "corpusId": 229152858, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/61fb9e3709aa396dd234ed96e4e4333905f69092", "title": "Mask-Align: Self-Supervised Neural Word Alignment", "abstract": "Word alignment, which aims to align translationally equivalent words between source and target sentences, plays an important role in many natural language processing tasks. Current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models, which does not leverage the full context in the target sequence. In this paper, we propose Mask-Align, a self-supervised word alignment model that takes advantage of the full context on the target side. Our model masks out each target token and predicts it conditioned on both source and the remaining target tokens. This two-step process is based on the assumption that the source token contributing most to recovering the masked target token should be aligned. We also introduce an attention variant called leaky attention, which alleviates the problem of unexpected high cross-attention weights on special tokens such as periods. Experiments on four language pairs show that our model outperforms previous unsupervised neural aligners and obtains new state-of-the-art results.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 28, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.369.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-14", "journal": {"pages": "4781-4791"}, "authors": [{"authorId": "2116885534", "name": "Chi Chen"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "2152797839", "name": "Yang Liu"}]}, {"paperId": "35e7760adfbf1d3f4da6246c9bb9b20f51de96a5", "externalIds": {"ArXiv": "2105.14913", "DBLP": "conf/acl/LiLHS20", "ACL": "2021.acl-long.370", "DOI": "10.18653/v1/2021.acl-long.370", "CorpusId": 235254233}, "corpusId": 235254233, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/35e7760adfbf1d3f4da6246c9bb9b20f51de96a5", "title": "GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation", "abstract": "Computer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT. There are two limitations in previous research in this line. First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far. Second, almost no public benchmarks are available for the autocompletion task of CAT. This might be among the reasons why research progress in CAT is much slower compared to automatic MT. In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark to facilitate research in this topic. In addition, we propose an effective method for GWLAN and compare it with several strong baselines. Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.370.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "4792-4802"}, "authors": [{"authorId": "91956362", "name": "Huayang Li"}, {"authorId": "2978364", "name": "Lemao Liu"}, {"authorId": "12875501", "name": "Guoping Huang"}, {"authorId": "2072684668", "name": "Shuming Shi"}]}, {"paperId": "3aa14f1d49db181a6cef26faa4b7cf60e5afeefb", "externalIds": {"DBLP": "journals/corr/abs-2106-09233", "ACL": "2021.acl-long.371", "ArXiv": "2106.09233", "DOI": "10.18653/v1/2021.acl-long.371", "CorpusId": 235458156}, "corpusId": 235458156, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3aa14f1d49db181a6cef26faa4b7cf60e5afeefb", "title": "De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention", "abstract": "Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching. Unfortunately, the learning of DS-NER is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models. In this paper, we fundamentally explain the dictionary bias via a Structural Causal Model (SCM), categorize the bias into intra-dictionary and inter-dictionary biases, and identify their causes. Based on the SCM, we learn de-biased DS-NER via causal interventions. For intra-dictionary bias, we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder. For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries. Experiments on four datasets and three DS-NER models show that our method can significantly improve the performance of DS-NER.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 28, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.371.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-17", "journal": {"name": "ArXiv", "volume": "abs/2106.09233"}, "authors": [{"authorId": "2145504165", "name": "Wenkai Zhang"}, {"authorId": "2116455765", "name": "Hongyu Lin"}, {"authorId": "2118233348", "name": "Xianpei Han"}, {"authorId": "2110832778", "name": "Le Sun"}]}, {"paperId": "22b6983a68d901925c83244dad1991c2bc2294b8", "externalIds": {"DBLP": "conf/acl/LiLZJ20", "ACL": "2021.acl-long.372", "ArXiv": "2106.14373", "DOI": "10.18653/v1/2021.acl-long.372", "CorpusId": 235658105}, "corpusId": 235658105, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/22b6983a68d901925c83244dad1991c2bc2294b8", "title": "A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition", "abstract": "Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention. The majority of previous work focuses on either overlapped or discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. The model includes two major steps. First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. Second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 61, "citationCount": 54, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.372.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-28", "journal": {"name": "ArXiv", "volume": "abs/2106.14373"}, "authors": [{"authorId": "2109530930", "name": "Fei Li"}, {"authorId": "47778813", "name": "Zhichao Lin"}, {"authorId": "2117849151", "name": "Meishan Zhang"}, {"authorId": "145628086", "name": "Donghong Ji"}]}, {"paperId": "c8ca05970ff3de83b6065f2789481399b388e3f1", "externalIds": {"ArXiv": "2105.09458", "DBLP": "journals/corr/abs-2105-09458", "ACL": "2021.acl-long.373", "DOI": "10.18653/v1/2021.acl-long.373", "CorpusId": 234790176}, "corpusId": 234790176, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c8ca05970ff3de83b6065f2789481399b388e3f1", "title": "MLBiNet: A Cross-Sentence Collective Event Detection Network", "abstract": "We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 41, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.373.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-20", "journal": {"pages": "4829-4839"}, "authors": [{"authorId": "104300808", "name": "Dongfang Lou"}, {"authorId": "10099514", "name": "Zhilin Liao"}, {"authorId": "152931849", "name": "Shumin Deng"}, {"authorId": "2153010067", "name": "Ningyu Zhang"}, {"authorId": "2608639", "name": "Ningyu Zhang"}, {"authorId": "49178307", "name": "Huajun Chen"}]}, {"paperId": "28e207ba02f3eab16803f3ee0653788b710d5b7d", "externalIds": {"ACL": "2021.acl-long.374", "DBLP": "conf/acl/TranPN20", "DOI": "10.18653/v1/2021.acl-long.374", "CorpusId": 236460010}, "corpusId": 236460010, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/28e207ba02f3eab16803f3ee0653788b710d5b7d", "title": "Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution", "abstract": "We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters. Deep learning methods have recently been applied for this task to deliver state-of-the-art performance. However, existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR, e.g., context words and entity mentions, to support the encoding of document-level context. In addition, consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR. This work addresses such limitations by introducing a novel deep learning model for ECR. At the core of our model are document structures to explicitly capture relevant objects for ECR. Our document structures introduce diverse knowledge sources (discourse, syntax, semantics) to compute edges/interactions between structure nodes for document-level representation learning. We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents. Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4840-4850"}, "authors": [{"authorId": "144356780", "name": "H. Tran"}, {"authorId": "2064597503", "name": "Duy Phung"}, {"authorId": "1811211", "name": "Thien Huu Nguyen"}]}, {"paperId": "576899ec8352a7fdfe5047fbe2949b7fe86feca6", "externalIds": {"DBLP": "conf/acl/TianJHL20", "ACL": "2021.acl-long.375", "DOI": "10.18653/v1/2021.acl-long.375", "CorpusId": 236459864}, "corpusId": 236459864, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/576899ec8352a7fdfe5047fbe2949b7fe86feca6", "title": "StereoRel: Relational Triple Extraction from a Stereoscopic Perspective", "abstract": "Relational triple extraction is critical to understanding massive text corpora and constructing large-scale knowledge graph, which has attracted increasing research interest. However, existing studies still face some challenging issues, including information loss, error propagation and ignoring the interaction between entity and relation. To intuitively explore the above issues and address them, in this paper, we provide a revealing insight into relational triple extraction from a stereoscopic perspective, which rationalizes the occurrence of these issues and exposes the shortcomings of existing methods. Further, a novel model is proposed for relational triple extraction, which maps relational triples to a three-dimension (3-D) space and leverages three decoders to extract them, aimed at simultaneously handling the above issues. A series of experiments are conducted on five public datasets, demonstrating that the proposed model outperforms the recent advanced baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.375.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4851-4861"}, "authors": [{"authorId": "2117125700", "name": "Xuetao Tian"}, {"authorId": "2113523247", "name": "Li Jing"}, {"authorId": "2148923324", "name": "Lu He"}, {"authorId": "2152943254", "name": "Feng Liu"}]}, {"paperId": "3bc0095232c098e737be3be5dd8110ab6b2936a9", "externalIds": {"DBLP": "conf/acl/CaoZ000CP20", "ACL": "2021.acl-long.376", "DOI": "10.18653/v1/2021.acl-long.376", "CorpusId": 236460024}, "corpusId": 236460024, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3bc0095232c098e737be3be5dd8110ab6b2936a9", "title": "Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks", "abstract": "Identifying causal relations of events is an important task in natural language processing area. However, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues. Existing methods cannot handle well the problem, especially in the condition of lacking training data. Nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge. Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task. Specifically, to make use of the descriptive knowledge, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge. To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning. Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 32, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.376.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4862-4872"}, "authors": [{"authorId": "49776272", "name": "Pengfei Cao"}, {"authorId": "87696608", "name": "Xinyu Zuo"}, {"authorId": "152829071", "name": "Yubo Chen"}, {"authorId": "77397868", "name": "Kang Liu"}, {"authorId": "11447228", "name": "Jun Zhao"}, {"authorId": "2145264600", "name": "Yuguang Chen"}, {"authorId": "49161576", "name": "Weihua Peng"}]}, {"paperId": "d7c0aff686894b370a13840890d42fe7992d6b95", "externalIds": {"DBLP": "conf/acl/QiYXLS20", "ACL": "2021.acl-long.377", "ArXiv": "2106.06361", "DOI": "10.18653/v1/2021.acl-long.377", "CorpusId": 235417102}, "corpusId": 235417102, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d7c0aff686894b370a13840890d42fe7992d6b95", "title": "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution", "abstract": "Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https://github.com/thunlp/BkdAtk-LWS.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 70, "influentialCitationCount": 20, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.377.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-11", "journal": {"pages": "4873-4883"}, "authors": [{"authorId": "51466208", "name": "Fanchao Qi"}, {"authorId": "1390925224", "name": "Yuan Yao"}, {"authorId": "2116855900", "name": "Sophia Xu"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "1753344", "name": "Maosong Sun"}]}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "externalIds": {"MAG": "3113057009", "DBLP": "journals/corr/abs-2012-07463", "ACL": "2021.acl-long.378", "ArXiv": "2012.07463", "DOI": "10.18653/v1/2021.acl-long.378", "CorpusId": 229152766}, "corpusId": 229152766, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning", "abstract": "The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. Diff pruning enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific \u201cdiff\u201d vector that extends the original pretrained parameters. This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task. Since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5% of the pretrained model\u2019s parameters per task and scales favorably in comparison to popular pruning approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 81, "citationCount": 181, "influentialCitationCount": 22, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.378.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-14", "journal": {"pages": "4884-4896"}, "authors": [{"authorId": "35578711", "name": "Demi Guo"}, {"authorId": "2531268", "name": "Alexander M. Rush"}, {"authorId": "152847918", "name": "Yoon Kim"}]}, {"paperId": "dab75d54f477c39529f2a88ae8ab26a1ea5bdb31", "externalIds": {"DBLP": "journals/corr/abs-2107-00967", "ACL": "2021.acl-long.379", "MAG": "3174906586", "ArXiv": "2107.00967", "DOI": "10.18653/v1/2021.acl-long.379", "CorpusId": 235727618}, "corpusId": 235727618, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dab75d54f477c39529f2a88ae8ab26a1ea5bdb31", "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling", "abstract": "Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.379.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-02", "journal": {"pages": "4897-4908"}, "authors": [{"authorId": "144219881", "name": "Xiang Hu"}, {"authorId": "2013337", "name": "Haitao Mi"}, {"authorId": "1515550185", "name": "Zujie Wen"}, {"authorId": "2115550906", "name": "Yafang Wang"}, {"authorId": "2118006538", "name": "Yi Su"}, {"authorId": "143851659", "name": "Jing Zheng"}, {"authorId": "144608002", "name": "Gerard de Melo"}]}, {"paperId": "961a9f9529a37484f216d5c4086d277c2fffa89e", "externalIds": {"ACL": "2021.acl-long.380", "DBLP": "conf/acl/HuJBWHHT20a", "DOI": "10.18653/v1/2021.acl-long.380", "CorpusId": 236459792}, "corpusId": 236459792, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/961a9f9529a37484f216d5c4086d277c2fffa89e", "title": "Risk Minimization for Zero-shot Sequence Labeling", "abstract": "Zero-shot sequence labeling aims to build a sequence labeler without human-annotated datasets. One straightforward approach is utilizing existing systems (source models) to generate pseudo-labeled datasets and train a target sequence labeler accordingly. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. In this paper, we propose a novel unified framework for zero-shot sequence labeling with minimum risk training and design a new decomposable risk function that models the relations between the predicted labels from the source models and the true labels. By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 47, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4909-4920"}, "authors": [{"authorId": "2125106901", "name": "Zechuan Hu"}, {"authorId": "50262192", "name": "Yong Jiang"}, {"authorId": "2997201", "name": "Nguyen Bach"}, {"authorId": null, "name": "Tao Wang"}, {"authorId": "2109670639", "name": "Zhongqiang Huang"}, {"authorId": "2117426607", "name": "Fei Huang"}, {"authorId": "40341553", "name": "Kewei Tu"}]}, {"paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e", "externalIds": {"DBLP": "conf/acl/HambardzumyanKM20", "ACL": "2021.acl-long.381", "ArXiv": "2101.00121", "DOI": "10.18653/v1/2021.acl-long.381", "CorpusId": 230437613}, "corpusId": 230437613, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e", "title": "WARP: Word-level Adversarial ReProgramming", "abstract": "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 214, "influentialCitationCount": 22, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.381.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-01", "journal": {"pages": "4921-4933"}, "authors": [{"authorId": "5641536", "name": "Karen Hambardzumyan"}, {"authorId": "1388829897", "name": "Hrant Khachatrian"}, {"authorId": "143823227", "name": "Jonathan May"}]}, {"paperId": "83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34", "externalIds": {"DBLP": "journals/corr/abs-2106-03993", "ACL": "2021.acl-long.382", "ArXiv": "2106.03993", "DOI": "10.18653/v1/2021.acl-long.382", "CorpusId": 235367771}, "corpusId": 235367771, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34", "title": "Lexicon Learning for Few Shot Sequence Modeling", "abstract": "Sequence-to-sequence transduction is the core problem in language processing applications as diverse as semantic parsing, machine translation, and instruction following. The neural network models that provide the dominant solution to these problems are brittle, especially in low-resource settings: they fail to generalize correctly or systematically from small datasets. Past work has shown that many failures of systematic generalization arise from neural models\u2019 inability to disentangle lexical phenomena from syntactic ones. To address this, we augment neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned, decontextualized, token-level translation rules. We describe how to initialize this mechanism using a variety of lexicon learning algorithms, and show that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science, formal semantics, and machine translation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 29, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "journal": {"pages": "4934-4946"}, "authors": [{"authorId": "1992708068", "name": "Ekin Aky\u00fcrek"}, {"authorId": "2112400", "name": "Jacob Andreas"}]}, {"paperId": "0c9a2adda11ed49d091948211fcfd517113b5243", "externalIds": {"DBLP": "conf/acl/LiZC20", "ACL": "2021.acl-long.383", "ArXiv": "2105.11601", "DOI": "10.18653/v1/2021.acl-long.383", "CorpusId": 235187032}, "corpusId": 235187032, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0c9a2adda11ed49d091948211fcfd517113b5243", "title": "Personalized Transformer for Explainable Recommendation", "abstract": "Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. In these tasks, user and item IDs are important identifiers for personalization. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer. Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendation-explanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 74, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.383.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-05-25", "journal": {"name": "ArXiv", "volume": "abs/2105.11601"}, "authors": [{"authorId": "2151529879", "name": "Lei Li"}, {"authorId": "1591136873", "name": "Yongfeng Zhang"}, {"authorId": "2146027242", "name": "Li Chen"}]}, {"paperId": "732a2f3532e41e05465e2b3a9b5eebc2ceb483c1", "externalIds": {"DBLP": "conf/acl/KrishnaKBL20", "ArXiv": "2005.01795", "ACL": "2021.acl-long.384", "DOI": "10.18653/v1/2021.acl-long.384", "CorpusId": 235303695}, "corpusId": 235303695, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/732a2f3532e41e05465e2b3a9b5eebc2ceb483c1", "title": "Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques", "abstract": "Following each patient visit, physicians draft long semi-structured clinical summaries called SOAP notes. While invaluable to clinicians and researchers, creating digital SOAP notes is burdensome, contributing to physician burnout. In this paper, we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients. After exploring a spectrum of methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an algorithm that (i) extracts important utterances relevant to each summary section; (ii) clusters together related utterances; and then (iii) generates one summary sentence per cluster. Cluster2Sent outperforms its purely abstractive counterpart by 8 ROUGE-1 points, and produces significantly more factual and coherent sentences as assessed by expert human evaluators. For reproducibility, we demonstrate similar benefits on the publicly available AMI dataset. Our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 57, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.384.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"pages": "4958-4972"}, "authors": [{"authorId": "38716503", "name": "Kundan Krishna"}, {"authorId": "10258164", "name": "Sopan Khosla"}, {"authorId": "1744846", "name": "Jeffrey P. Bigham"}, {"authorId": "2126415179", "name": "Z. Lipton"}]}, {"paperId": "133a5c9a15b734195d4ecb41c9f3fee01a8e8fd9", "externalIds": {"DBLP": "journals/corr/abs-2106-01609", "ACL": "2021.acl-long.385", "ArXiv": "2106.01609", "DOI": "10.18653/v1/2021.acl-long.385", "CorpusId": 235313435}, "corpusId": 235313435, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/133a5c9a15b734195d4ecb41c9f3fee01a8e8fd9", "title": "Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction", "abstract": "We investigate the problem of Chinese Grammatical Error Correction (CGEC) and present a new framework named Tail-to-Tail (TtT) non-autoregressive sequence prediction to address the deep issues hidden in CGEC. Considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a BERT-initialized Transformer Encoder as the backbone model to conduct information modeling and conveying. Considering that only relying on the same position substitution cannot handle the variable-length correction cases, various operations such substitution, deletion, insertion, and local paraphrasing are required jointly. Therefore, a Conditional Random Fields (CRF) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies. Since most tokens are correct and easily to be predicted/conveyed to the target, then the models may suffer from a severe class imbalance issue. To alleviate this problem, focal loss penalty strategies are integrated into the loss functions. Moreover, besides the typical fix-length error correction datasets, we also construct a variable-length corpus to conduct experiments. Experimental results on standard datasets, especially on the variable-length datasets, demonstrate the effectiveness of TtT in terms of sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and Correction.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 53, "citationCount": 21, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.385.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"pages": "4973-4984"}, "authors": [{"authorId": "2193560", "name": "Piji Li"}, {"authorId": "34720053", "name": "Shuming Shi"}]}, {"paperId": "fe9d8baf4dd0c5973f11094fbab95d34fd3a36fd", "externalIds": {"DBLP": "conf/acl/VogtLA20", "ACL": "2021.acl-long.386", "DOI": "10.18653/v1/2021.acl-long.386", "CorpusId": 236459947}, "corpusId": 236459947, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fe9d8baf4dd0c5973f11094fbab95d34fd3a36fd", "title": "Early Detection of Sexual Predators in Chats", "abstract": "An important risk that children face today is online grooming, where a so-called sexual predator establishes an emotional connection with a minor online with the objective of sexual abuse. Prior work has sought to automatically identify grooming chats, but only after an incidence has already happened in the context of legal prosecution. In this work, we instead investigate this problem from the point of view of prevention. We define and study the task of early sexual predator detection (eSPD) in chats, where the goal is to analyze a running chat from its beginning and predict grooming attempts as early and as accurately as possible. We survey existing datasets and their limitations regarding eSPD, and create a new dataset called PANC for more realistic evaluations. We present strong baselines built on BERT that also reach state-of-the-art results for conventional SPD. Finally, we consider coping with limited computational resources, as real-life applications require eSPD on mobile devices.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": null, "journal": {"pages": "4985-4999"}, "authors": [{"authorId": "2055420531", "name": "Matthias Vogt"}, {"authorId": "1693022", "name": "U. Leser"}, {"authorId": "2403712", "name": "A. Akbik"}]}, {"paperId": "84bab90970575f07eb2380453b51191f3aff6fcd", "externalIds": {"ACL": "2021.acl-long.387", "ArXiv": "2106.06471", "DBLP": "journals/corr/abs-2106-06471", "DOI": "10.18653/v1/2021.acl-long.387", "CorpusId": 235417555}, "corpusId": 235417555, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/84bab90970575f07eb2380453b51191f3aff6fcd", "title": "Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation", "abstract": "Medical report generation is one of the most challenging tasks in medical image analysis. Although existing approaches have achieved promising results, they either require a predefined template database in order to retrieve sentences or ignore the hierarchical nature of medical report generation. To address these issues, we propose MedWriter that incorporates a novel hierarchical retrieval mechanism to automatically extract both report and sentence-level templates for clinically accurate report generation. MedWriter first employs the Visual-Language Retrieval (VLR) module to retrieve the most relevant reports for the given images. To guarantee the logical coherence between generated sentences, the Language-Language Retrieval (LLR) module is introduced to retrieve relevant sentences based on the previous generated description. At last, a language decoder fuses image features and features from retrieved reports and sentences to generate meaningful medical reports. We verified the effectiveness of our model by automatic evaluation and human evaluation on two datasets, i.e., Open-I and MIMIC-CXR.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 19, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.387.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-25", "journal": {"pages": "5000-5009"}, "authors": [{"authorId": "48520402", "name": "Xingyi Yang"}, {"authorId": "1898157310", "name": "Muchao Ye"}, {"authorId": "36610242", "name": "Quanzeng You"}, {"authorId": "2988239", "name": "Fenglong Ma"}]}, {"paperId": "2fe7d9a0983b11fa817f14cb2da4855a6267a945", "externalIds": {"ACL": "2021.acl-long.388", "DBLP": "conf/acl/WangZLCZW20", "DOI": "10.18653/v1/2021.acl-long.388", "CorpusId": 236459950}, "corpusId": 236459950, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2fe7d9a0983b11fa817f14cb2da4855a6267a945", "title": "Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification", "abstract": "Hierarchical Text Classification (HTC) is a challenging task that categorizes a textual description within a taxonomic hierarchy. Most of the existing methods focus on modeling the text. Recently, researchers attempt to model the class representations with some resources (e.g., external dictionaries). However, the concept shared among classes which is a kind of domain-specific and fine-grained information has been ignored in previous work. In this paper, we propose a novel concept-based label embedding method that can explicitly represent the concept and model the sharing mechanism among classes for the hierarchical text classification. Experimental results on two widely used datasets prove that the proposed model outperforms several state-of-the-art methods. We release our complementary resources (concepts and definitions of classes) for these two datasets to benefit the research on HTC.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.388.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5010-5019"}, "authors": [{"authorId": "2121265994", "name": "Xuepeng Wang"}, {"authorId": "2157761808", "name": "Li Zhao"}, {"authorId": "2149123949", "name": "Bing Liu"}, {"authorId": "2118213443", "name": "Tao Chen"}, {"authorId": "1884418505", "name": "Feng Zhang"}, {"authorId": "2145384984", "name": "Di Wang"}]}, {"paperId": "30b21df1ae0c2b918e48435e739410e49db7638a", "externalIds": {"ArXiv": "2101.00265", "ACL": "2021.acl-long.389", "DBLP": "conf/acl/LuZL20", "DOI": "10.18653/v1/2021.acl-long.389", "CorpusId": 235165693}, "corpusId": 235165693, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/30b21df1ae0c2b918e48435e739410e49db7638a", "title": "VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words", "abstract": "Text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries. In this paper, we propose VisualSparta, a novel (Visual-text Sparse Transformer Matching) model that shows significant improvement in terms of both accuracy and efficiency. VisualSparta is capable of outperforming previous state-of-the-art scalable methods in MSCOCO and Flickr30K. We also show that it achieves substantial retrieving speed advantages, i.e., for a 1 million image index, VisualSparta using CPU gets ~391X speedup compared to CPU vector search and ~5.4X speedup compared to vector search with GPU acceleration. Experiments show that this speed advantage even gets bigger for larger datasets because VisualSparta can be efficiently implemented as an inverted index. To the best of our knowledge, VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets, with significant accuracy improvement compared to previous state-of-the-art methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 22, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.389.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-01", "journal": {"pages": "5020-5029"}, "authors": [{"authorId": "1500521596", "name": "Xiaopeng Lu"}, {"authorId": "8200875", "name": "Tiancheng Zhao"}, {"authorId": "2014351", "name": "Kyusong Lee"}]}, {"paperId": "65c2d2ffe45569101860a7defc7cccbd36b3602a", "externalIds": {"ACL": "2021.acl-long.390", "DBLP": "conf/acl/SunQLXZBLB20", "ArXiv": "2012.14862", "DOI": "10.18653/v1/2021.acl-long.390", "CorpusId": 235303662}, "corpusId": 235303662, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/65c2d2ffe45569101860a7defc7cccbd36b3602a", "title": "Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision", "abstract": "The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios. To democratize the benefits of Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains. Drawing on source-domain massive relevance supervision, MetaAdaptRank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic \u201cweak\u201d data based on their benefits to the target-domain ranking accuracy of Neu-IR models. Experiments on three TREC benchmarks in the web, news, and biomedical domains show that MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives from both its contrastive weak data synthesis and meta-reweighted data selection. The code and data of this paper can be obtained from https://github.com/thunlp/MetaAdaptRank.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 57, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.390.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-29", "journal": {"pages": "5030-5043"}, "authors": [{"authorId": "2109509345", "name": "Si Sun"}, {"authorId": "2043222587", "name": "Yingzhuo Qian"}, {"authorId": "2145312596", "name": "Zhenghao Liu"}, {"authorId": "144628574", "name": "Chenyan Xiong"}, {"authorId": "120097334", "name": "Kaitao Zhang"}, {"authorId": "143867128", "name": "Jie Bao"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "153335844", "name": "Paul Bennett"}]}, {"paperId": "0c049234515bfd17c642862af4cded5ba6d2fb82", "externalIds": {"DBLP": "conf/acl/LiLO20", "ACL": "2021.acl-long.391", "DOI": "10.18653/v1/2021.acl-long.391", "CorpusId": 236460055}, "corpusId": 236460055, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0c049234515bfd17c642862af4cded5ba6d2fb82", "title": "Semi-Supervised Text Classification with Balanced Deep Representation Distributions", "abstract": "Semi-Supervised Text Classification (SSTC) mainly works under the spirit of self-training. They initialize the deep classifier by training over labeled texts; and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts. Naturally, their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts. Unfortunately, they often suffer from low accuracy because of the margin bias problem caused by the large difference between representation distributions of labels in SSTC. To alleviate this problem, we apply the angular margin loss, and perform Gaussian linear transformation to achieve balanced label angle variances, i.e., the variance of label angles of texts within the same label. More accuracy of predicted pseudo-labels can be achieved by constraining all label angle variances balanced, where they are estimated over both labeled and pseudo-labeled texts during self-training loops. With this insight, we propose a novel SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S2TC-BDD). To evaluate S2TC-BDD, we compare it against the state-of-the-art SSTC methods. Empirical results demonstrate the effectiveness of S2TC-BDD, especially when the labeled texts are scarce.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 21, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.391.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5044-5053"}, "authors": [{"authorId": "2145399729", "name": "Changchun Li"}, {"authorId": "2108691206", "name": "Ximing Li"}, {"authorId": "3313180", "name": "Jihong Ouyang"}]}, {"paperId": "2a72251ed53ffe138b87d7c9c63b9dac65cdd65d", "externalIds": {"DBLP": "conf/acl/TangSJWZW20", "ACL": "2021.acl-long.392", "ArXiv": "2105.03599", "DOI": "10.18653/v1/2021.acl-long.392", "CorpusId": 234338073}, "corpusId": 234338073, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2a72251ed53ffe138b87d7c9c63b9dac65cdd65d", "title": "Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval", "abstract": "Recently, the retrieval models based on dense representations have been gradually applied in the first stage of the document retrieval tasks, showing better performance than traditional sparse vector space models. To obtain high efficiency, the basic structure of these models is Bi-encoder in most cases. However, this simple structure may cause serious information loss during the encoding of documents since the queries are agnostic. To address this problem, we design a method to mimic the queries to each of the documents by an iterative clustering process and represent the documents by multiple pseudo queries (i.e., the cluster centroids). To boost the retrieval process using approximate nearest neighbor search library, we also optimize the matching function with a two-step score calculation procedure. Experimental results on several popular ranking and QA datasets show that our model can achieve state-of-the-art results while still remaining high efficiency.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 27, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.392.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-08", "journal": {"name": "ArXiv", "volume": "abs/2105.03599"}, "authors": [{"authorId": "3449368", "name": "Hongyin Tang"}, {"authorId": "31215626", "name": "Xingwu Sun"}, {"authorId": "145982322", "name": "Beihong Jin"}, {"authorId": "2109593338", "name": "Jingang Wang"}, {"authorId": "2642200", "name": "Fuzheng Zhang"}, {"authorId": "2144356440", "name": "Wei Wu"}]}, {"paperId": "077c713bccd9d2c7fde68d4cbde06ab0f07a6855", "externalIds": {"ArXiv": "2105.11741", "DBLP": "journals/corr/abs-2105-11741", "ACL": "2021.acl-long.393", "DOI": "10.18653/v1/2021.acl-long.393", "CorpusId": 235187266}, "corpusId": 235187266, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/077c713bccd9d2c7fde68d4cbde06ab0f07a6855", "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer", "abstract": "Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 336, "influentialCitationCount": 53, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.393.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-25", "journal": {"name": "ArXiv", "volume": "abs/2105.11741"}, "authors": [{"authorId": "1500528818", "name": "Yuanmeng Yan"}, {"authorId": "2127717326", "name": "Rumei Li"}, {"authorId": "2592528", "name": "Sirui Wang"}, {"authorId": "2642200", "name": "Fuzheng Zhang"}, {"authorId": "2144356440", "name": "Wei Wu"}, {"authorId": "1753096", "name": "Weiran Xu"}]}, {"paperId": "3ca1430fb5bbf6ffa8f377f6b603648268f7546e", "externalIds": {"DBLP": "conf/acl/JiangZM00H0S20", "ACL": "2021.acl-long.394", "ArXiv": "2106.00261", "DOI": "10.18653/v1/2021.acl-long.394", "CorpusId": 235265899}, "corpusId": 235265899, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3ca1430fb5bbf6ffa8f377f6b603648268f7546e", "title": "Exploring Dynamic Selection of Branch Expansion Orders for Code Generation", "abstract": "Due to the great potential in facilitating software development, code generation has attracted increasing attention recently. Generally, dominant models are Seq2Tree models, which convert the input natural language description into a sequence of tree-construction actions corresponding to the pre-order traversal of an Abstract Syntax Tree (AST). However, such a traversal order may not be suitable for handling all multi-branch nodes. In this paper, we propose to equip the Seq2Tree model with a context-based Branch Selector, which is able to dynamically determine optimal expansion orders of branches for multi-branch nodes. Particularly, since the selection of expansion orders is a non-differentiable multi-step operation, we optimize the selector through reinforcement learning, and formulate the reward function as the difference of model losses obtained through different expansion orders. Experimental results and in-depth analysis on several commonly-used datasets demonstrate the effectiveness and generality of our approach. We have released our code at https://github.com/DeepLearnXMU/CG-RL.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.394.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"pages": "5076-5085"}, "authors": [{"authorId": "2156396323", "name": "Hui Jiang"}, {"authorId": "151481178", "name": "Chulun Zhou"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "2117829435", "name": "Biao Zhang"}, {"authorId": "48128428", "name": "Jie Zhou"}, {"authorId": "2610330", "name": "Degen Huang"}, {"authorId": "1920688", "name": "Qingqiang Wu"}, {"authorId": "34739384", "name": "Jinsong Su"}]}, {"paperId": "411a621e9822c2bd33e5022ee2627a9de5c4d238", "externalIds": {"ArXiv": "2106.02497", "DBLP": "conf/acl/PaulF20", "ACL": "2021.acl-long.395", "DOI": "10.18653/v1/2021.acl-long.395", "CorpusId": 235352659}, "corpusId": 235352659, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/411a621e9822c2bd33e5022ee2627a9de5c4d238", "title": "COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion", "abstract": "Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque. We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs. In this paper we present Coins, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and iii) uses them to guide task-specific output generation. We apply to a Narrative Story Completion task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies. By modularizing inference and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent. Our automatic and manual evaluations show that the model generates better story sentences than SOTA baselines, especially in terms of coherence. We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules. The recursive nature of holds the potential for controlled generation of longer sequences.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 48, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.395.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02497"}, "authors": [{"authorId": "2261760962", "name": "Debjit Paul"}, {"authorId": "143876555", "name": "A. Frank"}]}, {"paperId": "ebd088efdf62348fe4b6b353e3c84dec93bee54f", "externalIds": {"DBLP": "conf/acl/HuangGPLJ20", "ACL": "2021.acl-long.396", "DOI": "10.18653/v1/2021.acl-long.396", "CorpusId": 236459927}, "corpusId": 236459927, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ebd088efdf62348fe4b6b353e3c84dec93bee54f", "title": "Reasoning over Entity-Action-Location Graph for Procedural Text Understanding", "abstract": "Procedural text understanding aims at tracking the states (e.g., create, move, destroy) and locations of the entities mentioned in a given paragraph. To effectively track the states and locations, it is essential to capture the rich semantic relations between entities, actions, and locations in the paragraph. Although recent works have achieved substantial progress, most of them focus on leveraging the inherent constraints or incorporating external knowledge for state prediction. The rich semantic relations in the given paragraph are largely overlooked. In this paper, we propose a novel approach (REAL) to procedural text understanding, where we build a general framework to systematically model the entity-entity, entity-action, and entity-location relations using a graph neural network. We further develop algorithms for graph construction, representation learning, and state and location tracking. We evaluate the proposed approach on two benchmark datasets, ProPara, and Recipes. The experimental results show that our method outperforms strong baselines by a large margin, i.e., 5.0% on ProPara and 3.2% on Recipes, illustrating the utility of semantic relations and the effectiveness of the graph-based reasoning model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 10, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5100-5109"}, "authors": [{"authorId": "2143568814", "name": "Hao Huang"}, {"authorId": "2442662", "name": "Xiubo Geng"}, {"authorId": "145525190", "name": "J. Pei"}, {"authorId": "2062835", "name": "Guodong Long"}, {"authorId": "71790825", "name": "Daxin Jiang"}]}, {"paperId": "f8a7aa9c556703d27dd5bf98265de1ff89175635", "externalIds": {"ArXiv": "2106.06228", "DBLP": "journals/corr/abs-2106-06228", "ACL": "2021.acl-long.397", "DOI": "10.18653/v1/2021.acl-long.397", "CorpusId": 235417352}, "corpusId": 235417352, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f8a7aa9c556703d27dd5bf98265de1ff89175635", "title": "From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding", "abstract": "Semantic parsing is challenging due to the structure gap and the semantic gap between utterances and logical forms. In this paper, we propose an unsupervised semantic parsing method - Synchronous Semantic Decoding (SSD), which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained decoding. Specifically, we reformulate semantic parsing as a constrained paraphrasing problem: given an utterance, our model synchronously generates its canonical utterancel and meaning representation. During synchronously decoding: the utterance paraphrasing is constrained by the structure of the logical form, therefore the canonical utterance can be paraphrased controlledly; the semantic decoding is guided by the semantics of the canonical utterance, therefore its logical form can be generated unsupervisedly. Experimental results show that SSD is a promising approach and can achieve state-of-the-art unsupervised semantic parsing performance on multiple datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 61, "citationCount": 19, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.397.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-11", "journal": {"name": "ArXiv", "volume": "abs/2106.06228"}, "authors": [{"authorId": "2112539590", "name": "Shan Wu"}, {"authorId": "49864767", "name": "Bo Chen"}, {"authorId": "2111682010", "name": "Chunlei Xin"}, {"authorId": "2118233348", "name": "Xianpei Han"}, {"authorId": "2110832778", "name": "Le Sun"}, {"authorId": "2108009179", "name": "Weipeng Zhang"}, {"authorId": "2115522794", "name": "Jiansong Chen"}, {"authorId": "2158028618", "name": "Fan Yang"}, {"authorId": "2111683220", "name": "Xunliang Cai"}]}, {"paperId": "ccd42cb2b846336f93a79d93a162c3dc936f6db2", "externalIds": {"ACL": "2021.acl-long.398", "DBLP": "journals/corr/abs-2105-14478", "ArXiv": "2105.14478", "DOI": "10.18653/v1/2021.acl-long.398", "CorpusId": 235253844}, "corpusId": 235253844, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ccd42cb2b846336f93a79d93a162c3dc936f6db2", "title": "Pre-training Universal Language Representation", "abstract": "Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.398.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-30", "journal": {"name": "ArXiv", "volume": "abs/2105.14478"}, "authors": [{"authorId": "2110439273", "name": "Yian Li"}, {"authorId": "153716230", "name": "Hai Zhao"}]}, {"paperId": "a03844a1cb957feae7ded3a327cd3a445e2175ad", "externalIds": {"MAG": "3174881085", "DBLP": "journals/corr/abs-2105-10956", "ArXiv": "2105.10956", "ACL": "2021.acl-long.399", "DOI": "10.18653/v1/2021.acl-long.399", "CorpusId": 235166449}, "corpusId": 235166449, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a03844a1cb957feae7ded3a327cd3a445e2175ad", "title": "Structural Pre-training for Dialogue Comprehension", "abstract": "Pre-trained language models (PrLMs) have demonstrated superior performance due to their strong ability to learn universal language representations from self-supervised pre-training. However, even with the help of the powerful PrLMs, it is still challenging to effectively capture task-related knowledge from dialogue texts which are enriched by correlations among speaker-aware utterances. In this work, we present SPIDER, Structural Pre-traIned DialoguE Reader, to capture dialogue exclusive features. To simulate the dialogue-like features, we propose two training objectives in addition to the original LM objectives: 1) utterance order restoration, which predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization, which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets. Experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 64, "citationCount": 27, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.399.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-23", "journal": {"name": "ArXiv", "volume": "abs/2105.10956"}, "authors": [{"authorId": "3322871", "name": "Zhuosheng Zhang"}, {"authorId": "153716230", "name": "Hai Zhao"}]}, {"paperId": "ef18db2a18ac61e72783a613328842ce86ef00bf", "externalIds": {"DBLP": "journals/corr/abs-2107-13686", "ArXiv": "2107.13686", "ACL": "2021.acl-long.400", "DOI": "10.18653/v1/2021.acl-long.400", "CorpusId": 236460034}, "corpusId": 236460034, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ef18db2a18ac61e72783a613328842ce86ef00bf", "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models", "abstract": "Pre-trained language models (PLMs) have achieved great success in natural language processing. Most of PLMs follow the default setting of architecture hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks) in BERT. Few studies have been conducted to explore the design of architecture hyper-parameters in BERT, especially for the more efficient PLMs with tiny sizes, which are essential for practical deployment on resource-constrained devices. In this paper, we adopt the one-shot Neural Architecture Search (NAS) to automatically search architecture hyper-parameters. Specifically, we carefully design the techniques of one-shot learning and the search space to provide an adaptive and efficient development way of tiny PLMs for various latency constraints. We name our method AutoTinyBERT and evaluate its effectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show that our method outperforms both the SOTA search-based baseline (NAS-BERT) and the SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM, and MobileBERT). In addition, based on the obtained architectures, we propose a more efficient development method that is even faster than the development of a single PLM. The source code and models will be publicly available upon publication.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 35, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.400.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-29", "journal": {"pages": "5146-5157"}, "authors": [{"authorId": "1384668226", "name": "Yichun Yin"}, {"authorId": "2145775089", "name": "Cheng Chen"}, {"authorId": "50812138", "name": "Lifeng Shang"}, {"authorId": "2110310493", "name": "Xin Jiang"}, {"authorId": "2117025507", "name": "Xiao Chen"}, {"authorId": "30738758", "name": "Qun Liu"}]}, {"paperId": "0ce24e44400f91d86ba337377a5093d5ff8923c8", "externalIds": {"DBLP": "conf/acl/0010ZFXM20", "ACL": "2021.acl-long.401", "DOI": "10.18653/v1/2021.acl-long.401", "CorpusId": 236460170}, "corpusId": 236460170, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0ce24e44400f91d86ba337377a5093d5ff8923c8", "title": "Data Augmentation with Adversarial Training for Cross-Lingual NLI", "abstract": "Due to recent pretrained multilingual representation models, it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages. In practice, however, we still face the problem of scarce labeled data, leading to subpar results. In this paper, we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way. To this end, we propose two methods of training a generative model to induce synthesized examples, and then leverage the resulting data using an adversarial training regimen for more robustness. In a series of detailed experiments, we show that this fruitful combination leads to substantial gains in cross-lingual inference.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.401.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5158-5167"}, "authors": [{"authorId": "2031471015", "name": "Xin Dong"}, {"authorId": "2153095256", "name": "Yaxin Zhu"}, {"authorId": "2011378", "name": "Zuohui Fu"}, {"authorId": "2116459424", "name": "Dongkuan Xu"}, {"authorId": "144608002", "name": "Gerard de Melo"}]}, {"paperId": "a08fcd1b32ed7f6f0a7ad5500a4871516d175b7a", "externalIds": {"DBLP": "conf/acl/0004HLB020", "ACL": "2021.acl-long.402", "DOI": "10.18653/v1/2021.acl-long.402", "CorpusId": 236460287}, "corpusId": 236460287, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a08fcd1b32ed7f6f0a7ad5500a4871516d175b7a", "title": "Bootstrapped Unsupervised Sentence Representation Learning", "abstract": "As high-quality labeled data is scarce, unsupervised sentence representation learning has attracted much attention. In this paper, we propose a new framework with a two-branch Siamese Network which maximizes the similarity between two augmented views of each sentence. Specifically, given one augmented view of the input sentence, the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view. Meanwhile, the target network branch is bootstrapped with a moving average of the online network. The proposed method significantly outperforms other state-of-the-art unsupervised methods on semantic textual similarity (STS) and classification tasks. It can be adopted as a post-training procedure to boost the performance of the supervised methods. We further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual STS tasks. Our code is available at https://github.com/yanzhangnlp/BSL.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 25, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5168-5180"}, "authors": [{"authorId": "39831806", "name": "Yan Zhang"}, {"authorId": "22272507", "name": "Ruidan He"}, {"authorId": "2390951", "name": "Zuozhu Liu"}, {"authorId": "1996394", "name": "Lidong Bing"}, {"authorId": "2108493009", "name": "Haizhou Li"}]}, {"paperId": "ee4385aaae742a14a5db828a5076208c9b33da88", "externalIds": {"DBLP": "conf/acl/DuD0Q20", "ACL": "2021.acl-long.403", "DOI": "10.18653/v1/2021.acl-long.403", "CorpusId": 236459954}, "corpusId": 236459954, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ee4385aaae742a14a5db828a5076208c9b33da88", "title": "Learning Event Graph Knowledge for Abductive Reasoning", "abstract": "Abductive reasoning aims at inferring the most plausible explanation for observed events, which would play critical roles in various NLP applications, such as reading comprehension and question answering. To facilitate this task, a narrative text based abductive reasoning task \\alphaNLI is proposed, together with explorations about building reasoning framework using pretrained language models. However, abundant event commonsense knowledge is not well exploited for this task. To fill this gap, we propose a variational autoencoder based model ege-RoBERTa, which employs a latent variable to capture the necessary commonsense knowledge from event graph for guiding the abductive reasoning task. Experimental results show that through learning the external event graph knowledge, our approach outperforms the baseline methods on the \\alphaNLI task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.403.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5181-5190"}, "authors": [{"authorId": "49134163", "name": "L. Du"}, {"authorId": "145411800", "name": "Xiao Ding"}, {"authorId": "40282288", "name": "Ting Liu"}, {"authorId": "152277111", "name": "Bing Qin"}]}, {"paperId": "06994c2810c2720b302153fc73f8c4459f05fda7", "externalIds": {"ArXiv": "2105.07144", "DBLP": "conf/acl/WeiMC20", "ACL": "2021.acl-long.404", "DOI": "10.18653/v1/2021.acl-long.404", "CorpusId": 234741913}, "corpusId": 234741913, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/06994c2810c2720b302153fc73f8c4459f05fda7", "title": "A Cognitive Regularizer for Language Modeling", "abstract": "The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 51, "citationCount": 16, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.404.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-15", "journal": {"name": "ArXiv", "volume": "abs/2105.07144"}, "authors": [{"authorId": "119640649", "name": "Jason Wei"}, {"authorId": "150953620", "name": "Clara Meister"}, {"authorId": "2070989574", "name": "Ryan Cotterell"}]}, {"paperId": "aee7fd72f33bc8060ff32eb88f88b904802a9243", "externalIds": {"ArXiv": "2106.01229", "ACL": "2021.acl-long.405", "DBLP": "journals/corr/abs-2106-01229", "DOI": "10.18653/v1/2021.acl-long.405", "CorpusId": 235293738}, "corpusId": 235293738, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/aee7fd72f33bc8060ff32eb88f88b904802a9243", "title": "Lower Perplexity is Not Always Human-Like", "abstract": "In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization \u2014the lower perplexity a language model has, the more human-like the language model is\u2014 in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 34, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.405.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01229"}, "authors": [{"authorId": "83446147", "name": "Tatsuki Kuribayashi"}, {"authorId": "50856622", "name": "Yohei Oseki"}, {"authorId": "2116240662", "name": "Takumi Ito"}, {"authorId": "49807045", "name": "Ryo Yoshida"}, {"authorId": "1749558", "name": "Masayuki Asahara"}, {"authorId": "3040648", "name": "Kentaro Inui"}]}, {"paperId": "c0e3964e451a75b4bcf4defb5324ca91f54c4b83", "externalIds": {"ACL": "2021.acl-long.406", "DBLP": "conf/acl/WangW20", "DOI": "10.18653/v1/2021.acl-long.406", "CorpusId": 236460274}, "corpusId": 236460274, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c0e3964e451a75b4bcf4defb5324ca91f54c4b83", "title": "Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives", "abstract": "Lately proposed Word Sense Disambiguation (WSD) systems have approached the estimated upper bound of the task on standard evaluation benchmarks. However, these systems typically implement the disambiguation of words in a document almost independently, underutilizing sense and word dependency in context. In this paper, we convert the nearly isolated decisions into interrelated ones by exposing senses in context when learning sense embeddings in a similarity-based Sense Aware Context Exploitation (SACE) architecture. Meanwhile, we enhance the context embedding learning with selected sentences from the same document, rather than utilizing only the sentence where each ambiguous word appears. Experiments on both English and multilingual WSD datasets have shown the effectiveness of our approach, surpassing previous state-of-the-art by large margins (3.7% and 1.2% respectively), especially on few-shot (14.3%) and zero-shot (35.9%) scenarios.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 11, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.406.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5218-5229"}, "authors": [{"authorId": "2115448199", "name": "Ming Wang"}, {"authorId": "2143404491", "name": "Yinglin Wang"}]}, {"paperId": "169082d38acf2af7efd6f76a320f6f5e2e0f799f", "externalIds": {"ACL": "2021.acl-long.407", "MAG": "3175396083", "DBLP": "conf/acl/SuLLPZCH20", "DOI": "10.18653/v1/2021.acl-long.407", "CorpusId": 236460100}, "corpusId": 236460100, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/169082d38acf2af7efd6f76a320f6f5e2e0f799f", "title": "A Knowledge-Guided Framework for Frame Identification", "abstract": "Frame Identification (FI) is a fundamental and challenging task in frame semantic parsing. The task aims to find the exact frame evoked by a target word in a given sentence. It is generally regarded as a classification task in existing work, where frames are treated as discrete labels or represented using onehot embeddings. However, the valuable knowledge about frames is neglected. In this paper, we propose a Knowledge-Guided Frame Identification framework (KGFI) that integrates three types frame knowledge, including frame definitions, frame elements and frame-to-frame relations, to learn better frame representation, which guides the KGFI to jointly map target words and frames into the same embedding space and subsequently identify the best frame by calculating the dot-product similarity scores between the target word embedding and all of the frame embeddings. The extensive experimental results demonstrate KGFI significantly outperforms the state-of-the-art methods on two benchmark datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 12, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.407.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "5230-5240"}, "authors": [{"authorId": "2146500971", "name": "Xuefeng Su"}, {"authorId": "2109380034", "name": "Ru Li"}, {"authorId": "2127399967", "name": "Xiaoli Li"}, {"authorId": "9416872", "name": "Jeff Z. Pan"}, {"authorId": "2110890052", "name": "Hu Zhang"}, {"authorId": "2204251", "name": "Qinghua Chai"}, {"authorId": "21509758", "name": "Xiaoqi Han"}]}, {"paperId": "440a7cd665293cfb149a7434668fb3079e43e2aa", "externalIds": {"ArXiv": "2106.04302", "ACL": "2021.acl-long.408", "DBLP": "conf/acl/GuptaJ20", "DOI": "10.18653/v1/2021.acl-long.408", "CorpusId": 235367716}, "corpusId": 235367716, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/440a7cd665293cfb149a7434668fb3079e43e2aa", "title": "Obtaining Better Static Word Embeddings Using Contextual Embedding Models", "abstract": "The advent of contextual word embeddings \u2014 representations of words which incorporate semantic and syntactic information from their context\u2014has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 57, "citationCount": 23, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.408.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"pages": "5241-5253"}, "authors": [{"authorId": "2119997844", "name": "Prakhar Gupta"}, {"authorId": "2456863", "name": "Martin Jaggi"}]}, {"paperId": "74592257b9812f7d3a4a8a1cec31d6a1fd174c2f", "externalIds": {"ArXiv": "2106.02960", "DBLP": "conf/acl/DuHZSS20", "ACL": "2021.acl-long.409", "DOI": "10.18653/v1/2021.acl-long.409", "CorpusId": 235358348}, "corpusId": 235358348, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/74592257b9812f7d3a4a8a1cec31d6a1fd174c2f", "title": "Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation", "abstract": "A critical challenge faced by supervised word sense disambiguation (WSD) is the lack of large annotated datasets with sufficient coverage of words in their diversity of senses. This inspired recent research on few-shot WSD using meta-learning. While such work has successfully applied meta-learning to learn new word senses from very few examples, its performance still lags behind its fully-supervised counterpart. Aiming to further close this gap, we propose a model of semantic memory for WSD in a meta-learning setting. Semantic memory encapsulates prior experiences seen throughout the lifetime of the model, which aids better generalization in limited data settings. Our model is based on hierarchical variational inference and incorporates an adaptive memory update rule via a hypernetwork. We show our model advances the state of the art in few-shot WSD, supports effective learning in extremely data scarce (e.g. one-shot) scenarios and produces meaning prototypes that capture similar senses of distinct words.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 78, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.409.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-05", "journal": {"name": "ArXiv", "volume": "abs/2106.02960"}, "authors": [{"authorId": "46941376", "name": "Yingjun Du"}, {"authorId": "1661217828", "name": "Nithin Holla"}, {"authorId": "34798935", "name": "Xiantong Zhen"}, {"authorId": "145404204", "name": "Cees G. M. Snoek"}, {"authorId": "2362276", "name": "Ekaterina Shutova"}]}, {"paperId": "9d31cfe09f93a5d87657313c503b17619c2ae107", "externalIds": {"DBLP": "conf/acl/VulicPKG20", "ACL": "2021.acl-long.410", "DOI": "10.18653/v1/2021.acl-long.410", "CorpusId": 236460128}, "corpusId": 236460128, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9d31cfe09f93a5d87657313c503b17619c2ae107", "title": "LexFit: Lexical Fine-Tuning of Pretrained Language Models", "abstract": "Transformer-based language models (LMs) pretrained on large text collections implicitly store a wealth of lexical semantic knowledge, but it is non-trivial to extract that knowledge effectively from their parameters. Inspired by prior work on semantic specialization of static word embedding (WE) models, we show that it is possible to expose and enrich lexical knowledge from the LMs, that is, to specialize them to serve as effective and universal \u201cdecontextualized\u201d word encoders even when fed input words \u201cin isolation\u201d (i.e., without any context). Their transformation into such word encoders is achieved through a simple and efficient lexical fine-tuning procedure (termed LexFit) based on dual-encoder network structures. Further, we show that LexFit can yield effective word encoders even with limited lexical supervision and, via cross-lingual transfer, in different languages without any readily available external knowledge. Our evaluation over four established, structurally different lexical-level tasks in 8 languages indicates the superiority of LexFit-based WEs over standard static WEs (e.g., fastText) and WEs from vanilla LMs. Other extensive experiments and ablation studies further profile the LexFit framework, and indicate best practices and performance variations across LexFit variants, languages, and lexical tasks, also directly questioning the usefulness of traditional WE models in the era of large neural models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 75, "citationCount": 19, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5269-5283"}, "authors": [{"authorId": "1747849", "name": "Ivan Vulic"}, {"authorId": "2064995567", "name": "E. M. Ponti"}, {"authorId": "145762466", "name": "A. Korhonen"}, {"authorId": "2472657", "name": "Goran Glavas"}]}, {"paperId": "5df12460d9a742b08f82e8b79cb102a8be5dd9b4", "externalIds": {"DBLP": "conf/acl/HsuHMSG20", "ACL": "2021.acl-long.411", "ArXiv": "2012.15454", "DOI": "10.18653/v1/2021.acl-long.411", "CorpusId": 229923207}, "corpusId": 229923207, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5df12460d9a742b08f82e8b79cb102a8be5dd9b4", "title": "Text-Free Image-to-Speech Synthesis Using Learned Segmental Units", "abstract": "In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 122, "citationCount": 46, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.411.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"pages": "5284-5300"}, "authors": [{"authorId": "2957796", "name": "Wei-Ning Hsu"}, {"authorId": "30507748", "name": "David F. Harwath"}, {"authorId": "2116333524", "name": "Christopher Song"}, {"authorId": "145898106", "name": "James R. Glass"}]}, {"paperId": "e42cbd2dd928367da98b4fa056de2b9b29e39ed8", "externalIds": {"ACL": "2021.acl-long.412", "DBLP": "conf/acl/TangLJCZK20", "DOI": "10.18653/v1/2021.acl-long.412", "CorpusId": 236460152}, "corpusId": 236460152, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e42cbd2dd928367da98b4fa056de2b9b29e39ed8", "title": "CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network", "abstract": "Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities. The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure. However, the existing techniques require all modalities as input, thus are sensitive to missing modalities at predicting time. In this work, the coupled-translation fusion network (CTFN) is firstly proposed to model bi-direction interplay via couple learning, ensuring the robustness in respect to missing modalities. Specifically, the cyclic consistency constraint is presented to improve the translation performance, allowing us directly to discard decoder and only embraces encoder of Transformer. This could contribute to a much lighter model. Due to the couple learning, CTFN is able to conduct bi-direction cross-modality intercorrelation parallelly. Based on CTFN, a hierarchical architecture is further established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods. Moreover, the convolution block is utilized to further highlight explicit interactions among those translations. For evaluation, CTFN was verified on two multimodal benchmarks with extensive ablation studies. The experiments demonstrate that the proposed framework achieves state-of-the-art or often competitive performance. Additionally, CTFN still maintains robustness when considering missing modality.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 24, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.412.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5301-5311"}, "authors": [{"authorId": "144792078", "name": "Jiajia Tang"}, {"authorId": "2154305523", "name": "Kang Li"}, {"authorId": "67084597", "name": "Xuanyu Jin"}, {"authorId": "145683892", "name": "A. Cichocki"}, {"authorId": "144251712", "name": "Qibin Zhao"}, {"authorId": "1915470", "name": "Wanzeng Kong"}]}, {"paperId": "829580d6fc73fa601c4982e2b1b6832f2796270b", "externalIds": {"DBLP": "conf/acl/LuoKM20", "ACL": "2021.acl-long.413", "ArXiv": "2011.04393", "DOI": "10.18653/v1/2021.acl-long.413", "CorpusId": 235187469}, "corpusId": 235187469, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/829580d6fc73fa601c4982e2b1b6832f2796270b", "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings", "abstract": "In this work, we demonstrate that the contextualized word vectors derived from pretrained masked language model-based encoders share a common, perhaps undesirable pattern across layers. Namely, we find cases of persistent outlier neurons within BERT and RoBERTa\u2019s hidden state vectors that consistently bear the smallest or largest values in said vectors. In an attempt to investigate the source of this information, we introduce a neuron-level analysis method, which reveals that the outliers are closely related to information captured by positional embeddings. We also pre-train the RoBERTa-base models from scratch and find that the outliers disappear without using positional embeddings. These outliers, we find, are the major cause of anisotropy of encoders\u2019 raw vector spaces, and clipping them leads to increased similarity across vectors. We demonstrate this in practice by showing that clipped vectors can more accurately distinguish word senses, as well as lead to better sentence embeddings when mean pooling. In three supervised tasks, we find that clipping does not affect the performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 58, "citationCount": 26, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.413.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-09", "journal": {"pages": "5312-5327"}, "authors": [{"authorId": "23523733", "name": "Ziyang Luo"}, {"authorId": "25061910", "name": "Artur Kulmizev"}, {"authorId": "29422474", "name": "Xiaoxi Mao"}]}, {"paperId": "1fa487376af5d4293ec482d193ca1790176c4dbc", "externalIds": {"DBLP": "journals/corr/abs-2106-00085", "ArXiv": "2106.00085", "ACL": "2021.acl-long.414", "DOI": "10.18653/v1/2021.acl-long.414", "CorpusId": 235265909}, "corpusId": 235265909, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1fa487376af5d4293ec482d193ca1790176c4dbc", "title": "Language Model Evaluation Beyond Perplexity", "abstract": "We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework\u2013paired with significance tests\u2013for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type\u2013token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 39, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.414.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "5328-5339"}, "authors": [{"authorId": "150953620", "name": "Clara Meister"}, {"authorId": "2070989574", "name": "Ryan Cotterell"}]}, {"paperId": "0ea9df70ae9e4c9c4e99e0e53046eb041c18f2cf", "externalIds": {"DBLP": "conf/acl/SituZPMH20", "ACL": "2021.acl-long.415", "DOI": "10.18653/v1/2021.acl-long.415", "CorpusId": 236459859}, "corpusId": 236459859, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0ea9df70ae9e4c9c4e99e0e53046eb041c18f2cf", "title": "Learning to Explain: Generating Stable Explanations Fast", "abstract": "The importance of explaining the outcome of a machine learning model, especially a black-box model, is widely acknowledged. Recent approaches explain an outcome by identifying the contributions of input features to this outcome. In environments involving large black-box models or complex inputs, this leads to computationally demanding algorithms. Further, these algorithms often suffer from low stability, with explanations varying significantly across similar examples. In this paper, we propose a Learning to Explain (L2E) approach that learns the behaviour of an underlying explanation algorithm simultaneously from all training examples. Once the explanation algorithm is distilled into an explainer network, it can be used to explain new instances. Our experiments on three classification tasks, which compare our approach to six explanation algorithms, show that L2E is between 5 and 7.5\u00d710\u02c64 times faster than these algorithms, while generating more stable explanations, and having comparable faithfulness to the black-box model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 61, "citationCount": 20, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.415.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5340-5355"}, "authors": [{"authorId": "2121286471", "name": "Xuelin Situ"}, {"authorId": "48127057", "name": "Ingrid Zukerman"}, {"authorId": "2063030370", "name": "C\u00e9cile Paris"}, {"authorId": "1933516", "name": "Sameen Maruf"}, {"authorId": "2561045", "name": "Gholamreza Haffari"}]}, {"paperId": "babeda48b10a4d638252118f2238d05a06f4ec55", "externalIds": {"ACL": "2021.acl-long.416", "DBLP": "journals/corr/abs-2004-09456", "MAG": "3019416653", "ArXiv": "2004.09456", "DOI": "10.18653/v1/2021.acl-long.416", "CorpusId": 215828184}, "corpusId": 215828184, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/babeda48b10a4d638252118f2238d05a06f4ec55", "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "abstract": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 472, "influentialCitationCount": 91, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.416.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-20", "journal": {"pages": "5356-5371"}, "authors": [{"authorId": "50411022", "name": "Moin Nadeem"}, {"authorId": "78850252", "name": "Anna Bethke"}, {"authorId": "145732771", "name": "Siva Reddy"}]}, {"paperId": "648a5496a4be5690adaa888b04469de4799fff4f", "externalIds": {"DBLP": "conf/acl/JiangZY0020", "ACL": "2021.acl-long.417", "DOI": "10.18653/v1/2021.acl-long.417", "CorpusId": 236460314}, "corpusId": 236460314, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/648a5496a4be5690adaa888b04469de4799fff4f", "title": "Alignment Rationale for Natural Language Inference", "abstract": "Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors. Existing explanation methods usually pick prominent features such as words or phrases from the input text. However, for NLI, alignments among words or phrases are more enlightening clues to explain the model. To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI. The explanation is based on feature selection, which keeps few but sufficient alignments while maintaining the same prediction of the target model. Experimental results show that our method is more faithful and human-readable compared with many existing approaches. We further study and re-evaluate three typical models through our explanation beyond accuracy, and propose a simple method that greatly improves the model robustness.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 63, "citationCount": 12, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.417.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5372-5387"}, "authors": [{"authorId": "97234013", "name": "Zhongtao Jiang"}, {"authorId": "2145784135", "name": "Yuanzhe Zhang"}, {"authorId": "2155029396", "name": "Zhao Yang"}, {"authorId": "11447228", "name": "Jun Zhao"}, {"authorId": "77397868", "name": "Kang Liu"}]}, {"paperId": "e89804a4d2611450893a001c5ab5dcf8b5ad2b3a", "externalIds": {"DBLP": "journals/corr/abs-2106-02205", "ACL": "2021.acl-long.418", "ArXiv": "2106.02205", "DOI": "10.18653/v1/2021.acl-long.418", "CorpusId": 235352554}, "corpusId": 235352554, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e89804a4d2611450893a001c5ab5dcf8b5ad2b3a", "title": "Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators", "abstract": "This paper presents a novel pre-trained language models (PLM) compression approach based on the matrix product operator (short as MPO) from quantum many-body physics. It can decompose an original matrix into central tensors (containing the core information) and auxiliary tensors (with only a small proportion of parameters). With the decomposed MPO structure, we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors, and design an optimization algorithm for MPO-based approximation over stacked network architectures. Our approach can be applied to the original or the compressed PLMs in a general way, which derives a lighter network and significantly reduces the parameters to be fine-tuned. Extensive experiments have demonstrated the effectiveness of the proposed approach in model compression, especially the reduction in fine-tuning parameters (91% reduction on average). The code to reproduce the results of this paper can be found at https://github.com/RUCAIBox/MPOP.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 47, "citationCount": 12, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.418.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Physics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Physics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02205"}, "authors": [{"authorId": "2108129670", "name": "Peiyu Liu"}, {"authorId": "9136116", "name": "Ze-Feng Gao"}, {"authorId": "2542603", "name": "Wayne Xin Zhao"}, {"authorId": "2114113158", "name": "Z. Xie"}, {"authorId": "48462104", "name": "Zhong-Yi Lu"}, {"authorId": "153693432", "name": "Ji-rong Wen"}]}, {"paperId": "edd3a60cb7295af7d845f0be4ea62de6ef699602", "externalIds": {"DBLP": "conf/acl/ZhangHZYCZ20", "ArXiv": "2106.04753", "ACL": "2021.acl-long.419", "DOI": "10.18653/v1/2021.acl-long.419", "CorpusId": 236460242}, "corpusId": 236460242, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/edd3a60cb7295af7d845f0be4ea62de6ef699602", "title": "On Sample Based Explanation Methods for NLP: Faithfulness, Efficiency and Semantic Evaluation", "abstract": "In the recent advances of natural language processing, the scale of the state-of-the-art models and datasets is usually extensive, which challenges the application of sample-based explanation methods in many aspects, such as explanation interpretability, efficiency, and faithfulness. In this work, for the first time, we can improve the interpretability of explanations by allowing arbitrary text sequences as the explanation unit. On top of this, we implement a hessian-free method with a model faithfulness guarantee. Finally, to compare our method with the others, we propose a semantic-based evaluation metric that can better align with humans\u2019 judgment of explanations than the widely adopted diagnostic or re-training measures. The empirical results on multiple real data sets demonstrate the proposed method\u2019s superior performance to popular explanation techniques such as Influence Function or TracIn on semantic evaluation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-09", "journal": {"name": "ArXiv", "volume": "abs/2106.04753"}, "authors": [{"authorId": "31765881", "name": "Wei Zhang"}, {"authorId": "7945490", "name": "Ziming Huang"}, {"authorId": "24018646", "name": "Yada Zhu"}, {"authorId": "35984288", "name": "Guangnan Ye"}, {"authorId": "2114347016", "name": "Xiaodong Cui"}, {"authorId": "2153304639", "name": "Fan Zhang"}]}, {"paperId": "634e8fbeba53d45828846dd541ce0a0078c57b68", "externalIds": {"DBLP": "conf/acl/XuGTSSGZQJD20", "ACL": "2021.acl-long.420", "ArXiv": "2012.14116", "DOI": "10.18653/v1/2021.acl-long.420", "CorpusId": 229679923}, "corpusId": 229679923, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/634e8fbeba53d45828846dd541ce0a0078c57b68", "title": "Syntax-Enhanced Pre-trained Model", "abstract": "We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text. We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering. Results show that our model achieves state-of-the-art performance on six public benchmark datasets. We have two major findings. First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models. Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 36, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.420.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Materials Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-28", "journal": {"name": "ArXiv", "volume": "abs/2012.14116"}, "authors": [{"authorId": "22511787", "name": "Zenan Xu"}, {"authorId": "51223794", "name": "Daya Guo"}, {"authorId": "39483833", "name": "Duyu Tang"}, {"authorId": "2482836", "name": "Qinliang Su"}, {"authorId": "24962156", "name": "Linjun Shou"}, {"authorId": "50175330", "name": "Ming Gong"}, {"authorId": "81970097", "name": "Wanjun Zhong"}, {"authorId": "38472218", "name": "Xiaojun Quan"}, {"authorId": "46429989", "name": "Nan Duan"}, {"authorId": "71790825", "name": "Daxin Jiang"}]}, {"paperId": "3d06221331d8e61df2027b59f47846921294295b", "externalIds": {"DBLP": "conf/acl/ZhangZLCL20", "ACL": "2021.acl-long.421", "DOI": "10.18653/v1/2021.acl-long.421", "CorpusId": 236460301}, "corpusId": 236460301, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3d06221331d8e61df2027b59f47846921294295b", "title": "Matching Distributions between Model and Data: Cross-domain Knowledge Distillation for Unsupervised Domain Adaptation", "abstract": "Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge of source domain to the unlabeled target domain. Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains. However, this pipeline makes the source data risky and is inflexible for deploying the target model. This paper tackles a novel setting where only a trained source model is available and different network architectures can be adapted for target domain in terms of deployment environments. We propose a generic framework named Cross-domain Knowledge Distillation (CdKD) without needing any source data. CdKD matches the joint distributions between a trained source model and a set of target data during distilling the knowledge from the source model to the target domain. As a type of important knowledge in the source domain, for the first time, the gradient information is exploited to boost the transfer performance. Experiments on cross-domain text classification demonstrate that CdKD achieves superior performance, which verifies the effectiveness in this novel setting.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 33, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.421.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5423-5433"}, "authors": [{"authorId": null, "name": "Bo Zhang"}, {"authorId": "2115306399", "name": "Xiaoming Zhang"}, {"authorId": "2144443564", "name": "Yun Liu"}, {"authorId": "2110384161", "name": "Lei Cheng"}, {"authorId": "1707275", "name": "Zhoujun Li"}]}, {"paperId": "cea684bfe852e6fccdcb7c4d8676f8cab04dccc2", "externalIds": {"DBLP": "conf/acl/0003FWMX20", "ACL": "2021.acl-long.422", "DOI": "10.18653/v1/2021.acl-long.422", "CorpusId": 236459953}, "corpusId": 236459953, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cea684bfe852e6fccdcb7c4d8676f8cab04dccc2", "title": "Counterfactual Inference for Text Classification Debiasing", "abstract": "Today\u2019s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models\u2019 generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework \u2013 Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases \u2018poison\u2019 the trained model. In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model. Extensive experiments demonstrate Corsair\u2019s effectiveness, generalizability and fairness.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 70, "citationCount": 46, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.422.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5434-5445"}, "authors": [{"authorId": "2082474054", "name": "Chen Qian"}, {"authorId": "2163400298", "name": "Fuli Feng"}, {"authorId": "40650846", "name": "L. Wen"}, {"authorId": "48168083", "name": "Chunping Ma"}, {"authorId": "35930962", "name": "Pengjun Xie"}]}, {"paperId": "c09c9a507846b9ea0adff1afd1cc92513bba32f7", "externalIds": {"DBLP": "conf/acl/QiWWYY0020", "ArXiv": "2106.04408", "ACL": "2021.acl-long.423", "DOI": "10.18653/v1/2021.acl-long.423", "CorpusId": 235368202}, "corpusId": 235368202, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c09c9a507846b9ea0adff1afd1cc92513bba32f7", "title": "HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation", "abstract": "User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user embedding, in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest; 2) user interest in coarse-grained topics like sports; and 3) user interest in fine-grained topics like football. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 46, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.423.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"name": "ArXiv", "volume": "abs/2106.04408"}, "authors": [{"authorId": "50329599", "name": "Tao Qi"}, {"authorId": "2397264", "name": "Fangzhao Wu"}, {"authorId": "2118839668", "name": "Chuhan Wu"}, {"authorId": "80006471", "name": "Peiru Yang"}, {"authorId": "2152847933", "name": "Yang Yu"}, {"authorId": "144076239", "name": "Xing Xie"}, {"authorId": "1731776", "name": "Yongfeng Huang"}]}, {"paperId": "c7923f0e978310d7b7e38d7a1ba798b8bc50690a", "externalIds": {"DBLP": "conf/acl/QiWWH20", "ArXiv": "2106.01300", "ACL": "2021.acl-long.424", "DOI": "10.18653/v1/2021.acl-long.424", "CorpusId": 235294032}, "corpusId": 235294032, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c7923f0e978310d7b7e38d7a1ba798b8bc50690a", "title": "PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity", "abstract": "Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure time-aware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 41, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.424.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01300"}, "authors": [{"authorId": "50329599", "name": "Tao Qi"}, {"authorId": "2397264", "name": "Fangzhao Wu"}, {"authorId": "2118839668", "name": "Chuhan Wu"}, {"authorId": "1731776", "name": "Yongfeng Huang"}]}, {"paperId": "119c33321fc0e1db837ce293f1b65cc26c1cc34e", "externalIds": {"ACL": "2021.acl-long.425", "DBLP": "journals/corr/abs-2112-10322", "ArXiv": "2112.10322", "DOI": "10.18653/v1/2021.acl-long.425", "CorpusId": 236460000}, "corpusId": 236460000, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/119c33321fc0e1db837ce293f1b65cc26c1cc34e", "title": "Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims", "abstract": "False claims that have been previously fact-checked can still spread on social media. To mitigate their continual spread, detecting previously fact-checked claims is indispensable. Given a claim, existing works focus on providing evidence for detection by reranking candidate fact-checking articles (FC-articles) retrieved by BM25. However, these performances may be limited because they ignore the following characteristics of FC-articles: (1) claims are often quoted to describe the checked events, providing lexical information besides semantics; (2) sentence templates to introduce or debunk claims are common across articles, providing pattern information. Models that ignore the two aspects only leverage semantic relevance and may be misled by sentences that describe similar but irrelevant events. In this paper, we propose a novel reranker, MTM (Memory-enhanced Transformers for Matching) to rank FC-articles using key sentences selected with event (lexical and semantic) and pattern information. For event information, we propose a ROUGE-guided Transformer which is finetuned with regression of ROUGE. For pattern information, we generate pattern vectors for matching with sentences. By fusing event and pattern information, we select key sentences to represent an article and then predict if the article fact-checks the given claim using the claim, key sentences, and patterns. Experiments on two real-world datasets show that MTM outperforms existing methods. Human evaluation proves that MTM can capture key sentences for explanations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 58, "citationCount": 16, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.425.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-12-20", "journal": {"name": "ArXiv", "volume": "abs/2112.10322"}, "authors": [{"authorId": "46630548", "name": "Qiang Sheng"}, {"authorId": "144089410", "name": "Juan Cao"}, {"authorId": "2108063306", "name": "Xueyao Zhang"}, {"authorId": "2108569130", "name": "Xirong Li"}, {"authorId": "39223973", "name": "L. Zhong"}]}, {"paperId": "0cca27a289b595763d33b0a66ac1b3fc5b3ddc73", "externalIds": {"ACL": "2021.acl-long.426", "DBLP": "conf/acl/ZhouZHCH20", "DOI": "10.18653/v1/2021.acl-long.426", "CorpusId": 236460269}, "corpusId": 236460269, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0cca27a289b595763d33b0a66ac1b3fc5b3ddc73", "title": "Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble", "abstract": "Although deep neural networks have achieved prominent performance on many NLP tasks, they are vulnerable to adversarial examples. We propose Dirichlet Neighborhood Ensemble (DNE), a randomized method for training a robust model to defense synonym substitution-based attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models (e.g., BERT) for NLP applications. Through extensive experimentation, we demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 51, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.426.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5482-5492"}, "authors": [{"authorId": "2118765269", "name": "Yi Zhou"}, {"authorId": "2152196565", "name": "Xiaoqing Zheng"}, {"authorId": "1793529", "name": "Cho-Jui Hsieh"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}, {"authorId": "1790227", "name": "Xuanjing Huang"}]}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "externalIds": {"DBLP": "journals/corr/abs-2012-15832", "ACL": "2021.acl-long.427", "ArXiv": "2012.15832", "DOI": "10.18653/v1/2021.acl-long.427", "CorpusId": 229924221}, "corpusId": 229924221, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs", "abstract": "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 55, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.427.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-01", "journal": {"name": "ArXiv", "volume": "abs/2012.15832"}, "authors": [{"authorId": "40170001", "name": "Ofir Press"}, {"authorId": "1685669", "name": "Noah A. Smith"}, {"authorId": "35084211", "name": "M. Lewis"}]}, {"paperId": "26db43bc04dd8ca827f7700e3eaac04b5e5b7bad", "externalIds": {"DBLP": "conf/acl/MaoWL0H20", "ACL": "2021.acl-long.428", "DOI": "10.18653/v1/2021.acl-long.428", "CorpusId": 236460091}, "corpusId": 236460091, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/26db43bc04dd8ca827f7700e3eaac04b5e5b7bad", "title": "BanditMTL: Bandit-based Multi-task Learning for Text Classification", "abstract": "Task variance regularization, which can be used to improve the generalization of Multi-task Learning (MTL) models, remains unexplored in multi-task text classification. Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL in the multi-task text classification context is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.428.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5506-5516"}, "authors": [{"authorId": "1753922779", "name": "Yuren Mao"}, {"authorId": "1999172002", "name": "Zekai Wang"}, {"authorId": "2155130238", "name": "Weiwei Liu"}, {"authorId": "2873542", "name": "Xuemin Lin"}, {"authorId": "2566294", "name": "Wenbin Hu"}]}, {"paperId": "8527319f39e4bc0a85d404bdf4c71eb444aac395", "externalIds": {"ACL": "2021.acl-long.429", "DBLP": "conf/acl/KamigaitoH20", "ArXiv": "2106.07250", "DOI": "10.18653/v1/2021.acl-long.429", "CorpusId": 235421787}, "corpusId": 235421787, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8527319f39e4bc0a85d404bdf4c71eb444aac395", "title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding", "abstract": "In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.429.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-14", "journal": {"pages": "5517-5531"}, "authors": [{"authorId": "2300756", "name": "Hidetaka Kamigaito"}, {"authorId": "2087025575", "name": "Katsuhiko Hayashi"}]}, {"paperId": "98a00dbf695da32a63350fad4d6750acfe160eec", "externalIds": {"DBLP": "conf/acl/ChenTL0J20", "ACL": "2021.acl-long.430", "DOI": "10.18653/v1/2021.acl-long.430", "CorpusId": 236459782}, "corpusId": 236459782, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/98a00dbf695da32a63350fad4d6750acfe160eec", "title": "De-Confounded Variational Encoder-Decoder for Logical Table-to-Text Generation", "abstract": "Logical table-to-text generation aims to automatically generate fluent and logically faithful text from tables. The task remains challenging where deep learning models often generated linguistically fluent but logically inconsistent text. The underlying reason may be that deep learning models often capture surface-level spurious correlations rather than the causal relationships between the table \\boldsymbol{x} and the sentence \\boldsymbol{y}. Specifically, in the training stage, a model can get a low empirical loss without understanding \\boldsymbol{x} and use spurious statistical cues instead. In this paper, we propose a de-confounded variational encoder-decoder (DCVED) based on causal intervention, learning the objective p(\\boldsymbol{y}|\\textrm{do}(\\boldsymbol{x})). Firstly, we propose to use variational inference to estimate the confounders in the latent space and cooperate with the causal intervention based on Pearl\u2019s do-calculus to alleviate the spurious correlations. Secondly, to make the latent confounder meaningful, we propose a back-prediction process to predict the not-used entities but linguistically similar to the exactly selected ones. Finally, since our variational model can generate multiple candidates, we train a table-text selector to find out the best candidate sentence for the given table. An extensive set of experiments show that our model outperforms the baselines and achieves new state-of-the-art performance on two logical table-to-text datasets in terms of logical fidelity.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 22, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.430.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5532-5542"}, "authors": [{"authorId": "2108997748", "name": "Wenqing Chen"}, {"authorId": "2136089851", "name": "Jidong Tian"}, {"authorId": "2124530624", "name": "Yitian Li"}, {"authorId": "144111414", "name": "Hao He"}, {"authorId": "35692109", "name": "Yaohui Jin"}]}, {"paperId": "68a3d32416977e88cf1bfa4ad548d403f5f089d6", "externalIds": {"DBLP": "conf/acl/YangL00020", "ACL": "2021.acl-long.431", "DOI": "10.18653/v1/2021.acl-long.431", "CorpusId": 236459933}, "corpusId": 236459933, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/68a3d32416977e88cf1bfa4ad548d403f5f089d6", "title": "Rethinking Stealthiness of Backdoor Attack against NLP Models", "abstract": "Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https://github.com/lancopku/SOS.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 54, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.431.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5543-5557"}, "authors": [{"authorId": "2120801160", "name": "Wenkai Yang"}, {"authorId": "2149202150", "name": "Yankai Lin"}, {"authorId": "50492525", "name": "Peng Li"}, {"authorId": "2108485106", "name": "Jie Zhou"}, {"authorId": "2130282986", "name": "Xu Sun"}]}, {"paperId": "7df731af8fa55d8613d388ce68ed6d87f01835e6", "externalIds": {"DBLP": "conf/acl/ZhangXSZX20", "ACL": "2021.acl-long.432", "ArXiv": "2105.14980", "DOI": "10.18653/v1/2021.acl-long.432", "CorpusId": 235253726}, "corpusId": 235253726, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7df731af8fa55d8613d388ce68ed6d87f01835e6", "title": "Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition", "abstract": "Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers. Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models. We take a different point in this work, regarding all crowdsourced annotations as gold-standard with respect to the individual annotators. In this way, we find that crowdsourcing could be highly similar to domain adaptation, and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing. Here we take named entity recognition (NER) as a study case, suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features. We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only small-scale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 65, "citationCount": 16, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.432.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "5558-5570"}, "authors": [{"authorId": "2149173726", "name": "Xin Zhang"}, {"authorId": "2149131512", "name": "Guangwei Xu"}, {"authorId": "1789944", "name": "Yueheng Sun"}, {"authorId": "2678094", "name": "Meishan Zhang"}, {"authorId": "35930962", "name": "Pengjun Xie"}]}, {"paperId": "d3bd93b8fb79290a803d00fe32c30e53fbcff6de", "externalIds": {"ACL": "2021.acl-long.433", "MAG": "3173848082", "DBLP": "journals/corr/abs-2106-01809", "ArXiv": "2106.01809", "DOI": "10.18653/v1/2021.acl-long.433", "CorpusId": 235313693}, "corpusId": 235313693, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d3bd93b8fb79290a803d00fe32c30e53fbcff6de", "title": "Exploring Distantly-Labeled Rationales in Neural Network Models", "abstract": "Recent studies strive to incorporate various human rationales into neural networks to improve model performance, but few pay attention to the quality of the rationales. Most existing methods distribute their models\u2019 focus to distantly-labeled rationale words entirely and equally, while ignoring the potential important non-rationale words and not distinguishing the importance of different rationale words. In this paper, we propose two novel auxiliary loss functions to make better use of distantly-labeled rationales, which encourage models to maintain their focus on important words beyond labeled rationales (PINs) and alleviate redundant training on non-helpful rationales (NoIRs). Experiments on two representative classification tasks show that our proposed methods can push a classification model to effectively learn crucial clues from non-perfect rationales while maintaining the ability to spread its focus to other unlabeled important words, thus significantly outperform existing methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.433.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01809"}, "authors": [{"authorId": "2007771781", "name": "Quzhe Huang"}, {"authorId": "2110041566", "name": "Shengqi Zhu"}, {"authorId": "2115387922", "name": "Yansong Feng"}, {"authorId": "9072379", "name": "Dongyan Zhao"}]}, {"paperId": "193054e7e3a83d3e36926268e9c3706721973c57", "externalIds": {"DBLP": "journals/corr/abs-2105-02692", "ArXiv": "2105.02692", "ACL": "2021.acl-long.434", "DOI": "10.18653/v1/2021.acl-long.434", "CorpusId": 233864791}, "corpusId": 233864791, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/193054e7e3a83d3e36926268e9c3706721973c57", "title": "Learning to Perturb Word Embeddings for Out-of-distribution QA", "abstract": "QA models based on pretrained language models have achieved remarkable performance on various benchmark datasets. However, QA models do not generalize well to unseen data that falls outside the training distribution, due to distributional shifts. Data augmentation (DA) techniques which drop/replace words have shown to be effective in regularizing the model from overfitting to the training data. Yet, they may adversely affect the QA tasks since they incur semantic changes that may lead to wrong answers for the QA task. To tackle this problem, we propose a simple yet effective DA method based on a stochastic noise generator, which learns to perturb the word embedding of the input questions and context without changing their semantics. We validate the performance of the QA models trained with our word embedding perturbation on a single source dataset, on five different target domains. The results show that our method significantly outperforms the baseline DA methods. Notably, the model trained with ours outperforms the model trained with more than 240K artificially generated QA pairs.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 33, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.434.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-06", "journal": {"pages": "5583-5595"}, "authors": [{"authorId": "1472875852", "name": "Seanie Lee"}, {"authorId": "120434407", "name": "Minki Kang"}, {"authorId": "2108550899", "name": "Juho Lee"}, {"authorId": "2110796623", "name": "S. Hwang"}]}, {"paperId": "f5aa169162f2c7fdb311c4a7fedca182a07f05d9", "externalIds": {"DBLP": "journals/corr/abs-2105-13073", "MAG": "3164870073", "ArXiv": "2105.13073", "ACL": "2021.acl-long.435", "DOI": "10.18653/v1/2021.acl-long.435", "CorpusId": 235212591}, "corpusId": 235212591, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f5aa169162f2c7fdb311c4a7fedca182a07f05d9", "title": "Maria: A Visual Experience Powered Conversational Agent", "abstract": "Arguably, the visual perception of conversational agents to the physical world is a key way for them to exhibit the human-like intelligence. Image-grounded conversation is thus proposed to address this challenge. Existing works focus on exploring the multimodal dialog models that ground the conversation on a given image. In this paper, we take a step further to study image-grounded conversation under a fully open-ended setting where no paired dialog and image are assumed available. Specifically, we present Maria, a neural conversation agent powered by the visual world experiences which are retrieved from a large-scale image index. Maria consists of three flexible components, i.e., text-to-image retriever, visual concept detector and visual-knowledge-grounded response generator. The retriever aims to retrieve a correlated image to the dialog from an image index, while the visual concept detector extracts rich visual knowledge from the image. Then, the response generator is grounded on the extracted visual knowledge and dialog context to generate the target response. Extensive experiments demonstrate Maria outperforms previous state-of-the-art methods on automatic metrics and human evaluation, and can generate informative responses that have some visual commonsense of the physical world.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 60, "citationCount": 19, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.435.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-27", "journal": {"pages": "5596-5611"}, "authors": [{"authorId": "2007669688", "name": "Zujie Liang"}, {"authorId": null, "name": "Huang Hu"}, {"authorId": "2110091832", "name": "Can Xu"}, {"authorId": "8801869", "name": "Chongyang Tao"}, {"authorId": "2442662", "name": "Xiubo Geng"}, {"authorId": "2108965426", "name": "Yining Chen"}, {"authorId": "2105844411", "name": "Fan Liang"}, {"authorId": "71790825", "name": "Daxin Jiang"}]}, {"paperId": "b1999b5ede6820e7a17a5567009f4aadb3649f64", "externalIds": {"DBLP": "conf/acl/ZhangRR20", "ACL": "2021.acl-long.436", "DOI": "10.18653/v1/2021.acl-long.436", "CorpusId": 235386076}, "corpusId": 235386076, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b1999b5ede6820e7a17a5567009f4aadb3649f64", "title": "A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues", "abstract": "Conversational dialogue systems (CDSs) are hard to evaluate due to the complexity of natural language. Automatic evaluation of dialogues often shows insufficient correlation with human judgements. Human evaluation is reliable but labor-intensive. We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort. HMCEval casts dialogue evaluation as a sample assignment problem, where we need to decide to assign a sample to a human or a machine for evaluation. HMCEval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment, and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation, as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort. We assess the performance of HMCEval on the task of evaluating malevolence in dialogues. The experimental results show that HMCEval achieves around 99% evaluation accuracy with half of the human effort spared, showing that HMCEval provides reliable evaluation outcomes while reducing human effort by a large amount.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 47, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5612-5623"}, "authors": [{"authorId": "2145957097", "name": "Yangjun Zhang"}, {"authorId": "1749477", "name": "Pengjie Ren"}, {"authorId": "1696030", "name": "M. de Rijke"}]}, {"paperId": "0472f016ccf6609d73e11432883101b54b015150", "externalIds": {"DBLP": "journals/corr/abs-2106-03410", "ACL": "2021.acl-long.437", "ArXiv": "2106.03410", "DOI": "10.18653/v1/2021.acl-long.437", "CorpusId": 235358901}, "corpusId": 235358901, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0472f016ccf6609d73e11432883101b54b015150", "title": "Generating Relevant and Coherent Dialogue Responses using Self-Separated Conditional Variational AutoEncoders", "abstract": "Conditional Variational AutoEncoder (CVAE) effectively increases the diversity and informativeness of responses in open-ended dialogue generation tasks through enriching the context vector with sampled latent variables. However, due to the inherent one-to-many and many-to-one phenomena in human dialogues, the sampled latent variables may not correctly reflect the contexts\u2019 semantics, leading to irrelevant and incoherent generated responses. To resolve this problem, we propose Self-separated Conditional Variational AutoEncoder (abbreviated as SepaCVAE) that introduces group information to regularize the latent variables, which enhances CVAE by improving the responses\u2019 relevance and coherence while maintaining their diversity and informativeness. SepaCVAE actively divides the input data into groups, and then widens the absolute difference between data pairs from distinct groups, while narrowing the relative distance between data pairs in the same group. Empirical results from automatic evaluation and detailed analysis demonstrate that SepaCVAE can significantly boost responses in well-established open-domain dialogue datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 53, "citationCount": 18, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.437.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "journal": {"name": "ArXiv", "volume": "abs/2106.03410"}, "authors": [{"authorId": "2088605455", "name": "Bin Sun"}, {"authorId": "1519472504", "name": "Shaoxiong Feng"}, {"authorId": "2111161355", "name": "Yiwei Li"}, {"authorId": "2108326935", "name": "Jiamou Liu"}, {"authorId": "2158257423", "name": "Kan Li"}]}, {"paperId": "bb1839548939385aed7a8a8dfdf5e85c63a3e177", "externalIds": {"DBLP": "conf/acl/LiuRCRR020", "ACL": "2021.acl-long.438", "ArXiv": "2106.15903", "DOI": "10.18653/v1/2021.acl-long.438", "CorpusId": 235683604}, "corpusId": 235683604, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bb1839548939385aed7a8a8dfdf5e85c63a3e177", "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein Distance", "abstract": "Conversational Question Simplification (CQS) aims to simplify self-contained questions into conversational ones by incorporating some conversational characteristics, e.g., anaphora and ellipsis. Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training. In this work, we introduce a Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the minimum Levenshtein distance through explicit editing actions. RISE is able to pay attention to tokens that are related to conversational characteristics. To train RISE, we devise an Iterative Reinforce Training (IRT) algorithm with a Dynamic Programming based Sampling (DPS) process to improve exploration. Experimental results on two benchmark datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well on unseen data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.438.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-30", "journal": {"pages": "5638-5650"}, "authors": [{"authorId": "2145285599", "name": "Zhongkun Liu"}, {"authorId": "1749477", "name": "Pengjie Ren"}, {"authorId": "1721165", "name": "Zhumin Chen"}, {"authorId": "2780667", "name": "Z. Ren"}, {"authorId": "1696030", "name": "M. de Rijke"}, {"authorId": "92660691", "name": "Ming Zhou"}]}, {"paperId": "8b8d0e74a21486930b89c0ad356d60aa60d6322d", "externalIds": {"ArXiv": "2101.00151", "DBLP": "conf/acl/LeSMBGK20", "ACL": "2021.acl-long.439", "DOI": "10.18653/v1/2021.acl-long.439", "CorpusId": 230433768}, "corpusId": 230433768, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8b8d0e74a21486930b89c0ad356d60aa60d6322d", "title": "DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue", "abstract": "A video-grounded dialogue system is required to understand both dialogue, which contains semantic dependencies from turn to turn, and video, which contains visual cues of spatial and temporal scene variations. Building such dialogue systems is a challenging problem, involving various reasoning types on both visual and language inputs. Existing benchmarks do not have enough annotations to thoroughly analyze dialogue systems and understand their capabilities and limitations in isolation. These benchmarks are also not explicitly designed to minimise biases that models can exploit without actual reasoning. To address these limitations, in this paper, we present DVD, a Diagnostic Dataset for Video-grounded Dialogue. The dataset is designed to contain minimal biases and has detailed annotations for the different types of reasoning over the spatio-temporal space of video. Dialogues are synthesized over multiple question turns, each of which is injected with a set of cross-turn semantic relationships. We use DVD to analyze existing approaches, providing interesting insights into their abilities and limitations. In total, DVD is built from 11k CATER synthetic videos and contains 10 instances of 10-round dialogues for each video, resulting in more than 100k dialogues and 1M question-answer pairs. Our code and dataset are publicly available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 56, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.439.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-01", "journal": {"pages": "5651-5665"}, "authors": [{"authorId": "2064728738", "name": "Hung Le"}, {"authorId": "21669342", "name": "Chinnadhurai Sankar"}, {"authorId": "29072828", "name": "Seungwhan Moon"}, {"authorId": "1791052", "name": "Ahmad Beirami"}, {"authorId": "1979505", "name": "A. Geramifard"}, {"authorId": "2150275", "name": "Satwik Kottur"}]}, {"paperId": "64374ad854dba86b087ed14d96a376e646103904", "externalIds": {"ArXiv": "2107.06779", "DBLP": "journals/corr/abs-2107-06779", "ACL": "2021.acl-long.440", "DOI": "10.18653/v1/2021.acl-long.440", "CorpusId": 235829068}, "corpusId": 235829068, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/64374ad854dba86b087ed14d96a376e646103904", "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation", "abstract": "Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users\u2019 emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 65, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.440.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-14", "journal": {"name": "ArXiv", "volume": "abs/2107.06779"}, "authors": [{"authorId": "2118518300", "name": "Jingwen Hu"}, {"authorId": "2116486976", "name": "Yuchen Liu"}, {"authorId": "2109905684", "name": "Jinming Zhao"}, {"authorId": "1721329", "name": "Qin Jin"}]}, {"paperId": "753baa88a7f49f6605ae30f77a54dbb9e074c8b4", "externalIds": {"ACL": "2021.acl-long.441", "ArXiv": "2106.01112", "DBLP": "conf/acl/ZhangCDZFL020", "DOI": "10.18653/v1/2021.acl-long.441", "CorpusId": 235294194}, "corpusId": 235294194, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/753baa88a7f49f6605ae30f77a54dbb9e074c8b4", "title": "DynaEval: Unifying Turn and Dialogue Level Evaluation", "abstract": "A dialogue is essentially a multi-turn interaction among interlocutors. Effective evaluation metrics should reflect the dynamics of such interaction. Existing automatic metrics are focused very much on the turn-level quality, while ignoring such dynamics. To this end, we propose DynaEval, a unified automatic evaluation framework which is not only capable of performing turn-level evaluation, but also holistically considers the quality of the entire dialogue. In DynaEval, the graph convolutional network (GCN) is adopted to model a dialogue in totality, where the graph nodes denote each individual utterance and the edges represent the dependency between pairs of utterances. A contrastive loss is then applied to distinguish well-formed dialogues from carefully constructed negative samples. Experiments show that DynaEval significantly outperforms the state-of-the-art dialogue coherence model, and correlates strongly with human judgements across multiple dialogue evaluation aspects at both turn and dialogue level.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 64, "citationCount": 45, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.441.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"pages": "5676-5689"}, "authors": [{"authorId": "2111574800", "name": "Chen Zhang"}, {"authorId": "2188651547", "name": "Yiming Chen"}, {"authorId": "1405511901", "name": "L. F. D\u2019Haro"}, {"authorId": "2152821125", "name": "Yan Zhang"}, {"authorId": "3034330", "name": "Thomas Friedrichs"}, {"authorId": "2110959866", "name": "Grandee Lee"}, {"authorId": "2108493009", "name": "Haizhou Li"}]}, {"paperId": "0c21334be0228431d619a180c809b43be0065bdd", "externalIds": {"DBLP": "conf/acl/HuangTSG0J0D20", "ArXiv": "2105.13239", "ACL": "2021.acl-long.442", "DOI": "10.18653/v1/2021.acl-long.442", "CorpusId": 235212088}, "corpusId": 235212088, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0c21334be0228431d619a180c809b43be0065bdd", "title": "CoSQA: 20,000+ Web Queries for Code Search and Question Answering", "abstract": "Finding codes given natural language query is beneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce CoSQA dataset. It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance text-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that, evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the accuracy of code question answering by 5.1% and incorporating CoCLR brings a further improvement of 10.5%.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 54, "influentialCitationCount": 13, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.442.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-27", "journal": {"pages": "5690-5700"}, "authors": [{"authorId": "145505727", "name": "Junjie Huang"}, {"authorId": "39483833", "name": "Duyu Tang"}, {"authorId": "24962156", "name": "Linjun Shou"}, {"authorId": "50175330", "name": "Ming Gong"}, {"authorId": "2087261268", "name": "Ke Xu"}, {"authorId": "71790825", "name": "Daxin Jiang"}, {"authorId": "92660691", "name": "Ming Zhou"}, {"authorId": "46429989", "name": "Nan Duan"}]}, {"paperId": "148ed5a01f5d32957bebbdebea59a8f4531da5b9", "externalIds": {"ACL": "2021.acl-long.443", "DBLP": "conf/acl/LiY20", "ArXiv": "2012.05414", "DOI": "10.18653/v1/2021.acl-long.443", "CorpusId": 234353602}, "corpusId": 234353602, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/148ed5a01f5d32957bebbdebea59a8f4531da5b9", "title": "Rewriter-Evaluator Architecture for Neural Machine Translation", "abstract": "A few approaches have been developed to improve neural machine translation (NMT) models with multiple passes of decoding. However, their performance gains are limited because of lacking proper policies to terminate the multi-pass process. To address this issue, we introduce a novel architecture of Rewriter-Evaluator. Translating a source sentence involves multiple rewriting passes. In every pass, a rewriter generates a new translation to improve the past translation. Termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator. We also propose prioritized gradient descent (PGD) to jointly and efficiently train the rewriter and the evaluator. Extensive experiments on three machine translation tasks show that our architecture notably improves the performances of NMT models and significantly outperforms prior methods. An oracle experiment reveals that it can largely reduce performance gaps to the oracle policy. Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.443.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-10", "journal": {"pages": "5701-5710"}, "authors": [{"authorId": "48514481", "name": "Yangming Li"}, {"authorId": "39922478", "name": "K. Yao"}]}, {"paperId": "aa2bd42be1a96f3b8c5d9df70e6073a4799e0140", "externalIds": {"ACL": "2021.acl-long.444", "DBLP": "journals/corr/abs-2107-11164", "ArXiv": "2107.11164", "DOI": "10.18653/v1/2021.acl-long.444", "CorpusId": 236318502}, "corpusId": 236318502, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/aa2bd42be1a96f3b8c5d9df70e6073a4799e0140", "title": "Modeling Bilingual Conversational Characteristics for Neural Chat Translation", "abstract": "Neural chat translation aims to translate bilingual conversational text, which has a broad application in international exchanges and cooperation. Despite the impressive performance of sentence-level and context-aware Neural Machine Translation (NMT), there still remain challenges to translate bilingual conversational text due to its inherent characteristics such as role preference, dialogue coherence, and translation consistency. In this paper, we aim to promote the translation quality of conversational text by modeling the above properties. Specifically, we design three latent variational modules to learn the distributions of bilingual conversational characteristics. Through sampling from these learned distributions, the latent variables, tailored for role preference, dialogue coherence, and translation consistency, are incorporated into the NMT model for better translation. We evaluate our approach on the benchmark dataset BConTrasT (English<->German) and a self-collected bilingual dialogue corpus, named BMELD (English<->Chinese). Extensive experiments show that our approach notably boosts the performance over strong baselines by a large margin and significantly surpasses some state-of-the-art context-aware NMT models in terms of BLEU and TER. Additionally, we make the BMELD dataset publicly available for the research community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 75, "citationCount": 21, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.444.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-23", "journal": {"pages": "5711-5724"}, "authors": [{"authorId": "3389712", "name": "Yunlong Liang"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "47559028", "name": "Yufeng Chen"}, {"authorId": "2310092", "name": "Jinan Xu"}, {"authorId": "2108485135", "name": "Jie Zhou"}]}, {"paperId": "88a72a9bc844ceb809f0fcf94ae3a0ba69e558de", "externalIds": {"DBLP": "conf/acl/Xie0G020", "ACL": "2021.acl-long.445", "ArXiv": "2107.06569", "DOI": "10.18653/v1/2021.acl-long.445", "CorpusId": 235829792}, "corpusId": 235829792, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/88a72a9bc844ceb809f0fcf94ae3a0ba69e558de", "title": "Importance-based Neuron Allocation for Multilingual Neural Machine Translation", "abstract": "Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 17, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.445.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-14", "journal": {"name": "ArXiv", "volume": "abs/2107.06569"}, "authors": [{"authorId": "145106559", "name": "W. Xie"}, {"authorId": "144599698", "name": "Yang Feng"}, {"authorId": "93951729", "name": "Shuhao Gu"}, {"authorId": "144580031", "name": "Dong Yu"}]}, {"paperId": "fafd06196304bccd23d221cc8bfb431a48a288fc", "externalIds": {"ACL": "2021.acl-long.446", "DBLP": "conf/acl/HuangXSL20", "ArXiv": "2105.14809", "DOI": "10.18653/v1/2021.acl-long.446", "CorpusId": 235253993}, "corpusId": 235253993, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fafd06196304bccd23d221cc8bfb431a48a288fc", "title": "Transfer Learning for Sequence Generation: from Single-source to Multi-source", "abstract": "Multi-source sequence generation (MSG) is an important kind of sequence generation tasks that takes multiple sources, including automatic post-editing, multi-source translation, multi-document summarization, etc. As MSG tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks, transferring pretrained sequence-to-sequence models to MSG tasks is essential. Although directly finetuning pretrained models on MSG tasks and concatenating multiple sources into a single long sequence is regarded as a simple method to transfer pretrained models to MSG tasks, we conjecture that the direct finetuning method leads to catastrophic forgetting and solely relying on pretrained self-attention layers to capture cross-source information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to document-level translation, our framework outperforms strong baselines significantly.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 54, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.446.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "5738-5750"}, "authors": [{"authorId": "152638939", "name": "Xuancheng Huang"}, {"authorId": "2774294", "name": "Jingfang Xu"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "2152797839", "name": "Yang Liu"}]}, {"paperId": "7317dccaf8023b2719a2d0fe787a31b20a3232e1", "externalIds": {"DBLP": "conf/acl/ZhaoZSVRKS20", "ACL": "2021.acl-long.447", "ArXiv": "2012.15682", "DOI": "10.18653/v1/2021.acl-long.447", "CorpusId": 235303641}, "corpusId": 235303641, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7317dccaf8023b2719a2d0fe787a31b20a3232e1", "title": "A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters", "abstract": "Few-shot crosslingual transfer has been shown to outperform its zero-shot counterpart with pretrained encoders like multilingual BERT. Despite its growing popularity, little to no attention has been paid to standardizing and analyzing the design of few-shot experiments. In this work, we highlight a fundamental risk posed by this shortcoming, illustrating that the model exhibits a high degree of sensitivity to the selection of few shots. We conduct a large-scale experimental study on 40 sets of sampled few shots for six diverse NLP tasks across up to 40 languages. We provide an analysis of success and failure cases of few-shot transfer, which highlights the role of lexical features. Additionally, we show that a straightforward full model finetuning approach is quite effective for few-shot transfer, outperforming several state-of-the-art few-shot approaches. As a step towards standardizing few-shot crosslingual experimental designs, we make our sampled few shots publicly available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 90, "citationCount": 44, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.447.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"pages": "5751-5767"}, "authors": [{"authorId": "47374917", "name": "Mengjie Zhao"}, {"authorId": "2117913581", "name": "Yi Zhu"}, {"authorId": "2888926", "name": "Ehsan Shareghi"}, {"authorId": "1747849", "name": "Ivan Vulic"}, {"authorId": "1762757", "name": "Roi Reichart"}, {"authorId": "145762466", "name": "A. Korhonen"}, {"authorId": "144418438", "name": "Hinrich Sch\u00fctze"}]}, {"paperId": "bda345366be3960bbe451cbc57e386dbcb08903a", "externalIds": {"DBLP": "conf/acl/WuMRG20", "ACL": "2021.acl-long.448", "ArXiv": "2012.15573", "DOI": "10.18653/v1/2021.acl-long.448", "CorpusId": 229923926}, "corpusId": 229923926, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bda345366be3960bbe451cbc57e386dbcb08903a", "title": "Coreference Reasoning in Machine Reading Comprehension", "abstract": "Coreference resolution is essential for natural language understanding and has been long studied in NLP. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to evaluate the ability of MRC models to reason about coreference. However, as we show, coreference reasoning in MRC is a greater challenge than earlier thought; MRC datasets do not reflect the natural distribution and, consequently, the challenges of coreference reasoning. Specifically, success on these datasets does not reflect a model\u2019s proficiency in coreference reasoning. We propose a methodology for creating MRC datasets that better reflect the challenges of coreference reasoning and use it to create a sample evaluation set. The results on our dataset show that state-of-the-art models still struggle with these phenomena. Furthermore, we develop an effective way to use naturally occurring coreference phenomena from existing coreference resolution datasets when training MRC models. This allows us to show an improvement in the coreference reasoning abilities of state-of-the-art models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 43, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.448.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"name": "ArXiv", "volume": "abs/2012.15573"}, "authors": [{"authorId": "2145209642", "name": "Mingzhu Wu"}, {"authorId": "2182290", "name": "N. Moosavi"}, {"authorId": "144590225", "name": "D. Roth"}, {"authorId": "69033154", "name": "Iryna Gurevych"}]}, {"paperId": "80b52d5834dd1ee61c5d164dcb372af6993f81b8", "externalIds": {"ACL": "2021.acl-long.449", "DBLP": "conf/acl/Zhang0HT20", "DOI": "10.18653/v1/2021.acl-long.449", "CorpusId": 236460343}, "corpusId": 236460343, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/80b52d5834dd1ee61c5d164dcb372af6993f81b8", "title": "Adapting Unsupervised Syntactic Parsing Methodology for Discourse Dependency Parsing", "abstract": "One of the main bottlenecks in developing discourse dependency parsers is the lack of annotated training data. A potential solution is to utilize abundant unlabeled data by using unsupervised techniques, but there is so far little research in unsupervised discourse dependency parsing. Fortunately, unsupervised syntactic dependency parsing has been studied by decades, which could potentially be adapted for discourse parsing. In this paper, we propose a simple yet effective method to adapt unsupervised syntactic dependency parsing methodology for unsupervised discourse dependency parsing. We apply the method to adapt two state-of-the-art unsupervised syntactic dependency parsing methods. Experimental results demonstrate that our adaptation is effective. Moreover, we extend the adapted methods to the semi-supervised and supervised setting and surprisingly, we find that they outperform previous methods specially designed for supervised discourse parsing. Further analysis shows our adaptations result in superiority not only in parsing accuracy but also in time and space efficiency.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5782-5794"}, "authors": [{"authorId": "2107964289", "name": "Liwen Zhang"}, {"authorId": "2108296478", "name": "Ge Wang"}, {"authorId": "144836032", "name": "Wenjuan Han"}, {"authorId": "40341553", "name": "Kewei Tu"}]}, {"paperId": "4f5922b8ab49f4c5529e415656d13bab6c28ee52", "externalIds": {"DBLP": "conf/acl/NguyenNJ020", "ACL": "2021.acl-long.450", "ArXiv": "2106.15760", "DOI": "10.18653/v1/2021.acl-long.450", "CorpusId": 235683576}, "corpusId": 235683576, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4f5922b8ab49f4c5529e415656d13bab6c28ee52", "title": "A Conditional Splitting Framework for Efficient Constituency Parsing", "abstract": "We introduce a generic seq2seq parsing framework that casts constituency parsing problems (syntactic and discourse parsing) into a series of conditional splitting decisions. Our parsing model estimates the conditional probability distribution of possible splitting points in a given text span and supports efficient top-down decoding, which is linear in number of nodes. The conditional splitting formulation together with efficient beam search inference facilitate structural consistency without relying on expensive structured inference. Crucially, for discourse analysis we show that in our formulation, discourse segmentation can be framed as a special case of parsing which allows us to perform discourse parsing without requiring segmentation as a pre-requisite. Experiments show that our model achieves good results on the standard syntactic parsing tasks under settings with/without pre-trained representations and rivals state-of-the-art (SoTA) methods that are more computationally expensive than ours. In discourse parsing, our method outperforms SoTA by a good margin.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 53, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.450.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-30", "journal": {"name": "ArXiv", "volume": "abs/2106.15760"}, "authors": [{"authorId": "2117824172", "name": "Thanh-Tung Nguyen"}, {"authorId": "1399659909", "name": "Xuan-Phi Nguyen"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "2127399967", "name": "Xiaoli Li"}]}, {"paperId": "c09ff6965322ece56bce383266c75159234f59c4", "externalIds": {"DBLP": "journals/corr/abs-2106-01223", "ArXiv": "2106.01223", "ACL": "2021.acl-long.451", "DOI": "10.18653/v1/2021.acl-long.451", "CorpusId": 235294002}, "corpusId": 235294002, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c09ff6965322ece56bce383266c75159234f59c4", "title": "A Unified Generative Framework for Various NER Subtasks", "abstract": "Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 70, "citationCount": 170, "influentialCitationCount": 35, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.451.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01223"}, "authors": [{"authorId": "146948229", "name": "Hang Yan"}, {"authorId": "2067331064", "name": "Tao Gui"}, {"authorId": "2087363104", "name": "Junqi Dai"}, {"authorId": "3187768", "name": "Qipeng Guo"}, {"authorId": "2148904903", "name": "Zheng Zhang"}, {"authorId": "1767521", "name": "Xipeng Qiu"}]}, {"paperId": "38d052313e6cce936926f2168987b9edeb85a496", "externalIds": {"DBLP": "journals/corr/abs-2106-00334", "ArXiv": "2106.00334", "ACL": "2021.acl-long.452", "MAG": "3168043077", "DOI": "10.18653/v1/2021.acl-long.452", "CorpusId": 235266206}, "corpusId": 235266206, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/38d052313e6cce936926f2168987b9edeb85a496", "title": "An In-depth Study on Internal Structure of Chinese Words", "abstract": "Unlike English letters, Chinese characters have rich and specific meanings. Usually, the meaning of a word can be derived from its constituent characters in some way. Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information. This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships. First, based on newly compiled annotation guidelines, we manually annotate a word-internal structure treebank (WIST) consisting of over 30K multi-char words from Chinese Penn Treebank. To guarantee quality, each word is independently annotated by two annotators and inconsistencies are handled by a third senior annotator. Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation. Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser. Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.452.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"name": "ArXiv", "volume": "abs/2106.00334"}, "authors": [{"authorId": "2057184863", "name": "Chen Gong"}, {"authorId": "2110088606", "name": "Saihao Huang"}, {"authorId": "50986473", "name": "Houquan Zhou"}, {"authorId": "51003520", "name": "Zhenghua Li"}, {"authorId": "39767557", "name": "M. Zhang"}, {"authorId": "2108271253", "name": "Zhefeng Wang"}, {"authorId": "2422046", "name": "Baoxing Huai"}, {"authorId": "1677643972", "name": "N. Yuan"}]}, {"paperId": "0df9c19659388e55745c290ace520491c2985d9b", "externalIds": {"DBLP": "conf/acl/LiuDBJSM20", "ACL": "2021.acl-long.453", "DOI": "10.18653/v1/2021.acl-long.453", "CorpusId": 236460177}, "corpusId": 236460177, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0df9c19659388e55745c290ace520491c2985d9b", "title": "MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER", "abstract": "Named Entity Recognition (NER) for low-resource languages is a both practical and challenging research problem. This paper addresses zero-shot transfer for cross-lingual NER, especially when the amount of source-language training data is also limited. The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination. With the source-language data as well as the translated data, a generation-based multilingual data augmentation method is introduced to further increase diversity by generating synthetic labeled data in multiple languages. These augmented data enable the language model based NER models to generalize better with both the language-specific features from the target-language synthetic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 58, "citationCount": 54, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.453.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5834-5846"}, "authors": [{"authorId": "2145314839", "name": "Linlin Liu"}, {"authorId": "2064493724", "name": "Bosheng Ding"}, {"authorId": "1996394", "name": "Lidong Bing"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "2059080424", "name": "Luo Si"}, {"authorId": "1679209", "name": "C. Miao"}]}, {"paperId": "d0c383acba2f94caecff6f809debc032ec029d62", "externalIds": {"DBLP": "conf/acl/LiuFZX20", "ArXiv": "2105.07148", "ACL": "2021.acl-long.454", "DOI": "10.18653/v1/2021.acl-long.454", "CorpusId": 234741719}, "corpusId": 234741719, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d0c383acba2f94caecff6f809debc032ec029d62", "title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter", "abstract": "Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labeling tasks due to their respective strengths. However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labeling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer. Compared with existing methods, our model facilitates deep lexicon knowledge fusion at the lower layers of BERT. Experiments on ten Chinese datasets of three tasks including Named Entity Recognition, Word Segmentation, and Part-of-Speech Tagging, show that LEBERT achieves state-of-the-art results.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 57, "citationCount": 81, "influentialCitationCount": 12, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.454.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-15", "journal": {"name": "ArXiv", "volume": "abs/2105.07148"}, "authors": [{"authorId": "40474876", "name": "Wei Liu"}, {"authorId": "2119032391", "name": "Xiyan Fu"}, {"authorId": "2145914122", "name": "Yueqian Zhang"}, {"authorId": "49617098", "name": "Wenming Xiao"}]}, {"paperId": "ec15ff1fc5c9780fd91902f286a9e3fcd00b890d", "externalIds": {"DBLP": "conf/acl/WuZWH20", "ACL": "2021.acl-long.455", "DOI": "10.18653/v1/2021.acl-long.455", "CorpusId": 235432084}, "corpusId": 235432084, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ec15ff1fc5c9780fd91902f286a9e3fcd00b890d", "title": "Math Word Problem Solving with Explicit Numerical Values", "abstract": "In recent years, math word problem solving has received considerable attention and achieved promising results, but previous methods rarely take numerical values into consideration. Most methods treat the numerical values in the problems as number symbols, and ignore the prominent role of the numerical values in solving the problem. In this paper, we propose a novel approach called NumS2T, which enhances math word problem solving performance by explicitly incorporating numerical values into a sequence-to-tree network. In addition, a numerical properties prediction mechanism is used to capture the category and comparison information of numerals and measure their importance in global expressions. Experimental results on the Math23K and APE datasets demonstrate that our model achieves better performance than existing state-of-the-art models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 34, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.455.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5859-5869"}, "authors": [{"authorId": "1490734800", "name": "Qinzhuo Wu"}, {"authorId": "1409702669", "name": "Qi Zhang"}, {"authorId": "2118602528", "name": "Zhongyu Wei"}, {"authorId": "1790227", "name": "Xuanjing Huang"}]}, {"paperId": "7654dbd372d8b65e730e3bd477ff9fec96c16dc5", "externalIds": {"ACL": "2021.acl-long.456", "DBLP": "journals/corr/abs-2107-01431", "ArXiv": "2107.01431", "DOI": "10.18653/v1/2021.acl-long.456", "CorpusId": 235731777}, "corpusId": 235731777, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7654dbd372d8b65e730e3bd477ff9fec96c16dc5", "title": "Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks", "abstract": "Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain answers. Along with target expression supervision, our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning: a) self-supervised number prediction task predicting both number quantity and number locations; b) commonsense constant prediction task predicting what prior knowledge (e.g. how many legs a chicken has) is required; c) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping; d) duality exploiting task exploiting the quasi-duality between symbolic equation generation and problem\u2019s part-of-speech generation to enhance the understanding ability of a solver. Besides, to provide a more realistic and challenging benchmark for developing a universal and scalable solver, we also construct a new largescale MWP benchmark CM17K consisting of 4 kinds of MWPs (arithmetic, one-unknown linear, one-unknown non-linear, equation set) with more than 17K samples. Extensive experiments on Math23K and our CM17k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 54, "citationCount": 36, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.456.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-03", "journal": {"name": "ArXiv", "volume": "abs/2107.01431"}, "authors": [{"authorId": "9102638", "name": "Jinghui Qin"}, {"authorId": "13246332", "name": "Xiaodan Liang"}, {"authorId": "151261268", "name": "Yining Hong"}, {"authorId": "66239746", "name": "Jianheng Tang"}, {"authorId": "2148303324", "name": "Liang Lin"}]}, {"paperId": "4fdaf912ad474e4d52b2b8915535ab5a9e8ea94d", "externalIds": {"DBLP": "conf/acl/ZhangC0QYH20", "ACL": "2021.acl-long.457", "ArXiv": "2108.08983", "DOI": "10.18653/v1/2021.acl-long.457", "CorpusId": 236460015}, "corpusId": 236460015, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4fdaf912ad474e4d52b2b8915535ab5a9e8ea94d", "title": "SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining", "abstract": "Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbours of linked-entity. In SMedBERT, the mention-neighbour hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure. Apart from knowledge integration as external features, we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations. Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks. It also improves the performance of other tasks such as question answering, question matching and natural language inference.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.457.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-20", "journal": {"name": "ArXiv", "volume": "abs/2108.08983"}, "authors": [{"authorId": "2146342371", "name": "Taolin Zhang"}, {"authorId": "2152132786", "name": "Zerui Cai"}, {"authorId": "121899912", "name": "Chengyu Wang"}, {"authorId": "2056851632", "name": "Minghui Qiu"}, {"authorId": "2821227", "name": "Bite Yang"}, {"authorId": "143644849", "name": "Xiaofeng He"}]}, {"paperId": "aebdd7aab992437ba66cd81eab9306cc38bb8357", "externalIds": {"ACL": "2021.acl-long.458", "MAG": "3174392074", "DBLP": "conf/acl/ZhangIR20a", "DOI": "10.18653/v1/2021.acl-long.458", "CorpusId": 236459959}, "corpusId": 236459959, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/aebdd7aab992437ba66cd81eab9306cc38bb8357", "title": "What is Your Article Based On? Inferring Fine-grained Provenance", "abstract": "When evaluating an article and the claims it makes, a critical reader must be able to assess where the information presented comes from, and whether the various claims are mutually consistent and support the conclusion. This motivates the study of claim provenance, which seeks to trace and explain the origins of claims. In this paper, we introduce new techniques to model and reason about the provenance of multiple interacting claims, including how to capture fine-grained information about the context. Our solution hinges on first identifying the sentences that potentially contain important external information. We then develop a query generator with our novel rank-aware cross attention mechanism, which aims at generating metadata for the source article, based on the context and the signals collected from a search engine. This establishes relevant search queries, and it allows us to obtain source article candidates for each identified sentence and propose an ILP based algorithm to infer the best sources. We experiment with a newly created evaluation dataset, Politi-Prov, based on fact-checking articles from www.politifact.com; our experimental results show that our solution leads to a significant improvement over baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 21, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "5894-5903"}, "authors": [{"authorId": "46867002", "name": "Yi Zhang"}, {"authorId": "1804315", "name": "Z. Ives"}, {"authorId": "144590225", "name": "D. Roth"}]}, {"paperId": "db2bd466953f3ea49280988e1659b6ac3f639e45", "externalIds": {"DBLP": "conf/acl/ChenSSW20", "ACL": "2021.acl-long.459", "ArXiv": "2204.13258", "DOI": "10.18653/v1/2021.acl-long.459", "CorpusId": 236460168}, "corpusId": 236460168, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/db2bd466953f3ea49280988e1659b6ac3f639e45", "title": "Cross-modal Memory Networks for Radiology Report Generation", "abstract": "Medical imaging plays a significant role in clinical practice of medical diagnosis, where the text reports of the images are essential in understanding them and facilitating later treatments. By generating the reports automatically, it is beneficial to help lighten the burden of radiologists and significantly promote clinical automation, which already attracts much attention in applying artificial intelligence to medical domain. Previous studies mainly follow the encoder-decoder paradigm and focus on the aspect of text generation, with few studies considering the importance of cross-modal mappings and explicitly exploit such mappings to facilitate radiology report generation. In this paper, we propose a cross-modal memory networks (CMN) to enhance the encoder-decoder framework for radiology report generation, where a shared memory is designed to record the alignment between images and texts so as to facilitate the interaction and generation across modalities. Experimental results illustrate the effectiveness of our proposed model, where state-of-the-art performance is achieved on two widely used benchmark datasets, i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is able to better align information from radiology images and texts so as to help generating more accurate reports in terms of clinical indicators.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "referenceCount": 48, "citationCount": 68, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.459.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-04-28", "journal": {"name": "ArXiv", "volume": "abs/2204.13258"}, "authors": [{"authorId": "46843171", "name": "Zhihong Chen"}, {"authorId": "2115437031", "name": "Yaling Shen"}, {"authorId": "1922182598", "name": "Yan Song"}, {"authorId": "2112616424", "name": "Xiang Wan"}]}, {"paperId": "4026ae4e613e3e235ac9c999cccce31edeadf5f5", "externalIds": {"DBLP": "conf/acl/KanclerzFGKKPK20", "ACL": "2021.acl-long.460", "DOI": "10.18653/v1/2021.acl-long.460", "CorpusId": 236459875}, "corpusId": 236459875, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4026ae4e613e3e235ac9c999cccce31edeadf5f5", "title": "Controversy and Conformity: from Generalized to Personalized Aggressiveness Detection", "abstract": "There is content such as hate speech, offensive, toxic or aggressive documents, which are perceived differently by their consumers. They are commonly identified using classifiers solely based on textual content that generalize pre-agreed meanings of difficult problems. Such models provide the same results for each user, which leads to high misclassification rate observable especially for contentious, aggressive documents. Both document controversy and user nonconformity require new solutions. Therefore, we propose novel personalized approaches that respect individual beliefs expressed by either user conformity-based measures or various embeddings of their previous text annotations. We found that only a few annotations of most controversial documents are enough for all our personalization methods to significantly outperform classic, generalized solutions. The more controversial the content, the greater the gain. The personalized solutions may be used to efficiently filter unwanted aggressive content in the way adjusted to a given person.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 64, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.460.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5915-5926"}, "authors": [{"authorId": "2007286374", "name": "Kamil Kanclerz"}, {"authorId": "2120746557", "name": "Alicja Figas"}, {"authorId": "2120757466", "name": "Marcin Gruza"}, {"authorId": "1787971", "name": "Tomasz Kajdanowicz"}, {"authorId": "2905929", "name": "Jan Koco\u0144"}, {"authorId": "2004053871", "name": "Daria Puchalska"}, {"authorId": "1724788", "name": "Przemyslaw Kazienko"}]}, {"paperId": "73850065ad0a76b3ca85f356e11bed7eb71409f9", "externalIds": {"DBLP": "conf/acl/LiuH0B20", "ACL": "2021.acl-long.461", "DOI": "10.18653/v1/2021.acl-long.461", "CorpusId": 236459878}, "corpusId": 236459878, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/73850065ad0a76b3ca85f356e11bed7eb71409f9", "title": "Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews", "abstract": "As more and more product reviews are posted in both text and images, Multimodal Review Analysis (MRA) becomes an attractive research topic. Among the existing review analysis tasks, helpfulness prediction on review text has become predominant due to its importance for e-commerce platforms and online shops, i.e. helping customers quickly acquire useful product information. This paper proposes a new task Multimodal Review Helpfulness Prediction (MRHP) aiming to analyze the review helpfulness from text and visual modalities. Meanwhile, a novel Multi-perspective Coherent Reasoning method (MCR) is proposed to solve the MRHP task, which conducts joint reasoning over texts and images from both the product and the review, and aggregates the signals to predict the review helpfulness. Concretely, we first propose a product-review coherent reasoning module to measure the intra- and inter-modal coherence between the target product and the review. In addition, we also devise an intra-review coherent reasoning module to identify the coherence between the text content and images of the review, which is a piece of strong evidence for review helpfulness prediction. To evaluate the effectiveness of MCR, we present two newly collected multimodal review datasets as benchmark evaluation resources for the MRHP task. Experimental results show that our MCR method can lead to a performance increase of up to 8.5% as compared to the best performing text-only model. The source code and datasets can be obtained from https://github.com/jhliu17/MCR.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.461.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": null, "journal": {"pages": "5927-5936"}, "authors": [{"authorId": "2108373912", "name": "Junhao Liu"}, {"authorId": "2754618", "name": "Zhen Hai"}, {"authorId": "2144399430", "name": "Min Yang"}, {"authorId": "1996394", "name": "Lidong Bing"}]}, {"paperId": "3efd4b048dd7544333092332bccc3f0aea79f5c7", "externalIds": {"ACL": "2021.acl-long.462", "DBLP": "journals/corr/abs-2106-04970", "ArXiv": "2106.04970", "DOI": "10.18653/v1/2021.acl-long.462", "CorpusId": 235377417}, "corpusId": 235377417, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3efd4b048dd7544333092332bccc3f0aea79f5c7", "title": "Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding", "abstract": "In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield identical predictions to greedy decoding but with significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/Shallow-Aggressive-Decoding.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.462.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-09", "journal": {"name": "ArXiv", "volume": "abs/2106.04970"}, "authors": [{"authorId": "144783277", "name": "Xin Sun"}, {"authorId": "50251691", "name": "Tao Ge"}, {"authorId": "49807919", "name": "Furu Wei"}, {"authorId": "1781885", "name": "Houfeng Wang"}]}, {"paperId": "ce2ed0c0fc58ff34061bd1f8e200c8c5aff71908", "externalIds": {"ACL": "2021.acl-long.463", "DBLP": "conf/acl/ZhouC000NCL20", "DOI": "10.18653/v1/2021.acl-long.463", "CorpusId": 236459913}, "corpusId": 236459913, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ce2ed0c0fc58ff34061bd1f8e200c8c5aff71908", "title": "Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism", "abstract": "The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, existing works either ignore the long-tail of code frequency or the noisy clinical notes. To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation Mechanism. Specifically, an interactive shared representation network targets building connections among codes while modeling the co-occurrence, consequently alleviating the long-tail problem. Moreover, to cope with the noisy text issue, we encourage the model to focus on the clinical note\u2019s noteworthy part and extract valuable information through a self-distillation learning mechanism. Experimental results on two MIMIC datasets demonstrate the effectiveness of our method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 31, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.463.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5948-5957"}, "authors": [{"authorId": "144137458", "name": "Tong Zhou"}, {"authorId": "49776272", "name": "Pengfei Cao"}, {"authorId": "152829071", "name": "Yubo Chen"}, {"authorId": "77397868", "name": "Kang Liu"}, {"authorId": "11447228", "name": "Jun Zhao"}, {"authorId": "2096743949", "name": "Kun Niu"}, {"authorId": "1509643951", "name": "Weifeng Chong"}, {"authorId": "2035396", "name": "Shengping Liu"}]}, {"paperId": "9998875164c1d46c8c41be6f22171e90abf0ccbe", "externalIds": {"DBLP": "conf/acl/HuangLJZCWX20", "ACL": "2021.acl-long.464", "DOI": "10.18653/v1/2021.acl-long.464", "CorpusId": 236460127}, "corpusId": 236460127, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9998875164c1d46c8c41be6f22171e90abf0ccbe", "title": "PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check", "abstract": "Chinese Spelling Check (CSC) is a challenging task due to the complex characteristics of Chinese characters. Statistics reveal that most Chinese spelling errors belong to phonological or visual errors. However, previous methods rarely utilize phonological and morphological knowledge of Chinese characters or heavily rely on external resources to model their similarities. To address the above issues, we propose a novel end-to-end trainable model called PHMOSpell, which promotes the performance of CSC with multi-modal information. Specifically, we derive pinyin and glyph representations for Chinese characters from audio and visual modalities respectively, which are integrated into a pre-trained language model by a well-designed adaptive gating mechanism. To verify its effectiveness, we conduct comprehensive experiments and ablation tests. Experimental results on three shared benchmarks demonstrate that our model consistently outperforms previous state-of-the-art models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 38, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.464.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5958-5967"}, "authors": [{"authorId": "2189440923", "name": "Li Huang"}, {"authorId": "2109013120", "name": "Junjie Li"}, {"authorId": "2152124239", "name": "Weiwei Jiang"}, {"authorId": "2140306515", "name": "Zhiyu Zhang"}, {"authorId": "1391197327", "name": "Minchuan Chen"}, {"authorId": "47673210", "name": "Shaojun Wang"}, {"authorId": "2144570451", "name": "Jing Xiao"}]}, {"paperId": "694d83d3430d054a99eed027f58ebfe00de6e46f", "externalIds": {"DBLP": "conf/acl/ChengLLZLLZ20", "ArXiv": "2105.11698", "ACL": "2021.acl-long.465", "DOI": "10.18653/v1/2021.acl-long.465", "CorpusId": 235186933}, "corpusId": 235186933, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/694d83d3430d054a99eed027f58ebfe00de6e46f", "title": "Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting", "abstract": "This paper explores the task of Difficulty-Controllable Question Generation (DCQG), which aims at generating questions with required difficulty levels. Previous research on this task mainly defines the difficulty of a question as whether it can be correctly answered by a Question Answering (QA) system, lacking interpretability and controllability. In our work, we redefine question difficulty as the number of inference steps required to answer it and argue that Question Generation (QG) systems should have stronger control over the logic of generated questions. To this end, we propose a novel framework that progressively increases question difficulty through step-by-step rewriting under the guidance of an extracted reasoning chain. A dataset is automatically constructed to facilitate the research, on which extensive experiments are conducted to test the performance of our method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 22, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.465.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-25", "journal": {"name": "ArXiv", "volume": "abs/2105.11698"}, "authors": [{"authorId": "2117232627", "name": "Yi Cheng"}, {"authorId": "2118154632", "name": "Siyao Li"}, {"authorId": "7648396", "name": "Bang Liu"}, {"authorId": "2567863", "name": "Ruihui Zhao"}, {"authorId": "48831399", "name": "Sujian Li"}, {"authorId": "2268783", "name": "Chenghua Lin"}, {"authorId": "2145273405", "name": "Yefeng Zheng"}]}, {"paperId": "ef358603d387ca392b889b18b8314e08cdc5d83b", "externalIds": {"DBLP": "conf/acl/LiMYH20", "ACL": "2021.acl-long.466", "DOI": "10.18653/v1/2021.acl-long.466", "CorpusId": 236460336}, "corpusId": 236460336, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/ef358603d387ca392b889b18b8314e08cdc5d83b", "title": "Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation", "abstract": "Table-to-text generation aims at automatically generating natural text to help people conveniently obtain salient information in tables. Although neural models for table-to-text have achieved remarkable progress, some problems are still overlooked. Previous methods cannot deduce the factual results from the entity\u2019s (player or team) performance and the relations between entities. To solve this issue, we first build an entity graph from the input tables and introduce a reasoning module to perform reasoning on the graph. Moreover, there are different relations (e.g., the numeric size relation and the importance relation) between records in different dimensions. And these relations may contribute to the data-to-text generation. However, it is hard for a vanilla encoder to capture these. Consequently, we propose to utilize two auxiliary tasks, Number Ranking (NR) and Importance Ranking (IR), to supervise the encoder to capture the different relations. Experimental results on ROTOWIRE and RW-FG show that our method not only has a good generalization but also outperforms previous methods on several metrics: BLEU, Content Selection, Content Ordering.", "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing", "year": 2021, "referenceCount": 23, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.466.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"name": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing", "pages": "5979\u20135989", "volume": "1"}, "authors": [{"authorId": "2154884699", "name": "Liang Li"}, {"authorId": "2112563315", "name": "Can Ma"}, {"authorId": "35755264", "name": "Yinliang Yue"}, {"authorId": "2070034418", "name": "Dayong Hu"}]}, {"paperId": "3a25583c170d7b3eef568890ccffd8a500ac9b3e", "externalIds": {"DBLP": "conf/acl/YangLLQL20", "ACL": "2021.acl-long.467", "DOI": "10.18653/v1/2021.acl-long.467", "CorpusId": 236459948}, "corpusId": 236459948, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3a25583c170d7b3eef568890ccffd8a500ac9b3e", "title": "POS-Constrained Parallel Decoding for Non-autoregressive Generation", "abstract": "The multimodality problem has become a major challenge of existing non-autoregressive generation (NAG) systems. A common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as \u201cteacher AG\u201d). The success of such methods may largely depend on a latent assumption, i.e., the teacher AG is superior to the NAG model. However, in this work, we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation. To provide a feasible solution to the multimodality problem of NAG, we propose incorporating linguistic structure (Part-of-Speech sequence in particular) into NAG inference instead of relying on teacher AG. More specifically, the proposed POS-constrained Parallel Decoding (POSPD) method aims at providing a specific POS sequence to constrain the NAG model during decoding. Our experiments demonstrate that POSPD consistently improves NAG models on four text generation tasks to a greater extent compared to knowledge distillation. This observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.467.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "5990-6000"}, "authors": [{"authorId": "2004672900", "name": "Kexin Yang"}, {"authorId": "39165620", "name": "Wenqiang Lei"}, {"authorId": "2004587660", "name": "Dayiheng Liu"}, {"authorId": "15629561", "name": "Weizhen Qi"}, {"authorId": "2075420316", "name": "Jiancheng Lv"}]}, {"paperId": "58e13e1fed9b28e50efcaff611772801d7980c80", "externalIds": {"ACL": "2021.acl-long.468", "DBLP": "conf/acl/LiuYLZLZZS20", "ArXiv": "2106.06125", "DOI": "10.18653/v1/2021.acl-long.468", "CorpusId": 235417501}, "corpusId": 235417501, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/58e13e1fed9b28e50efcaff611772801d7980c80", "title": "Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation", "abstract": "A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary.This potentially weakens the effect when applying pretrained models into natural language generation (NLG) tasks, especially for the subword distributions between upstream and downstream tasks with significant discrepancy. Towards approaching this problem, we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step. Specifically, a plug-and-play embedding generator is introduced to produce the representation of any input token, according to pre-trained embeddings of its morphologically similar ones.Thus, embeddings of mismatch tokens in downstream tasks can also be efficiently initialized.We conduct experiments on a variety of NLG tasks under the pretrain-finetune fashion. Experimental results and extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 49, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.468.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-11", "journal": {"pages": "6001-6011"}, "authors": [{"authorId": "2120099874", "name": "Xin Liu"}, {"authorId": "21299583", "name": "Baosong Yang"}, {"authorId": "2004587660", "name": "Dayiheng Liu"}, {"authorId": "2111126458", "name": "Haibo Zhang"}, {"authorId": "1935569", "name": "Weihua Luo"}, {"authorId": "2156053262", "name": "Min Zhang"}, {"authorId": "2118012977", "name": "Haiying Zhang"}, {"authorId": "34739384", "name": "Jinsong Su"}]}, {"paperId": "1db86e01300e2f30fd08b46e63ea11656cb6dcf5", "externalIds": {"DBLP": "conf/acl/HePLLX20", "ACL": "2021.acl-long.469", "DOI": "10.18653/v1/2021.acl-long.469", "CorpusId": 236460248}, "corpusId": 236460248, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1db86e01300e2f30fd08b46e63ea11656cb6dcf5", "title": "TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models", "abstract": "In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 59, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.469.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6012-6025"}, "authors": [{"authorId": "2143775574", "name": "Jie He"}, {"authorId": "145560079", "name": "Bo Peng"}, {"authorId": "2048004675", "name": "Yi Liao"}, {"authorId": "2115900360", "name": "Qun Liu"}, {"authorId": "2694222", "name": "Deyi Xiong"}]}, {"paperId": "f4566761fe39c4b5273d696d9bc3f4195c9325bb", "externalIds": {"MAG": "3173001334", "ArXiv": "2105.03801", "DBLP": "conf/acl/ManakulG20", "ACL": "2021.acl-long.470", "DOI": "10.18653/v1/2021.acl-long.470", "CorpusId": 235262720}, "corpusId": 235262720, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f4566761fe39c4b5273d696d9bc3f4195c9325bb", "title": "Long-Span Summarization via Local Attention and Content Selection", "abstract": "Transformer-based models have achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks including document summarization. Typically these systems are trained by fine-tuning a large pre-trained model to the target task. One issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows. Thus, for long document summarization, it can be challenging to train or fine-tune these models. In this work, we exploit large pre-trained transformer-based models and address long-span dependencies in abstractive summarization using two methods: local self-attention; and explicit content selection. These approaches are compared on a range of network configurations. Experiments are carried out on standard long-span summarization tasks, including Spotify Podcast, arXiv, and PubMed datasets. We demonstrate that by combining these methods, we can achieve state-of-the-art results on all three tasks in the ROUGE scores. Moreover, without a large-scale GPU card, our approach can achieve comparable or better results than existing approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 65, "citationCount": 30, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.470.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-08", "journal": {"pages": "6026-6041"}, "authors": [{"authorId": "89355510", "name": "Potsawee Manakul"}, {"authorId": "1740397", "name": "M. Gales"}]}, {"paperId": "fe4b825d049c4e982da70cc10a2db8cbe5851cd8", "externalIds": {"DBLP": "conf/acl/FuZWLSY20", "ACL": "2021.acl-long.471", "DOI": "10.18653/v1/2021.acl-long.471", "CorpusId": 236460120}, "corpusId": 236460120, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fe4b825d049c4e982da70cc10a2db8cbe5851cd8", "title": "RepSum: Unsupervised Dialogue Summarization based on Replacement Strategy", "abstract": "In the field of dialogue summarization, due to the lack of training data, it is often difficult for supervised summary generation methods to learn vital information from dialogue context with limited data. Several attempts on unsupervised summarization for text by leveraging semantic information solely or auto-encoder strategy (i.e., sentence compression), it however cannot be adapted to the dialogue scene due to the limited words in utterances and huge gap between the dialogue and its summary. In this study, we propose a novel unsupervised strategy to address this challenge, which roots from the hypothetical foundation that a superior summary approximates a replacement of the original dialogue, and they are roughly equivalent for auxiliary (self-supervised) tasks, e.g., dialogue generation. The proposed strategy RepSum is applied to generate both extractive and abstractive summary with the guidance of the followed n\u02c6th utterance generation and classification tasks. Extensive experiments on various datasets demonstrate the superiority of the proposed model compared with the state-of-the-art methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 10, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.471.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6042-6051"}, "authors": [{"authorId": "153000866", "name": "Xiyan Fu"}, {"authorId": "51146612", "name": "Yating Zhang"}, {"authorId": "2118914502", "name": "Tianyi Wang"}, {"authorId": "1713802", "name": "Xiaozhong Liu"}, {"authorId": "2060934", "name": "Changlong Sun"}, {"authorId": "2149231521", "name": "Zhenglu Yang"}]}, {"paperId": "a62209a3f90a5bb23054b0a126f5f5f23b9e4b53", "externalIds": {"ACL": "2021.acl-long.472", "ArXiv": "2105.12041", "DBLP": "conf/acl/WuLXLCL0020", "DOI": "10.18653/v1/2021.acl-long.472", "CorpusId": 235187330}, "corpusId": 235187330, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a62209a3f90a5bb23054b0a126f5f5f23b9e4b53", "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph", "abstract": "Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 63, "citationCount": 33, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.472.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-25", "journal": {"pages": "6052-6067"}, "authors": [{"authorId": "2139644141", "name": "Wenhao Wu"}, {"authorId": "48624966", "name": "Wei Li"}, {"authorId": "2107521158", "name": "Xinyan Xiao"}, {"authorId": null, "name": "Jiachen Liu"}, {"authorId": "2314396", "name": "Ziqiang Cao"}, {"authorId": "48831399", "name": "Sujian Li"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "144270731", "name": "Haifeng Wang"}]}, {"paperId": "85bc3c3707e514430ed29dfedc454d5f77630f46", "externalIds": {"DBLP": "conf/acl/ChenALG00020", "ACL": "2021.acl-long.473", "DOI": "10.18653/v1/2021.acl-long.473", "CorpusId": 236460044}, "corpusId": 236460044, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/85bc3c3707e514430ed29dfedc454d5f77630f46", "title": "Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation", "abstract": "Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relation-aware Related work Generator (RRG), which generates an abstractive related work from the given multiple scientific papers in the same research area. Concretely, we propose a relation-aware multi-document encoder that relates one document to another according to their content dependency in a relation graph. The relation graph and the document representation are interacted and polished iteratively, complementing each other in the training process. We also contribute two public datasets composed of related work sections and their corresponding papers. Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines. We hope that this work will promote advances in related work generation task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 24, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.473.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": null, "journal": {"pages": "6068-6077"}, "authors": [{"authorId": "2116950235", "name": "Xiuying Chen"}, {"authorId": "1753416672", "name": "Hind Alamro"}, {"authorId": "9255404", "name": "Li Mingzhe"}, {"authorId": "2112311595", "name": "Shen Gao"}, {"authorId": "2928371", "name": "Xiangliang Zhang"}, {"authorId": "144060462", "name": "Dongyan Zhao"}, {"authorId": "144539156", "name": "Rui Yan"}]}, {"paperId": "630b19f8a856177dc9fcab3222cacb47f77985c3", "externalIds": {"DBLP": "journals/corr/abs-2105-11921", "ArXiv": "2105.11921", "ACL": "2021.acl-long.474", "DOI": "10.18653/v1/2021.acl-long.474", "CorpusId": 235186832}, "corpusId": 235186832, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/630b19f8a856177dc9fcab3222cacb47f77985c3", "title": "Focus Attention: Promoting Faithfulness and Diversity in Summarization", "abstract": "Professional summaries are written with document-level information, such as the theme of the document, in mind. This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step. With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. Further, we propose a Focus Sampling method to enable generation of diverse summaries, an area currently understudied in summarization. When evaluated on the BBC extreme summarization task, two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents, outperforming their vanilla counterparts on ROUGE and multiple faithfulness measures. We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-k or nucleus sampling-based decoding methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 91, "citationCount": 29, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.474.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-25", "journal": {"name": "ArXiv", "volume": "abs/2105.11921"}, "authors": [{"authorId": "19509693", "name": "Rahul Aralikatte"}, {"authorId": "143790499", "name": "Shashi Narayan"}, {"authorId": "2105162819", "name": "Joshua Maynez"}, {"authorId": "2204815", "name": "S. Rothe"}, {"authorId": "143957226", "name": "Ryan T. McDonald"}]}, {"paperId": "94b1bb4c23affc1b02a479e1b0347c1de0b17b11", "externalIds": {"DBLP": "conf/acl/XuL20", "ACL": "2021.acl-long.475", "ArXiv": "2012.14774", "DOI": "10.18653/v1/2021.acl-long.475", "CorpusId": 235266283}, "corpusId": 235266283, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/94b1bb4c23affc1b02a479e1b0347c1de0b17b11", "title": "Generating Query Focused Summaries from Query-Free Resources", "abstract": "The availability of large-scale datasets has driven the development of neural models that create generic summaries from single or multiple documents. In this work we consider query focused summarization (QFS), a task for which training data in the form of queries, documents, and summaries is not readily available. We propose to decompose QFS into (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model. Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 24, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.475.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-29", "journal": {"pages": "6096-6109"}, "authors": [{"authorId": "115986373", "name": "Yumo Xu"}, {"authorId": "1747893", "name": "Mirella Lapata"}]}, {"paperId": "a0c51191f7b3fa8207b4401903c10b87e2458129", "externalIds": {"ArXiv": "2107.03242", "ACL": "2021.acl-long.476", "DBLP": "journals/corr/abs-2107-03242", "DOI": "10.18653/v1/2021.acl-long.476", "CorpusId": 235755349}, "corpusId": 235755349, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a0c51191f7b3fa8207b4401903c10b87e2458129", "title": "Robustifying Multi-hop QA through Pseudo-Evidentiality Training", "abstract": "This paper studies the bias problem of multi-hop question answering models, of answering correctly without correct reasoning. One way to robustify these models is by supervising to not only answer right, but also with right reasoning chains. An existing direction is to annotate reasoning chains to train models, requiring expensive additional annotations. In contrast, we propose a new approach to learn evidentiality, deciding whether the answer prediction is supported by correct evidences, without such annotations. Instead, we compare counterfactual changes in answer confidence with and without evidence sentences, to generate \u201cpseudo-evidentiality\u201d annotations. We validate our proposed model on an original set and challenge set in HotpotQA, showing that our method is accurate and robust in multi-hop reasoning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 11, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.476.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-07", "journal": {"name": "ArXiv", "volume": "abs/2107.03242"}, "authors": [{"authorId": "79733119", "name": "Kyungjae Lee"}, {"authorId": "1716415", "name": "Seung-won Hwang"}, {"authorId": "2115652956", "name": "Sanghyun Han"}, {"authorId": "2135607329", "name": "Dohyeon Lee"}]}, {"paperId": "cd2f3398727bf3d5ccef42b33ad9097aeb1c44f1", "externalIds": {"ACL": "2021.acl-long.477", "DBLP": "conf/acl/YangWJJY20", "DOI": "10.18653/v1/2021.acl-long.477", "CorpusId": 236459960}, "corpusId": 236459960, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cd2f3398727bf3d5ccef42b33ad9097aeb1c44f1", "title": "xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering", "abstract": "Dense passage retrieval has been shown to be an effective approach for information retrieval tasks such as open domain question answering. Under this paradigm, a dual-encoder model is learned to encode questions and passages separately into vector representations, and all the passage vectors are then pre-computed and indexed, which can be efficiently retrieved by vector space search during inference time. In this paper, we propose a new contrastive learning method called Cross Momentum Contrastive learning (xMoCo), for learning a dual-encoder model for question-passage matching. Our method efficiently maintains a large pool of negative samples like the original MoCo, and by jointly optimizing question-to-passage and passage-to-question matching tasks, enables using separate encoders for questions and passages. We evaluate our method on various open-domain question answering dataset, and the experimental results show the effectiveness of the proposed method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 24, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.477.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6120-6129"}, "authors": [{"authorId": "144610884", "name": "Nan Yang"}, {"authorId": "49807919", "name": "Furu Wei"}, {"authorId": "24128606", "name": "Binxing Jiao"}, {"authorId": "71790825", "name": "Daxin Jiang"}, {"authorId": "7866194", "name": "Linjun Yang"}]}, {"paperId": "653927114923a82bbe92e4872e5dd555f078c056", "externalIds": {"DBLP": "conf/acl/KimKPK20", "ArXiv": "2106.11575", "ACL": "2021.acl-long.478", "DOI": "10.18653/v1/2021.acl-long.478", "CorpusId": 235593227}, "corpusId": 235593227, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/653927114923a82bbe92e4872e5dd555f078c056", "title": "Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering", "abstract": "One of the main challenges in conversational question answering (CQA) is to resolve the conversational dependency, such as anaphora and ellipsis. However, existing approaches do not explicitly train QA models on how to resolve the dependency, and thus these models are limited in understanding human dialogues. In this paper, we propose a novel framework, ExCorD (Explicit guidance on how to resolve Conversational Dependency) to enhance the abilities of QA models in comprehending conversational context. ExCorD first generates self-contained questions that can be understood without the conversation history, then trains a QA model with the pairs of original and self-contained questions using a consistency-based regularizer. In our experiments, we demonstrate that ExCorD significantly improves the QA models\u2019 performance by up to 1.2 F1 on QuAC, and 5.2 F1 on CANARD, while addressing the limitations of the existing approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 29, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.478.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-22", "journal": {"name": "ArXiv", "volume": "abs/2106.11575"}, "authors": [{"authorId": "1390543205", "name": "Gangwoo Kim"}, {"authorId": "3087706", "name": "Hyunjae Kim"}, {"authorId": "2118946797", "name": "Jungsoo Park"}, {"authorId": null, "name": "Jaewoo Kang"}]}, {"paperId": "cdd00f4a6741011313f331a6705c841ce2b4479c", "externalIds": {"ACL": "2021.acl-long.479", "DBLP": "conf/acl/ZangLWSZC20", "ArXiv": "2108.01453", "DOI": "10.18653/v1/2021.acl-long.479", "CorpusId": 236459946}, "corpusId": 236459946, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cdd00f4a6741011313f331a6705c841ce2b4479c", "title": "PhotoChat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling", "abstract": "We present a new human-human dialogue dataset - PhotoChat, the first dataset that casts light on the photo sharing behavior in online messaging. PhotoChat contains 12k dialogues, each of which is paired with a user photo that is shared during the conversation. Based on this dataset, we propose two tasks to facilitate research on image-text modeling: a photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn, and a photo retrieval task that retrieves the most relevant photo according to the dialogue context. In addition, for both tasks, we provide baseline models using the state-of-the-art models and report their benchmark performances. The best image retrieval model achieves 10.4% recall@1 (out of 1000 candidates) and the best photo intent prediction model achieves 58.1% F1 score, indicating that the dataset presents interesting yet challenging real-world problems. We are releasing PhotoChat to facilitate future research work among the community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 31, "citationCount": 23, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.479.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-06", "journal": {"name": "ArXiv", "volume": "abs/2108.01453"}, "authors": [{"authorId": "9761407", "name": "Xiaoxue Zang"}, {"authorId": "2116184149", "name": "Lijuan Liu"}, {"authorId": "2124703629", "name": "Maria Wang"}, {"authorId": "2157993372", "name": "Yang Song"}, {"authorId": "2144614295", "name": "Hao Zhang"}, {"authorId": "2108161860", "name": "Jindong Chen"}]}, {"paperId": "65ebd6debde06e4e41dab9fed0ff142e5ec12561", "externalIds": {"DBLP": "conf/acl/WuKBLK20", "ArXiv": "2105.14462", "ACL": "2021.acl-long.480", "DOI": "10.18653/v1/2021.acl-long.480", "CorpusId": 235254796}, "corpusId": 235254796, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/65ebd6debde06e4e41dab9fed0ff142e5ec12561", "title": "Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation", "abstract": "A neural multimodal machine translation (MMT) system is one that aims to perform better translation by extending conventional text-only translation models with multimodal information. Many recent studies report improvements when equipping their models with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of multimodal information in MMT by devising two interpretable MMT models. To our surprise, although our models replicate similar gains as recently developed multimodal-integrated systems achieved, our models learn to ignore the multimodal information. Upon further investigation, we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect. We report empirical findings that highlight the importance of MMT models\u2019 interpretability, and discuss how our findings will benefit future research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 53, "citationCount": 40, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.480.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-30", "journal": {"pages": "6153-6166"}, "authors": [{"authorId": "150358371", "name": "Zhiyong Wu"}, {"authorId": "47648549", "name": "Lingpeng Kong"}, {"authorId": "145001705", "name": "W. Bi"}, {"authorId": "2144440743", "name": "Xiang Li"}, {"authorId": "2064074010", "name": "B. Kao"}]}, {"paperId": "9c2f872cad55d691d484b42c2a8dd95b1222a958", "externalIds": {"DBLP": "conf/acl/SeoKPZ20", "ACL": "2021.acl-long.481", "ArXiv": "2106.10446", "DOI": "10.18653/v1/2021.acl-long.481", "CorpusId": 235490311}, "corpusId": 235490311, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9c2f872cad55d691d484b42c2a8dd95b1222a958", "title": "Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering", "abstract": "Video Question Answering is a task which requires an AI agent to answer questions grounded in video. This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information. We propose Motion-Appearance Synergistic Networks (MASN), which embed two cross-modal features grounded on motion and appearance information and selectively utilize them depending on the question\u2019s intentions. MASN consists of a motion module, an appearance module, and a motion-appearance fusion module. The motion module computes the action-oriented cross-modal joint representations, while the appearance module focuses on the appearance aspect of the input video. Finally, the motion-appearance fusion module takes each output of the motion module and the appearance module as input, and performs question-guided fusion. As a result, MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets. We also conduct qualitative analysis by visualizing the inference results of MASN.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 36, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.481.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-19", "journal": {"pages": "6167-6177"}, "authors": [{"authorId": "1679974562", "name": "Ahjeong Seo"}, {"authorId": "71119060", "name": "Gi-Cheon Kang"}, {"authorId": "121638716", "name": "J. Park"}, {"authorId": "1692756", "name": "Byoung-Tak Zhang"}]}, {"paperId": "92d9231cf56d7d5cf388cb33c4a2cfc00f8cbef8", "externalIds": {"ArXiv": "2105.12848", "DBLP": "journals/corr/abs-2105-12848", "ACL": "2021.acl-long.482", "DOI": "10.18653/v1/2021.acl-long.482", "CorpusId": 235212113}, "corpusId": 235212113, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/92d9231cf56d7d5cf388cb33c4a2cfc00f8cbef8", "title": "BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition", "abstract": "We study the problem of learning a named entity recognition (NER) tagger using noisy labels from multiple weak supervision sources. Though cheap to obtain, the labels from weak supervision sources are often incomplete, inaccurate, and contradictory, making it difficult to learn an accurate NER model. To address this challenge, we propose a conditional hidden Markov model (CHMM), which can effectively infer true labels from multi-source noisy labels in an unsupervised way. CHMM enhances the classic hidden Markov model with the contextual representation power of pre-trained language models. Specifically, CHMM learns token-wise transition and emission probabilities from the BERT embeddings of the input tokens to infer the latent true labels from noisy observations. We further refine CHMM with an alternate-training approach (CHMM-ALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM, and this BERT-NER\u2019s output is regarded as an additional weak source to train the CHMM in return. Experiments on four NER benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised NER models by wide margins.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 23, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.482.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-26", "journal": {"name": "ArXiv", "volume": "abs/2105.12848"}, "authors": [{"authorId": "1527089853", "name": "Yinghao Li"}, {"authorId": "47092843", "name": "Pranav Shetty"}, {"authorId": "2145288039", "name": "Lu Liu"}, {"authorId": "2152735278", "name": "Chao Zhang"}, {"authorId": "2143477036", "name": "Le Song"}]}, {"paperId": "04885b8ff6cad73eba0905e515428d1bdf3088b3", "externalIds": {"DBLP": "conf/acl/ChenST00Z20", "ArXiv": "2106.10855", "ACL": "2021.acl-long.483", "DOI": "10.18653/v1/2021.acl-long.483", "CorpusId": 235490132}, "corpusId": 235490132, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/04885b8ff6cad73eba0905e515428d1bdf3088b3", "title": "CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction", "abstract": "The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task. For the past decade, researchers apply the multi-instance learning (MIL) framework to find the most reliable feature from a bag of sentences. Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful sentence features in the datasets. In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. Therefore, the performance of distantly supervised RE models is bounded. In this paper, we go beyond typical MIL framework and propose a novel contrastive instance learning (CIL) framework. Specifically, we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance. Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on NYT10, GDS and KBP.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 31, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.483.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-21", "journal": {"name": "ArXiv", "volume": "abs/2106.10855"}, "authors": [{"authorId": "97807824", "name": "Tao Chen"}, {"authorId": "1993631009", "name": "Haizhou Shi"}, {"authorId": "2118071462", "name": "Siliang Tang"}, {"authorId": "1390759484", "name": "Zhigang Chen"}, {"authorId": "144894849", "name": "Fei Wu"}, {"authorId": "2056432541", "name": "Y. Zhuang"}]}, {"paperId": "72bcf9667e9809f5391a29c7adc5bb6bf702a471", "externalIds": {"ArXiv": "2106.11566", "DBLP": "journals/corr/abs-2106-11566", "ACL": "2021.acl-long.484", "DOI": "10.18653/v1/2021.acl-long.484", "CorpusId": 235593061}, "corpusId": 235593061, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/72bcf9667e9809f5391a29c7adc5bb6bf702a471", "title": "SENT: Sentence-level Distant Relation Extraction via Negative Training", "abstract": "Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type. Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance. In this work, we propose the use of negative training (NT), in which a model is trained using complementary labels regarding that \u201cthe instance does not belong to these complementary labels\u201d. Since the probability of selecting a true label as a complementary label is low, NT provides less noisy information. Furthermore, the model trained with NT is able to separate the noisy data from the training data. Based on NT, we propose a sentence-level framework, SENT, for distant relation extraction. SENT not only filters the noisy data to construct a cleaner dataset, but also performs a re-labeling process to transform the noisy data into useful training data, thus further benefiting the model\u2019s performance. Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 60, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.484.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-22", "journal": {"name": "ArXiv", "volume": "abs/2106.11566"}, "authors": [{"authorId": "151482614", "name": "Ruotian Ma"}, {"authorId": "2067331064", "name": "Tao Gui"}, {"authorId": "2107897400", "name": "Linyang Li"}, {"authorId": "1409702669", "name": "Qi Zhang"}, {"authorId": "2110347068", "name": "Yaqian Zhou"}, {"authorId": "1790227", "name": "Xuanjing Huang"}]}, {"paperId": "49804b6edc7644af7482171239ab9945c35cb77f", "externalIds": {"DBLP": "conf/acl/ZhouC0Y20", "ACL": "2021.acl-long.485", "DOI": "10.18653/v1/2021.acl-long.485", "CorpusId": 236460038}, "corpusId": 236460038, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/49804b6edc7644af7482171239ab9945c35cb77f", "title": "An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization", "abstract": "Medical named entity recognition (NER) and normalization (NEN) are fundamental for constructing knowledge graphs and building QA systems. Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks. The mispredicted mentions from NER will directly influence the results of NEN. Therefore, the NER module is the bottleneck of the whole system. Besides, the learnable features for both tasks are beneficial to improving the model performance. To avoid the disadvantages of existing models and exploit the generalized representation across the two tasks, we design an end-to-end progressive multi-task learning model for jointly modeling medical NER and NEN in an effective way. There are three level tasks with progressive difficulty in the framework. The progressive tasks can reduce the error propagation with the incremental task settings which implies the lower level tasks gain the supervised signals other than errors from the higher level tasks to improve their performances. Besides, the context features are exploited to enrich the semantic information of entity mentions extracted by NER. The performance of NEN profits from the enhanced entity mention features. The standard entities from knowledge bases are introduced into the NER module for extracting corresponding entity mentions correctly. The empirical results on two publicly available medical literature datasets demonstrate the superiority of our method over nine typical methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 16, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.485.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6214-6224"}, "authors": [{"authorId": "2109060719", "name": "Baohang Zhou"}, {"authorId": "3127329", "name": "Xiangrui Cai"}, {"authorId": "100996634", "name": "Ying Zhang"}, {"authorId": "1721029", "name": "Xiaojie Yuan"}]}, {"paperId": "7b66a5dd12d4b262138dc5864e908bc87f2d919a", "externalIds": {"ACL": "2021.acl-long.486", "DBLP": "journals/corr/abs-2106-09895", "ArXiv": "2106.09895", "DOI": "10.18653/v1/2021.acl-long.486", "CorpusId": 235485451}, "corpusId": 235485451, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7b66a5dd12d4b262138dc5864e908bc87f2d919a", "title": "PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction", "abstract": "Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction. Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency. In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence (PRGC). Specifically, we design a component to predict potential relations, which constrains the following entity extraction to the predicted relation subset rather than all relations; then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects; finally, a global correspondence component is designed to align the subject and object into a triple with low-complexity. Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples. The source code has been submitted as the supplementary material and will be made publicly available after the blind review.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 33, "citationCount": 88, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.486.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-06-18", "journal": {"name": "ArXiv", "volume": "abs/2106.09895"}, "authors": [{"authorId": "2115584612", "name": "Heng Zheng"}, {"authorId": "1381230088", "name": "Rui Wen"}, {"authorId": "2145307827", "name": "Xi Chen"}, {"authorId": "1845781852", "name": "Yifan Yang"}, {"authorId": "2145046450", "name": "Yunyan Zhang"}, {"authorId": "2030976630", "name": "Ziheng Zhang"}, {"authorId": "2608639", "name": "Ningyu Zhang"}, {"authorId": "2113838416", "name": "Bin Qin"}, {"authorId": "2153555985", "name": "Ming Xu"}, {"authorId": "2145273405", "name": "Yefeng Zheng"}]}, {"paperId": "4a534218cde7fd2a2efcc05b6f541b4c34596079", "externalIds": {"DBLP": "journals/corr/abs-2106-15167", "ACL": "2021.acl-long.487", "ArXiv": "2106.15167", "DOI": "10.18653/v1/2021.acl-long.487", "CorpusId": 235669988}, "corpusId": 235669988, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4a534218cde7fd2a2efcc05b6f541b4c34596079", "title": "Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition", "abstract": "Few-shot Named Entity Recognition (NER) exploits only a handful of annotations to iden- tify and classify named entity mentions. Pro- totypical network shows superior performance on few-shot NER. However, existing prototyp- ical methods fail to differentiate rich seman- tics in other-class words, which will aggravate overfitting under few shot scenario. To address the issue, we propose a novel model, Mining Undefined Classes from Other-class (MUCO), that can automatically induce different unde- fined classes from the other class to improve few-shot NER. With these extra-labeled unde- fined classes, our method will improve the dis- criminative ability of NER classifier and en- hance the understanding of predefined classes with stand-by semantic knowledge. Experi- mental results demonstrate that our model out- performs five state-of-the-art models in both 1- shot and 5-shots settings on four NER bench- marks. We will release the code upon accep- tance. The source code is released on https: //github.com/shuaiwa16/OtherClassNER.git.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 34, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.487.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-29", "journal": {"pages": "6236-6247"}, "authors": [{"authorId": "152439499", "name": "Meihan Tong"}, {"authorId": "2118512998", "name": "Shuai Wang"}, {"authorId": "2113744169", "name": "Bin Xu"}, {"authorId": "2112867078", "name": "Yixin Cao"}, {"authorId": "2108928441", "name": "Minghui Liu"}, {"authorId": "2055765060", "name": "Lei Hou"}, {"authorId": "8549842", "name": "Juan-Zi Li"}]}, {"paperId": "983fdd94067ff52972a931595bffb8933d6df968", "externalIds": {"DBLP": "conf/acl/LaiJZT20", "ACL": "2021.acl-long.488", "ArXiv": "2105.13456", "DOI": "10.18653/v1/2021.acl-long.488", "CorpusId": 235248430}, "corpusId": 235248430, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/983fdd94067ff52972a931595bffb8933d6df968", "title": "Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference", "abstract": "Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state-of-the-art results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 70, "citationCount": 37, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.488.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-27", "journal": {"name": "ArXiv", "volume": "abs/2105.13456"}, "authors": [{"authorId": "145242558", "name": "T. Lai"}, {"authorId": "2113323573", "name": "Heng Ji"}, {"authorId": "1736467", "name": "ChengXiang Zhai"}, {"authorId": "2536742", "name": "Quan Hung Tran"}]}, {"paperId": "41350b24e0766b40a90fede59adec1531340dea9", "externalIds": {"ACL": "2021.acl-long.489", "DBLP": "conf/acl/ZhangPJEMP20", "DOI": "10.18653/v1/2021.acl-long.489", "CorpusId": 236459985}, "corpusId": 236459985, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/41350b24e0766b40a90fede59adec1531340dea9", "title": "Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation", "abstract": "Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges. First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements. Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge. In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and extract scientific entities and events from English research papers. We perform Abstract Meaning Representation (AMR) to compress the wide context to uncover a clear semantic structure for each complex sentence. Besides, we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model\u2019s understanding of complex scientific concepts. We use an edge-conditioned graph attention network to encode the knowledge-enriched AMR graph for biomedical IE tasks. Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8% and 3.0% absolute F-score gains respectively. In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements, we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature, which can serve as a new benchmark for the biomedical IE community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 17, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.489.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6261-6270"}, "authors": [{"authorId": "2116461591", "name": "Zixuan Zhang"}, {"authorId": "2121305256", "name": "N. Parulian"}, {"authorId": "2113323573", "name": "Heng Ji"}, {"authorId": "144332315", "name": "Ahmed Elsayed"}, {"authorId": "1380290459", "name": "Skatje Myers"}, {"authorId": "38352944", "name": "M. Palmer"}]}, {"paperId": "2cfeea5d8aadaacaba1c693a6af8c3d651ab4fa5", "externalIds": {"DBLP": "conf/acl/VeysehLDN20", "ACL": "2021.acl-long.490", "DOI": "10.18653/v1/2021.acl-long.490", "CorpusId": 236460229}, "corpusId": 236460229, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2cfeea5d8aadaacaba1c693a6af8c3d651ab4fa5", "title": "Unleash GPT-2 Power for Event Detection", "abstract": "Event Detection (ED) aims to recognize mentions of events (i.e., event triggers) and their types in text. Recently, several ED datasets in various domains have been proposed. However, the major limitation of these resources is the lack of enough training data for individual event types which hinders the efficient training of data-hungry deep learning models. To overcome this issue, we propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for ED. To prevent the noises inevitable in automatically generated data from hampering training process, we propose to exploit a teacher-student architecture in which the teacher is supposed to learn anchor knowledge from the original data. The student is then trained on combination of the original and GPT-generated data while being led by the anchor knowledge from the teacher. Optimal transport is introduced to facilitate the anchor knowledge-based guidance between the two networks. We evaluate the proposed model on multiple ED benchmark datasets, gaining consistent improvement and establishing state-of-the-art results for ED.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 61, "citationCount": 36, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.490.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6271-6282"}, "authors": [{"authorId": "3460489", "name": "Amir Pouran Ben Veyseh"}, {"authorId": "1405279380", "name": "Viet Dac Lai"}, {"authorId": "2075390842", "name": "Franck Dernoncourt"}, {"authorId": "1811211", "name": "Thien Huu Nguyen"}]}, {"paperId": "2580aed3ac10d971f86d21f4c06db2de0cfb3c22", "externalIds": {"ArXiv": "2105.14485", "ACL": "2021.acl-long.491", "DBLP": "conf/acl/WangW0L000L020", "DOI": "10.18653/v1/2021.acl-long.491", "CorpusId": 235254286}, "corpusId": 235254286, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2580aed3ac10d971f86d21f4c06db2de0cfb3c22", "title": "CLEVE: Contrastive Pre-training for Event Extraction", "abstract": "Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised \u201cliberal\u201d EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 80, "citationCount": 66, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.491.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-30", "journal": {"pages": "6283-6297"}, "authors": [{"authorId": "1390880371", "name": "Ziqi Wang"}, {"authorId": "48631777", "name": "Xiaozhi Wang"}, {"authorId": "48506411", "name": "Xu Han"}, {"authorId": "2149202150", "name": "Yankai Lin"}, {"authorId": "2055765060", "name": "Lei Hou"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "50492525", "name": "Peng Li"}, {"authorId": "8549842", "name": "Juan-Zi Li"}, {"authorId": "49178343", "name": "Jie Zhou"}]}, {"paperId": "c952b73fd8ac6a7d4ad00d78f6b3b8d1caed6a8f", "externalIds": {"DBLP": "conf/acl/YangS000W20", "ACL": "2021.acl-long.492", "DOI": "10.18653/v1/2021.acl-long.492", "CorpusId": 236460259}, "corpusId": 236460259, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c952b73fd8ac6a7d4ad00d78f6b3b8d1caed6a8f", "title": "Document-level Event Extraction via Parallel Prediction Networks", "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 37, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.492.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6298-6308"}, "authors": [{"authorId": "1845787839", "name": "Hang Yang"}, {"authorId": "1381062467", "name": "Dianbo Sui"}, {"authorId": "152829071", "name": "Yubo Chen"}, {"authorId": "77397868", "name": "Kang Liu"}, {"authorId": "11447228", "name": "Jun Zhao"}, {"authorId": "1799672", "name": "Taifeng Wang"}]}, {"paperId": "0fc66c750d06da8dc47e7a3ae9f24af9ff3d6617", "externalIds": {"DBLP": "conf/acl/LiBY0HHS20", "ACL": "2021.acl-long.493", "ArXiv": "2105.11210", "DOI": "10.18653/v1/2021.acl-long.493", "CorpusId": 235166279}, "corpusId": 235166279, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0fc66c750d06da8dc47e7a3ae9f24af9ff3d6617", "title": "StructuralLM: Structural Pre-training for Form Understanding", "abstract": "Large pre-trained language models achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, they almost exclusively focus on text-only representation, while neglecting cell-level layout information that is important for form image understanding. In this paper, we propose a new pre-training approach, StructuralLM, to jointly leverage cell and layout information from scanned documents. Specifically, we pre-train StructuralLM with two new designs to make the most of the interactions of cell and layout information: 1) each cell as a semantic unit; 2) classification of cell positions. The pre-trained StructuralLM achieves new state-of-the-art results in different types of downstream tasks, including form understanding (from 78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and document image classification (from 94.43 to 96.08).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 25, "citationCount": 85, "influentialCitationCount": 20, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.493.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-24", "journal": {"pages": "6309-6318"}, "authors": [{"authorId": "143971529", "name": "Chenliang Li"}, {"authorId": "2555622", "name": "Bin Bi"}, {"authorId": "2047087220", "name": "Ming Yan"}, {"authorId": "38700603", "name": "Wei Wang"}, {"authorId": "2410938", "name": "Songfang Huang"}, {"authorId": "143857288", "name": "Fei Huang"}, {"authorId": "2059080424", "name": "Luo Si"}]}, {"paperId": "02b1bc3601aafbafc4cf8b27557e599a1690f295", "externalIds": {"ACL": "2021.acl-long.494", "DBLP": "conf/acl/LiCFMWH20", "DOI": "10.18653/v1/2021.acl-long.494", "CorpusId": 236460102}, "corpusId": 236460102, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/02b1bc3601aafbafc4cf8b27557e599a1690f295", "title": "Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis is a fine-grained sentiment classification task. Recently, graph neural networks over dependency trees have been explored to explicitly model connections between aspects and opinion words. However, the improvement is limited due to the inaccuracy of the dependency parsing results and the informal expressions and complexity of online reviews. To overcome these challenges, in this paper, we propose a dual graph convolutional networks (DualGCN) model that considers the complementarity of syntax structures and semantic correlations simultaneously. Particularly, to alleviate dependency parsing errors, we design a SynGCN module with rich syntactic knowledge. To capture semantic correlations, we design a SemGCN module with self-attention mechanism. Furthermore, we propose orthogonal and differential regularizers to capture semantic correlations between words precisely by constraining attention scores in the SemGCN module. The orthogonal regularizer encourages the SemGCN to learn semantically correlated words with less overlap for each word. The differential regularizer encourages the SemGCN to learn semantic features that the SynGCN fails to capture. Experimental results on three public datasets show that our DualGCN model outperforms state-of-the-art methods and verify the effectiveness of our model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 152, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.494.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": null, "journal": {"pages": "6319-6329"}, "authors": [{"authorId": "2462591", "name": "Ruifan Li"}, {"authorId": "144600412", "name": "Hao Chen"}, {"authorId": "39825530", "name": "Fangxiang Feng"}, {"authorId": "2118104922", "name": "Zhanyu Ma"}, {"authorId": "2115450950", "name": "Xiaojie Wang"}, {"authorId": "144547315", "name": "E. Hovy"}]}, {"paperId": "4bcd811142901c798c785a6aee4b6ae40bdabe19", "externalIds": {"DBLP": "conf/acl/HuZGXGGCS20", "ACL": "2021.acl-long.495", "ArXiv": "2105.14174", "DOI": "10.18653/v1/2021.acl-long.495", "CorpusId": 235254178}, "corpusId": 235254178, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4bcd811142901c798c785a6aee4b6ae40bdabe19", "title": "Multi-Label Few-Shot Learning for Aspect Category Detection", "abstract": "Aspect category detection (ACD) in sentiment analysis aims to identify the aspect categories mentioned in a sentence. In this paper, we formulate ACD in the few-shot learning scenario. However, existing few-shot learning approaches mainly focus on single-label predictions. These methods can not work well for the ACD task since a sentence may contain multiple aspect categories. Therefore, we propose a multi-label few-shot learning method based on the prototypical network. To alleviate the noise, we design two effective attention mechanisms. The support-set attention aims to extract better prototypes by removing irrelevant aspects. The query-set attention computes multiple prototype-specific representations for each query instance, which are then used to compute accurate distances with the corresponding prototypes. To achieve multi-label inference, we further learn a dynamic threshold per instance by a policy network. Extensive experimental results on three datasets demonstrate that the proposed method significantly outperforms strong baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 19, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.495.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-29", "journal": {"name": "ArXiv", "volume": "abs/2105.14174"}, "authors": [{"authorId": "10703821", "name": "Mengting Hu"}, {"authorId": "2516425", "name": "Shiwan Zhao"}, {"authorId": "3144353", "name": "Honglei Guo"}, {"authorId": "2056231460", "name": "Chao Xue"}, {"authorId": "2802864", "name": "H. Gao"}, {"authorId": "2724719", "name": "Tiegang Gao"}, {"authorId": "35391375", "name": "Renhong Cheng"}, {"authorId": "145640647", "name": "Zhong Su"}]}, {"paperId": "bdec7620d1df7fc9a32f19d7d90a5e47ee4bddb2", "externalIds": {"DBLP": "conf/acl/ChengWBS20", "ACL": "2021.acl-long.496", "DOI": "10.18653/v1/2021.acl-long.496", "CorpusId": 236460093}, "corpusId": 236460093, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bdec7620d1df7fc9a32f19d7d90a5e47ee4bddb2", "title": "Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding", "abstract": "Argument pair extraction (APE) is a research task for extracting arguments from two passages and identifying potential argument pairs. Prior research work treats this task as a sequence labeling problem and a binary classification problem on two passages that are directly concatenated together, which has a limitation of not fully utilizing the unique characteristics and inherent relations of two different passages. This paper proposes a novel attention-guided multi-layer multi-cross encoding scheme to address the challenges. The new model processes two passages with two individual sequence encoders and updates their representations using each other\u2019s representations through attention. In addition, the pair prediction part is formulated as a table-filling problem by updating the representations of two sequences\u2019 Cartesian product. Furthermore, an auxiliary attention loss is introduced to guide each argument to align to its paired argument. An extensive set of experiments show that the new model significantly improves the APE performance over several alternatives.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 9, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.496.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6341-6353"}, "authors": [{"authorId": "123962152", "name": "Liying Cheng"}, {"authorId": "2112664378", "name": "Tianyu Wu"}, {"authorId": "1996394", "name": "Lidong Bing"}, {"authorId": "2059080424", "name": "Luo Si"}]}, {"paperId": "975d02b643fb13f13a478f7b7a5bc4e3748baf5b", "externalIds": {"DBLP": "conf/acl/BaoFWDDX20", "ACL": "2021.acl-long.497", "DOI": "10.18653/v1/2021.acl-long.497", "CorpusId": 236459881}, "corpusId": 236459881, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/975d02b643fb13f13a478f7b7a5bc4e3748baf5b", "title": "A Neural Transition-based Model for Argumentation Mining", "abstract": "The goal of argumentation mining is to automatically extract argumentation structures from argumentative texts. Most existing methods determine argumentative relations by exhaustively enumerating all possible pairs of argument components, which suffer from low efficiency and class imbalance. Moreover, due to the complex nature of argumentation, there is, so far, no universal method that can address both tree and non-tree structured argumentation. Towards these issues, we propose a neural transition-based model for argumentation mining, which incrementally builds an argumentation graph by generating a sequence of actions, avoiding inefficient enumeration operations. Furthermore, our model can handle both tree and non-tree structured argumentation without introducing any structural constraints. Experimental results show that our model achieves the best performance on two public datasets of different structures.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 19, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.497.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6354-6364"}, "authors": [{"authorId": "2008034310", "name": "Jianzhu Bao"}, {"authorId": "20964758", "name": "Chuang Fan"}, {"authorId": "2109415297", "name": "Jipeng Wu"}, {"authorId": "2118919097", "name": "Yixue Dang"}, {"authorId": "2186356", "name": "Jiachen Du"}, {"authorId": "1753529", "name": "Ruifeng Xu"}]}, {"paperId": "f07029549bdb29a7afce1acd824fbe4e3dfb25d5", "externalIds": {"DBLP": "journals/corr/abs-2107-03444", "ACL": "2021.acl-long.498", "ArXiv": "2107.03444", "DOI": "10.18653/v1/2021.acl-long.498", "CorpusId": 234688073}, "corpusId": 234688073, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f07029549bdb29a7afce1acd824fbe4e3dfb25d5", "title": "Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text", "abstract": "This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate\u2019s reward, and encourages candidates that outperform the mean reward. Finally, we propose a realistic text comprehension task as an evaluation method for text simplification. When tested on the English news domain, the KiS model outperforms strong supervised baselines by more than 4 SARI points, and can help people complete a comprehension task an average of 18% faster while retaining accuracy, when compared to the original text.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 52, "citationCount": 35, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.498.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-07", "journal": {"pages": "6365-6378"}, "authors": [{"authorId": "46180754", "name": "Philippe Laban"}, {"authorId": "48839531", "name": "Tobias Schnabel"}, {"authorId": "144609235", "name": "Paul N. Bennett"}, {"authorId": "1716902", "name": "Marti A. Hearst"}]}, {"paperId": "798c61b2b985e918a74b9aa154e6bc3f01040763", "externalIds": {"DBLP": "conf/acl/GuanMFLDH20", "ArXiv": "2105.08963", "ACL": "2021.acl-long.499", "DOI": "10.18653/v1/2021.acl-long.499", "CorpusId": 234778188}, "corpusId": 234778188, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/798c61b2b985e918a74b9aa154e6bc3f01040763", "title": "Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence", "abstract": "Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 46, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.499.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-19", "journal": {"pages": "6379-6393"}, "authors": [{"authorId": "145902734", "name": "Jian Guan"}, {"authorId": "29422474", "name": "Xiaoxi Mao"}, {"authorId": "3120655", "name": "Changjie Fan"}, {"authorId": "2117940912", "name": "Zitao Liu"}, {"authorId": "3068001", "name": "Wenbiao Ding"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "7ffc1b425026e916cd6db37c79df3e08e8f47ee6", "externalIds": {"DBLP": "conf/acl/GuanZFLDMFH20", "ArXiv": "2105.08920", "ACL": "2021.acl-long.500", "DOI": "10.18653/v1/2021.acl-long.500", "CorpusId": 234778136}, "corpusId": 234778136, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7ffc1b425026e916cd6db37c79df3e08e8f47ee6", "title": "OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics", "abstract": "Automatic metrics are essential for developing natural language generation (NLG) models, particularly for open-ended language generation tasks such as story generation. However, existing automatic metrics are observed to correlate poorly with human evaluation. The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples. We evaluate existing metrics on OpenMEVA and observe that they have poor correlation with human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge (e.g., causal order between events), the generalization ability and robustness. Our study presents insights for developing NLG models and metrics in further research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 28, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.500.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-19", "journal": {"pages": "6394-6407"}, "authors": [{"authorId": "145902734", "name": "Jian Guan"}, {"authorId": "101371510", "name": "Zhexin Zhang"}, {"authorId": "2108500252", "name": "Zhuoer Feng"}, {"authorId": "2117940912", "name": "Zitao Liu"}, {"authorId": "3068001", "name": "Wenbiao Ding"}, {"authorId": "29422474", "name": "Xiaoxi Mao"}, {"authorId": "3120655", "name": "Changjie Fan"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "07cf32655f229ed0bfa76aad7e1afc60ea5bc9a5", "externalIds": {"ArXiv": "2106.00791", "DBLP": "journals/corr/abs-2106-00791", "ACL": "2021.acl-long.501", "DOI": "10.18653/v1/2021.acl-long.501", "CorpusId": 235293831}, "corpusId": 235293831, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/07cf32655f229ed0bfa76aad7e1afc60ea5bc9a5", "title": "DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation", "abstract": "We study the task of long-form opinion text generation, which faces at least two distinct challenges. First, existing neural generation models fall short of coherence, thus requiring efficient content planning. Second, diverse types of information are needed to guide the generator to cover both subjective and objective content. To this end, we propose DYPLOC, a generation framework that conducts dynamic planning of content while generating the output based on a novel design of mixed language models. To enrich the generation with diverse content, we further propose to use large pre-trained models to predict relevant concepts and to generate claims. We experiment with two challenging tasks on newly collected datasets: (1) argument generation with Reddit ChangeMyView, and (2) writing articles using New York Times\u2019 Opinion section. Automatic evaluation shows that our model significantly outperforms competitive comparisons. Human judges further confirm that our generations are more coherent with richer content.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 68, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.501.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"name": "ArXiv", "volume": "abs/2106.00791"}, "authors": [{"authorId": "7156360", "name": "Xinyu Hua"}, {"authorId": "2106628426", "name": "Ashwin Sreevatsa"}, {"authorId": "2153516659", "name": "Lu Wang"}]}, {"paperId": "0f60df76da1f55c4a904e3d9a40d30798c171103", "externalIds": {"ArXiv": "2107.00152", "ACL": "2021.acl-long.502", "DBLP": "journals/corr/abs-2107-00152", "DOI": "10.18653/v1/2021.acl-long.502", "CorpusId": 235678938}, "corpusId": 235678938, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0f60df76da1f55c4a904e3d9a40d30798c171103", "title": "Controllable Open-ended Question Generation with A New Question Type Ontology", "abstract": "We investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences. We first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words. A new dataset with 4,959 questions is labeled based on the new ontology. We then propose a novel question type-aware question generation framework, augmented by a semantic graph representation, to jointly predict question focuses and produce the question. Based on this framework, we further use both exemplars and automatically generated templates to improve controllability and diversity. Experiments on two newly collected large-scale datasets show that our model improves question quality over competitive comparisons based on automatic metrics. Human judges also rate our model outputs highly in answerability, coverage of scope, and overall quality. Finally, our model variants with templates can produce questions with enhanced controllability and diversity.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 59, "citationCount": 28, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.502.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-01", "journal": {"name": "ArXiv", "volume": "abs/2107.00152"}, "authors": [{"authorId": "6333082", "name": "Shuyang Cao"}, {"authorId": "2153516659", "name": "Lu Wang"}]}, {"paperId": "1fa1cbc15101ef0ff05c6a5c4f6d0b926f6d4d67", "externalIds": {"DBLP": "conf/acl/MitzalisCMS20", "ArXiv": "2106.03484", "ACL": "2021.acl-long.503", "DOI": "10.18653/v1/2021.acl-long.503", "CorpusId": 235358906}, "corpusId": 235358906, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1fa1cbc15101ef0ff05c6a5c4f6d0b926f6d4d67", "title": "BERTGen: Multi-task Generation through BERT", "abstract": "We present BERTGen, a novel, generative, decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT, respectively. BERTGen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. With a comprehensive set of evaluations, we show that BERTGen outperforms many strong baselines across the tasks explored. We also show BERTGen\u2019s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 58, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.503.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "journal": {"pages": "6440-6455"}, "authors": [{"authorId": "2993699", "name": "Faidon Mitzalis"}, {"authorId": "10791325", "name": "Ozan Caglayan"}, {"authorId": "3238408", "name": "P. Madhyastha"}, {"authorId": "1702974", "name": "Lucia Specia"}]}, {"paperId": "926eb6dbb08791dad76e4a0468731b02a85a5bba", "externalIds": {"DBLP": "conf/acl/0008YM020", "ArXiv": "2105.12967", "ACL": "2021.acl-long.504", "DOI": "10.18653/v1/2021.acl-long.504", "CorpusId": 235212428}, "corpusId": 235212428, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/926eb6dbb08791dad76e4a0468731b02a85a5bba", "title": "Selective Knowledge Distillation for Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks. As an active research field in NMT, knowledge distillation is widely applied to enhance the model\u2019s performance by transferring teacher model\u2019s knowledge on each training sample. However, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge. In this paper, we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples\u2019 partitions. Based on above protocol, we conduct extensive experiments and find that the teacher\u2019s knowledge is not the more, the better. Knowledge over specific samples may even hurt the whole performance of knowledge distillation. Finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT\u201914 English-German and WMT\u201919 Chinese-English. Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 32, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.504.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-27", "journal": {"name": "ArXiv", "volume": "abs/2105.12967"}, "authors": [{"authorId": "143924592", "name": "Fusheng Wang"}, {"authorId": "134233854", "name": "Jianhao Yan"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "48128428", "name": "Jie Zhou"}]}, {"paperId": "83145b7a391b792e24d8d38f74ed6b6ae7a149dc", "externalIds": {"ArXiv": "2105.03482", "DBLP": "conf/acl/FernandesYNM20", "ACL": "2021.acl-long.505", "DOI": "10.18653/v1/2021.acl-long.505", "CorpusId": 234342704}, "corpusId": 234342704, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/83145b7a391b792e24d8d38f74ed6b6ae7a149dc", "title": "Measuring and Increasing Context Usage in Context-Aware Machine Translation", "abstract": "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 31, "citationCount": 28, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.505.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-07", "journal": {"name": "ArXiv", "volume": "abs/2105.03482"}, "authors": [{"authorId": "2058640028", "name": "Patrick Fernandes"}, {"authorId": "1602993448", "name": "Kayo Yin"}, {"authorId": "1700325", "name": "Graham Neubig"}, {"authorId": "145644643", "name": "Andr\u00e9 F. T. Martins"}]}, {"paperId": "a8e44f0488ac573d0c9f5f8ba046f03212b34921", "externalIds": {"DBLP": "conf/acl/OrmazabalASLA20", "ACL": "2021.acl-long.506", "ArXiv": "2012.15715", "DOI": "10.18653/v1/2021.acl-long.506", "CorpusId": 229923154}, "corpusId": 229923154, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a8e44f0488ac573d0c9f5f8ba046f03212b34921", "title": "Beyond Offline Mapping: Learning Cross-lingual Word Embeddings through Context Anchoring", "abstract": "Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 8, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.506.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"pages": "6479-6489"}, "authors": [{"authorId": "146150609", "name": "Aitor Ormazabal"}, {"authorId": "2347956", "name": "Mikel Artetxe"}, {"authorId": "2078619062", "name": "Aitor Soroa Etxabe"}, {"authorId": "3255091", "name": "Gorka Labaka"}, {"authorId": "1733049", "name": "Eneko Agirre"}]}, {"paperId": "16dbfa7ad452277d2c568913d2550a9a58a43b62", "externalIds": {"DBLP": "journals/corr/abs-1911-04944", "ACL": "2021.acl-long.507", "MAG": "2988451549", "ArXiv": "1911.04944", "DOI": "10.18653/v1/2021.acl-long.507", "CorpusId": 207863306}, "corpusId": 207863306, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/16dbfa7ad452277d2c568913d2550a9a58a43b62", "title": "CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web", "abstract": "We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences. We use 32 snapshots of a curated common crawl corpus (Wenzel et al, 2019) totaling 71 billion unique sentences. Using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English. We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many TED benchmark. Further, we evaluate on competitive translation benchmarks such as WMT and WAT. Using only mined bitext, we set a new state of the art for a single system on the WMT\u201919 test set for English-German/Russian/Chinese. In particular, our English/German and English/Russian systems outperform the best single ones by over 4 BLEU points and are on par with best WMT\u201919 systems, which train on the WMT training data and augment it with backtranslation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop. All of the mined bitext will be freely available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 62, "citationCount": 167, "influentialCitationCount": 28, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.507.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-10", "journal": {"pages": "6490-6500"}, "authors": [{"authorId": "144518416", "name": "Holger Schwenk"}, {"authorId": "2293203", "name": "Guillaume Wenzek"}, {"authorId": "2068070", "name": "Sergey Edunov"}, {"authorId": "3024698", "name": "Edouard Grave"}, {"authorId": "2319608", "name": "Armand Joulin"}]}, {"paperId": "1a6c5f6ce26914c2a7af0217e8cd3e844f2b2f37", "externalIds": {"ACL": "2021.acl-long.508", "DBLP": "journals/corr/abs-2010-07003", "MAG": "3092746498", "ArXiv": "2010.07003", "DOI": "10.18653/v1/2021.acl-long.508", "CorpusId": 222341845}, "corpusId": 222341845, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1a6c5f6ce26914c2a7af0217e8cd3e844f2b2f37", "title": "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search", "abstract": "Despite transformers\u2019 impressive accuracy, their computational cost is often prohibitive to use with limited computational resources. Most previous approaches to improve inference efficiency require a separate model for each possible computational budget. In this paper, we extend PoWER-BERT (Goyal et al., 2020) and propose Length-Adaptive Transformer that can be used for various inference scenarios after one-shot training. We train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines a sequence length at each layer. We then conduct a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the efficiency metric under any given computational budget. Additionally, we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into token-level classification with Drop-and-Restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary. We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including span-based question answering and text classification. Code is available at https://github.com/clovaai/lengthadaptive-transformer.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 70, "citationCount": 44, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.508.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-10-14", "journal": {"pages": "6501-6511"}, "authors": [{"authorId": "2276429", "name": "Gyuwan Kim"}, {"authorId": "1979489", "name": "Kyunghyun Cho"}]}, {"paperId": "ef7c03ccd3a044d92a8c457c9bb969c6344f9d5c", "externalIds": {"DBLP": "conf/acl/HuangHSJCL20", "ACL": "2021.acl-long.509", "DOI": "10.18653/v1/2021.acl-long.509", "CorpusId": 236460174}, "corpusId": 236460174, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ef7c03ccd3a044d92a8c457c9bb969c6344f9d5c", "title": "GhostBERT: Generate More Features with Cheap Operations for BERT", "abstract": "Transformer-based pre-trained language models like BERT, though powerful in many tasks, are expensive in both memory and computation, due to their large number of parameters. Previous works show that some parameters in these models can be pruned away without severe accuracy drop. However, these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the model\u2019s representation ability. In this paper, we propose GhostBERT, which generates more features with very cheap operations from the remaining features. In this way, GhostBERT has similar memory and computational cost as the pruned model, but enjoys much larger representation power. The proposed ghost module can also be applied to unpruned BERT models to enhance their performance with negligible additional parameters and computation. Empirical results on the GLUE benchmark on three backbone models (i.e., BERT, RoBERTa and ELECTRA) verify the efficacy of our proposed method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 19, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.509.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6512-6523"}, "authors": [{"authorId": "1490968632", "name": "Zhiqi Huang"}, {"authorId": "48557122", "name": "Lu Hou"}, {"authorId": "50812138", "name": "Lifeng Shang"}, {"authorId": "2110310493", "name": "Xin Jiang"}, {"authorId": "2117025507", "name": "Xiao Chen"}, {"authorId": "30738758", "name": "Qun Liu"}]}, {"paperId": "e638b9e6ee09ab4fa748b748099e0f03d471d803", "externalIds": {"ACL": "2021.acl-long.510", "DBLP": "conf/acl/LiangZCJLHZC20", "ArXiv": "2105.12002", "DOI": "10.18653/v1/2021.acl-long.510", "CorpusId": 235186841}, "corpusId": 235186841, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e638b9e6ee09ab4fa748b748099e0f03d471d803", "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization", "abstract": "The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of \u201dlottery tickets\u201d, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as \u201dwinning tickets\u201d, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as \u201dsuper tickets\u201d. We further show that the phase transition is task and model dependent \u2014 as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 52, "citationCount": 41, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.510.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-25", "journal": {"name": "ArXiv", "volume": "abs/2105.12002"}, "authors": [{"authorId": "98703980", "name": "Chen Liang"}, {"authorId": "52194893", "name": "Simiao Zuo"}, {"authorId": "2108809403", "name": "Minshuo Chen"}, {"authorId": "2152630772", "name": "Haoming Jiang"}, {"authorId": "46522098", "name": "Xiaodong Liu"}, {"authorId": "50462546", "name": "Pengcheng He"}, {"authorId": "36345161", "name": "T. Zhao"}, {"authorId": "2109136147", "name": "Weizhu Chen"}]}, {"paperId": "7018b5d9e98fb9a2c4d6b5b14594e9187e234a82", "externalIds": {"ACL": "2021.acl-long.511", "DBLP": "conf/acl/ColomboPC20", "ArXiv": "2105.02685", "DOI": "10.18653/v1/2021.acl-long.511", "CorpusId": 233864576}, "corpusId": 233864576, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7018b5d9e98fb9a2c4d6b5b14594e9187e234a82", "title": "A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations", "abstract": "Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data either rely on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute. However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement. In contrast to adversarial methods, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi\u2019s divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 80, "citationCount": 35, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.511.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-06", "journal": {"pages": "6539-6550"}, "authors": [{"authorId": "46985469", "name": "Pierre Colombo"}, {"authorId": "2049106", "name": "C. Clavel"}, {"authorId": "1743922", "name": "P. Piantanida"}]}, {"paperId": "f4364b0ea43247935c88d29532ab58da85b3bb32", "externalIds": {"DBLP": "journals/corr/abs-2106-07400", "ArXiv": "2106.07400", "ACL": "2021.acl-long.512", "DOI": "10.18653/v1/2021.acl-long.512", "CorpusId": 235422227}, "corpusId": 235422227, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f4364b0ea43247935c88d29532ab58da85b3bb32", "title": "Determinantal Beam Search", "abstract": "Beam search is a go-to strategy for decoding neural sequence models. The algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates. Empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word. Yet in use-cases that call for multiple solutions, a diverse or representative set is often desired. To address this issue, we propose a reformulation of beam search, which we call determinantal beam search. Determinantal beam search has a natural relationship to determinantal point processes (DPPs), models over sets that inherently encode intra-set interactions. By posing iterations in beam search as a series of subdeterminant maximization problems, we can turn the algorithm into a diverse subset selection process. In a case study, we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model. We observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation, while providing a more general approach to optimizing for diversity.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.512.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-14", "journal": {"pages": "6551-6562"}, "authors": [{"authorId": "150953620", "name": "Clara Meister"}, {"authorId": "80039310", "name": "Martina Forster"}, {"authorId": "2070989574", "name": "Ryan Cotterell"}]}, {"paperId": "bf07a4087e53738a0ed5aa263e4a5ff730ae74e1", "externalIds": {"ACL": "2021.acl-long.513", "ArXiv": "2106.05221", "DBLP": "journals/corr/abs-2106-05221", "DOI": "10.18653/v1/2021.acl-long.513", "CorpusId": 235376991}, "corpusId": 235376991, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bf07a4087e53738a0ed5aa263e4a5ff730ae74e1", "title": "Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning", "abstract": "Graph convolutional network (GCN) has become popular in various natural language processing (NLP) tasks with its superiority in long-term and non-consecutive word interactions. However, existing single-hop graph reasoning in GCN may miss some important non-consecutive dependencies. In this study, we define the spectral graph convolutional network with the high-order dynamic Chebyshev approximation (HDGCN), which augments the multi-hop graph reasoning by fusing messages aggregated from direct and long-term dependencies into one convolutional layer. To alleviate the over-smoothing in high-order Chebyshev approximation, a multi-vote-based cross-attention (MVCAttn) with linear computation complexity is also proposed. The empirical results on four transductive and inductive NLP tasks and the ablation study verify the efficacy of the proposed model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.513.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"name": "ArXiv", "volume": "abs/2106.05221"}, "authors": [{"authorId": "92695867", "name": "Shuoran Jiang"}, {"authorId": "144159781", "name": "Qingcai Chen"}, {"authorId": "2146075472", "name": "Xin Liu"}, {"authorId": "33968873", "name": "Baotian Hu"}, {"authorId": "2107937930", "name": "Lisai Zhang"}]}, {"paperId": "578c9b095b166549625d2221ea4c1446eeb114f6", "externalIds": {"DBLP": "conf/acl/AdhikaryBV20", "ACL": "2021.acl-long.514", "DOI": "10.18653/v1/2021.acl-long.514", "CorpusId": 236459962}, "corpusId": 236459962, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/578c9b095b166549625d2221ea4c1446eeb114f6", "title": "Accelerating Text Communication via Abbreviated Sentence Input", "abstract": "Typing every character in a text message may require more time or effort than strictly necessary. Skipping spaces or other characters may be able to speed input and reduce a user\u2019s physical input effort. This can be particularly important for people with motor impairments. In a large crowdsourced study, we found workers frequently abbreviated text by omitting mid-word vowels. We designed a recognizer optimized for expanding noisy abbreviated input where users often omit spaces and mid-word vowels. We show using neural language models for selecting conversational-style training text and for rescoring the recognizer\u2019s n-best sentences improved accuracy. On noisy touchscreen data collected from hundreds of users, we found accurate abbreviated input was possible even if a third of characters was omitted. Finally, in a study where users had to dwell for a second on each key, sentence abbreviated input was competitive with a conventional keyboard with word predictions. After practice, users wrote abbreviated sentences at 9.6 words-per-minute versus word input at 9.9 words-per-minute.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 56, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.514.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6574-6588"}, "authors": [{"authorId": "147015338", "name": "Jiban Adhikary"}, {"authorId": "2131604385", "name": "Jamie Berger"}, {"authorId": "2314856", "name": "K. Vertanen"}]}, {"paperId": "4f92221c7cedb1bd6212276e1c122dcac9860750", "externalIds": {"ACL": "2021.acl-long.515", "ArXiv": "2105.03048", "DBLP": "journals/corr/abs-2105-03048", "DOI": "10.18653/v1/2021.acl-long.515", "CorpusId": 234093986}, "corpusId": 234093986, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4f92221c7cedb1bd6212276e1c122dcac9860750", "title": "Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates", "abstract": "Behavior of deep neural networks can be inconsistent between different versions. Regressions during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain. This work focuses on quantifying, reducing and analyzing regression errors in the NLP model updates. Using negative flip rate as regression measure, we show that regression has a prevalent presence across tasks in the GLUE benchmark. We formulate the regression-free model updates into a constrained optimization problem, and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method. We empirically analyze how model ensemble reduces regression. Finally, we conduct CheckList behavioral testing to understand the distribution of regressions across linguistic phenomena, and the efficacy of ensemble and distillation methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 46, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.515.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-07", "journal": {"name": "ArXiv", "volume": "abs/2105.03048"}, "authors": [{"authorId": "49291108", "name": "Yuqing Xie"}, {"authorId": "2117866", "name": "Yi-An Lai"}, {"authorId": "3331521", "name": "Yuanjun Xiong"}, {"authorId": "46867473", "name": "Yi Zhang"}, {"authorId": "1715959", "name": "Stefano Soatto"}]}, {"paperId": "ed7391f944d413d8d2e67d8d03bc1d4f8e83a327", "externalIds": {"DBLP": "journals/corr/abs-2109-08013", "ACL": "2021.acl-long.516", "ArXiv": "2109.08013", "DOI": "10.18653/v1/2021.acl-long.516", "CorpusId": 236459862}, "corpusId": 236459862, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ed7391f944d413d8d2e67d8d03bc1d4f8e83a327", "title": "Detecting Propaganda Techniques in Memes", "abstract": "Propaganda can be defined as a form of communication that aims to influence the opinions or the actions of people towards a specific goal; this is achieved by means of well-defined rhetorical and psychological devices. Propaganda, in the form we know it today, can be dated back to the beginning of the 17th century. However, it is with the advent of the Internet and the social media that propaganda has started to spread on a much larger scale than before, thus becoming major societal and political issue. Nowadays, a large fraction of propaganda in social media is multimodal, mixing textual with visual content. With this in mind, here we propose a new multi-label multimodal task: detecting the type of propaganda techniques used in memes. We further create and release a new corpus of 950 memes, carefully annotated with 22 propaganda techniques, which can appear in the text, in the image, or in both. Our analysis of the corpus shows that understanding both modalities together is essential for detecting these techniques. This is further confirmed in our experiments with several state-of-the-art multimodal models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 67, "citationCount": 53, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.516.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-07", "journal": {"name": "ArXiv", "volume": "abs/2109.08013"}, "authors": [{"authorId": "2105813793", "name": "Dimitar I. Dimitrov"}, {"authorId": "2097712075", "name": "Bishr Bin Ali"}, {"authorId": "2121286474", "name": "S. Shaar"}, {"authorId": "37784060", "name": "Firoj Alam"}, {"authorId": "144925193", "name": "F. Silvestri"}, {"authorId": "22593971", "name": "Hamed Firooz"}, {"authorId": "2026545715", "name": "Preslav Nakov"}, {"authorId": "34086979", "name": "Giovanni Da San Martino"}]}, {"paperId": "c5e4eafd85949e6aac9d8e98d5e03b2acf444046", "externalIds": {"ACL": "2021.acl-long.517", "DBLP": "conf/acl/KaushikKLY20", "ArXiv": "2106.00872", "DOI": "10.18653/v1/2021.acl-long.517", "CorpusId": 235294052}, "corpusId": 235294052, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c5e4eafd85949e6aac9d8e98d5e03b2acf444046", "title": "On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study", "abstract": "In adversarial data collection (ADC), a human workforce interacts with a model in real time, attempting to produce examples that elicit incorrect predictions. Researchers hope that models trained on these more challenging datasets will rely less on superficial patterns, and thus be less brittle. However, despite ADC\u2019s intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models. In this paper, we conduct a large-scale controlled study focused on question answering, assigning workers at random to compose questions either (i) adversarially (with a model in the loop); or (ii) in the standard fashion (without a model). Across a variety of models and datasets, we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of adversarial (vs standard) data, identifying key differences and offering guidance for future research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 53, "citationCount": 28, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.517.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"pages": "6618-6633"}, "authors": [{"authorId": "9264826", "name": "Divyansh Kaushik"}, {"authorId": "1743722", "name": "Douwe Kiela"}, {"authorId": "32219137", "name": "Zachary Chase Lipton"}, {"authorId": "2072801764", "name": "Wen-tau Yih"}]}, {"paperId": "1283ca87e6215b7393eba1653a4a2e4bf28d2868", "externalIds": {"DBLP": "conf/acl/LeeSKC20", "ArXiv": "2012.12624", "ACL": "2021.acl-long.518", "DOI": "10.18653/v1/2021.acl-long.518", "CorpusId": 229363636}, "corpusId": 229363636, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1283ca87e6215b7393eba1653a4a2e4bf28d2868", "title": "Learning Dense Representations of Phrases at Scale", "abstract": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 52, "citationCount": 83, "influentialCitationCount": 13, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.518.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-23", "journal": {"pages": "6634-6647"}, "authors": [{"authorId": "46664096", "name": "Jinhyuk Lee"}, {"authorId": "147610425", "name": "Mujeen Sung"}, {"authorId": "144323862", "name": "Jaewoo Kang"}, {"authorId": "50536468", "name": "Danqi Chen"}]}, {"paperId": "15df0e2c602ae8ccedcf50accea080c4ba76f8ba", "externalIds": {"DBLP": "conf/acl/SachanPSKPHC20", "ACL": "2021.acl-long.519", "ArXiv": "2101.00408", "DOI": "10.18653/v1/2021.acl-long.519", "CorpusId": 230437591}, "corpusId": 230437591, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/15df0e2c602ae8ccedcf50accea080c4ba76f8ba", "title": "End-to-End Training of Neural Retrievers for Open-Domain Question Answering", "abstract": "Recent work on training neural retrievers for open-domain question answering (OpenQA) has employed both supervised and unsupervised approaches. However, it remains unclear how unsupervised and supervised methods can be used most effectively for neural retrievers. In this work, we systematically study retriever pre-training. We first propose an approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans, followed by supervised finetuning using question-context pairs. This approach leads to absolute gains of 2+ points over the previous best result in the top-20 retrieval accuracy on Natural Questions and TriviaQA datasets. We next explore two approaches for end-to-end training of the reader and retriever components in OpenQA models, which differ in the manner the reader ingests the retrieved documents. Our experiments demonstrate the effectiveness of these approaches as we obtain state-of-the-art results. On the Natural Questions dataset, we obtain a top-20 retrieval accuracy of 84%, an improvement of 5 points over the recent DPR model. We also achieve good results on answer extraction, outperforming recent models like REALM and RAG by 3+ points.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 61, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.519.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-02", "journal": {"pages": "6648-6662"}, "authors": [{"authorId": "39670454", "name": "Devendra Singh Sachan"}, {"authorId": "66870756", "name": "M. Patwary"}, {"authorId": "1911755", "name": "M. Shoeybi"}, {"authorId": "2065893046", "name": "Neel Kant"}, {"authorId": "2056440915", "name": "Wei Ping"}, {"authorId": "49437682", "name": "William L. Hamilton"}, {"authorId": "2301680", "name": "Bryan Catanzaro"}]}, {"paperId": "47fe46e561e3b270407b7bffa176e35291f165a7", "externalIds": {"DBLP": "journals/corr/abs-2106-01515", "ACL": "2021.acl-long.520", "ArXiv": "2106.01515", "DOI": "10.18653/v1/2021.acl-long.520", "CorpusId": 235313508}, "corpusId": 235313508, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/47fe46e561e3b270407b7bffa176e35291f165a7", "title": "Question Answering Over Temporal Knowledge Graphs", "abstract": "Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120% in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 59, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.520.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01515"}, "authors": [{"authorId": "46961776", "name": "Apoorv Saxena"}, {"authorId": "40941894", "name": "Soumen Chakrabarti"}, {"authorId": "2406435", "name": "Partha P. Talukdar"}]}, {"paperId": "ea45423c8f44025a276f6e88aafffbf87a898c89", "externalIds": {"DBLP": "journals/corr/abs-2108-08485", "ACL": "2021.acl-long.521", "ArXiv": "2108.08485", "MAG": "3194302086", "DOI": "10.18653/v1/2021.acl-long.521", "CorpusId": 236459838}, "corpusId": 236459838, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ea45423c8f44025a276f6e88aafffbf87a898c89", "title": "Language Model Augmented Relevance Score", "abstract": "Although automated metrics are commonly used to evaluate NLG systems, they often correlate poorly with human judgements. Newer metrics such as BERTScore have addressed many weaknesses in prior metrics such as BLEU and ROUGE, which rely on n-gram matching. These newer methods, however, are still limited in that they do not consider the generation context, so they cannot properly reward generated text that is correct but deviates from the given reference. In this paper, we propose Language Model Augmented Relevance Score (MARS), a new context-aware metric for NLG evaluation. MARS leverages off-the-shelf language models, guided by reinforcement learning, to create augmented references that consider both the generation context and available human references, which are then used as additional references to score generated text. Compared with seven existing metrics in three common NLG tasks, MARS not only achieves higher correlation with human reference judgements, but also differentiates well-formed candidates from adversarial samples to a larger degree.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 67, "citationCount": 10, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.521.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-19", "journal": {"name": "ArXiv", "volume": "abs/2108.08485"}, "authors": [{"authorId": "7247867", "name": "Ruibo Liu"}, {"authorId": "119640649", "name": "Jason Wei"}, {"authorId": "1918441", "name": "Soroush Vosoughi"}]}, {"paperId": "02f033482b8045c687316ef81ba7aaae9f0a2e1c", "externalIds": {"ACL": "2021.acl-long.522", "DBLP": "conf/acl/LiuSLSBSC20", "ArXiv": "2105.03023", "DOI": "10.18653/v1/2021.acl-long.522", "CorpusId": 235313967}, "corpusId": 235313967, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/02f033482b8045c687316ef81ba7aaae9f0a2e1c", "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts", "abstract": "Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with \u201cexpert\u201d LMs and/or \u201canti-expert\u201d LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 52, "citationCount": 151, "influentialCitationCount": 43, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.522.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-07", "journal": {"pages": "6691-6706"}, "authors": [{"authorId": "94500147", "name": "Alisa Liu"}, {"authorId": "2729164", "name": "Maarten Sap"}, {"authorId": "50085131", "name": "Ximing Lu"}, {"authorId": "2705113", "name": "Swabha Swayamdipta"}, {"authorId": "1857797", "name": "Chandra Bhagavatula"}, {"authorId": "144365875", "name": "Noah A. Smith"}, {"authorId": "1699545", "name": "Yejin Choi"}]}, {"paperId": "15e71e497a67423bfedd0d63efe423c4660e5053", "externalIds": {"ArXiv": "2101.00288", "DBLP": "conf/acl/WuRHW20", "ACL": "2021.acl-long.523", "DOI": "10.18653/v1/2021.acl-long.523", "CorpusId": 235266322}, "corpusId": 235266322, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/15e71e497a67423bfedd0d63efe423c4660e5053", "title": "Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models", "abstract": "While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 66, "citationCount": 136, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.523.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-02", "journal": {"pages": "6707-6723"}, "authors": [{"authorId": "35232494", "name": "Tongshuang Sherry Wu"}, {"authorId": "78846919", "name": "Marco Tulio Ribeiro"}, {"authorId": "1803140", "name": "Jeffrey Heer"}, {"authorId": "1780531", "name": "Daniel S. Weld"}]}, {"paperId": "432da56e21e0d4c85f7ea58159f363732c4ecdd4", "externalIds": {"DBLP": "journals/corr/abs-2106-01228", "ArXiv": "2106.01228", "ACL": "2021.acl-long.524", "DOI": "10.18653/v1/2021.acl-long.524", "CorpusId": 235294009}, "corpusId": 235294009, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/432da56e21e0d4c85f7ea58159f363732c4ecdd4", "title": "Metaphor Generation with Conceptual Mappings", "abstract": "Generating metaphors is a difficult task as it requires understanding nuanced relationships between abstract concepts. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Guided by conceptual metaphor theory, we propose to control the generation process by encoding conceptual mappings between cognitive domains to generate meaningful metaphoric expressions. To achieve this, we develop two methods: 1) using FrameNet-based embeddings to learn mappings between domains and applying them at the lexical level (CM-Lex), and 2) deriving source/target pairs to train a controlled seq-to-seq generation model (CM-BART). We assess our methods through automatic and human evaluation for basic metaphoricity and conceptual metaphor presence. We show that the unsupervised CM-Lex model is competitive with recent deep learning metaphor generation systems, and CM-BART outperforms all other models both in automatic and human evaluations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 52, "citationCount": 23, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.524.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01228"}, "authors": [{"authorId": "31600242", "name": "Kevin Stowe"}, {"authorId": "51448832", "name": "Tuhin Chakrabarty"}, {"authorId": "3157053", "name": "Nanyun Peng"}, {"authorId": "2295928", "name": "S. Muresan"}, {"authorId": "1730400", "name": "Iryna Gurevych"}]}, {"paperId": "bd235d6ea116eec1fdb8a5410b61c4a92e19727f", "externalIds": {"DBLP": "conf/acl/KulkarniCFM20", "ACL": "2021.acl-long.525", "DOI": "10.18653/v1/2021.acl-long.525", "CorpusId": 236459874}, "corpusId": 236459874, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bd235d6ea116eec1fdb8a5410b61c4a92e19727f", "title": "Learning Latent Structures for Cross Action Phrase Relations in Wet Lab Protocols", "abstract": "Wet laboratory protocols (WLPs) are critical for conveying reproducible procedures in biological research. They are composed of instructions written in natural language describing the step-wise processing of materials by specific actions. This process flow description for reagents and materials synthesis in WLPs can be captured by material state transfer graphs (MSTGs), which encode global temporal and causal relationships between actions. Here, we propose methods to automatically generate a MSTG for a given protocol by extracting all action relationships across multiple sentences. We also note that previous corpora and methods focused primarily on local intra-sentence relationships between actions and entities and did not address two critical issues: (i) resolution of implicit arguments and (ii) establishing long-range dependencies across sentences. We propose a new model that incrementally learns latent structures and is better suited to resolving inter-sentence relations and implicit arguments. This model draws upon a new corpus WLP-MSTG which was created by extending annotations in the WLP corpora for inter-sentence relations and implicit arguments. Our model achieves an F1 score of 54.53% for temporal and causal relations in protocols from our corpus, which is a significant improvement over previous models - DyGIE++:28.17%; spERT:27.81%. We make our annotated WLP-MSTG corpus available to the research community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 1, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6737-6750"}, "authors": [{"authorId": "49223785", "name": "Chaitanya Kulkarni"}, {"authorId": "34372596", "name": "Jany Chan"}, {"authorId": "1398481836", "name": "E. Fosler-Lussier"}, {"authorId": "1707285", "name": "R. Machiraju"}]}, {"paperId": "15322b1ed9e8c062b9077a2bc48cd965f8d41d72", "externalIds": {"ACL": "2021.acl-long.526", "DBLP": "conf/acl/SawhneyGGMS20", "DOI": "10.18653/v1/2021.acl-long.526", "CorpusId": 236460233}, "corpusId": 236460233, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/15322b1ed9e8c062b9077a2bc48cd965f8d41d72", "title": "Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines", "abstract": "Risk prediction is an essential task in financial markets. Merger and Acquisition (M&A) calls provide key insights into the claims made by company executives about the restructuring of the financial firms. Extracting vocal and textual cues from M&A calls can help model the risk associated with such financial activities. To aid the analysis of M&A calls, we curate a dataset of conference call transcripts and their corresponding audio recordings for the time period ranging from 2016 to 2020. We introduce M3ANet, a baseline architecture that takes advantage of the multimodal multi-speaker input to forecast the financial risk associated with the M&A calls. Empirical results prove that the task is challenging, with the pro-posed architecture performing marginally better than strong BERT-based baselines. We release the M3A dataset and benchmark models to motivate future research on this challenging problem domain.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 57, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.526.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6751-6762"}, "authors": [{"authorId": "51042088", "name": "Ramit Sawhney"}, {"authorId": "2000936311", "name": "Mihir Goyal"}, {"authorId": "2000932039", "name": "Prakhar Goel"}, {"authorId": "144144799", "name": "Puneet Mathur"}, {"authorId": "1753278", "name": "R. Shah"}]}, {"paperId": "a2844b687ad11315a6a8d7c528560c21d40d7260", "externalIds": {"ACL": "2021.acl-long.527", "DBLP": "conf/acl/Jamara0KG20", "DOI": "10.18653/v1/2021.acl-long.527", "CorpusId": 236460204}, "corpusId": 236460204, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a2844b687ad11315a6a8d7c528560c21d40d7260", "title": "Mid-Air Hand Gestures for Post-Editing of Machine Translation", "abstract": "To translate large volumes of text in a globally connected world, more and more translators are integrating machine translation (MT) and post-editing (PE) into their translation workflows to generate publishable quality translations. While this process has been shown to save time and reduce errors, the task of translation is changing from mostly text production from scratch to fixing errors within useful but partly incorrect MT output. This is affecting the interface design of translation tools, where better support for text editing tasks is required. Here, we present the first study that investigates the usefulness of mid-air hand gestures in combination with the keyboard (GK) for text editing in PE of MT. Guided by a gesture elicitation study with 14 freelance translators, we develop a prototype supporting mid-air hand gestures for cursor placement, text selection, deletion, and reordering. These gestures combined with the keyboard facilitate all editing types required for PE. An evaluation of the prototype shows that the average editing duration of GK is only slightly slower than the standard mouse and keyboard (MK), even though participants are very familiar with the latter, and relative novices to the former. Furthermore, the qualitative analysis shows positive attitudes towards hand gestures for PE, especially when manipulating single words.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6763-6773"}, "authors": [{"authorId": "2121296950", "name": "Rashad Albo Jamara"}, {"authorId": "3181259", "name": "Nico Herbig"}, {"authorId": "2064260311", "name": "Antonio Kr\u00fcger"}, {"authorId": "7519068", "name": "Josef van Genabith"}]}, {"paperId": "fb1c90806fc5ec72987f58110aa255edbce6620d", "externalIds": {"ArXiv": "2105.04165", "DBLP": "conf/acl/LuGJQHLZ20", "ACL": "2021.acl-long.528", "MAG": "3176371077", "DOI": "10.18653/v1/2021.acl-long.528", "CorpusId": 234337054}, "corpusId": 234337054, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fb1c90806fc5ec72987f58110aa255edbce6620d", "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning", "abstract": "Geometry problem solving has attracted much attention in the NLP community recently. The task is challenging as it requires abstract problem understanding and symbolic reasoning with axiomatic knowledge. However, current datasets are either small in scale or not publicly available. Thus, we construct a new large-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with dense annotation in formal language. We further propose a novel geometry solving approach with formal language and symbolic reasoning, called Interpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the problem text and diagram into formal language automatically via rule-based text parsing and neural object detecting, respectively. Unlike implicit learning in existing methods, Inter-GPS incorporates theorem knowledge as conditional rules and performs symbolic reasoning step by step. Also, a theorem predictor is designed to infer the theorem application sequence fed to the symbolic solver for the more efficient and reasonable searching path. Extensive experiments on the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves significant improvements over existing methods. The project with code and data is available at https://lupantech.github.io/inter-gps.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 52, "citationCount": 34, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.528.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-10", "journal": {"pages": "6774-6786"}, "authors": [{"authorId": "2887562", "name": "Pan Lu"}, {"authorId": "48096046", "name": "Ran Gong"}, {"authorId": "2119326908", "name": "Shibiao Jiang"}, {"authorId": "47659905", "name": "Liang Qiu"}, {"authorId": "1713084", "name": "Siyuan Huang"}, {"authorId": "40250403", "name": "Xiaodan Liang"}, {"authorId": "145380991", "name": "Song-Chun Zhu"}]}, {"paperId": "3ce32d492aa14a2048d928957e9d15bcd92e8294", "externalIds": {"ArXiv": "2012.15115", "ACL": "2021.acl-long.529", "DBLP": "conf/acl/SchlichtkrullKO20", "DOI": "10.18653/v1/2021.acl-long.529", "CorpusId": 229923350}, "corpusId": 229923350, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3ce32d492aa14a2048d928957e9d15bcd92e8294", "title": "Joint Verification and Reranking for Open Fact Checking Over Tables", "abstract": "Structured information is an important knowledge source for automatic verification of factual claims. Nevertheless, the majority of existing research into this task has focused on textual data, and the few recent inquiries into structured data have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved. In this paper, we investigate verification over structured data in the open-domain setting, introducing a joint reranking-and-verification model which fuses evidence documents in the verification component. Our open-domain model achieves performance comparable to the closed-domain state-of-the-art on the TabFact dataset, and demonstrates performance gains from the inclusion of multiple tables as well as a significant improvement over a heuristic retrieval baseline.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 47, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.529.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-30", "journal": {"pages": "6787-6799"}, "authors": [{"authorId": "8804828", "name": "M. Schlichtkrull"}, {"authorId": "2067091563", "name": "Vladimir Karpukhin"}, {"authorId": "9185192", "name": "Barlas O\u011fuz"}, {"authorId": "35084211", "name": "M. Lewis"}, {"authorId": "144105277", "name": "Wen-tau Yih"}, {"authorId": "48662861", "name": "Sebastian Riedel"}]}, {"paperId": "99211c5cfe0da6e18330c38f64b2587bbbc4c389", "externalIds": {"ArXiv": "2106.15971", "DBLP": "conf/acl/Bilal0LPT20", "ACL": "2021.acl-long.530", "DOI": "10.18653/v1/2021.acl-long.530", "CorpusId": 235683542}, "corpusId": 235683542, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/99211c5cfe0da6e18330c38f64b2587bbbc4c389", "title": "Evaluation of Thematic Coherence in Microblogs", "abstract": "Collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners. A major question is how to evaluate the quality of such thematic clusters. Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence. We provide annotation guidelines and human annotations of thematic coherence by journalist experts. We subsequently investigate the efficacy of different automated evaluation metrics for the task. We consider a range of metrics including surface level metrics, ones for topic model coherence and text generation metrics (TGMs). While surface level metrics perform well, outperforming topic coherence metrics, they are not as consistent as TGMs. TGMs are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 53, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.530.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-30", "journal": {"pages": "6800-6814"}, "authors": [{"authorId": "2117010132", "name": "I. Bilal"}, {"authorId": "2153211753", "name": "Bo Wang"}, {"authorId": "48717312", "name": "M. Liakata"}, {"authorId": "144723416", "name": "R. Procter"}, {"authorId": "32728828", "name": "A. Tsakalidis"}]}, {"paperId": "c552c158776ad85814ef4fbca5b040d443573dda", "externalIds": {"DBLP": "conf/acl/LanJX20", "ArXiv": "2106.02569", "ACL": "2021.acl-long.531", "DOI": "10.18653/v1/2021.acl-long.531", "CorpusId": 235352798}, "corpusId": 235352798, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c552c158776ad85814ef4fbca5b040d443573dda", "title": "Neural semi-Markov CRF for Monolingual Word Alignment", "abstract": "Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 64, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.531.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"pages": "6815-6828"}, "authors": [{"authorId": "37852874", "name": "Wuwei Lan"}, {"authorId": "145755239", "name": "Chao Jiang"}, {"authorId": "145738420", "name": "Wei Xu"}]}, {"paperId": "ebeed3d81649ab67e6220d6db0c2c361cbc20784", "externalIds": {"DBLP": "journals/corr/abs-2004-11131", "ACL": "2021.acl-long.532", "ArXiv": "2004.11131", "MAG": "3018880344", "DOI": "10.18653/v1/2021.acl-long.532", "CorpusId": 216080642}, "corpusId": 216080642, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ebeed3d81649ab67e6220d6db0c2c361cbc20784", "title": "Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies", "abstract": "Organisations disclose their privacy practices by posting privacy policies on their websites. Even though internet users often care about their digital privacy, they usually do not read privacy policies, since understanding them requires a significant investment of time and effort. Natural language processing has been used to create experimental tools to interpret privacy policies, but there has been a lack of large privacy policy corpora to facilitate the creation of large-scale semi-supervised and unsupervised models to interpret and simplify privacy policies. Thus, we present the PrivaSeer Corpus of 1,005,380 English language website privacy policies collected from the web. The number of unique websites represented in PrivaSeer is about ten times larger than the next largest public collection of web privacy policies, and it surpasses the aggregate of unique websites represented in all other publicly available privacy policy corpora combined. We describe a corpus creation pipeline with stages that include a web crawler, language detection, document classification, duplicate and near-duplicate removal, and content extraction. We employ an unsupervised topic modelling approach to investigate the contents of policy documents in the corpus and discuss the distribution of topics in privacy policies at web scale. We further investigate the relationship between privacy policy domain PageRanks and text features of the privacy policies. Finally, we use the corpus to pretrain PrivBERT, a transformer-based privacy policy language model, and obtain state of the art results on the data practice classification and question answering tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 58, "citationCount": 26, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.532.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-23", "journal": {"name": "ArXiv", "volume": "abs/2004.11131"}, "authors": [{"authorId": "47467195", "name": "Mukund Srinath"}, {"authorId": "31950200", "name": "Shomir Wilson"}, {"authorId": "145157784", "name": "C. Lee Giles"}]}, {"paperId": "1cedc9f229368ef87c37b76ad5a686cd7c180610", "externalIds": {"DBLP": "conf/acl/WeiJ20", "ACL": "2021.acl-long.533", "ArXiv": "2105.12437", "DOI": "10.18653/v1/2021.acl-long.533", "CorpusId": 235195899}, "corpusId": 235195899, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1cedc9f229368ef87c37b76ad5a686cd7c180610", "title": "The statistical advantage of automatic NLG metrics at the system level", "abstract": "Estimating the expected output quality of generation systems is central to NLG. This paper qualifies the notion that automatic metrics are not as good as humans in estimating system-level quality. Statistically, humans are unbiased, high variance estimators, while metrics are biased, low variance estimators. We compare these estimators by their error in pairwise prediction (which generation system is better?) using the bootstrap. Measuring this error is complicated: predictions are evaluated against noisy, human predicted labels instead of the ground truth, and metric predictions fluctuate based on the test sets they were calculated on. By applying a bias-variance-noise decomposition, we adjust this error to a noise-free, infinite test set setting. Our analysis compares the adjusted error of metrics to humans and a derived, perfect segment-level annotator, both of which are unbiased estimators dependent on the number of judgments collected. In MT, we identify two settings where metrics outperform humans due to a statistical advantage in variance: when the number of human judgments used is small, and when the quality difference between compared systems is small.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 54, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.533.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-26", "journal": {"pages": "6840-6854"}, "authors": [{"authorId": "41150696", "name": "Johnny Tian-Zheng Wei"}, {"authorId": "3422908", "name": "Robin Jia"}]}, {"paperId": "18979d2dfa2c15a8930c494517407c7c7b6e3526", "externalIds": {"DBLP": "journals/corr/abs-2108-01387", "ACL": "2021.acl-long.534", "ArXiv": "2108.01387", "DOI": "10.18653/v1/2021.acl-long.534", "CorpusId": 236459932}, "corpusId": 236459932, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/18979d2dfa2c15a8930c494517407c7c7b6e3526", "title": "Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion", "abstract": "We present InferWiki, a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, assumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. To ensure it, we propose to utilize rule-guided train/test generation, instead of conventional random split. Second, InferWiki initiates the evaluation following the open-world assumption and improves the inferential difficulty of the closed-world assumption, by providing manually annotated negative and unknown triples. Third, we include various inference patterns (e.g., reasoning path length and types) for comprehensive evaluation. In experiments, we curate two settings of InferWiki varying in sizes and structures, and apply the construction process on CoDEx as comparative datasets. The results and empirical analyses demonstrate the necessity and high-quality of InferWiki. Nevertheless, the performance gap among various inferential assumptions and patterns presents the difficulty and inspires future research direction. Our datasets can be found in https://github.com/TaoMiner/inferwiki.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.534.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-03", "journal": {"name": "ArXiv", "volume": "abs/2108.01387"}, "authors": [{"authorId": "2112867078", "name": "Yixin Cao"}, {"authorId": "2117710478", "name": "Xiang Ji"}, {"authorId": "48574888", "name": "Xin Lv"}, {"authorId": "8549842", "name": "Juan-Zi Li"}, {"authorId": "2114783695", "name": "Yonggang Wen"}, {"authorId": "2119078220", "name": "Hanwang Zhang"}]}, {"paperId": "fa51076458b7bcf9a60f476d525755e47199a6d8", "externalIds": {"ArXiv": "2106.00829", "ACL": "2021.acl-long.535", "DBLP": "conf/acl/FabbriRRWLMR20", "DOI": "10.18653/v1/2021.acl-long.535", "CorpusId": 235294200}, "corpusId": 235294200, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fa51076458b7bcf9a60f476d525755e47199a6d8", "title": "ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining", "abstract": "While online conversations can cover a vast amount of information in many different formats, abstractive text summarization has primarily focused on modeling solely news articles. This research gap is due, in part, to the lack of standardized datasets for summarizing online discussions. To address this gap, we design annotation protocols motivated by an issues\u2013viewpoints\u2013assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments, discussion forums, community question answering forums, and email threads. We benchmark state-of-the-art models on our datasets and analyze characteristics associated with the data. To create a comprehensive benchmark, we also evaluate these models on widely-used conversation summarization datasets to establish strong baselines in this domain. Furthermore, we incorporate argument mining through graph construction to directly model the issues, viewpoints, and assertions present in a conversation and filter noisy input, showing comparable or improved results according to automatic and human evaluations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 78, "citationCount": 37, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.535.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"name": "ArXiv", "volume": "abs/2106.00829"}, "authors": [{"authorId": "46255971", "name": "Alexander R. Fabbri"}, {"authorId": "151002933", "name": "Faiaz Rahman"}, {"authorId": "2106627934", "name": "Imad Rizvi"}, {"authorId": "2203187", "name": "Borui Wang"}, {"authorId": "1391218521", "name": "Haoran Li"}, {"authorId": "2263803", "name": "Yashar Mehdad"}, {"authorId": "9215251", "name": "Dragomir R. Radev"}]}, {"paperId": "73cb36eb79e8d21a0e154e7984e4fe8a47cc0be9", "externalIds": {"DBLP": "conf/acl/NanSZNMNZWAX20", "ACL": "2021.acl-long.536", "ArXiv": "2105.04623", "DOI": "10.18653/v1/2021.acl-long.536", "CorpusId": 234358027}, "corpusId": 234358027, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/73cb36eb79e8d21a0e154e7984e4fe8a47cc0be9", "title": "Improving Factual Consistency of Abstractive Summarization via Question Answering", "abstract": "A commonly observed problem with the state-of-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents. The fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application. In this paper we present an approach to address factual consistency in summarization. We first propose an efficient automatic evaluation metric to measure factual consistency; next, we propose a novel learning algorithm that maximizes the proposed metric during model training. Through extensive experiments, we confirm that our method is effective in improving factual consistency and even overall quality of the summaries, as judged by both automatic metrics and human evaluation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 61, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.536.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-10", "journal": {"name": "ArXiv", "volume": "abs/2105.04623"}, "authors": [{"authorId": "144647318", "name": "Feng Nan"}, {"authorId": "1790831", "name": "C. D. Santos"}, {"authorId": "1787682", "name": "Henghui Zhu"}, {"authorId": "145878390", "name": "Patrick Ng"}, {"authorId": "145590324", "name": "K. McKeown"}, {"authorId": "1701451", "name": "Ramesh Nallapati"}, {"authorId": "2358258", "name": "Dejiao Zhang"}, {"authorId": "40296541", "name": "Zhiguo Wang"}, {"authorId": "2112031035", "name": "Andrew O. Arnold"}, {"authorId": "144028698", "name": "Bing Xiang"}]}, {"paperId": "f03afeef6f36da2f5b6531b65c996a7b8ea521dd", "externalIds": {"DBLP": "journals/corr/abs-2107-14691", "ArXiv": "2107.14691", "ACL": "2021.acl-long.537", "DOI": "10.18653/v1/2021.acl-long.537", "CorpusId": 236460081}, "corpusId": 236460081, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f03afeef6f36da2f5b6531b65c996a7b8ea521dd", "title": "EmailSum: Abstractive Email Thread Summarization", "abstract": "Recent years have brought about an interest in the challenging task of summarizing conversation threads (meetings, online discussions, etc.). Such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency. To spur research in thread summarization, we have developed an abstractive Email Thread Summarization (EmailSum) dataset, which contains human-annotated short (<30 words) and long (<100 words) summaries of 2,549 email threads (each containing 3 to 10 emails) over a wide variety of topics. We perform a comprehensive empirical study to explore different summarization techniques (including extractive and abstractive methods, single-document and hierarchical models, as well as transfer and semisupervised learning) and conduct human evaluations on both short and long summary generation tasks. Our results reveal the key challenges of current abstractive summarization models in this task, such as understanding the sender\u2019s intent and identifying the roles of sender and receiver. Furthermore, we find that widely used automatic evaluation metrics (ROUGE, BERTScore) are weakly correlated with human judgments on this email thread summarization task. Hence, we emphasize the importance of human evaluation and the development of better metrics by the community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 26, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.537.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-30", "journal": {"pages": "6895-6909"}, "authors": [{"authorId": "7670835", "name": "Shiyue Zhang"}, {"authorId": "1709797", "name": "Asli Celikyilmaz"}, {"authorId": "48441311", "name": "Jianfeng Gao"}, {"authorId": "143977268", "name": "Mohit Bansal"}]}, {"paperId": "146b054c3389213862eb00dde623ca7edba888e1", "externalIds": {"DBLP": "journals/corr/abs-2105-13648", "ACL": "2021.acl-long.538", "ArXiv": "2105.13648", "DOI": "10.18653/v1/2021.acl-long.538", "CorpusId": 235247840}, "corpusId": 235247840, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/146b054c3389213862eb00dde623ca7edba888e1", "title": "Cross-Lingual Abstractive Summarization with Limited Parallel Resources", "abstract": "Parallel cross-lingual summarization data is scarce, requiring models to better use the limited available cross-lingual resources. Existing methods to do so often adopt sequence-to-sequence networks with multi-task frameworks. Such approaches apply multiple decoders, each of which is utilized for a specific task. However, these independent decoders share no parameters, hence fail to capture the relationships between the discrete phrases of summaries in different languages, breaking the connections in order to transfer the knowledge of the high-resource languages to low-resource languages. To bridge these connections, we propose a novel Multi-Task framework for Cross-Lingual Abstractive Summarization (MCLAS) in a low-resource setting. Employing one unified decoder to generate the sequential concatenation of monolingual and cross-lingual summaries, MCLAS makes the monolingual summarization task a prerequisite of the CLS task. In this way, the shared decoder learns interactions involving alignments and summary patterns across languages, which encourages attaining knowledge transfer. Experiments on two CLS datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios. Moreover, in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using MCLAS, which benefits the CLS task under limited parallel resources.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 37, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.538.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-28", "journal": {"name": "ArXiv", "volume": "abs/2105.13648"}, "authors": [{"authorId": "144843219", "name": "Yu Bai"}, {"authorId": "145644809", "name": "Yang Gao"}, {"authorId": "4590286", "name": "Heyan Huang"}]}, {"paperId": "143310c074eb09d5e60adea4c42250dbe03bf9f2", "externalIds": {"DBLP": "journals/corr/abs-2106-01518", "ArXiv": "2106.01518", "ACL": "2021.acl-long.539", "DOI": "10.18653/v1/2021.acl-long.539", "CorpusId": 235313847}, "corpusId": 235313847, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/143310c074eb09d5e60adea4c42250dbe03bf9f2", "title": "Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution", "abstract": "Despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. We propose a two-step method to interpret summarization model decisions. We first analyze the model\u2019s behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in between? After isolating decisions that do depend on the input, we explore interpreting these decisions using several different attribution methods. We compare these techniques based on their ability to select content and reconstruct the model\u2019s predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 66, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.539.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"pages": "6925-6940"}, "authors": [{"authorId": "2121525724", "name": "Jiacheng Xu"}, {"authorId": "1814094", "name": "Greg Durrett"}]}, {"paperId": "8c748fbc290f7748cbf223a8c3674936808fc45d", "externalIds": {"DBLP": "conf/acl/JiangR20", "ACL": "2021.acl-long.540", "DOI": "10.18653/v1/2021.acl-long.540", "CorpusId": 235416586}, "corpusId": 235416586, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8c748fbc290f7748cbf223a8c3674936808fc45d", "title": "Learning Prototypical Functions for Physical Artifacts", "abstract": "Humans create things for a reason. Ancient people created spears for hunting, knives for cutting meat, pots for preparing food, etc. The prototypical function of a physical artifact is a kind of commonsense knowledge that we rely on to understand natural language. For example, if someone says \u201cShe borrowed the book\u201d then you would assume that she intends to read the book, or if someone asks \u201cCan I use your knife?\u201d then you would assume that they need to cut something. In this paper, we introduce a new NLP task of learning the prototypical uses for human-made physical objects. We use frames from FrameNet to represent a set of common functions for objects, and describe a manually annotated data set of physical objects labeled with their prototypical function. We also present experimental results for this task, including BERT-based models that use predictions from masked patterns as well as artifact sense definitions from WordNet and frame definitions from FrameNet.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "6941-6951"}, "authors": [{"authorId": "144830318", "name": "Tianyu Jiang"}, {"authorId": "1691993", "name": "E. Riloff"}]}, {"paperId": "ce2e6a287e2b1898665035b1a734d3a559c0933c", "externalIds": {"DBLP": "journals/corr/abs-2012-15421", "ACL": "2021.acl-long.541", "ArXiv": "2012.15421", "DOI": "10.18653/v1/2021.acl-long.541", "CorpusId": 229924335}, "corpusId": 229924335, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ce2e6a287e2b1898665035b1a734d3a559c0933c", "title": "Verb Knowledge Injection for Multilingual Event Processing", "abstract": "Linguistic probing of pretrained Transformer-based language models (LMs) revealed that they encode a range of syntactic and semantic properties of a language. However, they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic information. In this paper, we target a specific facet of linguistic knowledge, the interplay between verb meaning and argument structure. We investigate whether injecting explicit information on verbs\u2019 semantic-syntactic behaviour improves the performance of pretrained LMs in event extraction tasks, where accurate verb processing is paramount. Concretely, we impart the verb knowledge from curated lexical resources into dedicated adapter modules (verb adapters), allowing it to complement, in downstream tasks, the language knowledge obtained during LM-pretraining. We first demonstrate that injecting verb knowledge leads to performance gains in English event extraction. We then explore the utility of verb adapters for event extraction in other languages: we investigate 1) zero-shot language transfer with multilingual Transformers and 2) transfer via (noisy automatic) translation of English verb-based lexical knowledge. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 114, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.541.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"name": "ArXiv", "volume": "abs/2012.15421"}, "authors": [{"authorId": "46963731", "name": "Olga Majewska"}, {"authorId": "1747849", "name": "Ivan Vulic"}, {"authorId": "1666177566", "name": "Goran Glavavs"}, {"authorId": "3381663", "name": "E. Ponti"}, {"authorId": "145762466", "name": "A. Korhonen"}]}, {"paperId": "33661b3345cb29e37c85cc702d2a2a3b428e1445", "externalIds": {"MAG": "3094217771", "DBLP": "conf/acl/HofmannPS20b", "ACL": "2021.acl-long.542", "ArXiv": "2010.12684", "DOI": "10.18653/v1/2021.acl-long.542", "CorpusId": 225067080}, "corpusId": 225067080, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/33661b3345cb29e37c85cc702d2a2a3b428e1445", "title": "Dynamic Contextualized Word Embeddings", "abstract": "Static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts. Building on prior work on contextualized and dynamic word embeddings, we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. Based on a pretrained language model (PLM), dynamic contextualized word embeddings model time and social space jointly, which makes them attractive for a range of NLP tasks involving semantic variability. We highlight potential application scenarios by means of qualitative and quantitative analyses on four English datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 113, "citationCount": 30, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.542.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-10-23", "journal": {"pages": "6970-6984"}, "authors": [{"authorId": "1667898858", "name": "Valentin Hofmann"}, {"authorId": "1970864", "name": "J. Pierrehumbert"}, {"authorId": "144418438", "name": "Hinrich Sch\u00fctze"}]}, {"paperId": "3be4340615904d3b2452a5cba98d7f265b63a6f4", "externalIds": {"ArXiv": "2106.03111", "DBLP": "journals/corr/abs-2106-03111", "ACL": "2021.acl-long.543", "DOI": "10.18653/v1/2021.acl-long.543", "CorpusId": 235358847}, "corpusId": 235358847, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3be4340615904d3b2452a5cba98d7f265b63a6f4", "title": "Lexical Semantic Change Discovery", "abstract": "While there is a large amount of research in the field of Lexical Semantic Change Detection, only few approaches go beyond a standard benchmark evaluation of existing models. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 73, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.543.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-06", "journal": {"name": "ArXiv", "volume": "abs/2106.03111"}, "authors": [{"authorId": "2047306644", "name": "Sinan Kurtyigit"}, {"authorId": "1739504778", "name": "Maike Park"}, {"authorId": "3449121", "name": "Dominik Schlechtweg"}, {"authorId": "1716963", "name": "Jonas Kuhn"}, {"authorId": "7965906", "name": "Sabine Schulte im Walde"}]}, {"paperId": "4f9ced6a7a7b19e8607fc9028e8a8d55f89e9a52", "externalIds": {"DBLP": "journals/corr/abs-2106-02692", "ArXiv": "2106.02692", "ACL": "2021.acl-long.544", "DOI": "10.18653/v1/2021.acl-long.544", "CorpusId": 235358232}, "corpusId": 235358232, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4f9ced6a7a7b19e8607fc9028e8a8d55f89e9a52", "title": "The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity", "abstract": "Humans are increasingly interacting with machines through language, sometimes in contexts where the user may not know they are talking to a machine (like over the phone or a text chatbot). We aim to understand how system designers and researchers might allow their systems to confirm its non-human identity. We collect over 2,500 phrasings related to the intent of \u201cAre you a robot?\u201d. This is paired with over 2,500 adversarially selected utterances where only confirming the system is non-human would be insufficient or disfluent. We compare classifiers to recognize the intent and discuss the precision/recall and model complexity tradeoffs. Such classifiers could be integrated into dialog systems to avoid undesired deception. We then explore how both a generative research model (Blender) as well as two deployed systems (Amazon Alexa, Google Assistant) handle this intent, finding that systems often fail to confirm their non-human identity. Finally, we try to understand what a good response to the intent would be, and conduct a user study to compare the important aspects when responding to this intent.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.544.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02692"}, "authors": [{"authorId": "1986353424", "name": "David Gros"}, {"authorId": "40058381", "name": "Yu Li"}, {"authorId": "1564034697", "name": "Zhou Yu"}]}, {"paperId": "72a281af1054753d9db50f585139300abd7b515d", "externalIds": {"ACL": "2021.acl-long.545", "DBLP": "conf/acl/PinhanezCRACNPG20", "DOI": "10.18653/v1/2021.acl-long.545", "CorpusId": 236460227}, "corpusId": 236460227, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/72a281af1054753d9db50f585139300abd7b515d", "title": "Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems", "abstract": "In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. Developers often include such knowledge, structure as taxonomies, in the documentation of chatbots. By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition. In datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (EER) in more than 40% of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge. The meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (FAR) in two thirds of the chatbots, with decreases of 0.05 or more in FAR in almost 40% of the chatbots. When considering only the well-developed workspaces with a high level use of taxonomies, FAR decreased more than 0.05 in 77% of them, and more than 0.1 in 39% of the chatbots.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "7014-7027"}, "authors": [{"authorId": "1766240", "name": "Claudio S. Pinhanez"}, {"authorId": "1755187", "name": "P. Cavalin"}, {"authorId": "35364563", "name": "Victor Henrique Alves Ribeiro"}, {"authorId": "2410276", "name": "A. P. Appel"}, {"authorId": "3002927", "name": "Heloisa Candello"}, {"authorId": "1686473", "name": "J. Nogima"}, {"authorId": "3093972", "name": "M. Pichiliani"}, {"authorId": "40976565", "name": "M. Guerra"}, {"authorId": "14540226", "name": "Maira Gatti de Bayser"}, {"authorId": "1470833553", "name": "G. Malfatti"}, {"authorId": "36269133", "name": "H. Ferreira"}]}, {"paperId": "14312b5c345a0c7c210773436579500c507852e3", "externalIds": {"ACL": "2021.acl-long.546", "DBLP": "conf/acl/GaletzkaRS020", "DOI": "10.18653/v1/2021.acl-long.546", "CorpusId": 236317402}, "corpusId": 236317402, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/14312b5c345a0c7c210773436579500c507852e3", "title": "Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer", "abstract": "To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context. This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept. In this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights. The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.546.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "7028-7041"}, "authors": [{"authorId": "1598464476", "name": "Fabian Galetzka"}, {"authorId": "38444971", "name": "J. Rose"}, {"authorId": "1817455", "name": "David Schlangen"}, {"authorId": "71564931", "name": "Jens Lehmann"}]}, {"paperId": "72b2f7b5ebde06779b9842908fbae902cc4b9674", "externalIds": {"DBLP": "conf/acl/HuWH20", "ACL": "2021.acl-long.547", "ArXiv": "2106.01978", "DOI": "10.18653/v1/2021.acl-long.547", "CorpusId": 235313788}, "corpusId": 235313788, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/72b2f7b5ebde06779b9842908fbae902cc4b9674", "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations", "abstract": "Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective. Inspired by the Cognitive Theory of Emotion, we design multi-turn reasoning modules to extract and integrate emotional clues. The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 46, "citationCount": 84, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.547.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01978"}, "authors": [{"authorId": "120427046", "name": "Dou Hu"}, {"authorId": "2110687662", "name": "Lingwei Wei"}, {"authorId": "3311552", "name": "X. Huai"}]}, {"paperId": "3130fa914c733c9fffcf4c92d8dcaca93fadada6", "externalIds": {"ACL": "2021.acl-long.548", "ArXiv": "2106.07393", "DBLP": "journals/corr/abs-2106-07393", "DOI": "10.18653/v1/2021.acl-long.548", "CorpusId": 235422654}, "corpusId": 235422654, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3130fa914c733c9fffcf4c92d8dcaca93fadada6", "title": "Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability", "abstract": "When collecting annotations and labeled data from humans, a standard practice is to use inter-rater reliability (IRR) as a measure of data goodness (Hallgren, 2012). Metrics such as Krippendorff\u2019s alpha or Cohen\u2019s kappa are typically required to be above a threshold of 0.6 (Landis and Koch, 1977). These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances, especially on subjective topics. We present a new alternative to interpreting IRR that is more empirical and contextualized. It is based upon benchmarking IRR against baseline measures in a replication, one of which is a novel cross-replication reliability (xRR) measure based on Cohen\u2019s (1960) kappa. We call this approach the xRR framework. We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework. We argue this framework can be used to measure the quality of crowdsourced datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.548.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-11", "journal": {"name": "ArXiv", "volume": "abs/2106.07393"}, "authors": [{"authorId": "1644050353", "name": "KayYen Wong"}, {"authorId": "2990264", "name": "Praveen K. Paritosh"}, {"authorId": "1745337", "name": "Lora Aroyo"}]}, {"paperId": "62953ca1252c9febe07c7007a10911726f37792d", "externalIds": {"ArXiv": "2106.04571", "DBLP": "journals/corr/abs-2106-04571", "ACL": "2021.acl-long.549", "DOI": "10.18653/v1/2021.acl-long.549", "CorpusId": 235368000}, "corpusId": 235368000, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/62953ca1252c9febe07c7007a10911726f37792d", "title": "TIMEDIAL: Temporal Commonsense Reasoning in Dialog", "abstract": "Everyday conversations require understanding everyday events, which in turn, requires understanding temporal commonsense concepts interwoven with those events. Despite recent progress with massive pre-trained language models (LMs) such as T5 and GPT-3, their capability of temporal reasoning in dialogs remains largely under-explored. In this paper, we present the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set, TimeDial. We formulate TimeDial as a multiple choice cloze task with over 1.1K carefully curated dialogs. Empirical results demonstrate that even the best performing models struggle on this task compared to humans, with 23 absolute points of gap in accuracy. Furthermore, our analysis reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context, motivating future research for modeling temporal concepts in text and robust contextual reasoning about them. The dataset is publicly available at https://github.com/google-research-datasets/timedial.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 47, "citationCount": 35, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.549.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"pages": "7066-7076"}, "authors": [{"authorId": "3444092", "name": "Lianhui Qin"}, {"authorId": "2121241705", "name": "Aditya Gupta"}, {"authorId": "33145619", "name": "Shyam Upadhyay"}, {"authorId": "2265599", "name": "Luheng He"}, {"authorId": "1699545", "name": "Yejin Choi"}, {"authorId": "1779225", "name": "Manaal Faruqui"}]}, {"paperId": "55d760c0c9000378edc53ad75f77a72bfc50dc88", "externalIds": {"DBLP": "journals/corr/abs-2105-13266", "ACL": "2021.acl-long.550", "ArXiv": "2105.13266", "DOI": "10.18653/v1/2021.acl-long.550", "CorpusId": 235212053}, "corpusId": 235212053, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/55d760c0c9000378edc53ad75f77a72bfc50dc88", "title": "RAW-C: Relatedness of Ambiguous Words in Context (A New Lexical Resource for English)", "abstract": "Most words are ambiguous\u2014-i.e., they convey distinct meanings in different contexts\u2014-and even the meanings of unambiguous words are context-dependent. Both phenomena present a challenge for NLP. Recently, the advent of contextualized word embeddings has led to success on tasks involving lexical ambiguity, such as Word Sense Disambiguation. However, there are few tasks that directly evaluate how well these contextualized embeddings accommodate the more continuous, dynamic nature of word meaning\u2014-particularly in a way that matches human intuitions. We introduce RAW-C, a dataset of graded, human relatedness judgments for 112 ambiguous words in context (with 672 sentence pairs total), as well as human estimates of sense dominance. The average inter-annotator agreement (assessed using a leave-one-annotator-out method) was 0.79. We then show that a measure of cosine distance, computed using contextualized embeddings from BERT and ELMo, correlates with human judgments, but that cosine distance also systematically underestimates how similar humans find uses of the same sense of a word to be, and systematically overestimates how similar humans find uses of different-sense homonyms. Finally, we propose a synthesis between psycholinguistic theories of the mental lexicon and computational models of lexical semantics.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 51, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.550.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-27", "journal": {"name": "ArXiv", "volume": "abs/2105.13266"}, "authors": [{"authorId": "3393311", "name": "Sean Trott"}, {"authorId": "24316216", "name": "B. Bergen"}]}, {"paperId": "1e4cda8be54999ced1324777fa462a85e2c9746c", "externalIds": {"DBLP": "journals/corr/abs-2101-01785", "ACL": "2021.acl-long.551", "ArXiv": "2101.01785", "DOI": "10.18653/v1/2021.acl-long.551", "CorpusId": 230119728}, "corpusId": 230119728, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1e4cda8be54999ced1324777fa462a85e2c9746c", "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic", "abstract": "Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLM-R Large ( 3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 111, "citationCount": 215, "influentialCitationCount": 56, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.551.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-27", "journal": {"name": "ArXiv", "volume": "abs/2101.01785"}, "authors": [{"authorId": "1388437494", "name": "Muhammad Abdul-Mageed"}, {"authorId": "1397289779", "name": "AbdelRahim Elmadany"}, {"authorId": "17771023", "name": "El Moatez Billah Nagoudi"}]}, {"paperId": "191635f4457f06193920baaceba57d4c462b452d", "externalIds": {"ArXiv": "2106.07691", "DBLP": "journals/corr/abs-2106-07691", "ACL": "2021.acl-long.552", "DOI": "10.18653/v1/2021.acl-long.552", "CorpusId": 235436269}, "corpusId": 235436269, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/191635f4457f06193920baaceba57d4c462b452d", "title": "Improving Paraphrase Detection with the Adversarial Paraphrasing Task", "abstract": "If two sentences have the same meaning, it should follow that they are equivalent in their inferential properties, i.e., each sentence should textually entail the other. However, many paraphrase datasets currently in widespread use rely on a sense of paraphrase based on word overlap and syntax. Can we teach them instead to identify paraphrases in a way that draws on the inferential properties of the sentences, and is not over-reliant on lexical and syntactic similarities of a sentence pair? We apply the adversarial paradigm to this question, and introduce a new adversarial method of dataset creation for paraphrase identification: the Adversarial Paraphrasing Task (APT), which asks participants to generate semantically equivalent (in the sense of mutually implicative) but lexically and syntactically disparate paraphrases. These sentence pairs can then be used both to test paraphrase identification models (which get barely random accuracy) and then improve their performance. To accelerate dataset generation, we explore automation of APT using T5, and show that the resulting dataset also improves accuracy. We discuss implications for paraphrase detection and release our dataset in the hope of making paraphrase detection models better able to detect sentence-level meaning equivalence.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 54, "citationCount": 23, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.552.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-14", "journal": {"name": "ArXiv", "volume": "abs/2106.07691"}, "authors": [{"authorId": "133638581", "name": "Animesh Nighojkar"}, {"authorId": "2143879", "name": "John Licato"}]}, {"paperId": "c8206c0450c6928614577899b92fa389365c423d", "externalIds": {"DBLP": "conf/acl/EmamiPOSTC20", "ACL": "2021.acl-long.553", "DOI": "10.18653/v1/2021.acl-long.553", "CorpusId": 236459949}, "corpusId": 236459949, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c8206c0450c6928614577899b92fa389365c423d", "title": "ADEPT: An Adjective-Dependent Plausibility Task", "abstract": "A false contract is more likely to be rejected than a contract is, yet a false key is less likely than a key to open doors. While correctly interpreting and assessing the effects of such adjective-noun pairs (e.g., false key) on the plausibility of given events (e.g., opening doors) underpins many natural language understanding tasks, doing so often requires a significant degree of world knowledge and common-sense reasoning. We introduce ADEPT \u2013 a large-scale semantic plausibility task consisting of over 16 thousand sentences that are paired with slightly modified versions obtained by adding an adjective to a noun. Overall, we find that while the task appears easier for human judges (85% accuracy), it proves more difficult for transformer-based models like RoBERTa (71% accuracy). Our experiments also show that neither the adjective itself nor its taxonomic class suffice in determining the correct plausibility judgement, emphasizing the importance of endowing automatic natural language understanding systems with more context sensitivity and common-sense reasoning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "7117-7128"}, "authors": [{"authorId": "2075338284", "name": "Ali Emami"}, {"authorId": "108508643", "name": "Ian Porada"}, {"authorId": "2064011617", "name": "Alexandra Olteanu"}, {"authorId": "2065762420", "name": "Kaheer Suleman"}, {"authorId": "3382568", "name": "A. Trischler"}, {"authorId": "3159752", "name": "J. Cheung"}]}, {"paperId": "207a8328ab05d3a23f4d2882e70819bb1ad91b10", "externalIds": {"DBLP": "journals/corr/abs-2010-12854", "ACL": "2021.acl-long.554", "ArXiv": "2010.12854", "MAG": "3094082326", "DOI": "10.18653/v1/2021.acl-long.554", "CorpusId": 225067214}, "corpusId": 225067214, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/207a8328ab05d3a23f4d2882e70819bb1ad91b10", "title": "ReadOnce Transformers: Reusable Representations of Text for Transformers", "abstract": "We present ReadOnce Transformers, an approach to convert a transformer-based model into one that can build an information-capturing, task-independent, and compressed representation of text. The resulting representation is reusable across different examples and tasks, thereby requiring a document shared across many examples or tasks to only be read once. This leads to faster training and evaluation of models. Additionally, we extend standard text-to-text transformer models to Representation+Text-to-text models, and evaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and long-document summarization. Our one-time computed representation results in a 2x-5x speedup compared to standard text-to-text models, while the compression also allows existing language models to handle longer documents without the need for designing new pre-trained models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.554.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-10-24", "journal": {"pages": "7129-7141"}, "authors": [{"authorId": "1750926168", "name": "Shih-Ting Lin"}, {"authorId": "48229640", "name": "Ashish Sabharwal"}, {"authorId": "2236429", "name": "Tushar Khot"}]}, {"paperId": "33b06c74eea3f400b6f5ef14ef163aef1db42d16", "externalIds": {"DBLP": "conf/acl/LinCD20", "ACL": "2021.acl-long.555", "ArXiv": "2012.15786", "DOI": "10.18653/v1/2021.acl-long.555", "CorpusId": 229924289}, "corpusId": 229924289, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/33b06c74eea3f400b6f5ef14ef163aef1db42d16", "title": "Conditional Generation of Temporally-ordered Event Sequences", "abstract": "Models of narrative schema knowledge have proven useful for a range of event-related tasks, but they typically do not capture the temporal relationships between events. We propose a single model that addresses both temporal ordering, sorting given events into the order they occurred, and event infilling, predicting new events which fit into an existing temporally-ordered sequence. We use a BART-based conditional generation model that can capture both temporality and common event co-occurrence, meaning it can be flexibly applied to different tasks in this space. Our model is trained as a denoising autoencoder: we take temporally-ordered event sequences, shuffle them, delete some events, and then attempt to recover the original event sequence. This task teaches the model to make inferences given incomplete knowledge about the events in an underlying scenario. On the temporal ordering task, we show that our model is able to unscramble event sequences from existing datasets without access to explicitly labeled temporal training data, outperforming both a BERT-based pairwise model and a BERT-based pointer network. On event infilling, human evaluation shows that our model is able to generate events that fit better temporally into the input events when compared to GPT-2 story completion models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 66, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.555.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"pages": "7142-7157"}, "authors": [{"authorId": "1750926168", "name": "Shih-Ting Lin"}, {"authorId": "1729918", "name": "Nathanael Chambers"}, {"authorId": "1814094", "name": "Greg Durrett"}]}, {"paperId": "afaee3bd71a3ca31b883179270589e1588ee4eee", "externalIds": {"DBLP": "conf/acl/ZhouYFRSDYL20", "ACL": "2021.acl-long.556", "DOI": "10.18653/v1/2021.acl-long.556", "CorpusId": 236459847}, "corpusId": 236459847, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/afaee3bd71a3ca31b883179270589e1588ee4eee", "title": "Hate Speech Detection Based on Sentiment Knowledge Sharing", "abstract": "The wanton spread of hate speech on the internet brings great harm to society and families. It is urgent to establish and improve automatic detection and active avoidance mechanisms for hate speech. While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. In other words, getting more affective features from other affective resources will significantly affect the performance of hate speech detection. In this paper, we propose a hate speech detection framework based on sentiment knowledge sharing. While extracting the affective features of the target sentence itself, we make better use of the sentiment features from external resources, and finally fuse features from different feature extraction units to detect hate speech. Experimental results on two public datasets demonstrate the effectiveness of our model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 20, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.556.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "7158-7166"}, "authors": [{"authorId": "2140152126", "name": "Xianbing Zhou"}, {"authorId": "2059487704", "name": "Yang Yong"}, {"authorId": "3061076", "name": "Xiaochao Fan"}, {"authorId": "2027374061", "name": "Ge Ren"}, {"authorId": "49993077", "name": "Yunfeng Song"}, {"authorId": "38804628", "name": "Yufeng Diao"}, {"authorId": "2143920912", "name": "Liang Yang"}, {"authorId": "37553559", "name": "Hongfei Lin"}]}, {"paperId": "a61e3e99cc10bc3a5b58e420cf66a9954d3cb43e", "externalIds": {"ACL": "2021.acl-long.557", "DBLP": "journals/corr/abs-2107-06905", "ArXiv": "2107.06905", "DOI": "10.18653/v1/2021.acl-long.557", "CorpusId": 235899264}, "corpusId": 235899264, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a61e3e99cc10bc3a5b58e420cf66a9954d3cb43e", "title": "Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction", "abstract": "We propose a transition-based bubble parser to perform coordination structure identification and dependency-based syntactic analysis simultaneously. Bubble representations were proposed in the formal linguistics literature decades ago; they enhance dependency trees by encoding coordination boundaries and internal relationships within coordination structures explicitly. In this paper, we introduce a transition system and neural models for parsing these bubble-enhanced structures. Experimental results on the English Penn Treebank and the English GENIA corpus show that our parsers beat previous state-of-the-art approaches on the task of coordination structure prediction, especially for the subset of sentences with complex coordination structures.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 69, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.557.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-14", "journal": {"name": "ArXiv", "volume": "abs/2107.06905"}, "authors": [{"authorId": "2068916949", "name": "Tianze Shi"}, {"authorId": "145810617", "name": "Lillian Lee"}]}, {"paperId": "6efec9c3b3bb05b71c58786ffab6d40c31bcae96", "externalIds": {"DBLP": "conf/acl/FuHL20", "ACL": "2021.acl-long.558", "ArXiv": "2106.00641", "DOI": "10.18653/v1/2021.acl-long.558", "CorpusId": 235266246}, "corpusId": 235266246, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6efec9c3b3bb05b71c58786ffab6d40c31bcae96", "title": "SpanNER: Named Entity Re-/Recognition as Span Prediction", "abstract": "Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction. Despite its preliminary effectiveness, the span prediction model\u2019s architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems\u2019 outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all codes and datasets available: https://github.com/neulab/spanner, as well as an online system demo: http://spanner.sh. Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: http://explainaboard.nlpedia.ai/leaderboard/task-ner/.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 65, "citationCount": 61, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.558.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"pages": "7183-7195"}, "authors": [{"authorId": "41037252", "name": "Jinlan Fu"}, {"authorId": "1790227", "name": "Xuanjing Huang"}, {"authorId": "144118452", "name": "Pengfei Liu"}]}, {"paperId": "5042235979c7e91f0252e1d0bb182a55564a18f5", "externalIds": {"ArXiv": "2012.00857", "MAG": "3109067790", "DBLP": "journals/corr/abs-2012-00857", "ACL": "2021.acl-long.559", "DOI": "10.18653/v1/2021.acl-long.559", "CorpusId": 227247936}, "corpusId": 227247936, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5042235979c7e91f0252e1d0bb182a55564a18f5", "title": "StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling", "abstract": "There are two major classes of natural language grammars \u2014 the dependency grammar that models one-to-one correspondences between words and the constituency grammar that models the assembly of one or several corresponded words. While previous unsupervised parsing methods mostly focus on only inducing one class of grammars, we introduce a novel model, StructFormer, that can induce dependency and constituency structure at the same time. To achieve this, we propose a new parsing framework that can jointly generate a constituency tree and dependency graph. Then we integrate the induced dependency relations into the transformer, in a differentiable manner, through a novel dependency-constrained self-attention mechanism. Experimental results show that our model can achieve strong results on unsupervised constituency parsing, unsupervised dependency parsing, and masked language modeling at the same time.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 31, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.559.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-01", "journal": {"pages": "7196-7209"}, "authors": [{"authorId": "2714199", "name": "Yikang Shen"}, {"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "72117196", "name": "Che Zheng"}, {"authorId": "11774695", "name": "Dara Bahri"}, {"authorId": "1680617", "name": "Donald Metzler"}, {"authorId": "1760871", "name": "Aaron C. Courville"}]}, {"paperId": "ec844466feda60d664d5173ff011fea1c275f1fa", "externalIds": {"ArXiv": "2106.02082", "DBLP": "journals/corr/abs-2106-02082", "ACL": "2021.acl-long.560", "DOI": "10.18653/v1/2021.acl-long.560", "CorpusId": 235352806}, "corpusId": 235352806, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ec844466feda60d664d5173ff011fea1c275f1fa", "title": "Language Embeddings for Typology and Cross-lingual Transfer Learning", "abstract": "Cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data. We explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. We generate dense embeddings for 29 languages using a denoising autoencoder, and evaluate the embeddings using the World Atlas of Language Structures (WALS) and two extrinsic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.560.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"pages": "7210-7225"}, "authors": [{"authorId": "150978762", "name": "Dian Yu"}, {"authorId": "2107034039", "name": "Taiqi He"}, {"authorId": "1757166", "name": "Kenji Sagae"}]}, {"paperId": "6ac0dcfd9302ed128a035dfe77ebb9e665c01185", "externalIds": {"DBLP": "conf/acl/AldarrabM20", "ArXiv": "2012.15229", "ACL": "2021.acl-long.561", "DOI": "10.18653/v1/2021.acl-long.561", "CorpusId": 229923085}, "corpusId": 229923085, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6ac0dcfd9302ed128a035dfe77ebb9e665c01185", "title": "Can Sequence-to-Sequence Models Crack Substitution Ciphers?", "abstract": "Decipherment of historical ciphers is a challenging problem. The language of the target plaintext might be unknown, and ciphertext can have a lot of noise. State-of-the-art decipherment methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher, assuming the plaintext language is known. We propose an end-to-end multilingual model for solving simple substitution ciphers. We test our model on synthetic and real historical ciphers and show that our proposed method can decipher text without explicit language identification while still being robust to noise.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.561.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"pages": "7226-7235"}, "authors": [{"authorId": "79341148", "name": "Nada Aldarrab"}, {"authorId": "143823227", "name": "Jonathan May"}]}, {"paperId": "1d7e3bc217fe097ca39b366e98fbbe59fa6bff43", "externalIds": {"DBLP": "journals/corr/abs-2105-15087", "ACL": "2021.acl-long.562", "ArXiv": "2105.15087", "DOI": "10.18653/v1/2021.acl-long.562", "CorpusId": 235253968}, "corpusId": 235253968, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1d7e3bc217fe097ca39b366e98fbbe59fa6bff43", "title": "Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation", "abstract": "While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 53, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.562.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "7236-7249"}, "authors": [{"authorId": "40914545", "name": "Eleftheria Briakou"}, {"authorId": "2954727", "name": "Marine Carpuat"}]}, {"paperId": "c47cac224ff59892abfd6af316b0f9e082f97012", "externalIds": {"DBLP": "conf/acl/0001AR20", "ACL": "2021.acl-long.563", "DOI": "10.18653/v1/2021.acl-long.563", "CorpusId": 236460293}, "corpusId": 236460293, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c47cac224ff59892abfd6af316b0f9e082f97012", "title": "Discriminative Reranking for Neural Machine Translation", "abstract": "Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. These models have a long history in NLP, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. This takes as input both the source sentence as well as a list of hypotheses to output a ranked list. The reranker is trained to predict the observed distribution of a desired metric, e.g. BLEU, over the n-best list. Since such a discriminator contains hundreds of millions of parameters, we improve its generalization using pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 30, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.563.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "7250-7264"}, "authors": [{"authorId": "145732777", "name": "Ann Lee"}, {"authorId": "2325985", "name": "Michael Auli"}, {"authorId": "1706809", "name": "Marc'Aurelio Ranzato"}]}, {"paperId": "5441598e2b690a15198b7a38359e5936e4a46114", "externalIds": {"DBLP": "conf/acl/KaramchetiK0M20", "ArXiv": "2107.02331", "ACL": "2021.acl-long.564", "DOI": "10.18653/v1/2021.acl-long.564", "CorpusId": 235742735}, "corpusId": 235742735, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5441598e2b690a15198b7a38359e5936e4a46114", "title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering", "abstract": "Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers \u2013 groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 80, "citationCount": 57, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.564.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-06", "journal": {"name": "ArXiv", "volume": "abs/2107.02331"}, "authors": [{"authorId": "10737060", "name": "Siddharth Karamcheti"}, {"authorId": "145237361", "name": "Ranjay Krishna"}, {"authorId": "48004138", "name": "Li Fei-Fei"}, {"authorId": "144783904", "name": "Christopher D. Manning"}]}, {"paperId": "a16ae67070de155789a871cb27ecbf9eaa98b379", "externalIds": {"ArXiv": "2107.00061", "ACL": "2021.acl-long.565", "DBLP": "journals/corr/abs-2107-00061", "DOI": "10.18653/v1/2021.acl-long.565", "CorpusId": 235694265}, "corpusId": 235694265, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a16ae67070de155789a871cb27ecbf9eaa98b379", "title": "All That\u2019s \u2018Human\u2019 Is Not Gold: Evaluating Human Evaluation of Generated Text", "abstract": "Human evaluations are typically considered the gold standard in natural language generation, but as models\u2019 fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts\u2019 ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators\u2019 accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 175, "influentialCitationCount": 19, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.565.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-30", "journal": {"pages": "7282-7296"}, "authors": [{"authorId": "40684993", "name": "Elizabeth Clark"}, {"authorId": "50509991", "name": "Tal August"}, {"authorId": "38618739", "name": "Sofia Serrano"}, {"authorId": "3465456", "name": "Nikita Haduong"}, {"authorId": "40895369", "name": "Suchin Gururangan"}, {"authorId": "144365875", "name": "Noah A. Smith"}]}, {"paperId": "e399e78f2c236802aa50aef95554a3768079edb1", "externalIds": {"DBLP": "journals/corr/abs-2106-15195", "MAG": "3175955584", "ACL": "2021.acl-long.566", "ArXiv": "2106.15195", "DOI": "10.18653/v1/2021.acl-long.566", "CorpusId": 235670210}, "corpusId": 235670210, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e399e78f2c236802aa50aef95554a3768079edb1", "title": "Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers", "abstract": "This paper presents the first large-scale meta-evaluation of machine translation (MT). We annotated MT evaluations conducted in 769 research papers published from 2010 to 2020. Our study shows that practices for automatic MT evaluation have dramatically changed during the past decade and follow concerning trends. An increasing number of MT evaluations exclusively rely on differences between BLEU scores to draw conclusions, without performing any kind of statistical significance testing nor human evaluation, while at least 108 metrics claiming to be better than BLEU have been proposed. MT evaluations in recent papers tend to copy and compare automatic metric scores from previous work to claim the superiority of a method or an algorithm without confirming neither exactly the same training, validating, and testing data have been used nor the metric scores are comparable. Furthermore, tools for reporting standardized metric scores are still far from being widely adopted by the MT community. After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 79, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.566.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-29", "journal": {"name": "ArXiv", "volume": "abs/2106.15195"}, "authors": [{"authorId": "2064068087", "name": "Benjamin Marie"}, {"authorId": "46566611", "name": "Atsushi Fujita"}, {"authorId": "1731383", "name": "Rapha\u00ebl Rubino"}]}, {"paperId": "73ee65c312e64d7c5e581d55082d61c0f26707fb", "externalIds": {"DBLP": "conf/acl/00020LLL20", "ACL": "2021.acl-long.567", "ArXiv": "2105.11269", "DOI": "10.18653/v1/2021.acl-long.567", "CorpusId": 235166182}, "corpusId": 235166182, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/73ee65c312e64d7c5e581d55082d61c0f26707fb", "title": "Neural Machine Translation with Monolingual Translation Memory", "abstract": "Prior work has proved that Translation Memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 57, "citationCount": 54, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.567.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-24", "journal": {"pages": "7307-7318"}, "authors": [{"authorId": "46689291", "name": "Deng Cai"}, {"authorId": "2152546690", "name": "Yan Wang"}, {"authorId": "91956362", "name": "Huayang Li"}, {"authorId": "144594306", "name": "Wai Lam"}, {"authorId": "2978364", "name": "Lemao Liu"}]}, {"paperId": "e54ffc76d805c48660bb0fd20019ca82ac94ba0d", "externalIds": {"ArXiv": "2012.13255", "DBLP": "journals/corr/abs-2012-13255", "ACL": "2021.acl-long.568", "DOI": "10.18653/v1/2021.acl-long.568", "CorpusId": 229371560}, "corpusId": 229371560, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e54ffc76d805c48660bb0fd20019ca82ac94ba0d", "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning", "abstract": "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 176, "influentialCitationCount": 24, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.568.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-22", "journal": {"name": "ArXiv", "volume": "abs/2012.13255"}, "authors": [{"authorId": "2201435", "name": "Armen Aghajanyan"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}, {"authorId": "2118343423", "name": "Sonal Gupta"}]}, {"paperId": "2e3a7760c543a181b47245ffece91bff027c43c9", "externalIds": {"ArXiv": "2101.00010", "DBLP": "journals/corr/abs-2101-00010", "ACL": "2021.acl-long.569", "DOI": "10.18653/v1/2021.acl-long.569", "CorpusId": 230435766}, "corpusId": 230435766, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2e3a7760c543a181b47245ffece91bff027c43c9", "title": "UnNatural Language Inference", "abstract": "Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to understand human-like syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle to understand the meaning of ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word order invariant. For example, in MNLI dataset we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists in pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 109, "citationCount": 60, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.569.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-30", "journal": {"name": "ArXiv", "volume": "abs/2101.00010"}, "authors": [{"authorId": "40910779", "name": "Koustuv Sinha"}, {"authorId": "32899078", "name": "Prasanna Parthasarathi"}, {"authorId": "145134886", "name": "Joelle Pineau"}, {"authorId": "81840293", "name": "Adina Williams"}]}, {"paperId": "c4358134954d8e62939c3a8b9ba8e953d951f73b", "externalIds": {"ACL": "2021.acl-long.570", "ArXiv": "2105.05222", "DBLP": "conf/acl/YinMHGA20", "DOI": "10.18653/v1/2021.acl-long.570", "CorpusId": 234358053}, "corpusId": 234358053, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c4358134954d8e62939c3a8b9ba8e953d951f73b", "title": "Including Signed Languages in Natural Language Processing", "abstract": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 115, "citationCount": 60, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.570.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-05-11", "journal": {"pages": "7347-7360"}, "authors": [{"authorId": "1602993448", "name": "Kayo Yin"}, {"authorId": "73769459", "name": "Amit Moryossef"}, {"authorId": "1738680812", "name": "J. Hochgesang"}, {"authorId": "79775260", "name": "Yoav Goldberg"}, {"authorId": "2715920", "name": "Malihe Alikhani"}]}, {"paperId": "b086b812c867b1d07eb65bcdd206dd0891733f9d", "externalIds": {"DBLP": "conf/acl/XuZGZL20", "ACL": "2021.acl-long.571", "ArXiv": "2012.15671", "DOI": "10.18653/v1/2021.acl-long.571", "CorpusId": 235377446}, "corpusId": 235377446, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b086b812c867b1d07eb65bcdd206dd0891733f9d", "title": "Vocabulary Learning via Optimal Transport for Neural Machine Translation", "abstract": "The choice of token vocabulary affects the performance of machine translation. This paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without trial training. To answer these questions, we first provide an alternative understanding of vocabulary from the perspective of information theory. It motivates us to formulate the quest of vocabularization \u2013 finding the best token dictionary with a proper size \u2013 as an optimal transport (OT) problem. We propose VOLT, a simple and efficient solution without trial training. Empirical results show that VOLT beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation. For example, VOLT achieves 70% vocabulary size reduction and 0.5 BLEU gain on English-German translation. Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https://github.com/Jingjing-NLP/VOLT.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 53, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.571.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-31", "journal": {"pages": "7361-7373"}, "authors": [{"authorId": "47883405", "name": "Jingjing Xu"}, {"authorId": "2111824520", "name": "Hao Zhou"}, {"authorId": "2056157609", "name": "Chun Gan"}, {"authorId": "24018493", "name": "Zaixiang Zheng"}, {"authorId": "143900005", "name": "Lei Li"}]}, {"paperId": "50058cb053ede6c630dc39d4db07cea2b668367d", "externalIds": {"DBLP": "conf/acl/SweedS20", "ArXiv": "2106.04830", "ACL": "2021.acl-short.1", "DOI": "10.18653/v1/2021.acl-short.1", "CorpusId": 235377385}, "corpusId": 235377385, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/50058cb053ede6c630dc39d4db07cea2b668367d", "title": "Catchphrase: Automatic Detection of Cultural References", "abstract": "A snowclone is a customizable phrasal template that can be realized in multiple, instantly recognized variants. For example, \u201c* is the new *\" (Orange is the new black, 40 is the new 30). Snowclones are extensively used in social media. In this paper, we study snowclones originating from pop-culture quotes; our goal is to automatically detect cultural references in text. We introduce a new, publicly available data set of pop-culture quotes and their corresponding snowclone usages and train models on them. We publish code for Catchphrase, an internet browser plugin to automatically detect and mark references in real-time, and examine its performance via a user study. Aside from assisting people to better comprehend cultural references, we hope that detecting snowclones can complement work on paraphrasing and help tackling long-standing questions in social science about the dynamics of information propagation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 20, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.1.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-09", "journal": {"pages": "1-7"}, "authors": [{"authorId": "2108952323", "name": "Nir Sweed"}, {"authorId": "1805894", "name": "Dafna Shahaf"}]}, {"paperId": "e3c5ef26c54203c6b370e6d37ef8178ef231e5e1", "externalIds": {"ArXiv": "2107.03176", "ACL": "2021.acl-short.2", "DBLP": "journals/corr/abs-2107-03176", "DOI": "10.18653/v1/2021.acl-short.2", "CorpusId": 235755110}, "corpusId": 235755110, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e3c5ef26c54203c6b370e6d37ef8178ef231e5e1", "title": "On Training Instance Selection for Few-Shot Neural Text Generation", "abstract": "Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available. We hope that this work will call for more attention on this largely unexplored area.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 23, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.2.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-07", "journal": {"name": "ArXiv", "volume": "abs/2107.03176"}, "authors": [{"authorId": "48025720", "name": "Ernie Chang"}, {"authorId": "2562211", "name": "Xiaoyu Shen"}, {"authorId": "2047999043", "name": "Hui-Syuan Yeh"}, {"authorId": "2869436", "name": "Vera Demberg"}]}, {"paperId": "3029263ca51e6c2907f9f99277083cf6afb1adb7", "externalIds": {"DBLP": "journals/corr/abs-2101-00434", "ACL": "2021.acl-short.3", "ArXiv": "2101.00434", "DOI": "10.18653/v1/2021.acl-short.3", "CorpusId": 230435783}, "corpusId": 230435783, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3029263ca51e6c2907f9f99277083cf6afb1adb7", "title": "Coreference Resolution without Span Representations", "abstract": "The introduction of pretrained language models has reduced many complex task-specific NLP models to simple lightweight layers. An exception to this trend is coreference resolution, where a sophisticated task-specific model is appended to a pretrained transformer encoder. While highly effective, the model has a very large memory footprint \u2013 primarily due to dynamically-constructed span and span-pair representations \u2013 which hinders the processing of complete documents and the ability to train on multiple instances in a single batch. We introduce a lightweight end-to-end coreference model that removes the dependency on span representations, handcrafted features, and heuristics. Our model performs competitively with the current standard model, while being simpler and more efficient.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 18, "citationCount": 53, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.3.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-02", "journal": {"pages": "14-19"}, "authors": [{"authorId": "2044194129", "name": "Yuval Kirstain"}, {"authorId": "73775461", "name": "Ori Ram"}, {"authorId": "39455775", "name": "Omer Levy"}]}, {"paperId": "086c78fd5b53f016a2a9f067b12f0c47966382d7", "externalIds": {"ACL": "2021.acl-short.4", "DBLP": "conf/acl/ChenK20", "DOI": "10.18653/v1/2021.acl-short.4", "CorpusId": 236459831}, "corpusId": 236459831, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/086c78fd5b53f016a2a9f067b12f0c47966382d7", "title": "Enhancing Entity Boundary Detection for Better Chinese Named Entity Recognition", "abstract": "In comparison with English, due to the lack of explicit word boundary and tenses information, Chinese Named Entity Recognition (NER) is much more challenging. In this paper, we propose a boundary enhanced approach for better Chinese NER. In particular, our approach enhances the boundary information from two perspectives. On one hand, we enhance the representation of the internal dependency of phrases by an additional Graph Attention Network(GAT) layer. On the other hand, taking the entity head-tail prediction (i.e., boundaries) as an auxiliary task, we propose an unified framework to learn the boundary information and recognize the NE jointly. Experiments on both the OntoNotes and the Weibo corpora show the effectiveness of our approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 17, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.4.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "20-25"}, "authors": [{"authorId": "2109526406", "name": "Chun Chen"}, {"authorId": "47425794", "name": "Fang Kong"}]}, {"paperId": "3e724faf78a51d5cc3852d79a339b820150628d8", "externalIds": {"DBLP": "conf/acl/Zhan0WC20", "ArXiv": "2107.14402", "ACL": "2021.acl-short.5", "DOI": "10.18653/v1/2021.acl-short.5", "CorpusId": 236460223}, "corpusId": 236460223, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3e724faf78a51d5cc3852d79a339b820150628d8", "title": "Difficulty-Aware Machine Translation Evaluation", "abstract": "The high-quality translation results produced by machine translation (MT) systems still pose a huge challenge for automatic evaluation. Current MT evaluation pays the same attention to each sentence component, while the questions of real-world examinations (e.g., university examinations) have different difficulties and weightings. In this paper, we propose a novel difficulty-aware MT evaluation metric, expanding the evaluation dimension by taking translation difficulty into consideration. A translation that fails to be predicted by most MT systems will be treated as a difficult one and assigned a large weight in the final score function, and conversely. Experimental results on the WMT19 English-German Metrics shared tasks show that our proposed method outperforms commonly used MT metrics in terms of human correlation. In particular, our proposed method performs well even when all the MT systems are very competitive, which is when most existing metrics fail to distinguish between them. The source code is freely available at https://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.5.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-30", "journal": {"pages": "26-32"}, "authors": [{"authorId": "2051867267", "name": "Runzhe Zhan"}, {"authorId": "2151060023", "name": "Xuebo Liu"}, {"authorId": "1758353", "name": "Derek F. Wong"}, {"authorId": "1774304", "name": "Lidia S. Chao"}]}, {"paperId": "daae582be667cc24fafb2b93208e377c2c5dac37", "externalIds": {"ArXiv": "2012.12007", "DBLP": "journals/corr/abs-2012-12007", "ACL": "2021.acl-short.6", "DOI": "10.18653/v1/2021.acl-short.6", "CorpusId": 229349316}, "corpusId": 229349316, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/daae582be667cc24fafb2b93208e377c2c5dac37", "title": "Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition", "abstract": "Humor recognition has been widely studied as a text classification problem using data-driven approaches. However, most existing work does not examine the actual joke mechanism to understand humor. We break down any joke into two distinct components: the set-up and the punchline, and further explore the special relationship between them. Inspired by the incongruity theory of humor, we model the set-up as the part developing semantic uncertainty, and the punchline disrupting audience expectations. With increasingly powerful language models, we were able to feed the set-up along with the punchline into the GPT-2 language model, and calculate the uncertainty and surprisal values of the jokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found that these two features have better capabilities of telling jokes from non-jokes, compared with existing baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.6.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-22", "journal": {"name": "ArXiv", "volume": "abs/2012.12007"}, "authors": [{"authorId": "2118596012", "name": "Yubo Xie"}, {"authorId": "2108920576", "name": "Junze Li"}, {"authorId": "1781996", "name": "P. Pu"}]}, {"paperId": "1cb9df8d24eb0e346bc08b2e5ca3827807b6aff0", "externalIds": {"ACL": "2021.acl-short.7", "DBLP": "conf/acl/NangiCKKN20", "DOI": "10.18653/v1/2021.acl-short.7", "CorpusId": 236459866}, "corpusId": 236459866, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1cb9df8d24eb0e346bc08b2e5ca3827807b6aff0", "title": "Counterfactuals to Control Latent Disentangled Text Representations for Style Transfer", "abstract": "Disentanglement of latent representations into content and style spaces has been a commonly employed method for unsupervised text style transfer. These techniques aim to learn the disentangled representations and tweak them to modify the style of a sentence. In this paper, we propose a counterfactual-based method to modify the latent representation, by posing a \u2018what-if\u2019 scenario. This simple and disciplined approach also enables a fine-grained control on the transfer strength. We conduct experiments with the proposed methodology on multiple attribute transfer tasks like Sentiment, Formality and Excitement to support our hypothesis.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 14, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.7.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "40-48"}, "authors": [{"authorId": "1396843975", "name": "Sharmila Reddy Nangi"}, {"authorId": "2954043", "name": "Niyati Chhaya"}, {"authorId": "10258164", "name": "Sopan Khosla"}, {"authorId": "123066787", "name": "Nikhil Kaushik"}, {"authorId": "2121300690", "name": "Harshit Nyati"}]}, {"paperId": "da130d6538eeeacfb3a0da4cff106c098f74cdd4", "externalIds": {"DBLP": "conf/acl/EthayarajhJ20", "ArXiv": "2105.14652", "ACL": "2021.acl-short.8", "MAG": "3170204075", "DOI": "10.18653/v1/2021.acl-short.8", "CorpusId": 235254444}, "corpusId": 235254444, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/da130d6538eeeacfb3a0da4cff106c098f74cdd4", "title": "Attention Flows are Shapley Value Explanations", "abstract": "Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. In NLP, however, leave-one-out and attention-based explanations still predominate. Can we draw a connection between these different methods? We formally prove that \u2014 save for the degenerate case \u2014 attention weights and leave-one-out values cannot be Shapley Values. Attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. Perhaps surprisingly, we prove that attention flows are indeed Shapley Values, at least at the layerwise level. Given the many desirable theoretical qualities of Shapley Values \u2014 which has driven their adoption among the ML community \u2014 we argue that NLP practitioners should, when possible, adopt attention flow explanations alongside more traditional ones.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 20, "citationCount": 22, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.8.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "49-54"}, "authors": [{"authorId": "10324691", "name": "Kawin Ethayarajh"}, {"authorId": "1746807", "name": "Dan Jurafsky"}]}, {"paperId": "3d9c86d3bfd856299fcd1152e59d3f2fb7cded83", "externalIds": {"DBLP": "conf/acl/Liu020", "ACL": "2021.acl-short.9", "DOI": "10.18653/v1/2021.acl-short.9", "CorpusId": 236459989}, "corpusId": 236459989, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3d9c86d3bfd856299fcd1152e59d3f2fb7cded83", "title": "Video Paragraph Captioning as a Text Summarization Task", "abstract": "Video paragraph captioning aims to generate a set of coherent sentences to describe a video that contains several events. Most previous methods simplify this task by using ground-truth event segments. In this work, we propose a novel framework by taking this task as a text summarization task. We first generate lots of sentence-level captions focusing on different video clips and then summarize these captions to obtain the final paragraph caption. Our method does not depend on ground-truth event segments. Experiments on two popular datasets ActivityNet Captions and YouCookII demonstrate the advantages of our new framework. On the ActivityNet dataset, our method even outperforms some previous methods using ground-truth event segment labels.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 9, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.9.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "55-60"}, "authors": [{"authorId": "72217143", "name": "Hui Liu"}, {"authorId": "145078589", "name": "Xiaojun Wan"}]}, {"paperId": "abaa9cc845d3b18328d2014d780235fdd9695d1e", "externalIds": {"DBLP": "conf/acl/RosenbergGFR20", "ArXiv": "2106.04484", "ACL": "2021.acl-short.10", "DOI": "10.18653/v1/2021.acl-short.10", "CorpusId": 235367702}, "corpusId": 235367702, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/abaa9cc845d3b18328d2014d780235fdd9695d1e", "title": "Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions", "abstract": "Deep learning algorithms have shown promising results in visual question answering (VQA) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. To understand and better measure the generalization capabilities of VQA systems, we look at their robustness to counterfactually augmented data. Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. Using these augmentations, we propose a new robustness measure, Robustness to Augmented Data (RAD), which measures the consistency of model predictions between original and augmented examples. Through extensive experimentation, we show that RAD, unlike classical accuracy measures, can quantify when state-of-the-art systems are not robust to counterfactuals. We find substantial failure cases which reveal that current VQA systems are still brittle. Finally, we connect between robustness and generalization, demonstrating the predictive power of RAD for performance on unseen augmentations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.10.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"pages": "61-70"}, "authors": [{"authorId": "2107634759", "name": "Daniel Rosenberg"}, {"authorId": "2064713742", "name": "Itai Gat"}, {"authorId": "46609506", "name": "Amir Feder"}, {"authorId": "1762757", "name": "Roi Reichart"}]}, {"paperId": "dffd39f36deaf68e9916b945800757993895baf8", "externalIds": {"DBLP": "conf/acl/GhoshQCS20", "ACL": "2021.acl-short.11", "DOI": "10.18653/v1/2021.acl-short.11", "CorpusId": 236460192}, "corpusId": 236460192, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dffd39f36deaf68e9916b945800757993895baf8", "title": "How Helpful is Inverse Reinforcement Learning for Table-to-Text Generation?", "abstract": "Existing approaches for the Table-to-Text task suffer from issues such as missing information, hallucination and repetition. Many approaches to this problem use Reinforcement Learning (RL), which maximizes a single manually defined reward, such as BLEU. In this work, we instead pose the Table-to-Text task as Inverse Reinforcement Learning (IRL) problem. We explore using multiple interpretable unsupervised reward components that are combined linearly to form a composite reward function. The composite reward function and the description generator are learned jointly. We find that IRL outperforms strong RL baselines marginally. We further study the generalization of learned IRL rewards in scenarios involving domain adaptation. Our experiments reveal significant challenges in using IRL for this task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 31, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "71-79"}, "authors": [{"authorId": "144660491", "name": "Sayan Ghosh"}, {"authorId": "2121544060", "name": "Zheng Qi"}, {"authorId": "37202877", "name": "Snigdha Chaturvedi"}, {"authorId": "3284698", "name": "Shashank Srivastava"}]}, {"paperId": "f830aae938f3a7611a0ceabe2a6cd432660e5116", "externalIds": {"ArXiv": "2105.07698", "DBLP": "conf/acl/Hansen0L20", "ACL": "2021.acl-short.12", "DOI": "10.18653/v1/2021.acl-short.12", "CorpusId": 234742074}, "corpusId": 234742074, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f830aae938f3a7611a0ceabe2a6cd432660e5116", "title": "Automatic Fake News Detection: Are Models Learning to Reason?", "abstract": "Most fact checking models for automatic fake news detection are based on reasoning: given a claim with associated evidence, the models aim to estimate the claim veracity based on the supporting or refuting content within the evidence. When these models perform well, it is generally assumed to be due to the models having learned to reason over the evidence with regards to the claim. In this paper, we investigate this assumption of reasoning, by exploring the relationship and importance of both claim and evidence. Surprisingly, we find on political fact checking datasets that most often the highest effectiveness is obtained by utilizing only the evidence, as the impact of including the claim is either negligible or harmful to the effectiveness. This highlights an important problem in what constitutes evidence in existing approaches for automatic fake news detection.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 9, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.12.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-17", "journal": {"pages": "80-86"}, "authors": [{"authorId": "144613166", "name": "Casper Hansen"}, {"authorId": "144613163", "name": "Christian Hansen"}, {"authorId": "145980035", "name": "Lucas Chaves Lima"}]}, {"paperId": "2664a8ff0e5e779ea1d1ae8b4868e6d980bb40a6", "externalIds": {"ACL": "2021.acl-short.13", "DBLP": "journals/corr/abs-2012-01873", "ArXiv": "2012.01873", "MAG": "3108508534", "DOI": "10.18653/v1/2021.acl-short.13", "CorpusId": 227254771}, "corpusId": 227254771, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2664a8ff0e5e779ea1d1ae8b4868e6d980bb40a6", "title": "Saying No is An Art: Contextualized Fallback Responses for Unanswerable Dialogue Queries", "abstract": "Despite end-to-end neural systems making significant progress in the last decade for task-oriented as well as chit-chat based dialogue systems, most dialogue systems rely on hybrid approaches which use a combination of rule-based, retrieval and generative approaches for generating a set of ranked responses. Such dialogue systems need to rely on a fallback mechanism to respond to out-of-domain or novel user queries which are not answerable within the scope of the dialogue system. While, dialogue systems today rely on static and unnatural responses like \u201cI don\u2019t know the answer to that question\u201d or \u201cI\u2019m not sure about that\u201d, we design a neural approach which generates responses which are contextually aware with the user query as well as say no to the user. Such customized responses provide paraphrasing ability and contextualization as well as improve the interaction with the user and reduce dialogue monotonicity. Our simple approach makes use of rules over dependency parses and a text-to-text transformer fine-tuned on synthetic data of question-response pairs generating highly relevant, grammatical as well as diverse questions. We perform automatic and manual evaluations to demonstrate the efficacy of the system.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.13.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-03", "journal": {"name": "ArXiv", "volume": "abs/2012.01873"}, "authors": [{"authorId": "1490900960", "name": "A. Shrivastava"}, {"authorId": "4834571", "name": "Kaustubh D. Dhole"}, {"authorId": "2063973505", "name": "Abhinav Bhatt"}, {"authorId": "2030986125", "name": "Sharvani Raghunath"}]}, {"paperId": "b1db3f66d7b5cd8c7b72b824bac487c1b6a2b487", "externalIds": {"DBLP": "conf/acl/GanesanBBVT20", "ACL": "2021.acl-short.14", "ArXiv": "2106.06519", "DOI": "10.18653/v1/2021.acl-short.14", "CorpusId": 235417477}, "corpusId": 235417477, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b1db3f66d7b5cd8c7b72b824bac487c1b6a2b487", "title": "N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses", "abstract": "Spoken Language Understanding (SLU) systems parse speech into semantic structures like dialog acts and slots. This involves the use of an Automatic Speech Recognizer (ASR) to transcribe speech into multiple text alternatives (hypotheses). Transcription errors, ordinary in ASRs, impact downstream SLU performance negatively. Common approaches to mitigate such errors involve using richer information from the ASR, either in form of N-best hypotheses or word-lattices. We hypothesize that transformer models will learn better with a simpler utterance representation using the concatenation of the N-best ASR alternatives, where each alternative is separated by a special delimiter [SEP]. In our work, we test our hypothesis by using the concatenated N-best ASR alternatives as the input to the transformer encoder models, namely BERT and XLM-RoBERTa, and achieve equivalent performance to the prior state-of-the-art model on DSTC2 dataset. We also show that our approach significantly outperforms the prior state-of-the-art when subjected to the low data regime. Additionally, this methodology is accessible to users of third-party ASR APIs which do not provide word-lattice information.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 17, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.14.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-11", "journal": {"name": "ArXiv", "volume": "abs/2106.06519"}, "authors": [{"authorId": "2056971549", "name": "Karthik Ganesan"}, {"authorId": "147770413", "name": "P. Bamdev"}, {"authorId": "2228883806", "name": "Jaivarsan B"}, {"authorId": "2111682172", "name": "Amresh Venugopal"}, {"authorId": "2814621", "name": "A. Tushar"}]}, {"paperId": "63052e581f1b272eefdbf109a230c7ec87e1f79a", "externalIds": {"ACL": "2021.acl-short.15", "DBLP": "conf/acl/RenduchintalaDH20", "ArXiv": "2106.00169", "DOI": "10.18653/v1/2021.acl-short.15", "CorpusId": 235266085}, "corpusId": 235266085, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/63052e581f1b272eefdbf109a230c7ec87e1f79a", "title": "Gender bias amplification during Speed-Quality optimization in Neural Machine Translation", "abstract": "Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 27, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.15.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"pages": "99-109"}, "authors": [{"authorId": "2106411510", "name": "Adithya Renduchintala"}, {"authorId": "2036504384", "name": "Denise D\u00edaz"}, {"authorId": "1702066", "name": "Kenneth Heafield"}, {"authorId": "2116235416", "name": "Xian Li"}, {"authorId": "1700007", "name": "Mona T. Diab"}]}, {"paperId": "a3d22d18905e09eea53bcc2cd619037f4db736ea", "externalIds": {"DBLP": "journals/corr/abs-2106-06797", "ArXiv": "2106.06797", "ACL": "2021.acl-short.16", "DOI": "10.18653/v1/2021.acl-short.16", "CorpusId": 235422410}, "corpusId": 235422410, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a3d22d18905e09eea53bcc2cd619037f4db736ea", "title": "Machine Translation into Low-resource Language Varieties", "abstract": "State-of-the-art machine translation (MT) systems are typically trained to generate \u201cstandard\u201d target language; however, many languages have multiple varieties (regional varieties, dialects, sociolects, non-native varieties) that are different from the standard language. Such varieties are often low-resource, and hence do not benefit from contemporary NLP solutions, MT included. We propose a general framework to rapidly adapt MT systems to generate language varieties that are close to, but different from, the standard target language, using no parallel (source\u2013variety) data. This also includes adaptation of MT systems to low-resource typologically-related target languages. We experiment with adapting an English\u2013Russian MT system to generate Ukrainian and Belarusian, an English\u2013Norwegian Bokm\u00e5l system to generate Nynorsk, and an English\u2013Arabic system to generate four Arabic dialects, obtaining significant improvements over competitive baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 56, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.16.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-12", "journal": {"name": "ArXiv", "volume": "abs/2106.06797"}, "authors": [{"authorId": "51467955", "name": "Sachin Kumar"}, {"authorId": "49513989", "name": "Antonios Anastasopoulos"}, {"authorId": "2524073", "name": "S. Wintner"}, {"authorId": "145317727", "name": "Yulia Tsvetkov"}]}, {"paperId": "bc73d53ba859c56ab08c41c475d45a9ae6d021cb", "externalIds": {"DBLP": "conf/acl/MeisterLAC20", "ArXiv": "2106.01087", "ACL": "2021.acl-short.17", "DOI": "10.18653/v1/2021.acl-short.17", "CorpusId": 235293798}, "corpusId": 235293798, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bc73d53ba859c56ab08c41c475d45a9ae6d021cb", "title": "Is Sparse Attention more Interpretable?", "abstract": "Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. On three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists\u2014under sparse attention and otherwise. Further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. Rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 47, "citationCount": 22, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.17.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"pages": "122-129"}, "authors": [{"authorId": "150953620", "name": "Clara Meister"}, {"authorId": "2106626313", "name": "Stefan Lazov"}, {"authorId": "1736067", "name": "Isabelle Augenstein"}, {"authorId": "2070989574", "name": "Ryan Cotterell"}]}, {"paperId": "d8e7bad2681ce70277c900c77a22181d4b03d705", "externalIds": {"DBLP": "conf/acl/WennbergH20", "ACL": "2021.acl-short.18", "ArXiv": "2106.01950", "DOI": "10.18653/v1/2021.acl-short.18", "CorpusId": 235313860}, "corpusId": 235313860, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d8e7bad2681ce70277c900c77a22181d4b03d705", "title": "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models", "abstract": "Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Proof-of-concept experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.18.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"pages": "130-140"}, "authors": [{"authorId": "1387977694", "name": "Ulme Wennberg"}, {"authorId": "2763884", "name": "G. Henter"}]}, {"paperId": "e1afe296314493791b1c2c0af88cfe97279bdc69", "externalIds": {"ArXiv": "2106.03471", "DBLP": "journals/corr/abs-2106-03471", "MAG": "3170949577", "ACL": "2021.acl-short.19", "DOI": "10.18653/v1/2021.acl-short.19", "CorpusId": 235358922}, "corpusId": 235358922, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e1afe296314493791b1c2c0af88cfe97279bdc69", "title": "Relative Importance in Sentence Processing", "abstract": "Determining the relative importance of the elements in a sentence is a key factor for effortless natural language understanding. For human language processing, we can approximate patterns of relative importance by measuring reading fixations using eye-tracking technology. In neural language models, gradient-based saliency methods indicate the relative importance of a token for the target objective. In this work, we compare patterns of relative importance in English language processing by humans and models and analyze the underlying linguistic patterns. We find that human processing patterns in English correlate strongly with saliency-based importance in language models and not with attention-based importance. Our results indicate that saliency could be a cognitively more plausible metric for interpreting neural language models. The code is available on github: https://github.com/beinborn/relative_importance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 57, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.19.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "journal": {"pages": "141-150"}, "authors": [{"authorId": "3191620", "name": "Nora Hollenstein"}, {"authorId": "2752573", "name": "Lisa Beinborn"}]}, {"paperId": "f9ff3facd992951415e67ce08cd31c5ba8c04b4e", "externalIds": {"DBLP": "journals/corr/abs-2107-01791", "ACL": "2021.acl-short.20", "ArXiv": "2107.01791", "DOI": "10.18653/v1/2021.acl-short.20", "CorpusId": 235732204}, "corpusId": 235732204, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f9ff3facd992951415e67ce08cd31c5ba8c04b4e", "title": "Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models", "abstract": "Pretrained language models (PLM) achieve surprising performance on the Choice of Plausible Alternatives (COPA) task. However, whether PLMs have truly acquired the ability of causal reasoning remains a question. In this paper, we investigate the problem of semantic similarity bias and reveal the vulnerability of current COPA models by certain attacks. Previous solutions that tackle the superficial cues of unbalanced token distribution still encounter the same problem of semantic bias, even more seriously due to the utilization of more training data. We mitigate this problem by simply adding a regularization loss and experimental results show that this solution not only improves the model\u2019s generalization ability, but also assists the models to perform more robustly on a challenging dataset, BCOPA-CE, which has unbiased token distribution and is more difficult for models to distinguish cause and effect.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 20, "citationCount": 7, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.20.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-05", "journal": {"name": "ArXiv", "volume": "abs/2107.01791"}, "authors": [{"authorId": "2111856818", "name": "Mingyue Han"}, {"authorId": "2143404491", "name": "Yinglin Wang"}]}, {"paperId": "7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8", "externalIds": {"ACL": "2021.acl-short.21", "DBLP": "conf/acl/TraylorFP20", "DOI": "10.18653/v1/2021.acl-short.21", "CorpusId": 236459821}, "corpusId": 236459821, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8", "title": "AND does not mean OR: Using Formal Languages to Study Language Models\u2019 Representations", "abstract": "A current open question in natural language processing is to what extent language models, which are trained with access only to the form of language, are able to capture the meaning of language. This question is challenging to answer in general, as there is no clear line between meaning and form, but rather meaning constrains form in consistent ways. The goal of this study is to offer insights into a narrower but critical subquestion: Under what conditions should we expect that meaning and form covary sufficiently, such that a language model with access only to form might nonetheless succeed in emulating meaning? Focusing on several formal languages (propositional logic and a set of programming languages), we generate training corpora using a variety of motivated constraints, and measure a distributional language model\u2019s ability to differentiate logical symbols (AND, OR, and NOT). Our findings are largely negative: none of our simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current models are able to exploit.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 17, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.21.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "158-167"}, "authors": [{"authorId": "48666382", "name": "Aaron Traylor"}, {"authorId": "48522350", "name": "Roman Feiman"}, {"authorId": "2949185", "name": "Ellie Pavlick"}]}, {"paperId": "447a36764df662076e0be38e2ed14aed7f5bace6", "externalIds": {"ACL": "2021.acl-short.22", "DBLP": "journals/corr/abs-2107-05833", "ArXiv": "2107.05833", "DOI": "10.18653/v1/2021.acl-short.22", "CorpusId": 235829779}, "corpusId": 235829779, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/447a36764df662076e0be38e2ed14aed7f5bace6", "title": "Enforcing Consistency in Weakly Supervised Semantic Parsing", "abstract": "The predominant challenge in weakly supervised semantic parsing is that of spurious programs that evaluate to correct answers for the wrong reasons. Prior work uses elaborate search strategies to mitigate the prevalence of spurious programs; however, they typically consider only one input at a time. In this work we explore the use of consistency between the output programs for related inputs to reduce the impact of spurious programs. We bias the program search (and thus the model\u2019s training signal) towards programs that map the same phrase in related inputs to the same sub-parts in their respective programs. Additionally, we study the importance of designing logical formalisms that facilitate this kind of consistency-based training. We find that a more consistent formalism leads to improved model performance even without consistency-based training. When combined together, these two insights lead to a 10% absolute improvement over the best prior result on the Natural Language Visual Reasoning dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 17, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.22.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-13", "journal": {"pages": "168-174"}, "authors": [{"authorId": "2285178", "name": "Nitish Gupta"}, {"authorId": "34650964", "name": "Sameer Singh"}, {"authorId": "40642935", "name": "Matt Gardner"}]}, {"paperId": "4dc2c552bfb0abcf15db9cfe795da9e97551ee42", "externalIds": {"ArXiv": "2106.01933", "DBLP": "journals/corr/abs-2106-01933", "ACL": "2021.acl-short.23", "DOI": "10.18653/v1/2021.acl-short.23", "CorpusId": 235313546}, "corpusId": 235313546, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4dc2c552bfb0abcf15db9cfe795da9e97551ee42", "title": "An Improved Model for Voicing Silent Speech", "abstract": "In this paper, we present an improved model for voicing silent speech, where audio is synthesized from facial electromyography (EMG) signals. To give our model greater flexibility to learn its own input features, we directly use EMG signals as input in the place of hand-designed features used by prior work. Our model uses convolutional layers to extract features from the signals and Transformer layers to propagate information across longer distances. To provide better signal for learning, we also introduce an auxiliary task of predicting phoneme labels in addition to predicting speech audio features. On an open vocabulary intelligibility evaluation, our model improves the state of the art for this task by an absolute 25.8%.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 9, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.23.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"pages": "175-181"}, "authors": [{"authorId": "40583046", "name": "David Gaddy"}, {"authorId": "2061335724", "name": "Dana Klein"}]}, {"paperId": "5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec", "externalIds": {"DBLP": "journals/corr/abs-2105-02732", "ArXiv": "2105.02732", "ACL": "2021.acl-short.24", "DOI": "10.18653/v1/2021.acl-short.24", "CorpusId": 233864521}, "corpusId": 233864521, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec", "title": "What\u2019s in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus", "abstract": "Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit content, even after filtering procedures. We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 72, "citationCount": 55, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.24.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-06", "journal": {"pages": "182-189"}, "authors": [{"authorId": "2993731", "name": "A. Luccioni"}, {"authorId": "3739533", "name": "J. Viviano"}]}, {"paperId": "7bf530a2d01cd29b7c09116f2437b799aadd1b05", "externalIds": {"DBLP": "conf/acl/ObamuyideFS20", "ACL": "2021.acl-short.25", "DOI": "10.18653/v1/2021.acl-short.25", "CorpusId": 236460159}, "corpusId": 236460159, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7bf530a2d01cd29b7c09116f2437b799aadd1b05", "title": "Continual Quality Estimation with Online Bayesian Meta-Learning", "abstract": "Most current quality estimation (QE) models for machine translation are trained and evaluated in a static setting where training and test data are assumed to be from a fixed distribution. However, in real-life settings, the test data that a deployed QE model would be exposed to may differ from its training data. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. To address this challenge, we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users, while being robust to distributional shifts in training and test data. Experiments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 46, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "190-197"}, "authors": [{"authorId": "22313325", "name": "A. Obamuyide"}, {"authorId": "2006017", "name": "M. Fomicheva"}, {"authorId": "2065700826", "name": "Lucia Specia"}]}, {"paperId": "6515d5dc6a04050a9dd46ccebff9ef745ffe95f0", "externalIds": {"DBLP": "conf/acl/ShangMLYC20", "ACL": "2021.acl-short.26", "DOI": "10.18653/v1/2021.acl-short.26", "CorpusId": 236459992}, "corpusId": 236459992, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6515d5dc6a04050a9dd46ccebff9ef745ffe95f0", "title": "A Span-based Dynamic Local Attention Model for Sequential Sentence Classification", "abstract": "Sequential sentence classification aims to classify each sentence in the document based on the context in which sentences appear. Most existing work addresses this problem using a hierarchical sequence labeling network. However, they ignore considering the latent segment structure of the document, in which contiguous sentences often have coherent semantics. In this paper, we proposed a span-based dynamic local attention model that could explicitly capture the structural information by the proposed supervised dynamic local attention. We further introduce an auxiliary task called span-based classification to explore the span-level representations. Extensive experiments show that our model achieves better or competitive performance against state-of-the-art baselines on two benchmark datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 20, "citationCount": 5, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "198-203"}, "authors": [{"authorId": "2121295692", "name": "Xichen Shang"}, {"authorId": "2087919644", "name": "Qianli Ma"}, {"authorId": "148431025", "name": "Zhenxi Lin"}, {"authorId": "10676335", "name": "Jiangyue Yan"}, {"authorId": "3444611", "name": "Zipeng Chen"}]}, {"paperId": "3757de51963e31d0d88dc7a0fbffb8e5d6ba0a79", "externalIds": {"DBLP": "conf/acl/HesselS20", "ACL": "2021.acl-short.27", "DOI": "10.18653/v1/2021.acl-short.27", "CorpusId": 236460117}, "corpusId": 236460117, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3757de51963e31d0d88dc7a0fbffb8e5d6ba0a79", "title": "How effective is BERT without word ordering? Implications for language understanding and data privacy", "abstract": "Ordered word sequences contain the rich structures that define language. However, it\u2019s often not clear if or how modern pretrained language models utilize these structures. We show that the token representations and self-attention activations within BERT are surprisingly resilient to shuffling the order of input tokens, and that for several GLUE language understanding tasks, shuffling only minimally degrades performance, e.g., by 4% for QNLI. While bleak from the perspective of language understanding, our results have positive implications for cases where copyright or ethics necessitates the consideration of bag-of-words data (vs. full documents). We simulate such a scenario for three sensitive classification tasks, demonstrating minimal performance degradation vs. releasing full language sequences.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 63, "citationCount": 29, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.27.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "204-211"}, "authors": [{"authorId": "2689239", "name": "Jack Hessel"}, {"authorId": "144627155", "name": "Alexandra Schofield"}]}, {"paperId": "224ef724ca1ca3eed6475cf46a72109709371d81", "externalIds": {"ACL": "2021.acl-short.28", "DBLP": "conf/acl/CohenKZM20", "DOI": "10.18653/v1/2021.acl-short.28", "CorpusId": 236460328}, "corpusId": 236460328, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/224ef724ca1ca3eed6475cf46a72109709371d81", "title": "WikiSum: Coherent Summarization Dataset for Efficient Human-Evaluation", "abstract": "Recent works made significant advances on summarization tasks, facilitated by summarization datasets. Several existing datasets have the form of coherent-paragraph summaries. However, these datasets were curated from academic documents that were written for experts, thus making the essential step of assessing the summarization output through human-evaluation very demanding. To overcome these limitations, we present a dataset based on article summaries appearing on the WikiHow website, composed of how-to articles and coherent-paragraph summaries written in plain language. We compare our dataset attributes to existing ones, including readability and world-knowledge, showing our dataset makes human evaluation significantly easier and thus, more effective. A human evaluation conducted on PubMed and the proposed dataset reinforces our findings.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.28.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "212-219"}, "authors": [{"authorId": "2898845", "name": "Nachshon Cohen"}, {"authorId": "3387688", "name": "Oren Kalinsky"}, {"authorId": "7264689", "name": "Yftah Ziser"}, {"authorId": "1719404", "name": "Alessandro Moschitti"}]}, {"paperId": "389b98518980f218cdc0869fd852428686eef6dd", "externalIds": {"DBLP": "journals/corr/abs-2106-14019", "ACL": "2021.acl-short.29", "ArXiv": "2106.14019", "DOI": "10.18653/v1/2021.acl-short.29", "CorpusId": 235658821}, "corpusId": 235658821, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/389b98518980f218cdc0869fd852428686eef6dd", "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning", "abstract": "Despite the success of various text generation metrics such as BERTScore, it is still difficult to evaluate the image captions without enough reference captions due to the diversity of the descriptions. In this paper, we introduce a new metric UMIC, an Unreferenced Metric for Image Captioning which does not require reference captions to evaluate image captions. Based on Vision-and-Language BERT, we train UMIC to discriminate negative captions via contrastive learning. Also, we observe critical problems of the previous benchmark dataset (i.e., human annotations) on image captioning metric, and introduce a new collection of human annotations on the generated captions. We validate UMIC on four datasets, including our new dataset, and show that UMIC has a higher correlation than all previous metrics that require multiple references.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 25, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.29.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-26", "journal": {"name": "ArXiv", "volume": "abs/2106.14019"}, "authors": [{"authorId": "2109339794", "name": "Hwanhee Lee"}, {"authorId": "2110654003", "name": "Seunghyun Yoon"}, {"authorId": "2075390842", "name": "Franck Dernoncourt"}, {"authorId": "145262461", "name": "Trung Bui"}, {"authorId": "1731707", "name": "Kyomin Jung"}]}, {"paperId": "5e161653faa3a7739841d28c2d57ac76718b1e17", "externalIds": {"ArXiv": "2010.12627", "DBLP": "journals/corr/abs-2010-12627", "ACL": "2021.acl-short.30", "MAG": "3093536026", "DOI": "10.18653/v1/2021.acl-short.30", "CorpusId": 225067779}, "corpusId": 225067779, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5e161653faa3a7739841d28c2d57ac76718b1e17", "title": "Anchor-based Bilingual Word Embeddings for Low-Resource Languages", "abstract": "Good quality monolingual word embeddings (MWEs) can be built for languages which have large amounts of unlabeled text. MWEs can be aligned to bilingual spaces using only a few thousand word translation pairs. For low resource languages training MWEs monolingually results in MWEs of poor quality, and thus poor bilingual word embeddings (BWEs) as well. This paper proposes a new approach for building BWEs in which the vector space of the high resource source language is used as a starting point for training an embedding space for the low resource target language. By using the source vectors as anchors the vector spaces are automatically aligned during training. We experiment on English-German, English-Hiligaynon and English-Macedonian. We show that our approach results not only in improved BWEs and bilingual lexicon induction performance, but also in improved target language MWE quality as measured using monolingual word similarity.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.30.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-10-23", "journal": {"name": "ArXiv", "volume": "abs/2010.12627"}, "authors": [{"authorId": "26379222", "name": "Tobias Eder"}, {"authorId": "2113189", "name": "Viktor Hangya"}, {"authorId": "2277248", "name": "Alexander M. Fraser"}]}, {"paperId": "8db80de081890299c1bd6a6134cf1d7750fc80cf", "externalIds": {"ACL": "2021.acl-short.31", "DBLP": "conf/acl/YangYMHZLW20", "DOI": "10.18653/v1/2021.acl-short.31", "CorpusId": 236460124}, "corpusId": 236460124, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8db80de081890299c1bd6a6134cf1d7750fc80cf", "title": "Multilingual Agreement for Multilingual Neural Machine Translation", "abstract": "Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.31.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "233-239"}, "authors": [{"authorId": "37081450", "name": "Jian Yang"}, {"authorId": "2109472880", "name": "Yuwei Yin"}, {"authorId": "2118866998", "name": "Shuming Ma"}, {"authorId": "15086992", "name": "Haoyang Huang"}, {"authorId": "40232931", "name": "Dongdong Zhang"}, {"authorId": "1707275", "name": "Zhoujun Li"}, {"authorId": "49807919", "name": "Furu Wei"}]}, {"paperId": "9f49adcb70ee1edbec7eb60e7fe95dcca976b065", "externalIds": {"ArXiv": "2106.00749", "DBLP": "journals/corr/abs-2106-00749", "ACL": "2021.acl-short.32", "DOI": "10.18653/v1/2021.acl-short.32", "CorpusId": 235294133}, "corpusId": 235294133, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9f49adcb70ee1edbec7eb60e7fe95dcca976b065", "title": "Higher-order Derivatives of Weighted Finite-state Machines", "abstract": "Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time\u2014from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A\u02c62 N\u02c64) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.32.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"name": "ArXiv", "volume": "abs/2106.00749"}, "authors": [{"authorId": "51044403", "name": "Ran Zmigrod"}, {"authorId": "2396787", "name": "Tim Vieira"}, {"authorId": "1750769", "name": "Ryan Cotterell"}]}, {"paperId": "5011b61b11fe88d2f79dd5bb330691d23b73dddc", "externalIds": {"ACL": "2021.acl-short.33", "DBLP": "conf/acl/YadavGAD20", "ArXiv": "2107.00176", "DOI": "10.18653/v1/2021.acl-short.33", "CorpusId": 235694616}, "corpusId": 235694616, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5011b61b11fe88d2f79dd5bb330691d23b73dddc", "title": "Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards", "abstract": "The growth of online consumer health questions has led to the necessity for reliable and accurate question answering systems. A recent study showed that manual summarization of consumer health questions brings significant improvement in retrieving relevant answers. However, the automatic summarization of long questions is a challenging task due to the lack of training data and the complexity of the related subtasks, such as the question focus and type recognition. In this paper, we introduce a reinforcement learning-based framework for abstractive question summarization. We propose two novel rewards obtained from the downstream tasks of (i) question-type identification and (ii) question-focus recognition to regularize the question generation model. These rewards ensure the generation of semantically valid questions and encourage the inclusion of key medical entities/foci in the question summary. We evaluated our proposed method on two benchmark datasets and achieved higher performance over state-of-the-art models. The manual evaluation of the summaries reveals that the generated questions are more diverse and have fewer factual inconsistencies than the baseline summaries. The source code is available here: https://github.com/shwetanlp/CHQ-Summ.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 15, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.33.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-01", "journal": {"pages": "249-255"}, "authors": [{"authorId": "4137125", "name": "S. Yadav"}, {"authorId": "1865719481", "name": "D. Gupta"}, {"authorId": "2205800", "name": "Asma Ben Abacha"}, {"authorId": "1398175407", "name": "Dina Demner-Fushman"}]}, {"paperId": "f980bd9bcf0fac1d132cc4e1e0d6ebd51ec6fe9c", "externalIds": {"DBLP": "conf/acl/NaseemRMALKRGG20", "ACL": "2021.acl-short.34", "DOI": "10.18653/v1/2021.acl-short.34", "CorpusId": 236460098}, "corpusId": 236460098, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f980bd9bcf0fac1d132cc4e1e0d6ebd51ec6fe9c", "title": "A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering", "abstract": "Relation linking is a crucial component of Knowledge Base Question Answering systems. Existing systems use a wide variety of heuristics, or ensembles of multiple systems, heavily relying on the surface question text. However, the explicit semantic parse of the question is a rich source of relation information that is not taken advantage of. We propose a simple transformer-based neural model for relation linking that leverages the AMR semantic parse of a sentence. Our system significantly outperforms the state-of-the-art on 4 popular benchmark datasets. These are based on either DBpedia or Wikidata, demonstrating that our approach is effective across KGs.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 18, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.34.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "256-262"}, "authors": [{"authorId": "2138053379", "name": "Tahira Naseem"}, {"authorId": "30090681", "name": "Srinivas Ravishankar"}, {"authorId": "2689774", "name": "Nandana Mihindukulasooriya"}, {"authorId": "145749443", "name": "I. Abdelaziz"}, {"authorId": "2145430350", "name": "Young-Suk Lee"}, {"authorId": "2223082", "name": "Pavan Kapanipathi"}, {"authorId": "1781292", "name": "S. Roukos"}, {"authorId": "1711133", "name": "A. Gliozzo"}, {"authorId": "1703070", "name": "Alexander G. Gray"}]}, {"paperId": "5679431425a81c07bcafa521e5609cc05b3ec5dc", "externalIds": {"ACL": "2021.acl-short.35", "DBLP": "journals/corr/abs-2009-13815", "ArXiv": "2009.13815", "MAG": "3089484023", "DOI": "10.18653/v1/2021.acl-short.35", "CorpusId": 221995466}, "corpusId": 221995466, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5679431425a81c07bcafa521e5609cc05b3ec5dc", "title": "Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation", "abstract": "Early fusion models with cross-attention have shown better-than-human performance on some question answer benchmarks, while it is a poor fit for retrieval since it prevents pre-computation of the answer representations. We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate classification model with cross-attention between questions and answers. The cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model. The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at N (P@N) and Mean Reciprocal Rank (MRR).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 26, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.35.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-29", "journal": {"pages": "263-268"}, "authors": [{"authorId": "2781059", "name": "Yinfei Yang"}, {"authorId": "2057732958", "name": "Ning Jin"}, {"authorId": "2113283398", "name": "Kuo Lin"}, {"authorId": "51150315", "name": "Mandy Guo"}, {"authorId": "46724030", "name": "Daniel Matthew Cer"}]}, {"paperId": "e761a0d40cc7d6efddd0123d11045eba72c3d760", "externalIds": {"ACL": "2021.acl-short.36", "DBLP": "conf/acl/ShiLZ20", "DOI": "10.18653/v1/2021.acl-short.36", "CorpusId": 236460007}, "corpusId": 236460007, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e761a0d40cc7d6efddd0123d11045eba72c3d760", "title": "Enhancing Descriptive Image Captioning with Natural Language Inference", "abstract": "Generating descriptive sentences that convey non-trivial, detailed, and salient information about images is an important goal of image captioning. In this paper we propose a novel approach to encourage captioning models to produce more detailed captions using natural language inference, based on the motivation that, among different captions of an image, descriptive captions are more likely to entail less descriptive captions. Specifically, we construct directed inference graphs for reference captions based on natural language inference. A PageRank algorithm is then employed to estimate the descriptiveness score of each node. Built on that, we use reference sampling and weighted designated rewards to guide captioning to generate descriptive captions. The results on MSCOCO show that the proposed method outperforms the baselines significantly on a wide range of conventional and descriptiveness-related evaluation metrics.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 51, "citationCount": 17, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.36.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "269-277"}, "authors": [{"authorId": "145459175", "name": "Zhan Shi"}, {"authorId": "39080911", "name": "Hui Liu"}, {"authorId": "2130251018", "name": "Xiao-Dan Zhu"}]}, {"paperId": "f7d6bf58f5960c4ad18a698410f5bd9c15d092ae", "externalIds": {"DBLP": "conf/acl/FitzGeraldBBGKM20", "ArXiv": "2106.07352", "ACL": "2021.acl-short.37", "DOI": "10.18653/v1/2021.acl-short.37", "CorpusId": 235421827}, "corpusId": 235421827, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f7d6bf58f5960c4ad18a698410f5bd9c15d092ae", "title": "MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network", "abstract": "We present an instance-based nearest neighbor approach to entity linking. In contrast to most prior entity retrieval systems which represent each entity with a single vector, we build a contextualized mention-encoder that learns to place similar mentions of the same entity closer in vector space than mentions of different entities. This approach allows all mentions of an entity to serve as \u201cclass prototypes\u201d as inference involves retrieving from the full set of labeled entity mentions in the training set and applying the nearest mention neighbor\u2019s entity label. Our model is trained on a large multilingual corpus of mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor inference on an index of 700 million mentions. It is simpler to train, gives more interpretable predictions, and outperforms all other systems on two multilingual entity linking benchmarks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.37.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.07352"}, "authors": [{"authorId": "143883142", "name": "Nicholas FitzGerald"}, {"authorId": "35025872", "name": "Jan A. Botha"}, {"authorId": "2396669", "name": "D. Gillick"}, {"authorId": "2023469", "name": "D. Bikel"}, {"authorId": "15652489", "name": "T. Kwiatkowski"}, {"authorId": "143753639", "name": "A. McCallum"}]}, {"paperId": "dbacc7a4e6739bc85d2495220bcbc3f223eab013", "externalIds": {"DBLP": "conf/acl/SoseaC20", "ACL": "2021.acl-short.38", "DOI": "10.18653/v1/2021.acl-short.38", "CorpusId": 236460109}, "corpusId": 236460109, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dbacc7a4e6739bc85d2495220bcbc3f223eab013", "title": "eMLM: A New Pre-training Objective for Emotion Related Tasks", "abstract": "BERT has been shown to be extremely effective on a wide variety of natural language processing tasks, including sentiment analysis and emotion detection. However, the proposed pretraining objectives of BERT do not induce any sentiment or emotion-specific biases into the model. In this paper, we present Emotion Masked Language Modelling, a variation of Masked Language Modelling aimed at improving the BERT language representation model for emotion detection and sentiment analysis tasks. Using the same pre-training corpora as the original model, Wikipedia and BookCorpus, our BERT variation manages to improve the downstream performance on 4 tasks from emotion detection and sentiment analysis by an average of 1.2% F-1. Moreover, our approach shows an increased performance in our task-specific robustness tests.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "286-293"}, "authors": [{"authorId": "2008183567", "name": "Tiberiu Sosea"}, {"authorId": "1690656", "name": "Cornelia Caragea"}]}, {"paperId": "06463fb4232539f8e098fdab680fae01e7b1d4c7", "externalIds": {"DBLP": "conf/acl/AithalT20", "ACL": "2021.acl-short.39", "ArXiv": "2106.12056", "DOI": "10.18653/v1/2021.acl-short.39", "CorpusId": 235606193}, "corpusId": 235606193, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/06463fb4232539f8e098fdab680fae01e7b1d4c7", "title": "On Positivity Bias in Negative Reviews", "abstract": "Prior work has revealed that positive words occur more frequently than negative words in human expressions, which is typically attributed to positivity bias, a tendency for people to report positive views of reality. But what about the language used in negative reviews? Consistent with prior work, we show that English negative reviews tend to contain more positive words than negative words, using a variety of datasets. We reconcile this observation with prior findings on the pragmatics of negation, and show that negations are commonly associated with positive words in negative reviews. Furthermore, in negative reviews, the majority of sentences with positive words express negative opinions based on sentiment classifiers, indicating some form of negation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.39.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-06-22", "journal": {"name": "ArXiv", "volume": "abs/2106.12056"}, "authors": [{"authorId": "36319534", "name": "Madhusudhan M. Aithal"}, {"authorId": "40348583", "name": "Chenhao Tan"}]}, {"paperId": "bcba2d80ee681394c0ba43520e32bf56ab5ff3bd", "externalIds": {"MAG": "3022903564", "DBLP": "conf/acl/GuWWSY20", "ACL": "2021.acl-short.40", "ArXiv": "2004.13835", "DOI": "10.18653/v1/2021.acl-short.40", "CorpusId": 216642036}, "corpusId": 216642036, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bcba2d80ee681394c0ba43520e32bf56ab5ff3bd", "title": "PRAL: A Tailored Pre-Training Model for Task-Oriented Dialog Generation", "abstract": "Large pre-trained language generation models such as GPT-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. However, the performance of pre-trained models on task-oriented dialog tasks is still under-explored. We propose a Pre-trainedRole Alternating Language model (PRAL), explicitly designed for task-oriented conversational systems. We design several techniques: start position randomization, knowledge distillation, and history discount to improve pre-training performance. In addition, we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. We effectively adapt PRALon three downstream tasks. The results show that PRAL outperforms or is on par with state-of-the-art models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 17, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.40.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-24", "journal": {"name": "ArXiv", "volume": "abs/2004.13835"}, "authors": [{"authorId": "145612104", "name": "Jing Gu"}, {"authorId": "31060482", "name": "Qingyang Wu"}, {"authorId": "22539483", "name": "Chongruo Wu"}, {"authorId": "8299781", "name": "Weiyan Shi"}, {"authorId": "1564034697", "name": "Zhou Yu"}]}, {"paperId": "7cf872dbad1cd780a2ebf56f8bee76fb9497c018", "externalIds": {"DBLP": "journals/corr/abs-2106-10786", "ACL": "2021.acl-short.41", "ArXiv": "2106.10786", "DOI": "10.18653/v1/2021.acl-short.41", "CorpusId": 235489690}, "corpusId": 235489690, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7cf872dbad1cd780a2ebf56f8bee76fb9497c018", "title": "ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction", "abstract": "Natural reading orders of words are crucial for information extraction from form-like documents. Despite recent advances in Graph Convolutional Networks (GCNs) on modeling spatial layout patterns of documents, they have limited ability to capture reading orders of given word-level node representations in a graph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new positional encoding technique designed to apprehend the sequential presentation of words in documents. ROPE generates unique reading order codes for neighboring words relative to the target word given a word-level graph connectivity. We study two fundamental document entity extraction tasks including word labeling and word grouping on the public FUNSD dataset and a large-scale payment dataset. We show that ROPE consistently improves existing GCNs with a margin up to 8.4% F1-score.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 19, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.41.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-21", "journal": {"pages": "314-321"}, "authors": [{"authorId": "50521003", "name": "Chen-Yu Lee"}, {"authorId": "2116729195", "name": "Chun-Liang Li"}, {"authorId": "50097231", "name": "Chu Wang"}, {"authorId": "2143633431", "name": "Renshen Wang"}, {"authorId": "2114175058", "name": "Yasuhisa Fujii"}, {"authorId": "3407327", "name": "Siyang Qin"}, {"authorId": "2054252", "name": "Ashok Popat"}, {"authorId": "1945962", "name": "Tomas Pfister"}]}, {"paperId": "3047c31521e919b8bad3e9682358e33b0e3aa8bd", "externalIds": {"DBLP": "conf/acl/LyuZSR20", "ACL": "2021.acl-short.42", "DOI": "10.18653/v1/2021.acl-short.42", "CorpusId": 236459781}, "corpusId": 236459781, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3047c31521e919b8bad3e9682358e33b0e3aa8bd", "title": "Zero-shot Event Extraction via Transfer Learning: Challenges and Insights", "abstract": "Event extraction has long been a challenging task, addressed mostly with supervised methods that require expensive annotation and are not extensible to new event ontologies. In this work, we explore the possibility of zero-shot event extraction by formulating it as a set of Textual Entailment (TE) and/or Question Answering (QA) queries (e.g. \u201cA city was attacked\u201d entails \u201cThere is an attack\u201d), exploiting pretrained TE/QA models for direct transfer. On ACE-2005 and ERE, our system achieves acceptable results, yet there is still a large gap from supervised approaches, showing that current QA and TE technologies fail in transferring to a different domain. To investigate the reasons behind the gap, we analyze the remaining key challenges, their respective impact, and possible improvement directions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 21, "citationCount": 60, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.42.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "322-332"}, "authors": [{"authorId": "1904906987", "name": "Qing Lyu"}, {"authorId": "2111112132", "name": "Hongming Zhang"}, {"authorId": "46185356", "name": "Elior Sulem"}, {"authorId": "144590225", "name": "D. Roth"}]}, {"paperId": "620f2d75b08aa0788d3215d545c23bdd3bc1b337", "externalIds": {"DBLP": "journals/corr/abs-2105-11136", "ACL": "2021.acl-short.43", "ArXiv": "2105.11136", "DOI": "10.18653/v1/2021.acl-short.43", "CorpusId": 235166928}, "corpusId": 235166928, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/620f2d75b08aa0788d3215d545c23bdd3bc1b337", "title": "Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models", "abstract": "Pre-trained language models have achieved human-level performance on many Machine Reading Comprehension (MRC) tasks, but it remains unclear whether these models truly understand language or answer questions by exploiting statistical biases in datasets. Here, we demonstrate a simple yet effective method to attack MRC models and reveal the statistical biases in these models. We apply the method to the RACE dataset, for which the answer to each MRC question is selected from 4 options. It is found that several pre-trained language models, including BERT, ALBERT, and RoBERTa, show consistent preference to some options, even when these options are irrelevant to the question. When interfered by these irrelevant options, the performance of MRC models can be reduced from human-level performance to the chance-level performance. Human readers, however, are not clearly affected by these irrelevant options. Finally, we propose an augmented training method that can greatly reduce models\u2019 statistical biases.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 23, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.43.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-24", "journal": {"pages": "333-342"}, "authors": [{"authorId": "2110460088", "name": "Jieyu Lin"}, {"authorId": "48626329", "name": "Jiajie Zou"}, {"authorId": "2447541", "name": "N. Ding"}]}, {"paperId": "410164c34d3677ce5cf51571d7fbdfcb81eb1086", "externalIds": {"ArXiv": "2105.12762", "ACL": "2021.acl-short.44", "DBLP": "conf/acl/Kummerfeld20", "DOI": "10.18653/v1/2021.acl-short.44", "CorpusId": 235212512}, "corpusId": 235212512, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/410164c34d3677ce5cf51571d7fbdfcb81eb1086", "title": "Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing", "abstract": "Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S. federal minimum wage. Meanwhile, research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks. If most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair wage. Through analysis of worker discussions and guidance for researchers, we estimate that workers spend approximately 2.25 months of full time effort on poorly paid tasks in order to get the qualifications needed for better paid tasks. We discuss alternatives to this qualification and conduct a study of the correlation between qualifications and work quality on two NLP tasks. We find that it is possible to reduce the burden on workers while still collecting high quality data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.44.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Economics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-26", "journal": {"pages": "343-349"}, "authors": [{"authorId": "1727211", "name": "Jonathan K. Kummerfeld"}]}, {"paperId": "3a88f43236b092e3a86d66af94626445ec8671fa", "externalIds": {"DBLP": "journals/corr/abs-2106-01601", "ArXiv": "2106.01601", "ACL": "2021.acl-short.45", "DOI": "10.18653/v1/2021.acl-short.45", "CorpusId": 235313502}, "corpusId": 235313502, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3a88f43236b092e3a86d66af94626445ec8671fa", "title": "Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia", "abstract": "Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify social stereotypes, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of gender biases in a Wikipedia corpus. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. Then we detect events with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract events that have asymmetric associations with genders. Our study discovers that the Wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 0, "citationCount": 27, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.45.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Sociology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01601"}, "authors": [{"authorId": "145478138", "name": "Jiao Sun"}, {"authorId": "3157053", "name": "Nanyun Peng"}]}, {"paperId": "d60119f643be6294470f26c20eeaa2582a64e336", "externalIds": {"DBLP": "conf/acl/XuLGX20", "ACL": "2021.acl-short.46", "DOI": "10.18653/v1/2021.acl-short.46", "CorpusId": 236459870}, "corpusId": 236459870, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d60119f643be6294470f26c20eeaa2582a64e336", "title": "Modeling Task-Aware MIMO Cardinality for Efficient Multilingual Neural Machine Translation", "abstract": "Neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics. Previous work increases the modeling capacity by deepening or widening the Transformer. However, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.46.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "361-367"}, "authors": [{"authorId": "49507285", "name": "Hongfei Xu"}, {"authorId": "14147919", "name": "Qiuhui Liu"}, {"authorId": "7519068", "name": "Josef van Genabith"}, {"authorId": "2694222", "name": "Deyi Xiong"}]}, {"paperId": "6551ba18a52809f057518f3ebc627c77689bfc93", "externalIds": {"ArXiv": "2105.13022", "DBLP": "journals/corr/abs-2105-13022", "ACL": "2021.acl-short.47", "DOI": "10.18653/v1/2021.acl-short.47", "CorpusId": 235212132}, "corpusId": 235212132, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6551ba18a52809f057518f3ebc627c77689bfc93", "title": "Adaptive Nearest Neighbor Machine Translation", "abstract": "kNN-MT, recently proposed by Khandelwal et al. (2020a), successfully combines pre-trained neural machine translation (NMT) model with token-level k-nearest-neighbor (kNN) retrieval to improve the translation accuracy. However, the traditional kNN algorithm used in kNN-MT simply retrieves a same number of nearest neighbors for each target token, which may cause prediction errors when the retrieved neighbors include noises. In this paper, we propose Adaptive kNN-MT to dynamically determine the number of k for each target token. We achieve this by introducing a light-weight Meta-k Network, which can be efficiently trained with only a few training samples. On four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. Our implementation is open-sourced at https://github.com/zhengxxn/adaptive-knn-mt.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 20, "citationCount": 55, "influentialCitationCount": 12, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.47.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-27", "journal": {"name": "ArXiv", "volume": "abs/2105.13022"}, "authors": [{"authorId": "2143712892", "name": "Xin Zheng"}, {"authorId": "4947404", "name": "Zhirui Zhang"}, {"authorId": "2117224175", "name": "Junliang Guo"}, {"authorId": "2046010", "name": "Shujian Huang"}, {"authorId": "2152687324", "name": "Boxing Chen"}, {"authorId": "1935569", "name": "Weihua Luo"}, {"authorId": "1838162", "name": "Jiajun Chen"}]}, {"paperId": "e382a73c4f87020e5ee8af8feacc6053347a129f", "externalIds": {"ACL": "2021.acl-short.48", "DBLP": "conf/acl/ZhangCTFW0SYL20", "DOI": "10.18653/v1/2021.acl-short.48", "CorpusId": 236459909}, "corpusId": 236459909, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e382a73c4f87020e5ee8af8feacc6053347a129f", "title": "On Orthogonality Constraints for Transformers", "abstract": "Orthogonality constraints encourage matrices to be orthogonal for numerical stability. These plug-and-play constraints, which can be conveniently incorporated into model training, have been studied for popular architectures in natural language processing, such as convolutional neural networks and recurrent neural networks. However, a dedicated study on such constraints for transformers has been absent. To fill this gap, this paper studies orthogonality constraints for transformers, showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks. For example, on the large-scale WMT\u201916 En\u2192De benchmark, simply plugging-and-playing orthogonality constraints on the original transformer model (Vaswani et al., 2017) increases the BLEU from 28.4 to 29.6, coming close to the 29.7 BLEU achieved by the very competitive dynamic convolution (Wu et al., 2019).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 7, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "375-382"}, "authors": [{"authorId": "2085709", "name": "Aston Zhang"}, {"authorId": "2114014617", "name": "Alvin Chan"}, {"authorId": "97947517", "name": "Yi Tay"}, {"authorId": "49252800", "name": "Jie Fu"}, {"authorId": "2992833", "name": "Shuohang Wang"}, {"authorId": "2155802815", "name": "Shuai Zhang"}, {"authorId": "3395273", "name": "Huajie Shao"}, {"authorId": "2325031", "name": "Shuochao Yao"}, {"authorId": "38656724", "name": "R. Lee"}]}, {"paperId": "57a1258571a21817d89197dc84c986861fb6e580", "externalIds": {"DBLP": "journals/corr/abs-2106-03921", "MAG": "3166182577", "ArXiv": "2106.03921", "ACL": "2021.acl-short.49", "DOI": "10.18653/v1/2021.acl-short.49", "CorpusId": 235368289}, "corpusId": 235368289, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/57a1258571a21817d89197dc84c986861fb6e580", "title": "Measuring and Improving BERT\u2019s Mathematical Abilities by Predicting the Order of Reasoning.", "abstract": "Imagine you are in a supermarket. You have two bananas in your basket and want to buy four apples. How many fruits do you have in total? This seemingly straightforward question can be challenging for data-driven language models, even if trained at scale. However, we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence. Towards this goal, we investigate if a commonly used language model, BERT, possesses such mathematical abilities and, if so, to what degree. For that, we fine-tune BERT on a popular dataset for word math problems, AQuA-RAT, and conduct several tests to understand learned representations better. Since we teach models trained on natural language to do formal mathematics, we hypothesize that such models would benefit from training on semi-formal steps that explain how math results are derived. To better accommodate such training, we also propose new pretext tasks for learning mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or NROP). With this new model, we achieve significantly better outcomes than data-driven baselines and even on-par with more tailored models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 63, "citationCount": 19, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.49.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "journal": {"pages": "383-394"}, "authors": [{"authorId": "2107705802", "name": "Piotr Pikekos"}, {"authorId": "47407464", "name": "H. Michalewski"}, {"authorId": "145478807", "name": "Mateusz Malinowski"}]}, {"paperId": "9880c8e9ce969e002cbd9e49ed5d1d830445492b", "externalIds": {"DBLP": "journals/corr/abs-2105-09967", "ArXiv": "2105.09967", "ACL": "2021.acl-short.50", "DOI": "10.18653/v1/2021.acl-short.50", "CorpusId": 235125510}, "corpusId": 235125510, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9880c8e9ce969e002cbd9e49ed5d1d830445492b", "title": "Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter", "abstract": "Datasets with induced emotion labels are scarce but of utmost importance for many NLP tasks. We present a new, automated method for collecting texts along with their induced reaction labels. The method exploits the online use of reaction GIFs, which capture complex affective states. We show how to augment the data with induced emotion and induced sentiment labels. We use our method to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K tweets. We provide baselines for three new tasks, including induced sentiment prediction and multilabel classification of induced emotions. Our method and dataset open new research opportunities in emotion detection and affective computing.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.50.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-20", "journal": {"name": "ArXiv", "volume": "abs/2105.09967"}, "authors": [{"authorId": "1380557800", "name": "Boaz Shmueli"}, {"authorId": "145527877", "name": "Soumya Ray"}, {"authorId": "1746959", "name": "Lun-Wei Ku"}]}, {"paperId": "0c1d53fc87ca482037360b3547111158b505b26e", "externalIds": {"DBLP": "conf/acl/JiangPL20", "ACL": "2021.acl-short.51", "DOI": "10.18653/v1/2021.acl-short.51", "CorpusId": 235719102}, "corpusId": 235719102, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0c1d53fc87ca482037360b3547111158b505b26e", "title": "Exploring Listwise Evidence Reasoning with T5 for Fact Verification", "abstract": "This work explores a framework for fact verification that leverages pretrained sequence-to-sequence transformer models for sentence selection and label prediction, two key sub-tasks in fact verification. Most notably, improving on previous pointwise aggregation approaches for label prediction, we take advantage of T5 using a listwise approach coupled with data augmentation. With this enhancement, we observe that our label prediction stage is more robust to noise and capable of verifying complex claims by jointly reasoning over multiple pieces of evidence. Experimental results on the FEVER task show that our system attains a FEVER score of 75.87% on the blind test set. This puts our approach atop the competitive FEVER leaderboard at the time of our work, scoring higher than the second place submission by almost two points in label accuracy and over one point in FEVER score.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 26, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.51.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "402-410"}, "authors": [{"authorId": "46822553", "name": "Kelvin Jiang"}, {"authorId": "1816753042", "name": "Ronak Pradeep"}, {"authorId": "145580839", "name": "Jimmy J. Lin"}, {"authorId": "1759787", "name": "D. Cheriton"}]}, {"paperId": "314d9c650a6aebdbca90313a156b6cd28f85ba72", "externalIds": {"DBLP": "conf/acl/TsukagoshiST20", "ACL": "2021.acl-short.52", "ArXiv": "2105.04339", "DOI": "10.18653/v1/2021.acl-short.52", "CorpusId": 234342689}, "corpusId": 234342689, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/314d9c650a6aebdbca90313a156b6cd28f85ba72", "title": "DefSent: Sentence Embeddings using Definition Sentences", "abstract": "Sentence embedding methods using natural language inference (NLI) datasets have been successfully applied to various tasks. However, these methods are only available for limited languages due to relying heavily on the large NLI datasets. In this paper, we propose DefSent, a sentence embedding method that uses definition sentences from a word dictionary, which performs comparably on unsupervised semantics textual similarity (STS) tasks and slightly better on SentEval tasks than conventional methods. Since dictionaries are available for many languages, DefSent is more broadly applicable than methods using NLI datasets without constructing additional datasets. We demonstrate that DefSent performs comparably on unsupervised semantics textual similarity (STS) tasks and slightly better on SentEval tasks to the methods using large NLI datasets. Our code is publicly available at https://github.com/hpprc/defsent.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.52.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-10", "journal": {"pages": "411-418"}, "authors": [{"authorId": "2090616568", "name": "Hayato Tsukagoshi"}, {"authorId": "2293543", "name": "Ryohei Sasano"}, {"authorId": "2874038", "name": "Koichi Takeda"}]}, {"paperId": "c4d3794279e98f2c1a1c91c8b07121bffc13ae89", "externalIds": {"DBLP": "conf/acl/AlMarwaniD20", "ArXiv": "2106.00934", "ACL": "2021.acl-short.53", "DOI": "10.18653/v1/2021.acl-short.53", "CorpusId": 235294102}, "corpusId": 235294102, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c4d3794279e98f2c1a1c91c8b07121bffc13ae89", "title": "Discrete Cosine Transform as Universal Sentence Encoder", "abstract": "Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. While averaging is the most commonly used efficient sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging. However, as with most other sentence encoders, the DCT sentence encoder was only evaluated in English. To this end, we utilize DCT encoder to generate universal sentence representation for different languages such as German, French, Spanish and Russian. The experimental results clearly show the superior effectiveness of DCT encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.53.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.00934"}, "authors": [{"authorId": "3449703", "name": "Nada AlMarwani"}, {"authorId": "1700007", "name": "Mona T. Diab"}]}, {"paperId": "26afaeefbd0c38c5367919a0d5cf9c0ca6e91986", "externalIds": {"ACL": "2021.acl-short.54", "DBLP": "conf/acl/MirzaAW20", "DOI": "10.18653/v1/2021.acl-short.54", "CorpusId": 236460201}, "corpusId": 236460201, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/26afaeefbd0c38c5367919a0d5cf9c0ca6e91986", "title": "AligNarr: Aligning Narratives on Movies", "abstract": "High-quality alignment between movie scripts and plot summaries is an asset for learning to summarize stories and to generate dialogues. The alignment task is challenging as scripts and summaries substantially differ in details and abstraction levels as well as in linguistic register. This paper addresses the alignment problem by devising a fully unsupervised approach based on a global optimization model. Experimental results on ten movies show the viability of our method with 76% F1-score and its superiority over a previous baseline. We publish alignments for 914 movies to foster research in this new topic.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 17, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.54.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "427-433"}, "authors": [{"authorId": "2814289", "name": "Paramita Mirza"}, {"authorId": "1396548050", "name": "Mostafa Abouhamra"}, {"authorId": "1751591", "name": "G. Weikum"}]}, {"paperId": "d6218ce8a829c705678884d77ad791753755175e", "externalIds": {"ACL": "2021.acl-short.55", "DBLP": "conf/acl/RanasingheOM20", "ArXiv": "2106.00143", "DOI": "10.18653/v1/2021.acl-short.55", "CorpusId": 235265670}, "corpusId": 235265670, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d6218ce8a829c705678884d77ad791753755175e", "title": "An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers", "abstract": "Most studies on word-level Quality Estimation (QE) of machine translation focus on language-specific models. The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. To overcome these problems, we explore different approaches to multilingual, word-level QE. We show that multilingual QE models perform on par with the current language-specific models. In the cases of zero-shot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 25, "citationCount": 18, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.55.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "434-440"}, "authors": [{"authorId": "134805041", "name": "Tharindu Ranasinghe"}, {"authorId": "1786218", "name": "Constantin Orasan"}, {"authorId": "1746371", "name": "R. Mitkov"}]}, {"paperId": "755038a42abf9a9fa6d40ac7c0eb9fbe6298d3f0", "externalIds": {"ACL": "2021.acl-short.56", "ArXiv": "2105.14813", "DBLP": "journals/corr/abs-2105-14813", "DOI": "10.18653/v1/2021.acl-short.56", "CorpusId": 235254085}, "corpusId": 235254085, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/755038a42abf9a9fa6d40ac7c0eb9fbe6298d3f0", "title": "Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models", "abstract": "A sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a model to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the model. The generated adversarial examples are gradually added to the training set. Experimental results show that such an adversarial training method combined with the pre-training strategy can improve both the generalization and robustness of multiple CSC models across three different datasets, achieving state-of-the-art performance for CSC task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 21, "citationCount": 21, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.56.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"name": "ArXiv", "volume": "abs/2105.14813"}, "authors": [{"authorId": "2109665081", "name": "Chong Li"}, {"authorId": "1776014", "name": "Ce Zhang"}, {"authorId": "2152196565", "name": "Xiaoqing Zheng"}, {"authorId": "1790227", "name": "Xuanjing Huang"}]}, {"paperId": "4572ce690e5eff7d7d38c6f3f67561d0b263b34f", "externalIds": {"ArXiv": "2107.02102", "DBLP": "journals/corr/abs-2107-02102", "ACL": "2021.acl-short.57", "DOI": "10.18653/v1/2021.acl-short.57", "CorpusId": 235731757}, "corpusId": 235731757, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4572ce690e5eff7d7d38c6f3f67561d0b263b34f", "title": "Training Adaptive Computation for Open-Domain Question Answering with Computational Constraints", "abstract": "Adaptive Computation (AC) has been shown to be effective in improving the efficiency of Open-Domain Question Answering (ODQA) systems. However, the current AC approaches require tuning of all model parameters, and training state-of-the-art ODQA models requires significant computational resources that may not be available for most researchers. We propose Adaptive Passage Encoder, an AC method that can be applied to an existing ODQA model and can be trained efficiently on a single GPU. It keeps the parameters of the base ODQA model fixed, but it overrides the default layer-by-layer computation of the encoder with an AC policy that is trained to optimise the computational efficiency of the model. Our experimental results show that our method improves upon a state-of-the-art model on two datasets, and is also more accurate than previous AC methods due to the stronger base ODQA model. All source code and datasets are available at https://github.com/uclnlp/APE.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.57.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-05", "journal": {"name": "ArXiv", "volume": "abs/2107.02102"}, "authors": [{"authorId": "39417610", "name": "Yuxiang Wu"}, {"authorId": "3051815", "name": "Pasquale Minervini"}, {"authorId": "1918552", "name": "Pontus Stenetorp"}, {"authorId": "145941665", "name": "S. Riedel"}]}, {"paperId": "2effdadfa3723abfa39db66e844a3c485203849a", "externalIds": {"DBLP": "conf/acl/ZengX20", "ACL": "2021.acl-short.58", "DOI": "10.18653/v1/2021.acl-short.58", "CorpusId": 236460144}, "corpusId": 236460144, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2effdadfa3723abfa39db66e844a3c485203849a", "title": "An Empirical Study on Adversarial Attack on NMT: Languages and Positions Matter", "abstract": "In this paper, we empirically investigate adversarial attack on NMT from two aspects: languages (the source vs. the target language) and positions (front vs. rear). For autoregressive NMT models that generate target words from left to right, we observe that adversarial attack on the source language is more effective than on the target language, and that attacking front positions of target sentences or positions of source sentences aligned to the front positions of corresponding target sentences is more effective than attacking other positions. We further exploit the attention distribution of the victim model to attack source sentences at positions that have a strong association with front target words. Experiment results demonstrate that our attention-based adversarial attack is more effective than adversarial attacks by sampling positions randomly or according to gradients.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "454-460"}, "authors": [{"authorId": null, "name": "Zhiyuan Zeng"}, {"authorId": "2694222", "name": "Deyi Xiong"}]}, {"paperId": "037adc1f49e42b681a482c181df46bab054944ff", "externalIds": {"ArXiv": "2106.00933", "DBLP": "conf/acl/ZhuPZ20", "ACL": "2021.acl-short.59", "DOI": "10.18653/v1/2021.acl-short.59", "CorpusId": 235293958}, "corpusId": 235293958, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/037adc1f49e42b681a482c181df46bab054944ff", "title": "OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres", "abstract": "SOTA coreference resolution produces increasingly impressive scores on the OntoNotes benchmark. However lack of comparable data following the same scheme for more genres makes it difficult to evaluate generalizability to open domain data. This paper provides a dataset and comprehensive evaluation showing that the latest neural LM based end-to-end systems degrade very substantially out of domain. We make an OntoNotes-like coreference dataset called OntoGUM publicly available, converted from GUM, an English corpus covering 12 genres, using deterministic rules, which we evaluate. Thanks to the rich syntactic and discourse annotations in GUM, we are able to create the largest human-annotated coreference corpus following the OntoNotes guidelines, and the first to be evaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation across 12 genres shows nearly 15-20% degradation for both deterministic and deep learning systems, indicating a lack of generalizability or covert overfitting in existing coreference resolution models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.59.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.00933"}, "authors": [{"authorId": "2117870253", "name": "Yilun Zhu"}, {"authorId": "1735131", "name": "Sameer Pradhan"}, {"authorId": "2548420", "name": "Amir Zeldes"}]}, {"paperId": "0f9b5b32229a7245e43754430c0c88f8e7f0d8af", "externalIds": {"DBLP": "conf/acl/VickersAMB20", "ACL": "2021.acl-short.60", "DOI": "10.18653/v1/2021.acl-short.60", "CorpusId": 236460186}, "corpusId": 236460186, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0f9b5b32229a7245e43754430c0c88f8e7f0d8af", "title": "In Factuality: Efficient Integration of Relevant Facts for Visual Question Answering", "abstract": "Visual Question Answering (VQA) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities. Current models are trained on labelled data that may be insufficient to learn complex knowledge representations. In this paper, we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model (Vision+Language BERT) by integrating facts extracted from an external knowledge base. Evaluation on the KVQA dataset benchmark demonstrates that our method outperforms competitive baselines by 19%, achieving new state-of-the-art results. We also perform an extensive analysis highlighting the limitations of our best performing model through an ablation study.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 49, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.60.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "468-475"}, "authors": [{"authorId": "144748442", "name": "P. Vickers"}, {"authorId": "3238627", "name": "Nikolaos Aletras"}, {"authorId": "38722908", "name": "Emilio Monti"}, {"authorId": "2934336", "name": "Lo\u00efc Barrault"}]}, {"paperId": "25c9dbcad66fedec76a8941db46bda2e9bd8f698", "externalIds": {"ArXiv": "2105.14682", "DBLP": "journals/corr/abs-2105-14682", "ACL": "2021.acl-short.61", "DOI": "10.18653/v1/2021.acl-short.61", "CorpusId": 235254775}, "corpusId": 235254775, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/25c9dbcad66fedec76a8941db46bda2e9bd8f698", "title": "Zero-shot Fact Verification by Claim Generation", "abstract": "Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model\u2019s F1 from 50% to 77%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 33, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.61.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"pages": "476-483"}, "authors": [{"authorId": "3470231", "name": "Liangming Pan"}, {"authorId": "2109664620", "name": "Wenhu Chen"}, {"authorId": "22253126", "name": "Wenhan Xiong"}, {"authorId": "37596605", "name": "Min-Yen Kan"}, {"authorId": "152876475", "name": "W. Wang"}]}, {"paperId": "0bf230ee96c7ec49fe9d2aadcaec598d68ad94f1", "externalIds": {"ACL": "2021.acl-short.62", "ArXiv": "2105.06947", "DBLP": "journals/corr/abs-2105-06947", "MAG": "3176400576", "DOI": "10.18653/v1/2021.acl-short.62", "CorpusId": 234681927}, "corpusId": 234681927, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0bf230ee96c7ec49fe9d2aadcaec598d68ad94f1", "title": "Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer", "abstract": "Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content \u2013the two core aspects of the task\u2013 we achieve a new state-of-the-art.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 38, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.62.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-14", "journal": {"pages": "484-494"}, "authors": [{"authorId": "66376493", "name": "Huiyuan Lai"}, {"authorId": "2065048323", "name": "Antonio Toral"}, {"authorId": "2742475", "name": "M. Nissim"}]}, {"paperId": "725f0f75f0b210c31378ee1358345dbfb6322578", "externalIds": {"DBLP": "journals/corr/abs-2106-03806", "ArXiv": "2106.03806", "ACL": "2021.acl-short.63", "DOI": "10.18653/v1/2021.acl-short.63", "CorpusId": 235363975}, "corpusId": 235363975, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/725f0f75f0b210c31378ee1358345dbfb6322578", "title": "Deep Context- and Relation-Aware Learning for Aspect-based Sentiment Analysis", "abstract": "Existing works for aspect-based sentiment analysis (ABSA) have adopted a unified approach, which allows the interactive relations among subtasks. However, we observe that these methods tend to predict polarities based on the literal meaning of aspect and opinion terms and mainly consider relations implicitly among subtasks at the word level. In addition, identifying multiple aspect\u2013opinion pairs with their polarities is much more challenging. Therefore, a comprehensive understanding of contextual information w.r.t. the aspect and opinion are further required in ABSA. In this paper, we propose Deep Contextualized Relation-Aware Network (DCRAN), which allows interactive relations among subtasks with deep contextual information based on two modules (i.e., Aspect and Opinion Propagation and Explicit Self-Supervised Strategies). Especially, we design novel self-supervised strategies for ABSA, which have strengths in dealing with multiple aspects. Experimental results show that DCRAN significantly outperforms previous state-of-the-art methods by large margins on three widely used benchmarks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.63.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "journal": {"name": "ArXiv", "volume": "abs/2106.03806"}, "authors": [{"authorId": "1390597717", "name": "Shinhyeok Oh"}, {"authorId": "2115276420", "name": "Dongyub Lee"}, {"authorId": "89016637", "name": "Taesun Whang"}, {"authorId": "118266343", "name": "Ilnam Park"}, {"authorId": "2107086011", "name": "Gaeun Seo"}, {"authorId": "2210474389", "name": "EungGyun Kim"}, {"authorId": "2108880610", "name": "Harksoo Kim"}]}, {"paperId": "768ff532981f47a954d8bc6563a5b3e909cfbcad", "externalIds": {"MAG": "3176690085", "DBLP": "conf/acl/Zhang0DBL20", "ACL": "2021.acl-short.64", "DOI": "10.18653/v1/2021.acl-short.64", "CorpusId": 236460053}, "corpusId": 236460053, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/768ff532981f47a954d8bc6563a5b3e909cfbcad", "title": "Towards Generative Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) has received increasing attention recently. Most existing work tackles ABSA in a discriminative manner, designing various task-specific classification networks for the prediction. Despite their effectiveness, these methods ignore the rich label semantics in ABSA problems and require extensive task-specific designs. In this paper, we propose to tackle various ABSA tasks in a unified generative framework. Two types of paradigms, namely annotation-style and extraction-style modeling, are designed to enable the training process by formulating each ABSA task as a text generation problem. We conduct experiments on four ABSA tasks across multiple benchmark datasets where our proposed generative approach achieves new state-of-the-art results in almost all cases. This also validates the strong generality of the proposed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 33, "citationCount": 82, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.64.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "504-510"}, "authors": [{"authorId": "150341144", "name": "Wenxuan Zhang"}, {"authorId": "40613621", "name": "Xin Li"}, {"authorId": "145843537", "name": "Yang Deng"}, {"authorId": "1996394", "name": "Lidong Bing"}, {"authorId": "144594306", "name": "Wai Lam"}]}, {"paperId": "36ced528de0e87a02ca997a223aeabd7647cd0d9", "externalIds": {"ArXiv": "2105.12523", "DBLP": "journals/corr/abs-2105-12523", "MAG": "3175995572", "ACL": "2021.acl-short.65", "DOI": "10.18653/v1/2021.acl-short.65", "CorpusId": 235195674}, "corpusId": 235195674, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/36ced528de0e87a02ca997a223aeabd7647cd0d9", "title": "Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation", "abstract": "Recently, token-level adaptive training has achieved promising improvement in machine translation, where the cross-entropy loss function is adjusted by assigning different training weights to different tokens, in order to alleviate the token imbalance problem. However, previous approaches only use static word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks like machine translation. In this paper, we propose a novel bilingual mutual information (BMI) based adaptive objective, which measures the learning difficulty for each target token from the perspective of bilingualism, and assigns an adaptive weight accordingly to improve token-level adaptive training. This method assigns larger training weights to tokens with higher BMI, so that easy tokens are updated with coarse granularity while difficult tokens are updated with fine granularity. Experimental results on WMT14 English-to-German and WMT19 Chinese-to-English demonstrate the superiority of our approach compared with the Transformer baseline and previous token-level adaptive training approaches. Further analyses confirm that our method can improve the lexical diversity.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 9, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.65.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-26", "journal": {"name": "ArXiv", "volume": "abs/2105.12523"}, "authors": [{"authorId": "2115683431", "name": "Yang Xu"}, {"authorId": "2108060455", "name": "Yijin Liu"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "2124819243", "name": "Jiajun Zhang"}, {"authorId": "2310092", "name": "Jinan Xu"}, {"authorId": "48128428", "name": "Jie Zhou"}]}, {"paperId": "95f3b5b09e3f0e0d2038c57b57325bfe03279fbf", "externalIds": {"DBLP": "conf/acl/GengYXSX020", "ACL": "2021.acl-short.66", "ArXiv": "2107.08173", "DOI": "10.18653/v1/2021.acl-short.66", "CorpusId": 236087633}, "corpusId": 236087633, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/95f3b5b09e3f0e0d2038c57b57325bfe03279fbf", "title": "Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking", "abstract": "This ability to learn consecutive tasks without forgetting how to perform previously trained problems is essential for developing an online dialogue system. This paper proposes an effective continual learning method for the task-oriented dialogue system with iterative network pruning, expanding, and masking (TPEM), which preserves performance on previously encountered tasks while accelerating learning progress on subsequent tasks. Specifically, TPEM (i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts network expanding to create free weights for new tasks, and (iii) introduces task-specific network masking to alleviate the negative impact of fixed weights of old tasks on new tasks. We conduct extensive experiments on seven different tasks from three benchmark datasets and show empirically that TPEM leads to significantly improved results over the strong competitors.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.66.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-17", "journal": {"pages": "517-523"}, "authors": [{"authorId": "2120019456", "name": "Binzong Geng"}, {"authorId": "2612614", "name": "Fajie Yuan"}, {"authorId": "2149107900", "name": "Qiancheng Xu"}, {"authorId": "2115382645", "name": "Ying Shen"}, {"authorId": "1753529", "name": "Ruifeng Xu"}, {"authorId": "2110950699", "name": "Min Yang"}]}, {"paperId": "ce9e010c5cb2323dfed3af687b12875f01c759bc", "externalIds": {"DBLP": "conf/acl/MathurJDMTM20", "ACL": "2021.acl-short.67", "DOI": "10.18653/v1/2021.acl-short.67", "CorpusId": 236459967}, "corpusId": 236459967, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ce9e010c5cb2323dfed3af687b12875f01c759bc", "title": "TIMERS: Document-level Temporal Relation Extraction", "abstract": "We present TIMERS - a TIME, Rhetorical and Syntactic-aware model for document-level temporal relation classification in the English language. Our proposed method leverages rhetorical discourse features and temporal arguments from semantic role labels, in addition to traditional local syntactic features, trained through a Gated Relational-GCN. Extensive experiments show that the proposed model outperforms previous methods by 5-18% on the TDDiscourse, TimeBank-Dense, and MATRES datasets due to our discourse-level modeling.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 23, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.67.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "524-533"}, "authors": [{"authorId": "144144799", "name": "Puneet Mathur"}, {"authorId": "39878379", "name": "R. Jain"}, {"authorId": "2075390842", "name": "Franck Dernoncourt"}, {"authorId": "2852035", "name": "Vlad I. Morariu"}, {"authorId": "2536742", "name": "Quan Hung Tran"}, {"authorId": "1699159", "name": "Dinesh Manocha"}]}, {"paperId": "c21f366b6e024318e57e6b972ba697c769e88957", "externalIds": {"DBLP": "conf/acl/QinCTS20", "ACL": "2021.acl-short.68", "DOI": "10.18653/v1/2021.acl-short.68", "CorpusId": 236460202}, "corpusId": 236460202, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c21f366b6e024318e57e6b972ba697c769e88957", "title": "Improving Arabic Diacritization with Regularized Decoding and Adversarial Training", "abstract": "Arabic diacritization is a fundamental task for Arabic language processing. Previous studies have demonstrated that automatically generated knowledge can be helpful to this task. However, these studies regard the auto-generated knowledge instances as gold references, which limits their effectiveness since such knowledge is not always accurate and inferior instances can lead to incorrect predictions. In this paper, we propose to use regularized decoding and adversarial training to appropriately learn from such noisy knowledge for diacritization. Experimental results on two benchmark datasets show that, even with quite flawed auto-generated knowledge, our model can still learn adequate diacritics and outperform all previous studies, on both datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "534-542"}, "authors": [{"authorId": "2112655647", "name": "Han Qin"}, {"authorId": "2116379168", "name": "Guimin Chen"}, {"authorId": "2152947211", "name": "Yuanhe Tian"}, {"authorId": "1922182598", "name": "Yan Song"}]}, {"paperId": "0bfa9275561f45bc772c4d544fd75f5d65143c5e", "externalIds": {"DBLP": "conf/acl/LiSHDC20", "ACL": "2021.acl-short.69", "DOI": "10.18653/v1/2021.acl-short.69", "CorpusId": 236459780}, "corpusId": 236459780, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0bfa9275561f45bc772c4d544fd75f5d65143c5e", "title": "When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation", "abstract": "Subword segmentation algorithms have been a de facto choice when building neural machine translation systems. However, most of them need to learn a segmentation model based on some heuristics, which may produce sub-optimal segmentation. This can be problematic in some scenarios when the target language has rich morphological changes or there is not enough data for learning compact composition rules. Translating at fully character level has the potential to alleviate the issue, but empirical performances of character-based models has not been fully explored. In this paper, we present an in-depth comparison between character-based and subword-based NMT systems under three settings: translating to typologically diverse languages, training with low resource, and adapting to unseen domains. Experiment results show strong competitiveness of character-based models. Further analyses show that compared to subword-based models, character-based models are better at handling morphological phenomena, generating rare and unknown words, and more suitable for transferring to unseen domains.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "543-549"}, "authors": [{"authorId": "2108959287", "name": "Jiahuan Li"}, {"authorId": "52154746", "name": "Yutong Shen"}, {"authorId": "2124946880", "name": "Shujian Huang"}, {"authorId": "3035069", "name": "Xinyu Dai"}, {"authorId": "1838162", "name": "Jiajun Chen"}]}, {"paperId": "e7ff0dedf94fbba7abb36f80ebb75716eba51e09", "externalIds": {"DBLP": "conf/acl/ZhangHLWZZ20", "ACL": "2021.acl-short.70", "DOI": "10.18653/v1/2021.acl-short.70", "CorpusId": 236460189}, "corpusId": 236460189, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e7ff0dedf94fbba7abb36f80ebb75716eba51e09", "title": "More than Text: Multi-modal Chinese Word Segmentation", "abstract": "Chinese word segmentation (CWS) is undoubtedly an important basic task in natural language processing. Previous works only focus on the textual modality, but there are often audio and video utterances (such as news broadcast and face-to-face dialogues), where textual, acoustic and visual modalities normally exist. To this end, we attempt to combine the multi-modality (mainly the converted text and actual voice information) to perform CWS. In this paper, we annotate a new dataset for CWS containing text and audio. Moreover, we propose a time-dependent multi-modal interactive model based on Transformer framework to integrate multi-modal information for word sequence labeling. The experimental results on three different training sets show the effectiveness of our approach with fusing text and audio.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 46, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "550-557"}, "authors": [{"authorId": "153165260", "name": "Dong Zhang"}, {"authorId": "2111375200", "name": "Zhengang Hu"}, {"authorId": "2109167274", "name": "Shoushan Li"}, {"authorId": "2005245", "name": "Hanqian Wu"}, {"authorId": "7703092", "name": "Qiaoming Zhu"}, {"authorId": "143740945", "name": "Guodong Zhou"}]}, {"paperId": "0de74abad261ba383786b06065a15f025edf048a", "externalIds": {"ACL": "2021.acl-short.71", "DBLP": "conf/acl/XieZ20", "DOI": "10.18653/v1/2021.acl-short.71", "CorpusId": 236460030}, "corpusId": 236460030, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0de74abad261ba383786b06065a15f025edf048a", "title": "A Mixture-of-Experts Model for Antonym-Synonym Discrimination", "abstract": "Discrimination between antonyms and synonyms is an important and challenging NLP task. Antonyms and synonyms often share the same or similar contexts and thus are hard to make a distinction. This paper proposes two underlying hypotheses and employs the mixture-of-experts framework as a solution. It works on the basis of a divide-and-conquer strategy, where a number of localized experts focus on their own domains (or subspaces) to learn their specialties, and a gating mechanism determines the space partitioning and the expert mixture. Experimental results have shown that our method achieves the state-of-the-art performance on the task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 3, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.71.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "558-564"}, "authors": [{"authorId": "37168208", "name": "Zhipeng Xie"}, {"authorId": "2055711058", "name": "Nan Zeng"}]}, {"paperId": "0e2dd43d4487f9707eba1482a593293a5934a59e", "externalIds": {"ACL": "2021.acl-short.72", "ArXiv": "2105.14398", "DBLP": "conf/acl/0001VKC20", "DOI": "10.18653/v1/2021.acl-short.72", "CorpusId": 235254228}, "corpusId": 235254228, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0e2dd43d4487f9707eba1482a593293a5934a59e", "title": "Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking", "abstract": "Injecting external domain-specific knowledge (e.g., UMLS) into pretrained language models (LMs) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (BEL). However, such abundant expert knowledge is available only for a handful of languages (e.g., English). In this work, by proposing a novel cross-lingual biomedical entity linking task (XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages, we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual LMs beyond the standard monolingual English BEL task. The scores indicate large gaps to English performance. We then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. To this end, we propose and evaluate a series of cross-lingual transfer methods for the XL-BEL task, and demonstrate that general-domain bitext helps propagate the available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 26, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.72.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-30", "journal": {"pages": "565-574"}, "authors": [{"authorId": "144097210", "name": "Fangyu Liu"}, {"authorId": "1747849", "name": "Ivan Vulic"}, {"authorId": "145762466", "name": "A. Korhonen"}, {"authorId": "50638196", "name": "Nigel Collier"}]}, {"paperId": "36e69a05b315c7f2c51379a261b70154482a4c74", "externalIds": {"DBLP": "conf/acl/RajaeeP20", "ArXiv": "2106.01183", "ACL": "2021.acl-short.73", "DOI": "10.18653/v1/2021.acl-short.73", "CorpusId": 235294217}, "corpusId": 235294217, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/36e69a05b315c7f2c51379a261b70154482a4c74", "title": "A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space", "abstract": "The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. Our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in verb representations dominates sense semantics. We show that removing dominant directions of verb representations can transform the space to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 33, "citationCount": 22, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.73.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01183"}, "authors": [{"authorId": "31280249", "name": "S. Rajaee"}, {"authorId": "1717641", "name": "Mohammad Taher Pilehvar"}]}, {"paperId": "8f0686ccf1a706a4cf797e0fd9ee0b9ac007dc91", "externalIds": {"ACL": "2021.acl-short.74", "DBLP": "journals/corr/abs-2106-08364", "ArXiv": "2106.08364", "DOI": "10.18653/v1/2021.acl-short.74", "CorpusId": 235446822}, "corpusId": 235446822, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8f0686ccf1a706a4cf797e0fd9ee0b9ac007dc91", "title": "Unsupervised Enrichment of Persona-grounded Dialog with Background Stories", "abstract": "Humans often refer to personal narratives, life experiences, and events to make a conversation more engaging and rich. While persona-grounded dialog models are able to generate responses that follow a given persona, they often miss out on stating detailed experiences or events related to a persona, often leaving conversations shallow and dull. In this work, we equip dialog models with \u2018background stories\u2019 related to a persona by leveraging fictional narratives from existing story datasets (e.g. ROCStories). Since current dialog datasets do not contain such narratives as responses, we perform an unsupervised adaptation of a retrieved story for generating a dialog response using a gradient-based rewriting technique. Our proposed method encourages the generated response to be fluent (i.e., highly likely) with the dialog history, minimally different from the retrieved story to preserve event ordering and consistent with the original persona. We demonstrate that our method can generate responses that are more diverse, and are rated more engaging and human-like by human evaluators, compared to outputs from existing dialog models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.74.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-15", "journal": {"name": "ArXiv", "volume": "abs/2106.08364"}, "authors": [{"authorId": "3165738", "name": "Bodhisattwa Prasad Majumder"}, {"authorId": "1400419309", "name": "Taylor Berg-Kirkpatrick"}, {"authorId": "35660011", "name": "Julian McAuley"}, {"authorId": "2006291", "name": "Harsh Jhamtani"}]}, {"paperId": "1a1807c0458d0b2d754b5659111691df0c3a2241", "externalIds": {"DBLP": "conf/acl/ChandraKV20", "ACL": "2021.acl-short.75", "DOI": "10.18653/v1/2021.acl-short.75", "CorpusId": 235650866}, "corpusId": 235650866, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1a1807c0458d0b2d754b5659111691df0c3a2241", "title": "Beyond Laurel/Yanny: An Autoencoder-Enabled Search for Polyperceivable Audio", "abstract": "The famous \u201claurel/yanny\u201d phenomenon references an audio clip that elicits dramatically different responses from different listeners. For the original clip, roughly half the population hears the word \u201claurel,\u201d while the other half hears \u201cyanny.\u201d How common are such \u201cpolyperceivable\u201d audio clips? In this paper we apply ML techniques to study the prevalence of polyperceivability in spoken language. We devise a metric that correlates with polyperceivability of audio clips, use it to efficiently find new \u201claurel/yanny\u201d-type examples, and validate these results with human experiments. Our results suggest that polyperceivable examples are surprisingly prevalent in natural language, existing for >2% of English words.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.75.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "593-598"}, "authors": [{"authorId": "2070680937", "name": "Kartik Chandra"}, {"authorId": "2081067237", "name": "Chuma Kabaghe"}, {"authorId": "1806083", "name": "G. Valiant"}]}, {"paperId": "47019e041ee30165a1dcc36ce9847561ade4df59", "externalIds": {"DBLP": "conf/acl/KoupaeeDCB20", "ACL": "2021.acl-short.76", "DOI": "10.18653/v1/2021.acl-short.76", "CorpusId": 236460103}, "corpusId": 236460103, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/47019e041ee30165a1dcc36ce9847561ade4df59", "title": "Don\u2019t Let Discourse Confine Your Model: Sequence Perturbations for Improved Event Language Models", "abstract": "Event language models represent plausible sequences of events. Most existing approaches train autoregressive models on text, which successfully capture event co-occurrence but unfortunately constrain the model to follow the discourse order in which events are presented. Other domains may employ different discourse orders, and for many applications, we may care about different notions of ordering (e.g., temporal) or not care about ordering at all (e.g., when predicting related events in a schema). We propose a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and out-of-domain events data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "599-604"}, "authors": [{"authorId": "144007901", "name": "Mahnaz Koupaee"}, {"authorId": "1814094", "name": "Greg Durrett"}, {"authorId": "1729918", "name": "Nathanael Chambers"}, {"authorId": "35217367", "name": "Niranjan Balasubramanian"}]}, {"paperId": "0f49917a1dfafac3d8353d084467ce04cd58aec2", "externalIds": {"ACL": "2021.acl-short.77", "ArXiv": "2012.14210", "DBLP": "conf/acl/0001G20", "DOI": "10.18653/v1/2021.acl-short.77", "CorpusId": 229680000}, "corpusId": 229680000, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0f49917a1dfafac3d8353d084467ce04cd58aec2", "title": "The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes", "abstract": "Information Retrieval using dense low-dimensional representations recently became popular and showed out-performance to traditional sparse-representations like BM25. However, no previous work investigated how dense representations perform with large index sizes. We show theoretically and empirically that the performance for dense representations decreases quicker than sparse representations for increasing index sizes. In extreme cases, this can even lead to a tipping point where at a certain index size sparse representations outperform dense representations. We show that this behavior is tightly connected to the number of dimensions of the representations: The lower the dimension, the higher the chance for false positives, i.e. returning irrelevant documents", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 18, "citationCount": 34, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.77.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-28", "journal": {"pages": "605-611"}, "authors": [{"authorId": "2959414", "name": "Nils Reimers"}, {"authorId": "1730400", "name": "Iryna Gurevych"}]}, {"paperId": "413c6974ee908c4f54ed1970ce554db133c4a63d", "externalIds": {"DBLP": "journals/corr/abs-2105-11246", "ArXiv": "2105.11246", "ACL": "2021.acl-short.78", "DOI": "10.18653/v1/2021.acl-short.78", "CorpusId": 235166842}, "corpusId": 235166842, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/413c6974ee908c4f54ed1970ce554db133c4a63d", "title": "Cross-lingual Text Classification with Heterogeneous Graph Neural Network", "abstract": "Cross-lingual text classification aims at training a classifier on the source language and transferring the knowledge to target languages, which is very useful for low-resource languages. Recent multilingual pretrained language models (mPLM) achieve impressive results in cross-lingual classification tasks, but rarely consider factors beyond semantic similarity, causing performance degradation between some language pairs. In this paper we propose a simple yet effective method to incorporate heterogeneous information within and across languages for cross-lingual text classification using graph convolutional networks (GCN). In particular, we construct a heterogeneous graph by treating documents and words as nodes, and linking nodes with different relations, which include part-of-speech roles, semantic similarity, and document translations. Extensive experiments show that our graph-based method significantly outperforms state-of-the-art models on all tasks, and also achieves consistent performance gain over baselines in low-resource settings where external tools like translators are unavailable.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 13, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.78.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-24", "journal": {"pages": "612-620"}, "authors": [{"authorId": "39747646", "name": "ZiYun Wang"}, {"authorId": "2110773815", "name": "Xuan Liu"}, {"authorId": "2119185269", "name": "Pei-Yin Yang"}, {"authorId": "2131176413", "name": "Shixing Liu"}, {"authorId": "2108086267", "name": "Zhisheng Wang"}]}, {"paperId": "7cfcbf280a3d6ee52a3a3900673bcc3ec3608fb4", "externalIds": {"DBLP": "conf/acl/DebnathRAA20", "ACL": "2021.acl-short.79", "ArXiv": "2105.14115", "DOI": "10.18653/v1/2021.acl-short.79", "CorpusId": 235254629}, "corpusId": 235254629, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7cfcbf280a3d6ee52a3a3900673bcc3ec3608fb4", "title": "Towards more equitable question answering systems: How much more data do you need?", "abstract": "Question answering (QA) in English has been widely explored, but multilingual datasets are relatively new, with several methods attempting to bridge the gap between high- and low-resourced languages using data augmentation through translation and cross-lingual transfer. In this project we take a step back and study which approaches allow us to take the most advantage of existing resources in order to produce QA systems in many languages. Specifically, we perform extensive analysis to measure the efficacy of few-shot approaches augmented with automatic translations and permutations of context-question-answer pairs. In addition, we make suggestions for future dataset development efforts that make better use of a fixed annotation budget, with a goal of increasing the language coverage of QA datasets and systems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.79.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-28", "journal": {"pages": "621-629"}, "authors": [{"authorId": "51299843", "name": "Arnab Debnath"}, {"authorId": "30927960", "name": "Navid Rajabi"}, {"authorId": "48646706", "name": "F. Alam"}, {"authorId": "49513989", "name": "Antonios Anastasopoulos"}]}, {"paperId": "f62601045de4f0db88aac0c7d84a61ea8de69bc4", "externalIds": {"DBLP": "conf/acl/DehghaniHA20", "ACL": "2021.acl-short.80", "DOI": "10.18653/v1/2021.acl-short.80", "CorpusId": 236459858}, "corpusId": 236459858, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f62601045de4f0db88aac0c7d84a61ea8de69bc4", "title": "Embedding Time Differences in Context-sensitive Neural Networks for Learning Time to Event", "abstract": "We propose an effective context-sensitive neural model for time to event (TTE) prediction task, which aims to predict the amount of time to/from the occurrence of given events in streaming content. We investigate this problem in the context of a multi-task learning framework, which we enrich with time difference embeddings. In addition, we develop a multi-genre dataset of English events about soccer competitions and academy awards ceremonies, and their relevant tweets obtained from Twitter. Our model is 1.4 and 3.3 hours more accurate than the current state-of-the-art model in estimating TTE on English and Dutch tweets respectively. We examine different aspects of our model to illustrate its source of improvement.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 21, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "630-636"}, "authors": [{"authorId": "3103769", "name": "Nazanin Dehghani"}, {"authorId": "121590975", "name": "H. Hajipoor"}, {"authorId": "1785703", "name": "H. Amiri"}]}, {"paperId": "523745e29f6cb1890f18352d449fd3597910c485", "externalIds": {"ArXiv": "2106.10434", "DBLP": "journals/corr/abs-2106-10434", "ACL": "2021.acl-short.81", "DOI": "10.18653/v1/2021.acl-short.81", "CorpusId": 235490646}, "corpusId": 235490646, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/523745e29f6cb1890f18352d449fd3597910c485", "title": "Improving Compositional Generalization in Classification Tasks via Structure Annotations", "abstract": "Compositional generalization is the ability to generalize systematically to a new data distribution by combining known components. Although humans seem to have a great ability to generalize compositionally, state-of-the-art neural models struggle to do so. In this work, we study compositional generalization in classification tasks and present two main contributions. First, we study ways to convert a natural language sequence-to-sequence dataset to a classification dataset that also requires compositional generalization. Second, we show that providing structural hints (specifically, providing parse trees and entity links as attention masks for a Transformer model) helps compositional generalization.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 17, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.81.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-19", "journal": {"pages": "637-645"}, "authors": [{"authorId": "2116890696", "name": "Juyong Kim"}, {"authorId": "145969795", "name": "Pradeep Ravikumar"}, {"authorId": "1643737606", "name": "J. Ainslie"}, {"authorId": "2066060119", "name": "Santiago Ontan'on"}]}, {"paperId": "9e42cb2b133bc41600824a1002cb05844c6a46a9", "externalIds": {"ACL": "2021.acl-short.82", "DBLP": "conf/acl/Ye020", "ArXiv": "2101.00420", "DOI": "10.18653/v1/2021.acl-short.82", "CorpusId": 235436348}, "corpusId": 235436348, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9e42cb2b133bc41600824a1002cb05844c6a46a9", "title": "Learning to Generate Task-Specific Adapters from Task Description", "abstract": "Pre-trained text-to-text transformers such as BART have achieved impressive performance across a range of NLP tasks. Recent study further shows that they can learn to generalize to novel tasks, by including task descriptions as part of the source sequence and training the model with (source, target) examples. At test time, these fine-tuned models can make inferences on new tasks using the new task descriptions as part of the input. However, this approach has potential limitations, as the model learns to solve individual (source, target) examples (i.e., at the instance level), instead of learning to solve tasks by taking all examples within a task as a whole (i.e., at the task level). To this end, we introduce Hypter, a framework that improves text-to-text transformer\u2019s generalization ability to unseen tasks by training a hypernetwork to generate task-specific, light-weight adapters from task descriptions. Experiments on ZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves upon fine-tuning baselines. Notably, when using BART-Large as the main network, Hypter brings 11.3% comparative improvement on ZEST dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 19, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.82.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-02", "journal": {"pages": "646-653"}, "authors": [{"authorId": "2075312420", "name": "Qinyuan Ye"}, {"authorId": "1384550891", "name": "Xiang Ren"}]}, {"paperId": "525d5835b66612b11c2714645d92297b221e9daf", "externalIds": {"ACL": "2021.acl-short.83", "DBLP": "conf/acl/DuHLYPZ20", "DOI": "10.18653/v1/2021.acl-short.83", "CorpusId": 236459929}, "corpusId": 236459929, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/525d5835b66612b11c2714645d92297b221e9daf", "title": "QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining", "abstract": "Slot-filling is an essential component for building task-oriented dialog systems. In this work, we focus on the zero-shot slot-filling problem, where the model needs to predict slots and their values, given utterances from new domains without training on the target domain. Prior methods directly encode slot descriptions to generalize to unseen slot types. However, raw slot descriptions are often ambiguous and do not encode enough semantic information, limiting the models\u2019 zero-shot capability. To address this problem, we introduce QA-driven slot filling (QASF), which extracts slot-filler spans from utterances with a span-based QA model. We use a linguistically motivated questioning strategy to turn descriptions into questions, allowing the model to generalize to unseen slot types. Moreover, our QASF model can benefit from weak supervision signals from QA pairs synthetically generated from unlabeled conversations. Our full system substantially outperforms baselines by over 5% on the SNIPS benchmark.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 18, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.83.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "654-664"}, "authors": [{"authorId": "13728923", "name": "Xinya Du"}, {"authorId": "2265599", "name": "Luheng He"}, {"authorId": "2118913345", "name": "Qi Li"}, {"authorId": "150978762", "name": "Dian Yu"}, {"authorId": "2616463", "name": "Panupong Pasupat"}, {"authorId": "1703465", "name": "Yuan Zhang"}]}, {"paperId": "9fd8fbc97890909869197a888cfc42bde48e94ec", "externalIds": {"ACL": "2021.acl-short.84", "DBLP": "conf/acl/WuXSJZS20", "ArXiv": "2105.13665", "DOI": "10.18653/v1/2021.acl-short.84", "CorpusId": 235248045}, "corpusId": 235248045, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9fd8fbc97890909869197a888cfc42bde48e94ec", "title": "Domain-Adaptive Pretraining Methods for Dialogue Understanding", "abstract": "Language models like BERT and SpanBERT pretrained on open-domain data have obtained impressive gains on various NLP tasks. In this paper, we probe the effectiveness of domain-adaptive pretraining objectives on downstream tasks. In particular, three objectives, including a novel objective focusing on modeling predicate-argument relations, are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domain-adaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks, achieving the new state-of-the-art performances.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 18, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.84.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-28", "journal": {"pages": "665-669"}, "authors": [{"authorId": "2109295256", "name": "Han Wu"}, {"authorId": "2113451979", "name": "Kun Xu"}, {"authorId": "50258954", "name": "Linfeng Song"}, {"authorId": "50496698", "name": "Lifeng Jin"}, {"authorId": "2108558798", "name": "Haisong Zhang"}, {"authorId": "29042285", "name": "Linqi Song"}]}, {"paperId": "8e8c9e907f9be0c098d90eec387da88dd0111238", "externalIds": {"ArXiv": "2007.04792", "MAG": "3041154998", "DBLP": "journals/corr/abs-2007-04792", "ACL": "2021.acl-short.85", "DOI": "10.18653/v1/2021.acl-short.85", "CorpusId": 220424620}, "corpusId": 220424620, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8e8c9e907f9be0c098d90eec387da88dd0111238", "title": "Targeting the Benchmark: On Methodology in Current Natural Language Processing Research", "abstract": "It has become a common pattern in our field: One group introduces a language task, exemplified by a dataset, which they argue is challenging enough to serve as a benchmark. They also provide a baseline model for it, which then soon is improved upon by other groups. Often, research efforts then move on, and the pattern repeats itself. What is typically left implicit is the argumentation for why this constitutes progress, and progress towards what. In this paper, we try to step back for a moment from this pattern and work out possible argumentations and their parts.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 42, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.85.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-07", "journal": {"pages": "670-674"}, "authors": [{"authorId": "1817455", "name": "David Schlangen"}]}, {"paperId": "378e2dc920f312fdb1c5ab095dfc2a0de9208199", "externalIds": {"DBLP": "journals/corr/abs-2106-09248", "ArXiv": "2106.09248", "ACL": "2021.acl-short.86", "DOI": "10.18653/v1/2021.acl-short.86", "CorpusId": 235457983}, "corpusId": 235457983, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/378e2dc920f312fdb1c5ab095dfc2a0de9208199", "title": "X-Fact: A New Benchmark Dataset for Multilingual Fact Checking", "abstract": "In this work, we introduce : the largest publicly available multilingual dataset for factual verification of naturally existing real-world claims. The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers. The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization, and zero-shot capabilities of the multilingual models. Using state-of-the-art multilingual transformer-based models, we develop several automated fact-checking models that, along with textual claims, make use of additional metadata and evidence from news stories retrieved using a search engine. Empirically, our best model attains an F-score of around 40%, suggesting that our dataset is a challenging benchmark for the evaluation of multilingual fact-checking models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 40, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.86.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-17", "journal": {"pages": "675-682"}, "authors": [{"authorId": "15634433", "name": "Ashim Gupta"}, {"authorId": "3052879", "name": "Vivek Srikumar"}]}, {"paperId": "a1d578646cf42f2f69ee996742af484d03cc9121", "externalIds": {"DBLP": "conf/acl/KaleSAXCJ20", "ArXiv": "2106.02171", "ACL": "2021.acl-short.87", "DOI": "10.18653/v1/2021.acl-short.87", "CorpusId": 235352948}, "corpusId": 235352948, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a1d578646cf42f2f69ee996742af484d03cc9121", "title": "nmT5 - Is parallel data still relevant for pre-training massively multilingual language models?", "abstract": "Recently, mT5 - a massively multilingual version of T5 - leveraged a unified text-to-text format to attain state-of-the-art results on a wide variety of multilingual NLP tasks. In this paper, we investigate the impact of incorporating parallel data into mT5 pre-training. We find that multi-tasking language modeling with objectives such as machine translation during pre-training is a straightforward way to improve performance on downstream multilingual and cross-lingual tasks. However, the gains start to diminish as the model capacity increases, suggesting that parallel data might not be as essential for larger models. At the same time, even at larger model sizes, we find that pre-training with parallel data still provides benefits in the limited labelled data regime", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.87.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.02171"}, "authors": [{"authorId": "26688118", "name": "Mihir Kale"}, {"authorId": "9356387", "name": "Aditya Siddhant"}, {"authorId": "40832517", "name": "Noah Constant"}, {"authorId": "2109675545", "name": "Melvin Johnson"}, {"authorId": "1388360943", "name": "Rami Al-Rfou"}, {"authorId": "2692973", "name": "Linting Xue"}]}, {"paperId": "390f174d102c72172249254f3f1048721c0c3161", "externalIds": {"DBLP": "conf/acl/SrivastavaG20", "ACL": "2021.acl-short.88", "ArXiv": "2106.04262", "DOI": "10.18653/v1/2021.acl-short.88", "CorpusId": 235368201}, "corpusId": 235368201, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/390f174d102c72172249254f3f1048721c0c3161", "title": "Question Generation for Adaptive Education", "abstract": "Intelligent and adaptive online education systems aim to make high-quality education available for a diverse range of students. However, existing systems usually depend on a pool of hand-made questions, limiting how fine-grained and open-ended they can be in adapting to individual students. We explore targeted question generation as a controllable sequence generation task. We first show how to fine-tune pre-trained language models for deep knowledge tracing (LM-KT). This model accurately predicts the probability of a student answering a question correctly, and generalizes to questions not seen in training. We then use LM-KT to specify the objective and data for training a model to generate questions conditioned on the student and target difficulty. Our results show we succeed at generating novel, well-calibrated language translation questions for second language learners from a real online education platform.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 17, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.88.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"pages": "692-701"}, "authors": [{"authorId": "31620766", "name": "Megha Srivastava"}, {"authorId": "2066984008", "name": "Noah D. Goodman"}]}, {"paperId": "c3661fc0091545c6b71f78141bdd2075614fe266", "externalIds": {"DBLP": "journals/corr/abs-2106-03830", "MAG": "3170606572", "ACL": "2021.acl-short.89", "ArXiv": "2106.03830", "DOI": "10.18653/v1/2021.acl-short.89", "CorpusId": 235364002}, "corpusId": 235364002, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c3661fc0091545c6b71f78141bdd2075614fe266", "title": "A Simple Recipe for Multilingual Grammatical Error Correction", "abstract": "This paper presents a simple recipe to trainstate-of-the-art multilingual Grammatical Error Correction (GEC) models. We achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples. The second ingredient is to use large-scale multilingual language models (up to 11B parameters). Once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on GEC benchmarks in four languages: English, Czech, German and Russian. Having established a new set of baselines for GEC, we make our results easily reproducible and accessible by releasing a CLANG-8 dataset. It is produced by using our best model, which we call gT5, to clean the targets of a widely used yet noisy Lang-8 dataset. cLang-8 greatly simplifies typical GEC training pipelines composed of multiple fine-tuning stages \u2013 we demonstrate that performing a single fine-tuning stepon cLang-8 with the off-the-shelf language models yields further accuracy improvements over an already top-performing gT5 model for English.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 85, "influentialCitationCount": 20, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.89.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "journal": {"pages": "702-707"}, "authors": [{"authorId": "2204815", "name": "S. Rothe"}, {"authorId": "1931758", "name": "Jonathan Mallinson"}, {"authorId": "3288074", "name": "Eric Malmi"}, {"authorId": "32632038", "name": "Sebastian Krause"}, {"authorId": "3091861", "name": "Aliaksei Severyn"}]}, {"paperId": "2551990a1ccdffb1a4d1d9040b2d493ba6d26dd1", "externalIds": {"DBLP": "conf/acl/HeCWZMXX20", "ACL": "2021.acl-short.90", "DOI": "10.18653/v1/2021.acl-short.90", "CorpusId": 236460151}, "corpusId": 236460151, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2551990a1ccdffb1a4d1d9040b2d493ba6d26dd1", "title": "Towards Visual Question Answering on Pathology Images", "abstract": "Pathology imaging is broadly used for identifying the causes and effects of diseases or injuries. Given a pathology image, being able to answer questions about the clinical findings contained in the image is very important for medical decision making. In this paper, we aim to develop a pathological visual question answering framework to analyze pathology images and answer medical questions related to these images. To build such a framework, we create PathVQA, a VQA dataset with 32,795 questions asked from 4,998 pathology images. We also propose a three-level optimization framework which performs self-supervised pretraining and VQA finetuning end-to-end to learn powerful visual and textual representations jointly and automatically identifies and excludes noisy self-supervised examples from pretraining. We perform experiments on our created PathVQA dataset and the results demonstrate the effectiveness of our proposed methods. The datasets and code are available at https://github.com/UCSD-AI4H/PathVQA", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 20, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.90.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "708-718"}, "authors": [{"authorId": "2149253469", "name": "Xuehai He"}, {"authorId": "2113440594", "name": "Zhuo Cai"}, {"authorId": "2149192867", "name": "Wenlan Wei"}, {"authorId": "2129513945", "name": "Yichen Zhang"}, {"authorId": "37756359", "name": "Luntian Mou"}, {"authorId": "143977260", "name": "E. Xing"}, {"authorId": "40526720", "name": "P. Xie"}]}, {"paperId": "027353b63d9bf423ff675d751ad7505c3fb53614", "externalIds": {"DBLP": "conf/acl/MurugesanAKTSC20", "ACL": "2021.acl-short.91", "DOI": "10.18653/v1/2021.acl-short.91", "CorpusId": 236459800}, "corpusId": 236459800, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/027353b63d9bf423ff675d751ad7505c3fb53614", "title": "Efficient Text-based Reinforcement Learning by Jointly Leveraging State and Commonsense Graph Representations", "abstract": "Text-based games (TBGs) have emerged as useful benchmarks for evaluating progress at the intersection of grounded language understanding and reinforcement learning (RL). Recent work has proposed the use of external knowledge to improve the efficiency of RL agents for TBGs. In this paper, we posit that to act efficiently in TBGs, an agent must be able to track the state of the game while retrieving and using relevant commonsense knowledge. Thus, we propose an agent for TBGs that induces a graph representation of the game state and jointly grounds it with a graph of commonsense knowledge from ConceptNet. This combination is achieved through bidirectional knowledge graph attention between the two symbolic representations. We show that agents that incorporate commonsense into the game state graph outperform baseline agents.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 25, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "719-725"}, "authors": [{"authorId": "3377711", "name": "K. Murugesan"}, {"authorId": "32946276", "name": "Mattia Atzeni"}, {"authorId": "2223082", "name": "Pavan Kapanipathi"}, {"authorId": "2940762", "name": "Kartik Talamadupula"}, {"authorId": "2790926", "name": "Mrinmaya Sachan"}, {"authorId": "143903370", "name": "Murray Campbell"}]}, {"paperId": "5eb4dc9b012286f22724bafff0edd3dacff09fc6", "externalIds": {"DBLP": "conf/acl/LeiBB20", "ACL": "2021.acl-short.92", "ArXiv": "2108.00061", "DOI": "10.18653/v1/2021.acl-short.92", "CorpusId": 236460035}, "corpusId": 236460035, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5eb4dc9b012286f22724bafff0edd3dacff09fc6", "title": "mTVR: Multilingual Moment Retrieval in Videos", "abstract": "We introduce mTVR, a large-scale multilingual video moment retrieval dataset, containing 218K English and Chinese queries from 21.8K TV show video clips. The dataset is collected by extending the popular TVR dataset (in English) with paired Chinese queries and subtitles. Compared to existing moment retrieval datasets, mTVR is multilingual, larger, and comes with diverse annotations. We further propose mXML, a multilingual moment retrieval model that learns and operates on data from both languages, via encoder parameter sharing and language neighborhood constraints. We demonstrate the effectiveness of mXML on the newly collected mTVR dataset, where mXML outperforms strong monolingual baselines while using fewer parameters. In addition, we also provide detailed dataset analyses and model ablations. Data and code are publicly available at https://github.com/jayleicn/mTVRetrieval", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.92.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-30", "journal": {"pages": "726-734"}, "authors": [{"authorId": "2052835090", "name": "Jie Lei"}, {"authorId": "1685538", "name": "Tamara L. Berg"}, {"authorId": "143977268", "name": "Mohit Bansal"}]}, {"paperId": "626576bcda2404c328c8848331e0b34f64394571", "externalIds": {"DBLP": "conf/acl/ChenDAH20", "ACL": "2021.acl-short.93", "DOI": "10.18653/v1/2021.acl-short.93", "CorpusId": 236459975}, "corpusId": 236459975, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/626576bcda2404c328c8848331e0b34f64394571", "title": "Explicitly Capturing Relations between Entity Mentions via Graph Neural Networks for Domain-specific Named Entity Recognition", "abstract": "Named entity recognition (NER) is well studied for the general domain, and recent systems have achieved human-level performance for identifying common entity types. However, the NER performance is still moderate for specialized domains that tend to feature complicated contexts and jargonistic entity types. To address these challenges, we propose explicitly connecting entity mentions based on both global coreference relations and local dependency relations for building better entity mention representations. In our experiments, we incorporate entity mention relations by Graph Neural Networks and show that our system noticeably improves the NER performance on two datasets from different domains. We further show that the proposed lightweight system can effectively elevate the NER performance to a higher level even when only a tiny amount of labeled data is available, which is desirable for domain-specific NER.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.93.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "735-742"}, "authors": [{"authorId": "2901524", "name": "Pei Chen"}, {"authorId": "47929135", "name": "Haibo Ding"}, {"authorId": "50007145", "name": "J. Araki"}, {"authorId": "1455107421", "name": "Ruihong Huang"}]}, {"paperId": "cbc0a2ddb8df70ae7e68b20038dfb8274f84b850", "externalIds": {"ArXiv": "2105.05498", "DBLP": "conf/acl/LeeYC20", "ACL": "2021.acl-short.94", "DOI": "10.18653/v1/2021.acl-short.94", "CorpusId": 234469688}, "corpusId": 234469688, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cbc0a2ddb8df70ae7e68b20038dfb8274f84b850", "title": "Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction", "abstract": "Accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases appear in the translation output. However, in many cases, those methods are studied on general domain corpora, where the terms are mostly uni- and bi-grams (>98%). In this paper, we instead tackle a more challenging setup consisting of domain-specific corpora with much longer n-gram and highly specialized terms. Inspired by the recent success of masked span prediction models, we propose a simple and effective training strategy that achieves consistent improvements on both terminology and sentence-level translation for three domain-specific corpora in two language pairs.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.94.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-12", "journal": {"name": "ArXiv", "volume": "abs/2105.05498"}, "authors": [{"authorId": "2115216766", "name": "Gyubok Lee"}, {"authorId": "153909426", "name": "S. Yang"}, {"authorId": "3242613", "name": "E. Choi"}]}, {"paperId": "64a16a16a869d4396c658c32293647f28168f7e0", "externalIds": {"ArXiv": "2105.14189", "DBLP": "conf/acl/WangZW20", "ACL": "2021.acl-short.95", "DOI": "10.18653/v1/2021.acl-short.95", "CorpusId": 235254521}, "corpusId": 235254521, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/64a16a16a869d4396c658c32293647f28168f7e0", "title": "Quotation Recommendation and Interpretation Based on Transformation from Queries to Quotations", "abstract": "To help individuals express themselves better, quotation recommendation is receiving growing attention. Nevertheless, most prior efforts focus on modeling quotations and queries separately and ignore the relationship between the quotations and the queries. In this work, we introduce a transformation matrix that directly maps the query representations to quotation representations. To better learn the mapping relationship, we employ a mapping loss that minimizes the distance of two semantic spaces (one for quotation and another for mapped-query). Furthermore, we explore using the words in history queries to interpret the figurative language of quotations, where quotation-aware attention is applied on top of history queries to highlight the indicator words. Experiments on two datasets in English and Chinese show that our model outperforms previous state-of-the-art models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 13, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.95.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-29", "journal": {"pages": "754-758"}, "authors": [{"authorId": "48170367", "name": "Lingzhi Wang"}, {"authorId": "46180553", "name": "Xingshan Zeng"}, {"authorId": "1784988", "name": "Kam-Fai Wong"}]}, {"paperId": "07d85a6c8a31f43ee6e128f7ef0a1bf1494567cd", "externalIds": {"ArXiv": "2004.03974", "DBLP": "journals/corr/abs-2004-03974", "MAG": "3015202090", "ACL": "2021.acl-short.96", "DOI": "10.18653/v1/2021.acl-short.96", "CorpusId": 215415911}, "corpusId": 215415911, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/07d85a6c8a31f43ee6e128f7ef0a1bf1494567cd", "title": "Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence", "abstract": "Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 46, "citationCount": 159, "influentialCitationCount": 36, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.96.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-08", "journal": {"pages": "759-766"}, "authors": [{"authorId": "49224924", "name": "Federico Bianchi"}, {"authorId": "1392653707", "name": "Silvia Terragni"}, {"authorId": "2022288", "name": "Dirk Hovy"}]}, {"paperId": "d7040675dcd91157a4fbf4f8e4f34a93eec81f29", "externalIds": {"DBLP": "conf/acl/WangNBB20", "ACL": "2021.acl-short.97", "DOI": "10.18653/v1/2021.acl-short.97", "CorpusId": 236460196}, "corpusId": 236460196, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d7040675dcd91157a4fbf4f8e4f34a93eec81f29", "title": "Input Representations for Parsing Discourse Representation Structures: Comparing English with Chinese", "abstract": "Neural semantic parsers have obtained acceptable results in the context of parsing DRSs (Discourse Representation Structures). In particular models with character sequences as input showed remarkable performance for English. But how does this approach perform on languages with a different writing system, like Chinese, a language with a large vocabulary of characters? Does rule-based tokenisation of the input help, and which granularity is preferred: characters, or words? The results are promising. Even with DRSs based on English, good results for Chinese are obtained. Tokenisation offers a small advantage for English, but not for Chinese. Overall, characters are preferred as input, both for English and Chinese.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 52, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.97.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "767-775"}, "authors": [{"authorId": "80578055", "name": "Chunliu Wang"}, {"authorId": "66583219", "name": "Rik van Noord"}, {"authorId": "3242253", "name": "Arianna Bisazza"}, {"authorId": "3461596", "name": "Johan Bos"}]}, {"paperId": "88053d888db7ee16fa2632fea4bfd0dfca15df06", "externalIds": {"ArXiv": "2101.00259", "ACL": "2021.acl-short.98", "DBLP": "conf/acl/NorouziTC20", "DOI": "10.18653/v1/2021.acl-short.98", "CorpusId": 236459804}, "corpusId": 236459804, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/88053d888db7ee16fa2632fea4bfd0dfca15df06", "title": "Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data", "abstract": "Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks. As a result, models for this application usually need additional prior knowledge to be built into the architecture or algorithm. The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice. This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design. By exploiting a relatively sizeable monolingual corpus of the target programming language, which is cheap to mine from the web, we achieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa. Both are SOTA to the best of our knowledge. This positive evidence highlights a potentially easier path toward building accurate semantic parsers in practice.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.98.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-01", "journal": {"pages": "776-785"}, "authors": [{"authorId": "80200948", "name": "Sajad Norouzi"}, {"authorId": "1995851674", "name": "Keyi Tang"}, {"authorId": "2125469779", "name": "Yanshuai Cao"}]}, {"paperId": "5a3826bfed12bbff27f8dd701928d741e5d3d960", "externalIds": {"ACL": "2021.acl-short.99", "DBLP": "conf/acl/MaYLZ20", "DOI": "10.18653/v1/2021.acl-short.99", "CorpusId": 236460138}, "corpusId": 236460138, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5a3826bfed12bbff27f8dd701928d741e5d3d960", "title": "Issues with Entailment-based Zero-shot Text Classification", "abstract": "The general format of natural language inference (NLI) makes it tempting to be used for zero-shot text classification by casting any target label into a sentence of hypothesis and verifying whether or not it could be entailed by the input, aiming at generic classification applicable on any specified label space. In this opinion piece, we point out a few overlooked issues that are yet to be discussed in this line of work. We observe huge variance across different classification datasets amongst standard BERT-based NLI models and surprisingly find that pre-trained BERT without any fine-tuning can yield competitive performance against BERT fine-tuned for NLI. With the concern that these models heavily rely on spurious lexical patterns for prediction, we also experiment with preliminary approaches for more robust NLI, but the results are in general negative. Our observations reveal implicit but challenging difficulties in entailment-based zero-shot text classification.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 57, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.99.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "786-796"}, "authors": [{"authorId": "2168066", "name": "Tingting Ma"}, {"authorId": "3303069", "name": "Jin-ge Yao"}, {"authorId": "50554693", "name": "Chin-Yew Lin"}, {"authorId": "145382463", "name": "T. Zhao"}]}, {"paperId": "513e6d3ac68f170feebaee7c9b53a1b8e9533773", "externalIds": {"ArXiv": "2105.06717", "ACL": "2021.acl-short.100", "DBLP": "conf/acl/MoghimifarQZHB20", "DOI": "10.18653/v1/2021.acl-short.100", "CorpusId": 234682053}, "corpusId": 234682053, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/513e6d3ac68f170feebaee7c9b53a1b8e9533773", "title": "Neural-Symbolic Commonsense Reasoner with Relation Predictors", "abstract": "Commonsense reasoning aims to incorporate sets of commonsense facts, retrieved from Commonsense Knowledge Graphs (CKG), to draw conclusion about ordinary situations. The dynamic nature of commonsense knowledge postulates models capable of performing multi-hop reasoning over new situations. This feature also results in having large-scale sparse Knowledge Graphs, where such reasoning process is needed to predict relations between new events. However, existing approaches in this area are limited by considering CKGs as a limited set of facts, thus rendering them unfit for reasoning over new unseen situations and events. In this paper, we present a neural-symbolic reasoner, which is capable of reasoning over large-scale dynamic CKGs. The logic rules for reasoning over CKGs are learned during training by our model. In addition to providing interpretable explanation, the learned logic rules help to generalise prediction to newly introduced events. Experimental results on the task of link prediction on CKGs prove the effectiveness of our model by outperforming the state-of-the-art models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.100.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-14", "journal": {"pages": "797-802"}, "authors": [{"authorId": "2804001", "name": "Farhad Moghimifar"}, {"authorId": "14564042", "name": "Lizhen Qu"}, {"authorId": "2080123731", "name": "Terry Yue Zhuo"}, {"authorId": "2561045", "name": "Gholamreza Haffari"}, {"authorId": "51278862", "name": "Mahsa Baktashmotlagh"}]}, {"paperId": "b6bffec600168834ae031491796e20b2feb1702f", "externalIds": {"ACL": "2021.acl-short.101", "DBLP": "conf/acl/StajnerYGF20", "DOI": "10.18653/v1/2021.acl-short.101", "CorpusId": 236459869}, "corpusId": 236459869, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b6bffec600168834ae031491796e20b2feb1702f", "title": "What Motivates You? Benchmarking Automatic Detection of Basic Needs from Short Posts", "abstract": "According to the self-determination theory, the levels of satisfaction of three basic needs (competence, autonomy and relatedness) have implications on people\u2019s everyday life and career. We benchmark the novel task of automatically detecting those needs on short posts in English, by modelling it as a ternary classification task, and as three binary classification tasks. A detailed manual analysis shows that the latter has advantages in the real-world scenario, and that our best models achieve similar performances as a trained human annotator.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "803-810"}, "authors": [{"authorId": "1952894", "name": "Sanja \u0160tajner"}, {"authorId": "28026360", "name": "Seren Yenikent"}, {"authorId": "46188821", "name": "Bilal Ghanem"}, {"authorId": "1403862010", "name": "Marc Franco-Salvador"}]}, {"paperId": "765976a7035bb93b2cb58e1d86fc731b5fd0ae48", "externalIds": {"ACL": "2021.acl-short.102", "DBLP": "conf/acl/YamadaST20", "ArXiv": "2105.13466", "DOI": "10.18653/v1/2021.acl-short.102", "CorpusId": 235248108}, "corpusId": 235248108, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/765976a7035bb93b2cb58e1d86fc731b5fd0ae48", "title": "Semantic Frame Induction using Masked Word Embeddings and Two-Step Clustering", "abstract": "Recent studies on semantic frame induction show that relatively high performance has been achieved by using clustering-based methods with contextualized word embeddings. However, there are two potential drawbacks to these methods: one is that they focus too much on the superficial information of the frame-evoking verb and the other is that they tend to divide the instances of the same verb into too many different frame clusters. To overcome these drawbacks, we propose a semantic frame induction method using masked word embeddings and two-step clustering. Through experiments on the English FrameNet data, we demonstrate that using the masked word embeddings is effective for avoiding too much reliance on the surface information of frame-evoking verbs and that two-step clustering can improve the number of resulting frame clusters for the instances of the same verb.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 16, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.102.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-27", "journal": {"pages": "811-816"}, "authors": [{"authorId": "1490775466", "name": "Kosuke Yamada"}, {"authorId": "2293543", "name": "Ryohei Sasano"}, {"authorId": "2874038", "name": "Koichi Takeda"}]}, {"paperId": "eacb5dc57a167aeda3b23c28abfc2b51095f1b7c", "externalIds": {"DBLP": "journals/corr/abs-2106-01463", "ArXiv": "2106.01463", "ACL": "2021.acl-short.103", "DOI": "10.18653/v1/2021.acl-short.103", "CorpusId": 235313889}, "corpusId": 235313889, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/eacb5dc57a167aeda3b23c28abfc2b51095f1b7c", "title": "Lightweight Adapter Tuning for Multilingual Speech Translation", "abstract": "Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pre-trained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 60, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.103.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01463"}, "authors": [{"authorId": "91955511", "name": "Hang Le"}, {"authorId": "145503806", "name": "J. Pino"}, {"authorId": "20132361", "name": "Changhan Wang"}, {"authorId": "3016273", "name": "Jiatao Gu"}, {"authorId": "2504683", "name": "D. Schwab"}, {"authorId": "143823463", "name": "L. Besacier"}]}, {"paperId": "3762b6b18f78d6e67fd4570812274994b889bd5d", "externalIds": {"ACL": "2021.acl-short.104", "DBLP": "journals/corr/abs-2107-05393", "ArXiv": "2107.05393", "DOI": "10.18653/v1/2021.acl-short.104", "CorpusId": 235328547}, "corpusId": 235328547, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3762b6b18f78d6e67fd4570812274994b889bd5d", "title": "Parameter Selection: Why We Should Pay More Attention to It", "abstract": "The importance of parameter selection in supervised learning is well known. However, due to the many parameter combinations, an incomplete or an insufficient procedure is often applied. This situation may cause misleading or confusing conclusions. In this opinion paper, through an intriguing example we point out that the seriousness goes beyond what is generally recognized. In the topic of multilabel classification for medical code prediction, one influential paper conducted a proper parameter selection on a set, but when moving to a subset of frequently occurring labels, the authors used the same parameters without a separate tuning. The set of frequent labels became a popular benchmark in subsequent studies, which kept pushing the state of the art. However, we discovered that most of the results in these studies cannot surpass the approach in the original paper if a parameter tuning had been conducted at the time. Thus it is unclear how much progress the subsequent developments have actually brought. The lesson clearly indicates that without enough attention on parameter selection, the research progress in our field can be uncertain or even illusive.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.104.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-08", "journal": {"pages": "825-830"}, "authors": [{"authorId": "1644967666", "name": "Jie-Jyun Liu"}, {"authorId": "2109191206", "name": "Tsung-Han Yang"}, {"authorId": "2111637671", "name": "Si-An Chen"}, {"authorId": "2146148973", "name": "Chih-Jen Lin"}]}, {"paperId": "4b4788db41d21f531465524c582b2a65fa2d33c7", "externalIds": {"DBLP": "conf/acl/OhashiTKA20", "ACL": "2021.acl-short.105", "DOI": "10.18653/v1/2021.acl-short.105", "CorpusId": 236460278}, "corpusId": 236460278, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4b4788db41d21f531465524c582b2a65fa2d33c7", "title": "Distinct Label Representations for Few-Shot Text Classification", "abstract": "Few-shot text classification aims to classify inputs whose label has only a few examples. Previous studies overlooked the semantic relevance between label representations. Therefore, they are easily confused by labels that are relevant. To address this problem, we propose a method that generates distinct label representations that embed information specific to each label. Our method is applicable to conventional few-shot classification models. Experimental results show that our method significantly improved the performance of few-shot text classification across models and datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.105.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "831-836"}, "authors": [{"authorId": "1755134768", "name": "Sora Ohashi"}, {"authorId": "67032534", "name": "Junya Takayama"}, {"authorId": "1981103", "name": "Tomoyuki Kajiwara"}, {"authorId": "3043844", "name": "Yuki Arase"}]}, {"paperId": "448c377dfd28ceb9c1e55dbdc55872fb6e34ba57", "externalIds": {"DBLP": "conf/acl/CastellucciFC020", "ACL": "2021.acl-short.106", "DOI": "10.18653/v1/2021.acl-short.106", "CorpusId": 236460265}, "corpusId": 236460265, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/448c377dfd28ceb9c1e55dbdc55872fb6e34ba57", "title": "Learning to Solve NLP Tasks in an Incremental Number of Languages", "abstract": "In real scenarios, a multilingual model trained to solve NLP tasks on a set of languages can be required to support new languages over time. Unfortunately, the straightforward retraining on a dataset containing annotated examples for all the languages is both expensive and time-consuming, especially when the number of target languages grows. Moreover, the original annotated material may no longer be available due to storage or business constraints. Re-training only with the new language data will inevitably result in Catastrophic Forgetting of previously acquired knowledge. We propose a Continual Learning strategy that updates a model to support new languages over time, while maintaining consistent results on previously learned languages. We define a Teacher-Student framework where the existing model \u201cteaches\u201d to a student model its knowledge about the languages it supports, while the student is also trained on a new language. We report an experimental evaluation in several tasks including Sentence Classification, Relational Learning and Sequence Labeling.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.106.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "837-847"}, "authors": [{"authorId": "3285152", "name": "Giuseppe Castellucci"}, {"authorId": "2778426", "name": "Simone Filice"}, {"authorId": "1784172", "name": "D. Croce"}, {"authorId": "144654543", "name": "Roberto Basili"}]}, {"paperId": "84daddd294fa3cc12596b5785f81c2a153d2fb1d", "externalIds": {"DBLP": "journals/corr/abs-2106-01040", "ACL": "2021.acl-short.107", "ArXiv": "2106.01040", "DOI": "10.18653/v1/2021.acl-short.107", "CorpusId": 235294151}, "corpusId": 235294151, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/84daddd294fa3cc12596b5785f81c2a153d2fb1d", "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling", "abstract": "Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 25, "citationCount": 31, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.107.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01040"}, "authors": [{"authorId": "2118839668", "name": "Chuhan Wu"}, {"authorId": "2397264", "name": "Fangzhao Wu"}, {"authorId": "50329599", "name": "Tao Qi"}, {"authorId": "1731776", "name": "Yongfeng Huang"}]}, {"paperId": "cdcffb2f1678d7252bfd9b902d3cd676a5217005", "externalIds": {"DBLP": "conf/acl/HanPW20", "ACL": "2021.acl-short.108", "ArXiv": "2108.02340", "DOI": "10.18653/v1/2021.acl-short.108", "CorpusId": 236460041}, "corpusId": 236460041, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cdcffb2f1678d7252bfd9b902d3cd676a5217005", "title": "Robust Transfer Learning with Pretrained Language Models through Adapters", "abstract": "Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 34, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.108.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-05", "journal": {"name": "ArXiv", "volume": "abs/2108.02340"}, "authors": [{"authorId": "144836032", "name": "Wenjuan Han"}, {"authorId": "2063096824", "name": "Bo Pang"}, {"authorId": "39092098", "name": "Y. Wu"}]}, {"paperId": "fb51dc284e42927d018858fcc6618d16cbdfc042", "externalIds": {"ArXiv": "2106.03020", "DBLP": "journals/corr/abs-2106-03020", "ACL": "2021.acl-short.109", "MAG": "3166996085", "DOI": "10.18653/v1/2021.acl-short.109", "CorpusId": 235358498}, "corpusId": 235358498, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fb51dc284e42927d018858fcc6618d16cbdfc042", "title": "Embracing Ambiguity: Shifting the Training Target of NLI Models", "abstract": "Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels. While many research works do not pay much attention to this fact, several recent efforts have been made to acknowledge and embrace the existence of ambiguity, such as UNLI and ChaosNLI. In this paper, we explore the option of training directly on the estimated label distribution of the annotators in the NLI task, using a learning loss based on this ambiguity distribution instead of the gold-labels. We prepare AmbiNLI, a trial dataset obtained from readily available sources, and show it is possible to reduce ChaosNLI divergence scores when finetuning on this data, a promising first step towards learning how to capture linguistic ambiguity. Additionally, we show that training on the same amount of data but targeting the ambiguity distribution instead of gold-labels can result in models that achieve higher performance and learn better representations for downstream tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 11, "citationCount": 15, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.109.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-06", "journal": {"pages": "862-869"}, "authors": [{"authorId": "144984319", "name": "Johannes Mario Meissner"}, {"authorId": "3406736", "name": "Napat Thumwanit"}, {"authorId": "2673984", "name": "Saku Sugawara"}, {"authorId": "1705519", "name": "Akiko Aizawa"}]}, {"paperId": "8c54e2bc0dde115a00886cb026a04daff3f4b4de", "externalIds": {"DBLP": "journals/corr/abs-2105-14289", "ArXiv": "2105.14289", "ACL": "2021.acl-short.110", "DOI": "10.18653/v1/2021.acl-short.110", "CorpusId": 235253927}, "corpusId": 235253927, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8c54e2bc0dde115a00886cb026a04daff3f4b4de", "title": "Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning", "abstract": "Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. A key challenge of OOD detection is to learn discriminative semantic features. Traditional cross-entropy loss only focuses on whether a sample is correctly classified, and does not explicitly distinguish the margins between categories. In this paper, we propose a supervised contrastive learning objective to minimize intra-class variance by pulling together in-domain intents belonging to the same class and maximize inter-class variance by pushing apart samples from different classes. Besides, we employ an adversarial augmentation mechanism to obtain pseudo diverse views of a sample in the latent space. Experiments on two public datasets prove the effectiveness of our method capturing discriminative representations for OOD detection.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 43, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.110.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-29", "journal": {"name": "ArXiv", "volume": "abs/2105.14289"}, "authors": [{"authorId": "2101321888", "name": "Zhiyuan Zeng"}, {"authorId": "2058349088", "name": "Keqing He"}, {"authorId": "1500528818", "name": "Yuanmeng Yan"}, {"authorId": null, "name": "Zijun Liu"}, {"authorId": "50117922", "name": "Yanan Wu"}, {"authorId": "2146234760", "name": "Hong Xu"}, {"authorId": "2309680", "name": "Huixing Jiang"}, {"authorId": "1753096", "name": "Weiran Xu"}]}, {"paperId": "eadaf893e56adc383e07c119fefe85a2d1b9832c", "externalIds": {"ACL": "2021.acl-short.111", "ArXiv": "2106.00291", "DBLP": "journals/corr/abs-2106-00291", "DOI": "10.18653/v1/2021.acl-short.111", "CorpusId": 235266201}, "corpusId": 235266201, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/eadaf893e56adc383e07c119fefe85a2d1b9832c", "title": "Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialogue State Tracking", "abstract": "Existing dialog state tracking (DST) models are trained with dialog data in a random order, neglecting rich structural information in a dataset. In this paper, we propose to use curriculum learning (CL) to better leverage both the curriculum structure and schema structure for task-oriented dialogs. Specifically, we propose a model-agnostic framework called Schema-aware Curriculum Learning for Dialog State Tracking (SaCLog), which consists of a preview module that pre-trains a DST model with schema information, a curriculum module that optimizes the model with CL, and a review module that augments mispredicted data to reinforce the CL training. We show that our proposed approach improves DST performance over both a transformer-based and RNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 44, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.111.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-06-01", "journal": {"pages": "879-885"}, "authors": [{"authorId": "30087809", "name": "Yinpei Dai"}, {"authorId": "2118386175", "name": "Hangyu Li"}, {"authorId": "1527090216", "name": "Yongbin Li"}, {"authorId": "2152147863", "name": "Jian Sun"}, {"authorId": "2087380523", "name": "Fei Huang"}, {"authorId": "2059080424", "name": "Luo Si"}, {"authorId": "150345740", "name": "Xiaodan Zhu"}]}, {"paperId": "c6304247f04870393d56579cdd5e093a25c121ee", "externalIds": {"ACL": "2021.acl-short.112", "DBLP": "conf/acl/ZhouLTZYHJCCYZW20", "DOI": "10.18653/v1/2021.acl-short.112", "CorpusId": 236460133}, "corpusId": 236460133, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c6304247f04870393d56579cdd5e093a25c121ee", "title": "On the Generation of Medical Dialogs for COVID-19", "abstract": "Under the pandemic of COVID-19, people experiencing COVID19-related symptoms have a pressing need to consult doctors. Because of the shortage of medical professionals, many people cannot receive online consultations timely. To address this problem, we aim to develop a medical dialog system that can provide COVID19-related consultations. We collected two dialog datasets \u2013 CovidDialog \u2013 (in English and Chinese respectively) containing conversations between doctors and patients about COVID-19. While the largest of their kind, these two datasets are still relatively small compared with general-domain dialog datasets. Training complex dialog generation models on small datasets bears high risk of overfitting. To alleviate overfitting, we develop a multi-task learning approach, which regularizes the data-deficient dialog generation task with a masked token prediction task. Experiments on the CovidDialog datasets demonstrate the effectiveness of our approach. We perform both human evaluation and automatic evaluation of dialogs generated by our method. Results show that the generated responses are promising in being doctor-like, relevant to conversation history, clinically informative and correct. The code and the data are available at https://github.com/UCSD-AI4H/COVID-Dialogue.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.112.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "886-896"}, "authors": [{"authorId": "2112494296", "name": "Meng Zhou"}, {"authorId": "2003804697", "name": "Zechen Li"}, {"authorId": "10918587", "name": "Bowen Tan"}, {"authorId": "2061248094", "name": "Guangtao Zeng"}, {"authorId": "32412901", "name": "Wenmian Yang"}, {"authorId": "2149253469", "name": "Xuehai He"}, {"authorId": "1613055688", "name": "Zeqian Ju"}, {"authorId": "79952429", "name": "Subrato Chakravorty"}, {"authorId": "2107976513", "name": "Shu Chen"}, {"authorId": "48520402", "name": "Xingyi Yang"}, {"authorId": "2129513945", "name": "Yichen Zhang"}, {"authorId": "2152862795", "name": "Qingyang Wu"}, {"authorId": "1564034697", "name": "Zhou Yu"}, {"authorId": "2113451979", "name": "Kun Xu"}, {"authorId": "143977260", "name": "E. Xing"}, {"authorId": "40526720", "name": "P. Xie"}]}, {"paperId": "8f31038de5cadc3171735c0410511c044d216463", "externalIds": {"ACL": "2021.acl-short.113", "DBLP": "conf/acl/LeeSCCM20", "ArXiv": "2107.08685", "DOI": "10.18653/v1/2021.acl-short.113", "CorpusId": 236087603}, "corpusId": 236087603, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8f31038de5cadc3171735c0410511c044d216463", "title": "Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images", "abstract": "In multi-modal dialogue systems, it is important to allow the use of images as part of a multi-turn conversation. Training such dialogue systems generally requires a large-scale dataset consisting of multi-turn dialogues that involve images, but such datasets rarely exist. In response, this paper proposes a 45k multi-modal dialogue dataset created with minimal human intervention. Our method to create such a dataset consists of (1) preparing and pre-processing text dialogue datasets, (2) creating image-mixed dialogues by using a text-to-image replacement technique, and (3) employing a contextual-similarity-based filtering step to ensure the contextual coherence of the dataset. To evaluate the validity of our dataset, we devise a simple retrieval model for dialogue sentence prediction tasks. Automatic metrics and human evaluation results on such tasks show that our dataset can be effectively used as training data for multi-modal dialogue systems which require an understanding of images and text in a context-aware manner. Our dataset and generation code is available at https://github.com/shh1574/multi-modal-dialogue-dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 18, "citationCount": 13, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.113.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-19", "journal": {"name": "ArXiv", "volume": "abs/2107.08685"}, "authors": [{"authorId": "1666584032", "name": "Nyoungwoo Lee"}, {"authorId": "2032186731", "name": "Suwon Shin"}, {"authorId": "1795455", "name": "J. Choo"}, {"authorId": "145530103", "name": "Ho\u2010Jin Choi"}, {"authorId": "2675306", "name": "S. Myaeng"}]}, {"paperId": "f683f1d7c3366f3c9fd967540112900c6918387a", "externalIds": {"ACL": "2021.acl-short.114", "DBLP": "conf/acl/Nozza20", "DOI": "10.18653/v1/2021.acl-short.114", "CorpusId": 236460111}, "corpusId": 236460111, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f683f1d7c3366f3c9fd967540112900c6918387a", "title": "Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection", "abstract": "Reducing and counter-acting hate speech on Social Media is a significant concern. Most of the proposed automatic methods are conducted exclusively on English and very few consistently labeled, non-English resources have been proposed. Learning to detect hate speech on English and transferring to unseen languages seems an immediate solution. This work is the first to shed light on the limits of this zero-shot, cross-lingual transfer learning framework for hate speech detection. We use benchmark data sets in English, Italian, and Spanish to detect hate speech towards immigrants and women. Investigating post-hoc explanations of the model, we discover that non-hateful, language-specific taboo interjections are misinterpreted as signals of hate speech. Our findings demonstrate that zero-shot, cross-lingual models cannot be used as they are, but need to be carefully designed.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 54, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.114.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "907-914"}, "authors": [{"authorId": "2101317501", "name": "Debora Nozza"}]}, {"paperId": "75decd34aeed7057ab3962be5310ec2faa7ec9ba", "externalIds": {"ACL": "2021.acl-short.115", "ArXiv": "2106.02208", "DBLP": "conf/acl/UnanuePP20", "DOI": "10.18653/v1/2021.acl-short.115", "CorpusId": 235352574}, "corpusId": 235352574, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/75decd34aeed7057ab3962be5310ec2faa7ec9ba", "title": "BERTTune: Fine-Tuning Neural Machine Translation with BERTScore", "abstract": "Neural machine translation models are often biased toward the limited translation references seen during training. To amend this form of overfitting, in this paper we propose fine-tuning the models with a novel training objective based on the recently-proposed BERTScore evaluation metric. BERTScore is a scoring function based on contextual embeddings that overcomes the typical limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing translations that are different from the references, yet close in the contextual embedding space, to be treated as substantially correct. To be able to use BERTScore as a training objective, we propose three approaches for generating soft predictions, allowing the network to remain completely differentiable end-to-end. Experiments carried out over four, diverse language pairs show improvements of up to 0.58 pp (3.28%) in BLEU score and up to 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 36, "citationCount": 18, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.115.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02208"}, "authors": [{"authorId": "19229462", "name": "Inigo Jauregi Unanue"}, {"authorId": "2093095932", "name": "Jacob Parnell"}, {"authorId": "35153150", "name": "M. Piccardi"}]}, {"paperId": "956a9ea85bda349eff58588c1ad64015560414e0", "externalIds": {"DBLP": "conf/acl/ShiD20", "ACL": "2021.acl-short.116", "DOI": "10.18653/v1/2021.acl-short.116", "CorpusId": 236459901}, "corpusId": 236459901, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/956a9ea85bda349eff58588c1ad64015560414e0", "title": "Entity Enhancement for Implicit Discourse Relation Classification in the Biomedical Domain", "abstract": "Implicit discourse relation classification is a challenging task, in particular when the text domain is different from the standard Penn Discourse Treebank (PDTB; Prasad et al., 2008) training corpus domain (Wall Street Journal in 1990s). We here tackle the task of implicit discourse relation classification on the biomedical domain, for which the Biomedical Discourse Relation Bank (BioDRB; Prasad et al., 2011) is available. We show that entity information can be used to improve discourse relational argument representation. In a first step, we show that explicitly marked instances that are content-wise similar to the target relations can be used to achieve good performance in the cross-domain setting using a simple unsupervised voting pipeline. As a further step, we show that with the linked entity information from the first step, a transformer which is augmented with entity-related information (KBERT; Liu et al., 2020) sets the new state of the art performance on the dataset, outperforming the large pre-trained BioBERT (Lee et al., 2020) model by 2% points.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "925-931"}, "authors": [{"authorId": null, "name": "Wei Shi"}, {"authorId": "2869436", "name": "Vera Demberg"}]}, {"paperId": "7064225b8736e4687d1d0ad37ee3e0c88b780b3e", "externalIds": {"ACL": "2021.acl-short.117", "DBLP": "conf/acl/ShenBB20", "ArXiv": "2105.12392", "DOI": "10.18653/v1/2021.acl-short.117", "CorpusId": 235196117}, "corpusId": 235196117, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7064225b8736e4687d1d0ad37ee3e0c88b780b3e", "title": "Unsupervised Pronoun Resolution via Masked Noun-Phrase Prediction", "abstract": "In this work, we propose Masked Noun-Phrase Prediction (MNPP), a pre-training strategy to tackle pronoun resolution in a fully unsupervised setting. Firstly, We evaluate our pre-trained model on various pronoun resolution datasets without any finetuning. Our method outperforms all previous unsupervised methods on all datasets by large margins. Secondly, we proceed to a few-shot setting where we finetune our pre-trained model on WinoGrande-S and XS separately. Our method outperforms RoBERTa-large baseline with large margins, meanwhile, achieving a higher AUC score after further finetuning on the remaining three official splits of WinoGrande.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 58, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.117.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-26", "journal": {"pages": "932-941"}, "authors": [{"authorId": "118558736", "name": "Minghan Shen"}, {"authorId": "120722271", "name": "Pratyay Banerjee"}, {"authorId": "2064619864", "name": "Chitta Baral"}]}, {"paperId": "c61641ee5849efb5bafc786f2fd3e976e19f81c8", "externalIds": {"DBLP": "conf/acl/LiBYWH20", "ACL": "2021.acl-short.118", "DOI": "10.18653/v1/2021.acl-short.118", "CorpusId": 236459988}, "corpusId": 236459988, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c61641ee5849efb5bafc786f2fd3e976e19f81c8", "title": "Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction", "abstract": "Recently, question answering (QA) based on machine reading comprehension has become popular. This work focuses on generative QA which aims to generate an abstractive answer to a given question instead of extracting an answer span from a provided passage. Generative QA often suffers from two critical problems: (1) summarizing content irrelevant to a given question, (2) drifting away from a correct answer during generation. In this paper, we address these problems by a novel Rationale-Enriched Answer Generator (REAG), which incorporates an extractive mechanism into a generative model. Specifically, we add an extraction task on the encoder to obtain the rationale for an answer, which is the most relevant piece of text in an input document to a given question. Based on the extracted rationale and original input, the decoder is expected to generate an answer with high confidence. We jointly train REAG on the MS MARCO QA+NLG task and the experimental results show that REAG improves the quality and semantic accuracy of answers over baseline models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 17, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "942-947"}, "authors": [{"authorId": "143971529", "name": "Chenliang Li"}, {"authorId": "2555622", "name": "Bin Bi"}, {"authorId": "2114009661", "name": "Mingshi Yan"}, {"authorId": "38700603", "name": "Wei Wang"}, {"authorId": "2410938", "name": "Songfang Huang"}]}, {"paperId": "ffd2e261889a56c7b54d03679f8827ebb118c1fb", "externalIds": {"ArXiv": "2105.14241", "DBLP": "journals/corr/abs-2105-14241", "ACL": "2021.acl-short.119", "DOI": "10.18653/v1/2021.acl-short.119", "CorpusId": 235254069}, "corpusId": 235254069, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ffd2e261889a56c7b54d03679f8827ebb118c1fb", "title": "Demoting the Lead Bias in News Summarization via Alternating Adversarial Learning", "abstract": "In news articles the lead bias is a common phenomenon that usually dominates the learning signals for neural extractive summarizers, severely limiting their performance on data with different or even no bias. In this paper, we introduce a novel technique to demote lead bias and make the summarizer focus more on the content semantics. Experiments on two news corpora with different degrees of lead bias show that our method can effectively demote the model\u2019s learned lead bias and improve its generality on out-of-distribution data, with little to no performance loss on in-distribution data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.119.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-29", "journal": {"name": "ArXiv", "volume": "abs/2105.14241"}, "authors": [{"authorId": "15493820", "name": "Linzi Xing"}, {"authorId": "49617120", "name": "Wen Xiao"}, {"authorId": "1825424", "name": "G. Carenini"}]}, {"paperId": "7718a9b2b33bc6b9c1d4c4b9a3e1d473ffe5c330", "externalIds": {"DBLP": "conf/acl/TangL0H0020", "ACL": "2021.acl-short.120", "ArXiv": "2004.11142", "DOI": "10.18653/v1/2021.acl-short.120", "CorpusId": 236169717}, "corpusId": 236169717, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7718a9b2b33bc6b9c1d4c4b9a3e1d473ffe5c330", "title": "DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Applications", "abstract": "Machine reading comprehension (MRC) is a crucial task in natural language processing and has achieved remarkable advancements. However, most of the neural MRC models are still far from robust and fail to generalize well in real-world applications. In order to comprehensively verify the robustness and generalization of MRC models, we introduce a real-world Chinese dataset \u2013 DuReader_robust . It is designed to evaluate the MRC models from three aspects: over-sensitivity, over-stability and generalization. Comparing to previous work, the instances in DuReader_robust are natural texts, rather than the altered unnatural texts. It presents the challenges when applying MRC models to real-world applications. The experimental results show that MRC models do not perform well on the challenge test set. Moreover, we analyze the behavior of existing models on the challenge test set, which may provide suggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/DuReader.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 17, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.120.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-23", "journal": {"pages": "955-963"}, "authors": [{"authorId": "1576488202", "name": "Hongxuan Tang"}, {"authorId": "2115262377", "name": "Hongyu Li"}, {"authorId": "46700619", "name": "Jing Liu"}, {"authorId": "144873792", "name": "Yu Hong"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "144270731", "name": "Haifeng Wang"}]}, {"paperId": "dd75f713420cd8eb3e0b3ce870b2680cd93b39fd", "externalIds": {"ArXiv": "2106.00990", "DBLP": "conf/acl/TsaiLWS20", "ACL": "2021.acl-short.121", "DOI": "10.18653/v1/2021.acl-short.121", "CorpusId": 235293892}, "corpusId": 235293892, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dd75f713420cd8eb3e0b3ce870b2680cd93b39fd", "title": "Sequence to General Tree: Knowledge-Guided Geometry Word Problem Solving", "abstract": "With the recent advancements in deep learning, neural solvers have gained promising results in solving math word problems. However, these SOTA solvers only generate binary expression trees that contain basic arithmetic operators and do not explicitly use the math formulas. As a result, the expression trees they produce are lengthy and uninterpretable because they need to use multiple operators and constants to represent one single formula. In this paper, we propose sequence-to-general tree (S2G) that learns to generate interpretable and executable operation trees where the nodes can be formulas with an arbitrary number of arguments. With nodes now allowed to be formulas, S2G can learn to incorporate mathematical domain knowledge into problem-solving, making the results more interpretable. Experiments show that S2G can achieve a better performance against strong baselines on problems that require domain knowledge.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 37, "citationCount": 10, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.121.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"pages": "964-972"}, "authors": [{"authorId": "2072527027", "name": "S. Tsai"}, {"authorId": "3092327", "name": "Chao-Chun Liang"}, {"authorId": "46506453", "name": "Hsin-Min Wang"}, {"authorId": "145718668", "name": "Keh-Yih Su"}]}, {"paperId": "d27d44ed1caa7f973f9fedb11878e4b5ab452320", "externalIds": {"DBLP": "conf/acl/GuoZJ0L20", "ACL": "2021.acl-short.122", "DOI": "10.18653/v1/2021.acl-short.122", "CorpusId": 236460112}, "corpusId": 236460112, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d27d44ed1caa7f973f9fedb11878e4b5ab452320", "title": "Multi-Scale Progressive Attention Network for Video Question Answering", "abstract": "Understanding the multi-scale visual information in a video is essential for Video Question Answering (VideoQA). Therefore, we propose a novel Multi-Scale Progressive Attention Network (MSPAN) to achieve relational reasoning between cross-scale video information. We construct clips of different lengths to represent different scales of the video. Then, the clip-level features are aggregated into node features by using max-pool, and a graph is generated for each scale of clips. For cross-scale feature interaction, we design a message passing strategy between adjacent scale graphs, i.e., top-down scale interaction and bottom-up scale interaction. Under the question\u2019s guidance of progressive attention, we realize the fusion of all-scale video features. Experimental evaluations on three benchmarks: TGIF-QA, MSVD-QA and MSRVTT-QA show our method has achieved state-of-the-art performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 17, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.122.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "973-978"}, "authors": [{"authorId": "1557391036", "name": "Zhicheng Guo"}, {"authorId": "2115974348", "name": "Jiaxuan Zhao"}, {"authorId": "144125122", "name": "L. Jiao"}, {"authorId": "2110952179", "name": "Xu Liu"}, {"authorId": "47681424", "name": "Lingling Li"}]}, {"paperId": "15493bbb5387d60e7c77cee34528acd3acae7b65", "externalIds": {"DBLP": "conf/acl/YamadaAH20", "ArXiv": "2106.00882", "ACL": "2021.acl-short.123", "DOI": "10.18653/v1/2021.acl-short.123", "CorpusId": 235293983}, "corpusId": 235293983, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/15493bbb5387d60e7c77cee34528acd3acae7b65", "title": "Efficient Passage Retrieval with Hashing for Open-domain Question Answering", "abstract": "Most state-of-the-art open-domain question answering systems use a neural retrieval model to encode passages into continuous vectors and extract them from a knowledge source. However, such retrieval models often require large memory to run because of the massive size of their passage index. In this paper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural retrieval model that integrates a learning-to-hash technique into the state-of-the-art Dense Passage Retriever (DPR) to represent the passage index using compact binary codes rather than continuous vectors. BPR is trained with a multi-task objective over two tasks: efficient candidate generation based on binary codes and accurate reranking based on continuous vectors. Compared with DPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss of accuracy on two standard open-domain question answering benchmarks: Natural Questions and TriviaQA. Our code and trained models are available at https://github.com/studio-ousia/bpr.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 25, "citationCount": 58, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.123.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.00882"}, "authors": [{"authorId": "2303128", "name": "Ikuya Yamada"}, {"authorId": "35584853", "name": "Akari Asai"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}]}, {"paperId": "4e9e5d7430b2dbea903fc8d49ff34c0dc043472e", "externalIds": {"DBLP": "conf/acl/YangZNZP20", "ArXiv": "2106.02401", "ACL": "2021.acl-short.124", "DOI": "10.18653/v1/2021.acl-short.124", "CorpusId": 235352728}, "corpusId": 235352728, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4e9e5d7430b2dbea903fc8d49ff34c0dc043472e", "title": "Entity Concept-enhanced Few-shot Relation Extraction", "abstract": "Few-shot relation extraction (FSRE) is of great importance in long-tail distribution problem, especially in special domain with low-resource data. Most existing FSRE algorithms fail to accurately classify the relations merely based on the information of the sentences together with the recognized entity pairs, due to limited samples and lack of knowledge. To address this problem, in this paper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction scheme (ConceptFERE), which introduces the inherent concepts of entities to provide clues for relation prediction and boost the relations classification performance. Firstly, a concept-sentence attention module is developed to select the most appropriate concept from multiple concepts of each entity by calculating the semantic similarity between sentences and concepts. Secondly, a self-attention based fusion module is presented to bridge the gap of concept embedding and sentence embedding from different semantic spaces. Extensive experiments on the FSRE benchmark dataset FewRel have demonstrated the effectiveness and the superiority of the proposed ConceptFERE scheme as compared to the state-of-the-art baselines. Code is available at https://github.com/LittleGuoKe/ConceptFERE.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 17, "citationCount": 39, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.124.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-04", "journal": {"pages": "987-991"}, "authors": [{"authorId": "2154931396", "name": "Shan Yang"}, {"authorId": "2108026046", "name": "Yongfei Zhang"}, {"authorId": "2052804171", "name": "Guanglin Niu"}, {"authorId": "2118377199", "name": "Qinghua Zhao"}, {"authorId": "3290437", "name": "Shiliang Pu"}]}, {"paperId": "2f34693c39aaca2617db8965f82515003e359ebf", "externalIds": {"DBLP": "conf/acl/LiangL20", "ACL": "2021.acl-short.125", "DOI": "10.18653/v1/2021.acl-short.125", "CorpusId": 236460025}, "corpusId": 236460025, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2f34693c39aaca2617db8965f82515003e359ebf", "title": "Improving Model Generalization: A Chinese Named Entity Recognition Case Study", "abstract": "Generalization is an important ability that helps to ensure that a machine learning model can perform well on unseen data. In this paper, we study the effect of data bias on model generalization, using Chinese Named Entity Recognition (NER) as a case study. Specifically, we analyzed five benchmarking datasets for Chinese NER, and observed the following two types of data bias that can compromise model generalization ability. Firstly, the test sets of all the five datasets contain a significant proportion of entities that have been seen in the training sets. Such test data would therefore not be able to reflect the true generalization ability of a model. Secondly, all datasets are dominated by a few fat-head entities, i.e., entities appearing with particularly high frequency. As a result, a model might be able to produce high prediction accuracy simply by keyword memorization without leveraging context knowledge. To address these data biases, we first refine each test set by excluding seen entities from it, so as to better evaluate a model\u2019s generalization ability. Then, we propose a simple yet effective entity resampling method to make entities within the same category distributed equally, encouraging a model to leverage both name and context knowledge in the training process. Experimental results demonstrate that the proposed entity resampling method significantly improves a model\u2019s ability in detecting unseen entities, especially for company, organization and position categories.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 21, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "992-997"}, "authors": [{"authorId": "3190313", "name": "Guanqing Liang"}, {"authorId": "2142726", "name": "C. Leung"}]}, {"paperId": "026f955701579ba6ae0caf972c90729daf5a5e9e", "externalIds": {"ArXiv": "2106.01793", "ACL": "2021.acl-short.126", "DBLP": "conf/acl/HuangZF0L020", "DOI": "10.18653/v1/2021.acl-short.126", "CorpusId": 235313346}, "corpusId": 235313346, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/026f955701579ba6ae0caf972c90729daf5a5e9e", "title": "Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction", "abstract": "Document-level Relation Extraction (RE) is a more challenging task than sentence RE as it often requires reasoning over multiple sentences. Yet, human annotators usually use a small number of sentences to identify the relationship between a given entity pair. In this paper, we present an embarrassingly simple but effective method to heuristically select evidence sentences for document-level RE, which can be easily combined with BiLSTM to achieve good performance on benchmark datasets, even better than fancy graph neural network based methods. We have released our code at https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 19, "citationCount": 28, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.126.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"pages": "998-1004"}, "authors": [{"authorId": "2007771781", "name": "Quzhe Huang"}, {"authorId": "2110041566", "name": "Shengqi Zhu"}, {"authorId": "2115387922", "name": "Yansong Feng"}, {"authorId": "2106717300", "name": "Yuan Ye"}, {"authorId": "7827757", "name": "Yuxuan Lai"}, {"authorId": "9072379", "name": "Dongyan Zhao"}]}, {"paperId": "3316bc75e1d46e58008e89f109da1fb2f6b6efb4", "externalIds": {"ArXiv": "2105.03505", "DBLP": "conf/acl/LiYLQR20", "ACL": "2021.acl-short.127", "DOI": "10.18653/v1/2021.acl-short.127", "CorpusId": 234334083}, "corpusId": 234334083, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3316bc75e1d46e58008e89f109da1fb2f6b6efb4", "title": "Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders", "abstract": "Learning prerequisite chains is an important task for one to pick up knowledge efficiently in both known and unknown domains. For example, one may be an expert in the natural language processing (NLP) domain, but want to determine the best order in which to learn new concepts in an unfamiliar Computer Vision domain (CV). Both domains share some common concepts, such as machine learning basics and deep learning models. In this paper, we solve the task of unsupervised cross-domain concept prerequisite chain learning, using an optimized variational graph autoencoder. Our model learns to transfer concept prerequisite relations from an information-rich domain (source domain) to an information-poor domain (target domain), substantially surpassing other baseline models. In addition, we expand an existing dataset by introducing two new domains\u2014-CV and Bioinformatics (BIO). The annotated data and resources as well as the code will be made publicly available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 19, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.127.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-07", "journal": {"pages": "1005-1011"}, "authors": [{"authorId": "46331602", "name": "Irene Li"}, {"authorId": "2090570150", "name": "Vanessa Yan"}, {"authorId": "2118909153", "name": "Tianxiao Li"}, {"authorId": "32562916", "name": "Rihao Qu"}, {"authorId": "9215251", "name": "Dragomir R. Radev"}]}, {"paperId": "3843f69718a4052da1ec93e83b6a0f5508a69e2e", "externalIds": {"DBLP": "conf/acl/AmiriMK20", "MAG": "3176087585", "ACL": "2021.acl-short.128", "DOI": "10.18653/v1/2021.acl-short.128", "CorpusId": 236460160}, "corpusId": 236460160, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3843f69718a4052da1ec93e83b6a0f5508a69e2e", "title": "Attentive Multiview Text Representation for Differential Diagnosis", "abstract": "We present a text representation approach that can combine different views (representations) of the same input through effective data fusion and attention strategies for ranking purposes. We apply our model to the problem of differential diagnosis, which aims to find the most probable diseases that match with clinical descriptions of patients, using data from the Undiagnosed Diseases Network. Our model outperforms several ranking approaches (including a commercially-supported system) by effectively prioritizing and combining representations obtained from traditional and recent text representation techniques. We elaborate on several aspects of our model and shed light on its improved performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.128.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "1012-1019"}, "authors": [{"authorId": "1785703", "name": "H. Amiri"}, {"authorId": "1754057", "name": "Mitra Mohtarami"}, {"authorId": "1740538", "name": "I. Kohane"}]}, {"paperId": "b9c0d572402d38976d4bf99c78f4cc65e6d8f1e4", "externalIds": {"ACL": "2021.acl-short.129", "DBLP": "journals/corr/abs-2106-01491", "ArXiv": "2106.01491", "DOI": "10.18653/v1/2021.acl-short.129", "CorpusId": 235313843}, "corpusId": 235313843, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b9c0d572402d38976d4bf99c78f4cc65e6d8f1e4", "title": "MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain", "abstract": "Crowdworker-constructed natural language inference (NLI) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (CITATION). We investigate whether MedNLI, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (CITATION). We find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as modifiers related to responsiveness, duration, and probability. Neutral hypotheses feature conditions and behaviors that co-occur with, or cause, the condition(s) in the premise. Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health. Adversarial filtering demonstrates that performance degrades when evaluated on the difficult subset. We provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 18, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.129.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-02", "journal": {"pages": "1020-1027"}, "authors": [{"authorId": "98634534", "name": "Christine Herlihy"}, {"authorId": "2034613", "name": "Rachel Rudinger"}]}, {"paperId": "06f71821237da3d3b4289618846fe4fab71eca82", "externalIds": {"DBLP": "conf/acl/SibliniSK20", "ACL": "2021.acl-short.130", "DOI": "10.18653/v1/2021.acl-short.130", "CorpusId": 236460249}, "corpusId": 236460249, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/06f71821237da3d3b4289618846fe4fab71eca82", "title": "Towards a more Robust Evaluation for Conversational Question Answering", "abstract": "With the explosion of chatbot applications, Conversational Question Answering (CQA) has generated a lot of interest in recent years. Among proposals, reading comprehension models which take advantage of the conversation history (previous QA) seem to answer better than those which only consider the current question. Nevertheless, we note that the CQA evaluation protocol has a major limitation. In particular, models are allowed, at each turn of the conversation, to access the ground truth answers of the previous turns. Not only does this severely prevent their applications in fully autonomous chatbots, it also leads to unsuspected biases in their behavior. In this paper, we highlight this effect and propose new tools for evaluation and training in order to guard against the noted issues. The new results that we bring come to reinforce methods of the current state of the art.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.130.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "1028-1034"}, "authors": [{"authorId": "17268868", "name": "Wissam Siblini"}, {"authorId": "2121303070", "name": "Baris Sayil"}, {"authorId": "1970650", "name": "Y. Kessaci"}]}, {"paperId": "a14ef90b8fa01b3ddc231eb491c76a6f7458976e", "externalIds": {"ArXiv": "2105.03229", "DBLP": "journals/corr/abs-2105-03229", "ACL": "2021.acl-short.131", "DOI": "10.18653/v1/2021.acl-short.131", "CorpusId": 234095341}, "corpusId": 234095341, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a14ef90b8fa01b3ddc231eb491c76a6f7458976e", "title": "VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension", "abstract": "Existing models on Machine Reading Comprehension (MRC) require complex model architecture for effectively modeling long texts with paragraph representation and classification, thereby making inference computationally inefficient for production use. In this work, we propose VAULT: a light-weight and parallel-efficient paragraph representation for MRC based on contextualized representation from long document input, trained using a new Gaussian distribution-based objective that pays close attention to the partially correct instances that are close to the ground-truth. We validate our VAULT architecture showing experimental results on two benchmark MRC datasets that require long context modeling; one Wikipedia-based (Natural Questions (NQ)) and the other on TechNotes (TechQA). VAULT can achieve comparable performance on NQ with a state-of-the-art (SOTA) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain \u2013 TechQA \u2013 with large improvement over a model fine-tuned on a previously published large PLM.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.131.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-07", "journal": {"pages": "1035-1042"}, "authors": [{"authorId": "4428136", "name": "Haoyang Wen"}, {"authorId": "1388016369", "name": "Anthony Ferritto"}, {"authorId": "144016781", "name": "Heng Ji"}, {"authorId": "1707117", "name": "Radu Florian"}, {"authorId": "2707234", "name": "Avirup Sil"}]}, {"paperId": "2e386384fe19600ff130210708c996104bebab67", "externalIds": {"ACL": "2021.acl-short.132", "DBLP": "conf/acl/DuF20", "DOI": "10.18653/v1/2021.acl-short.132", "CorpusId": 236460048}, "corpusId": 236460048, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2e386384fe19600ff130210708c996104bebab67", "title": "Avoiding Overlap in Data Augmentation for AMR-to-Text Generation", "abstract": "Leveraging additional unlabeled data to boost model performance is common practice in machine learning and natural language processing. For generation tasks, if there is overlap between the additional data and the target text evaluation data, then training on the additional data is training on answers of the test set. This leads to overly-inflated scores with the additional data compared to real-world testing scenarios and problems when comparing models. We study the AMR dataset and Gigaword, which is popularly used for improving AMR-to-text generators, and find significant overlap between Gigaword and a subset of the AMR dataset. We propose methods for excluding parts of Gigaword to remove this overlap, and show that our approach leads to a more realistic evaluation of the task of AMR-to-text generation. Going forward, we give simple best-practice recommendations for leveraging additional data in AMR-to-text generation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "1043-1048"}, "authors": [{"authorId": "2072591366", "name": "Wenchao Du"}, {"authorId": "144683841", "name": "Jeffrey Flanigan"}]}, {"paperId": "1b460f0733a5569280b8e25a89820f32b2a0d687", "externalIds": {"DBLP": "conf/acl/YangZM20", "ArXiv": "2106.02792", "ACL": "2021.acl-short.133", "DOI": "10.18653/v1/2021.acl-short.133", "CorpusId": 235358754}, "corpusId": 235358754, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1b460f0733a5569280b8e25a89820f32b2a0d687", "title": "Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains", "abstract": "Social media has become a valuable resource for the study of suicidal ideation and the assessment of suicide risk. Among social media platforms, Reddit has emerged as the most promising one due to its anonymity and its focus on topic-based communities (subreddits) that can be indicative of someone\u2019s state of mind or interest regarding mental health disorders such as r/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on suicide risk assessment has been the small amount of labeled data. We propose an empirical investigation into several classes of weakly-supervised approaches, and show that using pseudo-labeling based on related issues around mental health (e.g., anxiety, depression) helps improve model performance for suicide risk assessment.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 19, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.133.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-05", "journal": {"name": "ArXiv", "volume": "abs/2106.02792"}, "authors": [{"authorId": "46962597", "name": "Chenghao Yang"}, {"authorId": "2115673319", "name": "Yudong Zhang"}, {"authorId": "2295928", "name": "S. Muresan"}]}, {"paperId": "f3bb8cef420df35456d3abc7194ddcc0dd5439fb", "externalIds": {"DBLP": "journals/corr/abs-2107-03448", "ArXiv": "2107.03448", "ACL": "2021.acl-short.134", "DOI": "10.18653/v1/2021.acl-short.134", "CorpusId": 235656966}, "corpusId": 235656966, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f3bb8cef420df35456d3abc7194ddcc0dd5439fb", "title": "Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test", "abstract": "The Shuffle Test is the most common task to evaluate whether NLP models can measure coherence in text. Most recent work uses direct supervision on the task; we show that by simply finetuning a RoBERTa model, we can achieve a near perfect accuracy of 97.8%, a state-of-the-art. We argue that this outstanding performance is unlikely to lead to a good model of text coherence, and suggest that the Shuffle Test should be approached in a Zero-Shot setting: models should be evaluated without being trained on the task itself. We evaluate common models in this setting, such as Generative and Bi-directional Transformers, and find that larger architectures achieve high-performance out-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of the original by increasing the size of blocks shuffled. Even though human reader performance remains high (around 95% accuracy), model performance drops from 94% to 78% as block size increases, creating a conceptually simple challenge to benchmark NLP models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.134.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-07", "journal": {"name": "ArXiv", "volume": "abs/2107.03448"}, "authors": [{"authorId": "46180754", "name": "Philippe Laban"}, {"authorId": "2115177932", "name": "Luke Dai"}, {"authorId": "2091417351", "name": "Lucas Bandarkar"}]}, {"paperId": "b31eb3428320342dfde042693ff2ca106dabed0d", "externalIds": {"ArXiv": "2106.01890", "DBLP": "journals/corr/abs-2106-01890", "ACL": "2021.acl-short.135", "DOI": "10.18653/v1/2021.acl-short.135", "CorpusId": 235313879}, "corpusId": 235313879, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b31eb3428320342dfde042693ff2ca106dabed0d", "title": "SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization", "abstract": "In this paper, we present a conceptually simple while empirically powerful framework for abstractive summarization, SimCLS, which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem (i.e., quality estimation) assisted by contrastive learning. Experimental results show that, with minor modification over existing top-scoring systems, SimCLS can improve the performance of existing top-performing models by a large margin. Particularly, 2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on the CNN/DailyMail dataset, driving the state-of-the-art performance to a new level. We have open-sourced our codes and results: https://github.com/yixinL7/SimCLS. Results of our proposed models have been deployed into ExplainaBoard platform, which allows researchers to understand our systems in a more fine-grained way.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 39, "citationCount": 151, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.135.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01890"}, "authors": [{"authorId": "2108176413", "name": "Yixin Liu"}, {"authorId": "145779142", "name": "Peng Liu"}]}, {"paperId": "c0498f3bbba3bca5f23934b61704b028e7dc1705", "externalIds": {"ArXiv": "2105.06456", "ACL": "2021.acl-short.136", "DBLP": "conf/acl/RogozGI20", "DOI": "10.18653/v1/2021.acl-short.136", "CorpusId": 234482874}, "corpusId": 234482874, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c0498f3bbba3bca5f23934b61704b028e7dc1705", "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles", "abstract": "In this work, we introduce a corpus for satire detection in Romanian news. We gathered 55,608 public news articles from multiple real and satirical news sources, composing one of the largest corpora for satire detection regardless of language and the only one for the Romanian language. We provide an official split of the text samples, such that training news articles belong to different sources than test news articles, thus ensuring that models do not achieve high performance simply due to overfitting. We conduct experiments with two state-of-the-art deep neural models, resulting in a set of strong baselines for our novel corpus. Our results show that the machine-level accuracy for satire detection in Romanian is quite low (under 73% on the test set) compared to the human-level accuracy (87%), leaving enough room for improvement in future research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.136.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-13", "journal": {"name": "ArXiv", "volume": "abs/2105.06456"}, "authors": [{"authorId": "2091914188", "name": "Ana-Cristina Rogoz"}, {"authorId": "15344643", "name": "Mihaela G\u0103man"}, {"authorId": "1817759", "name": "Radu Tudor Ionescu"}]}, {"paperId": "77a25c1577610c0edb2024c902fb2c6610228d9a", "externalIds": {"DBLP": "conf/acl/MengTZDYWH20", "ACL": "2021.acl-short.137", "ArXiv": "2106.00130", "DOI": "10.18653/v1/2021.acl-short.137", "CorpusId": 235265871}, "corpusId": 235265871, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/77a25c1577610c0edb2024c902fb2c6610228d9a", "title": "Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents", "abstract": "Faceted summarization provides briefings of a document from different perspectives. Readers can quickly comprehend the main points of a long document with the help of a structured outline. However, little research has been conducted on this subject, partially due to the lack of large-scale faceted summarization datasets. In this study, we present FacetSum, a faceted summarization benchmark built on Emerald journal articles, covering a diverse range of domains. Different from traditional document-summary pairs, FacetSum provides multiple summaries, each targeted at specific sections of a long document, including the purpose, method, findings, and value. Analyses and empirical results on our dataset reveal the importance of bringing structure into summaries. We believe FacetSum will spur further advances in summarization research and foster the development of NLP systems that can leverage the structured information in both long texts and summaries.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 40, "citationCount": 25, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.137.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-31", "journal": {"name": "ArXiv", "volume": "abs/2106.00130"}, "authors": [{"authorId": "2949282", "name": "Rui Meng"}, {"authorId": "51488415", "name": "Khushboo Thaker"}, {"authorId": "2152838965", "name": "Lei Zhang"}, {"authorId": "2115458445", "name": "Yue Dong"}, {"authorId": "2854297", "name": "Xingdi Yuan"}, {"authorId": "1664686681", "name": "Tong Wang"}, {"authorId": "1846800386", "name": "Daqing He"}]}, {"paperId": "10102684ef60e2fa06ea0cfa3d034303ad7dfacc", "externalIds": {"ACL": "2021.acl-short.138", "DBLP": "journals/corr/abs-2106-00352", "ArXiv": "2106.00352", "DOI": "10.18653/v1/2021.acl-short.138", "CorpusId": 235266064}, "corpusId": 235266064, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/10102684ef60e2fa06ea0cfa3d034303ad7dfacc", "title": "Replicating and Extending \u201cBecause Their Treebanks Leak\u201d: Graph Isomorphism, Covariants, and Parser Performance", "abstract": "S\u00f8gaard (2020) obtained results suggesting the fraction of trees occurring in the test data isomorphic to trees in the training set accounts for a non-trivial variation in parser performance. Similar to other statistical analyses in NLP, the results were based on evaluating linear regressions. However, the study had methodological issues and was undertaken using a small sample size leading to unreliable results. We present a replication study in which we also bin sentences by length and find that only a small subset of sentences vary in performance with respect to graph isomorphism. Further, the correlation observed between parser performance and graph isomorphism in the wild disappears when controlling for covariants. However, in a controlled experiment, where covariants are kept fixed, we do observe a correlation. We suggest that conclusions drawn from statistical analyses like this need to be tempered and that controlled experiments can complement them by more readily teasing factors apart.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.138.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "journal": {"name": "ArXiv", "volume": "abs/2106.00352"}, "authors": [{"authorId": "23174933", "name": "Mark Anderson"}, {"authorId": "1700187", "name": "Anders S\u00f8gaard"}, {"authorId": "2106599468", "name": "Carlos G'omez Rodr'iguez"}]}, {"paperId": "0a54d2fe19878a3381767be62382a9799717934d", "externalIds": {"DBLP": "conf/acl/BhatnagarGK20", "ACL": "2021.acl-short.139", "ArXiv": "2106.06875", "DOI": "10.18653/v1/2021.acl-short.139", "CorpusId": 235422668}, "corpusId": 235422668, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0a54d2fe19878a3381767be62382a9799717934d", "title": "Don\u2019t Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data", "abstract": "High-performing machine translation (MT) systems can help overcome language barriers while making it possible for everyone to communicate and use language technologies in the language of their choice. However, such systems require large amounts of parallel sentences for training, and translators can be difficult to find and expensive. Here, we present a data collection strategy for MT which, in contrast, is cheap and simple, as it does not require bilingual speakers. Based on the insight that humans pay specific attention to movements, we use graphics interchange formats (GIFs) as a pivot to collect parallel sentences from monolingual annotators. We use our strategy to collect data in Hindi, Tamil and English. As a baseline, we also collect data using images as a pivot. We perform an intrinsic evaluation by manually evaluating a subset of the sentence pairs and an extrinsic evaluation by finetuning mBART (Liu et al., 2020) on the collected data. We find that sentences collected via GIFs are indeed of higher quality.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 22, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-short.139.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-12", "journal": {"pages": "1099-1106"}, "authors": [{"authorId": "34023411", "name": "Rajat Bhatnagar"}, {"authorId": "47079359", "name": "Ananya Ganesh"}, {"authorId": "3422953", "name": "Katharina Kann"}]}, {"paperId": "1e446c33455568e4d4660d360f10761351a6df6b", "externalIds": {"DBLP": "journals/corr/abs-2110-05892", "ACL": "2021.acl-srw.1", "ArXiv": "2110.05892", "DOI": "10.18653/v1/2021.acl-srw.1", "CorpusId": 236317335}, "corpusId": 236317335, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1e446c33455568e4d4660d360f10761351a6df6b", "title": "Investigation on Data Adaptation Techniques for Neural Named Entity Recognition", "abstract": "Data processing is an important step in various natural language processing tasks. As the commonly used datasets in named entity recognition contain only a limited number of samples, it is important to obtain additional labeled data in an efficient and reliable manner. A common practice is to utilize large monolingual unlabeled corpora. Another popular technique is to create synthetic data from the original labeled data (data augmentation). In this work, we investigate the impact of these two methods on the performance of three different named entity recognition tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.1.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-10-12", "journal": {"name": "ArXiv", "volume": "abs/2110.05892"}, "authors": [{"authorId": "2035572365", "name": "E. Tokarchuk"}, {"authorId": "17687726", "name": "David Thulke"}, {"authorId": "2108446987", "name": "Weiyue Wang"}, {"authorId": "32703822", "name": "Christian Dugast"}, {"authorId": "145322333", "name": "H. Ney"}]}, {"paperId": "97107a9b2d60a52ccfc53b6c2ae2f786927dcc7c", "externalIds": {"DBLP": "journals/corr/abs-2105-08021", "ArXiv": "2105.08021", "ACL": "2021.acl-srw.2", "DOI": "10.18653/v1/2021.acl-srw.2", "CorpusId": 234742446}, "corpusId": 234742446, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/97107a9b2d60a52ccfc53b6c2ae2f786927dcc7c", "title": "Stage-wise Fine-tuning for Graph-to-Text Generation", "abstract": "Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes model on Wikipedia before adapting to the graph-to-text generation. In addition to using the traditional token and position embeddings to encode the knowledge graph (KG), we propose a novel tree-level embedding method to capture the inter-dependency structures of the input graph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 42, "citationCount": 13, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.2.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-17", "journal": {"name": "ArXiv", "volume": "abs/2105.08021"}, "authors": [{"authorId": "1786863", "name": "Qingyun Wang"}, {"authorId": "3014143", "name": "Semih Yavuz"}, {"authorId": "2060138164", "name": "Victoria Lin"}, {"authorId": "2113323573", "name": "Heng Ji"}, {"authorId": "8937909", "name": "Nazneen Rajani"}]}, {"paperId": "20a3d791febed1b727e11dce72b017960148f05d", "externalIds": {"DBLP": "conf/acl/WangYGN21", "ACL": "2021.acl-srw.3", "DOI": "10.18653/v1/2021.acl-srw.3", "CorpusId": 236457320}, "corpusId": 236457320, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/20a3d791febed1b727e11dce72b017960148f05d", "title": "Transformer-Based Direct Hidden Markov Model for Machine Translation", "abstract": "The neural hidden Markov model has been proposed as an alternative to attention mechanism in machine translation with recurrent neural networks. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the hidden Markov model to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge compared to a model with first-order dependency, which performs similarly but is significantly slower in training and decoding.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "23-32"}, "authors": [{"authorId": "2108446987", "name": "Weiyue Wang"}, {"authorId": null, "name": "Zijian Yang"}, {"authorId": "2118545885", "name": "Yingbo Gao"}, {"authorId": "145322333", "name": "H. Ney"}]}, {"paperId": "44928ee327335883eacce30d3baeb7dfdaa1fed2", "externalIds": {"MAG": "3089119299", "DBLP": "journals/corr/abs-2009-10680", "ACL": "2021.acl-srw.4", "ArXiv": "2009.10680", "DOI": "10.18653/v1/2021.acl-srw.4", "CorpusId": 221836671}, "corpusId": 221836671, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/44928ee327335883eacce30d3baeb7dfdaa1fed2", "title": "AutoRC: Improving BERT Based Relation Classification Models via Architecture Search", "abstract": "Although BERT based relation classification (RC) models have achieved significant improvements over the traditional deep learning models, it seems that no consensus can be reached on what is the optimal architecture, since there are many design choices available. In this work, we design a comprehensive search space for BERT based RC models and employ a modified version of efficient neural architecture search (ENAS) method to automatically discover the design choices mentioned above. Experiments on eight benchmark RC tasks show that our method is efficient and effective in finding better architectures than the baseline BERT based RC models. Ablation study demonstrates the necessity of our search space design and the effectiveness of our search method. We also show that our framework can also apply to other entity related tasks like coreference resolution and span based named entity recognition (NER).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.4.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-22", "journal": {"pages": "33-43"}, "authors": [{"authorId": "2152348673", "name": "Wei Zhu"}, {"authorId": "2145748428", "name": "Xiaoling Wang"}, {"authorId": "1767521", "name": "Xipeng Qiu"}, {"authorId": "2072724069", "name": "Yuan Ni"}, {"authorId": "2052157466", "name": "G. Xie"}]}, {"paperId": "fe53ea09903af7e0a32a29c2c9578ad3350749d7", "externalIds": {"DBLP": "journals/corr/abs-2105-14515", "ArXiv": "2105.14515", "ACL": "2021.acl-srw.5", "DOI": "10.18653/v1/2021.acl-srw.5", "CorpusId": 235253839}, "corpusId": 235253839, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fe53ea09903af7e0a32a29c2c9578ad3350749d7", "title": "How Low is Too Low? A Computational Perspective on Extremely Low-Resource Languages", "abstract": "Despite the recent advancements of attention-based deep learning architectures across a majority of Natural Language Processing tasks, their application remains limited in a low-resource setting because of a lack of pre-trained models for such languages. In this study, we make the first attempt to investigate the challenges of adapting these techniques to an extremely low-resource language \u2013 Sumerian cuneiform \u2013 one of the world\u2019s oldest written languages attested from at least the beginning of the 3rd millennium BC. Specifically, we introduce the first cross-lingual information extraction pipeline for Sumerian, which includes part-of-speech tagging, named entity recognition, and machine translation. We introduce InterpretLR, an interpretability toolkit for low-resource NLP and use it alongside human evaluations to gauge the trained models. Notably, all our techniques and most components of our pipeline can be generalised to any low-resource language. We publicly release all our implementations including a novel data set with domain-specific pre-processing to promote further research in this domain.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 4, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.5.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-30", "journal": {"name": "ArXiv", "volume": "abs/2105.14515"}, "authors": [{"authorId": "80494139", "name": "Rachit Bansal"}, {"authorId": "1753690778", "name": "Himanshu Choudhary"}, {"authorId": "2003196161", "name": "Ravneet Punia"}, {"authorId": "40173007", "name": "Niko Schenk"}, {"authorId": "2106412345", "name": "Jacob L Dahl"}, {"authorId": "2106411920", "name": "'Emilie Pag'e-Perron"}]}, {"paperId": "d7c2369e65865eedfa612283a26924d90b440fc1", "externalIds": {"DBLP": "conf/acl/UedaW21", "ACL": "2021.acl-srw.6", "DOI": "10.18653/v1/2021.acl-srw.6", "CorpusId": 237012341}, "corpusId": 237012341, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d7c2369e65865eedfa612283a26924d90b440fc1", "title": "On the Relationship between Zipf\u2019s Law of Abbreviation and Interfering Noise in Emergent Languages", "abstract": "This paper studies whether emergent languages in a signaling game follow Zipf\u2019s law of abbreviation (ZLA), especially when the communication ability of agents is limited because of interfering noises. ZLA is a well-known tendency in human languages where the more frequently a word is used, the shorter it will be. Surprisingly, previous work demonstrated that emergent languages do not obey ZLA at all when neural agents play a signaling game. It also reported that a ZLA-like tendency appeared by adding an explicit penalty on word lengths, which can be considered some external factors in reality such as articulatory effort. We hypothesize, on the other hand, that there might be not only such external factors but also some internal factors related to cognitive abilities. We assume that it could be simulated by modeling the effect of noises on the agents\u2019 environment. In our experimental setup, the hidden states of the LSTM-based speaker and listener were added with Gaussian noise, while the channel was subject to discrete random replacement. Our results suggest that noise on a speaker is one of the factors for ZLA or at least causes emergent languages to approach ZLA, while noise on a listener and a channel is not.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "60-70"}, "authors": [{"authorId": "153704369", "name": "Ryo Ueda"}, {"authorId": "41130997", "name": "Koki Washio"}]}, {"paperId": "8ff620f704a4151fd7abba1db792463fbd32bfe5", "externalIds": {"DBLP": "journals/corr/abs-2103-00751", "ArXiv": "2103.00751", "ACL": "2021.acl-srw.7", "DOI": "10.18653/v1/2021.acl-srw.7", "CorpusId": 232075836}, "corpusId": 232075836, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8ff620f704a4151fd7abba1db792463fbd32bfe5", "title": "Long Document Summarization in a Low Resource Setting using Pretrained Language Models", "abstract": "Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pre-trained abstractive summarizer BART, which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with independent human labeling by domain experts.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 25, "citationCount": 27, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.7.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-03-01", "journal": {"name": "ArXiv", "volume": "abs/2103.00751"}, "authors": [{"authorId": "21204259", "name": "Ahsaas Bajaj"}, {"authorId": "2051712441", "name": "Pavitra Dangati"}, {"authorId": "26161085", "name": "Kalpesh Krishna"}, {"authorId": "2212670354", "name": "Pradhiksha Ashok Kumar"}, {"authorId": "114693523", "name": "Rheeya Uppaal"}, {"authorId": "97619082", "name": "Bradford T. Windsor"}, {"authorId": "31558543", "name": "Eliot Brenner"}, {"authorId": "3066005", "name": "Dominic Dotterrer"}, {"authorId": "143863023", "name": "R. Das"}, {"authorId": "143753639", "name": "A. McCallum"}]}, {"paperId": "aab3c69ce6dab0b912e4a05e65c8ef0316c25ce0", "externalIds": {"DBLP": "conf/acl/SamaranGOCN21", "ACL": "2021.acl-srw.8", "DOI": "10.18653/v1/2021.acl-srw.8", "CorpusId": 237329472}, "corpusId": 237329472, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/aab3c69ce6dab0b912e4a05e65c8ef0316c25ce0", "title": "Attending Self-Attention: A Case Study of Visually Grounded Supervision in Vision-and-Language Transformers", "abstract": "The impressive performances of pre-trained visually grounded language models have motivated a growing body of research investigating what has been learned during the pre-training. As a lot of these models are based on Transformers, several studies on the attention mechanisms used by the models to learn to associate phrases with their visual grounding in the image have been conducted. In this work, we investigate how supervising attention directly to learn visual grounding can affect the behavior of such models. We compare three different methods on attention supervision and their impact on the performances of a state-of-the-art visually grounded language model on two popular vision-and-language tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "81-86"}, "authors": [{"authorId": "118440364", "name": "Jules Samaran"}, {"authorId": "26385137", "name": "Noa Garc\u00eda"}, {"authorId": "3186326", "name": "Mayu Otani"}, {"authorId": "2427516", "name": "Chenhui Chu"}, {"authorId": "2210102679", "name": "Yuta Nakashima"}]}, {"paperId": "8a3cb6d0a5be98c01bba7830cd9042a664bb7a5d", "externalIds": {"DBLP": "conf/acl/GuSCK21", "ACL": "2021.acl-srw.9", "DOI": "10.18653/v1/2021.acl-srw.9", "CorpusId": 237330071}, "corpusId": 237330071, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8a3cb6d0a5be98c01bba7830cd9042a664bb7a5d", "title": "Video-guided Machine Translation with Spatial Hierarchical Attention Network", "abstract": "Video-guided machine translation, as one type of multimodal machine translations, aims to engage video contents as auxiliary information to address the word sense ambiguity problem in machine translation. Previous studies only use features from pretrained action detection models as motion representations of the video to solve the verb sense ambiguity, leaving the noun sense ambiguity a problem. To address this problem, we propose a video-guided machine translation system by using both spatial and motion representations in videos. For spatial features, we propose a hierarchical attention network to model the spatial information from object-level to video-level. Experiments on the VATEX dataset show that our system achieves 35.86 BLEU-4 score, which is 0.51 score higher than the single model of the SOTA method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 17, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "87-92"}, "authors": [{"authorId": "2152036670", "name": "Weiqi Gu"}, {"authorId": "2980506", "name": "Haiyue Song"}, {"authorId": "2427516", "name": "Chenhui Chu"}, {"authorId": "1795664", "name": "S. Kurohashi"}]}, {"paperId": "db22b7d1cc4e38e0413a4aabdfe5755facda65e9", "externalIds": {"DBLP": "conf/acl/Chua21", "ACL": "2021.acl-srw.10", "DOI": "10.18653/v1/2021.acl-srw.10", "CorpusId": 237332190}, "corpusId": 237332190, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/db22b7d1cc4e38e0413a4aabdfe5755facda65e9", "title": "Stylistic approaches to predicting Reddit popularity in diglossia", "abstract": "Past work investigating what makes a Reddit post popular has indicated that style is a far better predictor than content, where posts conforming to a subreddit\u2019s community style are better received. However, what about a diglossia, when there are two community styles? In Singapore, the basilect (\u2018Singlish\u2019) co-exists with an acrolect (standard English), each with contrasting advantages of community identity and prestige respectively. In this paper, I apply stylistic approaches to predicting Reddit post scores in a diglossia. Using data from the Singaporean and British subreddits, I show that while the acrolect\u2019s prestige attracts more upvotes, the most popular posts also draw on Singlish vocabulary to appeal to the community identity.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 14, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.10.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "93-100"}, "authors": [{"authorId": "119585914", "name": "H. Chua"}]}, {"paperId": "5cac47b57b9ba4d92e5579b3f53f6eda88678b86", "externalIds": {"DBLP": "conf/acl/TestoniB21", "ACL": "2021.acl-srw.11", "DOI": "10.18653/v1/2021.acl-srw.11", "CorpusId": 237329050}, "corpusId": 237329050, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5cac47b57b9ba4d92e5579b3f53f6eda88678b86", "title": "\u201cI\u2019ve Seen Things You People Wouldn\u2019t Believe\u201d: Hallucinating Entities in GuessWhat?!", "abstract": "Natural language generation systems have witnessed important progress in the last years, but they are shown to generate tokens that are unrelated to the source input. This problem affects computational models in many NLP tasks, and it is particularly unpleasant in multimodal systems. In this work, we assess the rate of object hallucination in multimodal conversational agents playing the GuessWhat?! referential game. Better visual processing has been shown to mitigate this issue in image captioning; hence, we adapt to the GuessWhat?! task the best visual processing models at disposal, and propose two new models to play the Questioner agent. We show that the new models generate few hallucinations compared to other renowned models available in the literature. Moreover, their hallucinations are less severe (affect task-accuracy less) and are more human-like. We also analyse where hallucinations tend to occur more often through the dialogue: hallucinations are less frequent in earlier turns, cause a cascade hallucination effect, and are often preceded by negative answers, which have been shown to be harder to ground.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.11.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "101-111"}, "authors": [{"authorId": "50829868", "name": "Alberto Testoni"}, {"authorId": "145040726", "name": "R. Bernardi"}]}, {"paperId": "a829889d277bd42d990e079433cec64bc8d3ff1c", "externalIds": {"DBLP": "conf/acl/KumarASM21", "ACL": "2021.acl-srw.12", "DOI": "10.18653/v1/2021.acl-srw.12", "CorpusId": 237330072}, "corpusId": 237330072, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a829889d277bd42d990e079433cec64bc8d3ff1c", "title": "How do different factors Impact the Inter-language Similarity? A Case Study on Indian languages", "abstract": "India is one of the most linguistically diverse nations of the world and is culturally very rich. Most of these languages are somewhat similar to each other on account of sharing a common ancestry or being in contact for a long period of time. Nowadays, researchers are constantly putting efforts in utilizing the language relatedness to improve the performance of various NLP systems such as cross lingual semantic search, machine translation, sentiment analysis systems, etc. So in this paper, we performed an extensive case study on similarity involving languages of the Indian subcontinent. Language similarity prediction is defined as the task of measuring how similar the two languages are on the basis of their lexical, morphological and syntactic features. In this study, we concentrate only on the approach to calculate lexical similarity between Indian languages by looking at various factors such as size and type of corpus, similarity algorithms, subword segmentation, etc. The main takeaways from our work are: (i) Relative order of the language similarities largely remain the same, regardless of the factors mentioned above, (ii) Similarity within the same language family is higher, (iii) Languages share more lexical features at the subword level.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 19, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "112-118"}, "authors": [{"authorId": "2109550902", "name": "Sourav Kumar"}, {"authorId": "2121331370", "name": "Salil Aggarwal"}, {"authorId": "35246152", "name": "D. Sharma"}, {"authorId": "1829635", "name": "R. Mamidi"}]}, {"paperId": "0293686b2df1fde97e102b5a48c51e9286bd5504", "externalIds": {"DBLP": "conf/acl/AntypasCPR21", "ACL": "2021.acl-srw.13", "DOI": "10.18653/v1/2021.acl-srw.13", "CorpusId": 237332198}, "corpusId": 237332198, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0293686b2df1fde97e102b5a48c51e9286bd5504", "title": "COVID-19 and Misinformation: A Large-Scale Lexical Analysis on Twitter", "abstract": "Social media is often used by individuals and organisations as a platform to spread misinformation. With the recent coronavirus pandemic we have seen a surge of misinformation on Twitter, posing a danger to public health. In this paper, we compile a large COVID-19 Twitter misinformation corpus and perform an analysis to discover patterns with respect to vocabulary usage. Among others, our analysis reveals that the variety of topics and vocabulary usage are considerably more limited and negative in tweets related to misinformation than in randomly extracted tweets. In addition to our qualitative analysis, our experimental results show that a simple linear model based only on lexical features is effective in identifying misinformation-related tweets (with accuracy over 80%), providing evidence to the fact that the vocabulary used in misinformation largely differs from generic tweets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 26, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.13.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "119-126"}, "authors": [{"authorId": "2122908904", "name": "Dimosthenis Antypas"}, {"authorId": "1387447871", "name": "Jos\u00e9 Camacho-Collados"}, {"authorId": "1762890", "name": "A. Preece"}, {"authorId": "2150992", "name": "David Rogers"}]}, {"paperId": "f151e4c63ead5c9cea344d753cec36ed804241b2", "externalIds": {"ACL": "2021.acl-srw.14", "DBLP": "conf/acl/SmirnovaSC21", "DOI": "10.18653/v1/2021.acl-srw.14", "CorpusId": 237328496}, "corpusId": 237328496, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f151e4c63ead5c9cea344d753cec36ed804241b2", "title": "Situation-Based Multiparticipant Chat Summarization: a Concept, an Exploration-Annotation Tool and an Example Collection", "abstract": "Currently, text chatting is one of the primary means of communication. However, modern text chat still in general does not offer any navigation or even full-featured search, although the high volumes of messages demand it. In order to mitigate these inconveniences, we formulate the problem of situation-based summarization and propose a special data annotation tool intended for developing training and gold-standard data. A situation is a subset of messages revolving around a single event in both temporal and contextual senses: e.g, a group of friends arranging a meeting in chat, agreeing on date, time, and place. Situations can be extracted via information retrieval, natural language processing, and machine learning techniques. Since the task is novel, neither training nor gold-standard datasets for it have been created yet. In this paper, we present the formulation of the situation-based summarization problem. Next, we describe Chat Corpora Annotator (CCA): the first annotation system designed specifically for exploring and annotating chat log data. We also introduce a custom query language for semi-automatic situation extraction. Finally, we present the first gold-standard dataset for situation-based summarization. The software source code and the dataset are publicly available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "127-137"}, "authors": [{"authorId": "2064355663", "name": "A. Smirnova"}, {"authorId": "116108499", "name": "E. Slobodkin"}, {"authorId": "2532412", "name": "G. Chernishev"}]}, {"paperId": "022be579841554c84d09a09a9af67a1314ad8a8f", "externalIds": {"ACL": "2021.acl-srw.15", "DBLP": "conf/acl/InoueAKA21", "DOI": "10.18653/v1/2021.acl-srw.15", "CorpusId": 237332940}, "corpusId": 237332940, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/022be579841554c84d09a09a9af67a1314ad8a8f", "title": "Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings", "abstract": "In this study, we propose a model that extends the continuous space topic model (CSTM), which flexibly controls word probability in a document, using pre-trained word embeddings. To develop the proposed model, we pre-train word embeddings, which capture the semantics of words and plug them into the CSTM. Intrinsic experimental results show that the proposed model exhibits a superior performance over the CSTM in terms of perplexity and convergence speed. Furthermore, extrinsic experimental results show that the proposed model is useful for a document classification task when compared with the baseline model. We qualitatively show that the latent coordinates obtained by training the proposed model are better than those of the baseline model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 21, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "138-147"}, "authors": [{"authorId": "2069396323", "name": "Seiichi Inoue"}, {"authorId": "1580485699", "name": "Taichi Aida"}, {"authorId": "2936411", "name": "Mamoru Komachi"}, {"authorId": "1937455", "name": "Manabu Asai"}]}, {"paperId": "188c4b9a13bf98e0c8818d43baf75b1930342912", "externalIds": {"ArXiv": "2004.02251", "DBLP": "journals/corr/abs-2004-02251", "ACL": "2021.acl-srw.16", "MAG": "3015163529", "DOI": "10.18653/v1/2021.acl-srw.16", "CorpusId": 235368160}, "corpusId": 235368160, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/188c4b9a13bf98e0c8818d43baf75b1930342912", "title": "Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2", "abstract": "The semantics of a text is manifested not only by what is read but also by what is not read. In this article, we will study how those implicit \u201cnot read\u201d information such as end-of-paragraph () and end-of-sequence () affect the quality of text generation. Specifically, we find that the pre-trained language model GPT2 can generate better continuations by learning to generate the in the fine-tuning stage. Experimental results on English story generation show that can lead to higher BLEU scores and lower perplexity. We also conduct experiments on a self-collected Chinese essay dataset with Chinese-GPT2, a character level LM without and during pre-training. Experimental results show that the Chinese GPT2 can generate better essay endings with .", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 18, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.16.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-05", "journal": {"name": "ArXiv", "volume": "abs/2004.02251"}, "authors": [{"authorId": "37374479", "name": "He Bai"}, {"authorId": "2055357849", "name": "Peng Shi"}, {"authorId": "145580839", "name": "Jimmy J. Lin"}, {"authorId": "40379164", "name": "Luchen Tan"}, {"authorId": "50033756", "name": "Kun Xiong"}, {"authorId": "2153578299", "name": "Wen Gao"}, {"authorId": "2146651412", "name": "Jie Liu"}, {"authorId": "2150652992", "name": "Ming Li"}]}, {"paperId": "0f296ee7d1e0a786f1224aeea09daedabcbc69d5", "externalIds": {"DBLP": "conf/acl/NishikawaRT21", "ACL": "2021.acl-srw.17", "ArXiv": "2006.00262", "DOI": "10.18653/v1/2021.acl-srw.17", "CorpusId": 235303672}, "corpusId": 235303672, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0f296ee7d1e0a786f1224aeea09daedabcbc69d5", "title": "Data Augmentation with Unsupervised Machine Translation Improves the Structural Similarity of Cross-lingual Word Embeddings", "abstract": "Unsupervised cross-lingual word embedding(CLWE) methods learn a linear transformation matrix that maps two monolingual embedding spaces that are separately trained with monolingual corpora. This method relies on the assumption that the two embedding spaces are structurally similar, which does not necessarily hold true in general. In this paper, we argue that using a pseudo-parallel corpus generated by an unsupervised machine translation model facilitates the structural similarity of the two embedding spaces and improves the quality of CLWEs in the unsupervised mapping method. We show that our approach outperforms other alternative approaches given the same amount of data, and, through detailed analysis, we show that data augmentation with the pseudo data from unsupervised machine translation is especially effective for mapping-based CLWEs because (1) the pseudo data makes the source and target corpora (partially) parallel; (2) the pseudo data contains information on the original language that helps to learn similar embedding spaces between the source and target languages.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.17.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-30", "journal": {"pages": "163-173"}, "authors": [{"authorId": "1734796921", "name": "Sosuke Nishikawa"}, {"authorId": "1466451143", "name": "Ryokan Ri"}, {"authorId": "143946906", "name": "Yoshimasa Tsuruoka"}]}, {"paperId": "91e3921aea5563f036c3fd0b5470bd3a3656c756", "externalIds": {"ACL": "2021.acl-srw.18", "DBLP": "conf/acl/KrimanJ21", "DOI": "10.18653/v1/2021.acl-srw.18", "CorpusId": 237329696}, "corpusId": 237329696, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/91e3921aea5563f036c3fd0b5470bd3a3656c756", "title": "Joint Detection and Coreference Resolution of Entities and Events with Document-level Context Aggregation", "abstract": "Constructing knowledge graphs from unstructured text is an important task that is relevant to many domains. Most previous work focuses on extracting information from sentences or paragraphs, due to the difficulty of analyzing longer contexts. In this paper we propose a new jointly trained model that can be used for various information extraction tasks at the document level. The tasks performed by this system are entity and event identification, typing, and coreference resolution. In order to improve entity and event typing, we utilize context-aware representations aggregated from the detected mentions of the corresponding entities and events across the entire document. By extending our system to document-level, we can improve our results by incorporating cross-sentence dependencies and additional contextual information that might not be available at the sentence level, which allows for more globally optimized predictions. We evaluate our system on documents from the ACE05-E+ dataset and find significant improvement over the sentence-level SOTA on entity and event trigger identification and classification.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.18.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "174-179"}, "authors": [{"authorId": "1380253623", "name": "Samuel Kriman"}, {"authorId": "144016781", "name": "Heng Ji"}]}, {"paperId": "503e089a4413900a7d930a9b8d43b29796dedb1e", "externalIds": {"DBLP": "conf/acl/SinghACW21", "ACL": "2021.acl-srw.19", "DOI": "10.18653/v1/2021.acl-srw.19", "CorpusId": 237319690}, "corpusId": 237319690, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/503e089a4413900a7d930a9b8d43b29796dedb1e", "title": "\u201cHold on honey, men at work\u201d: A semi-supervised approach to detecting sexism in sitcoms", "abstract": "Television shows play an important role inpropagating societal norms. Owing to the popularity of the situational comedy (sitcom) genre, it contributes significantly to the over-all development of society. In an effort to analyze the content of television shows belong-ing to this genre, we present a dataset of dialogue turns from popular sitcoms annotated for the presence of sexist remarks. We train a text classification model to detect sexism using domain adaptive learning. We apply the model to our dataset to analyze the evolution of sexist content over the years. We propose a domain-specific semi-supervised architecture for the aforementioned detection of sexism.Through extensive experiments, we show that our model often yields better classification performance over generic deep learn-ing based sentence classification that does not employ domain-specific training. We find that while sexism decreases over time on average,the proportion of sexist dialogue for the most sexist sitcom actually increases. A quantitative analysis along with a detailed error analysis presents the case for our proposed methodology", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "180-185"}, "authors": [{"authorId": "2108928626", "name": "Smriti Singh"}, {"authorId": "1455135470", "name": "Tanvi Anand"}, {"authorId": "48372562", "name": "Arijit Ghosh Chowdhury"}, {"authorId": "2138053020", "name": "Zeerak Talat"}]}, {"paperId": "8dc279bcd385bde9f2950583153ddfc5e20cd631", "externalIds": {"ACL": "2021.acl-srw.20", "DBLP": "conf/acl/StadlerMA21", "DOI": "10.18653/v1/2021.acl-srw.20", "CorpusId": 237239585}, "corpusId": 237239585, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8dc279bcd385bde9f2950583153ddfc5e20cd631", "title": "Observing the Learning Curve of NMT Systems With Regard to Linguistic Phenomena", "abstract": "In this paper we present our observations and evaluations by observing the linguistic performance of the system on several steps on the training process of various English-to-German Neural Machine Translation models. The linguistic performance is measured through a semi-automatic process using a test suite. Among several linguistic observations, we find that the translation quality of some linguistic categories decreased within the recorded iterations. Additionally, we notice some drops of the translation quality of certain categories when using a larger corpus.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "186-196"}, "authors": [{"authorId": "2072101570", "name": "P. Stadler"}, {"authorId": "22330068", "name": "Vivien Macketanz"}, {"authorId": "2837687", "name": "Eleftherios Avramidis"}]}, {"paperId": "5f1a436277476cbcac5eaa7c0b925b051ef05ff7", "externalIds": {"ACL": "2021.acl-srw.21", "ArXiv": "2004.03238", "DBLP": "conf/acl/ShinodaSA21", "DOI": "10.18653/v1/2021.acl-srw.21", "CorpusId": 235353080}, "corpusId": 235353080, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5f1a436277476cbcac5eaa7c0b925b051ef05ff7", "title": "Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation", "abstract": "Question answering (QA) models for reading comprehension have achieved human-level accuracy on in-distribution test sets. However, they have been demonstrated to lack robustness to challenge sets, whose distribution is different from that of training sets. Existing data augmentation methods mitigate this problem by simply augmenting training sets with synthetic examples sampled from the same distribution as the challenge sets. However, these methods assume that the distribution of a challenge set is known a priori, making them less applicable to unseen challenge sets. In this study, we focus on question-answer pair generation (QAG) to mitigate this problem. While most existing QAG methods aim to improve the quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better robustness. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our method can improve the accuracy of 12 challenge sets, as well as the in-distribution accuracy.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 92, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.21.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-07", "journal": {"pages": "197-214"}, "authors": [{"authorId": "50088672", "name": "Kazutoshi Shinoda"}, {"authorId": "2673984", "name": "Saku Sugawara"}, {"authorId": "1705519", "name": "Akiko Aizawa"}]}, {"paperId": "f69a66f9832c7c7aae05bf5fed4e133c53688915", "externalIds": {"DBLP": "conf/acl/CerezoBB21", "ACL": "2021.acl-srw.22", "DOI": "10.18653/v1/2021.acl-srw.22", "CorpusId": 236909376}, "corpusId": 236909376, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f69a66f9832c7c7aae05bf5fed4e133c53688915", "title": "Tools Impact on the Quality of Annotations for Chat Untangling", "abstract": "The quality of the annotated data directly influences in the success of supervised NLP models. However, creating annotated datasets is often time-consuming and expensive. Although the annotation tool takes an important role, we know little about how it influences annotation quality. We compare the quality of annotations for the task of chat-untangling made by non-experts annotators using two different tools. The first is SLATE, an existing command-line based tool, and the second is Parlay, a new tool we developed that integrates mouse interaction and visual links. Our experimental results indicate that, while both tools perform similarly in terms of annotation quality, Parlay offers a significantly better user experience.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 25, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.22.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "215-220"}, "authors": [{"authorId": "2122658310", "name": "Jhonny Cerezo"}, {"authorId": "2998375", "name": "Felipe Bravo-Marquez"}, {"authorId": "1790474", "name": "Alexandre Bergel"}]}, {"paperId": "a010fe2f9404a951c3a9f50cba2006a551690917", "externalIds": {"DBLP": "conf/acl/SimoulinC21", "ACL": "2021.acl-srw.23", "DOI": "10.18653/v1/2021.acl-srw.23", "CorpusId": 237331496}, "corpusId": 237331496, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a010fe2f9404a951c3a9f50cba2006a551690917", "title": "How Many Layers and Why? An Analysis of the Model Depth in Transformers", "abstract": "In this study, we investigate the role of the multiple layers in deep transformer models. We design a variant of Albert that dynamically adapts the number of layers for each token of the input. The key specificity of Albert is that weights are tied across layers. Therefore, the stack of encoder layers iteratively repeats the application of the same transformation function on the input. We interpret the repetition of this application as an iterative process where the token contextualized representations are progressively refined. We analyze this process at the token level during pre-training, fine-tuning, and inference. We show that tokens do not require the same amount of iterations and that difficult or crucial tokens for the task are subject to more iterations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "221-228"}, "authors": [{"authorId": "32566481", "name": "Antoine Simoulin"}, {"authorId": "2704047", "name": "B. Crabb\u00e9"}]}, {"paperId": "6799e796fb26d081253027feb5ec33ae6664ee23", "externalIds": {"DBLP": "conf/acl/KadotaniKAO21", "ACL": "2021.acl-srw.24", "DOI": "10.18653/v1/2021.acl-srw.24", "CorpusId": 237329957}, "corpusId": 237329957, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6799e796fb26d081253027feb5ec33ae6664ee23", "title": "Edit Distance Based Curriculum Learning for Paraphrase Generation", "abstract": "Curriculum learning has improved the quality of neural machine translation, where only source-side features are considered in the metrics to determine the difficulty of translation. In this study, we apply curriculum learning to paraphrase generation for the first time. Different from machine translation, paraphrase generation allows a certain level of discrepancy in semantics between source and target, which results in diverse transformations from lexical substitution to reordering of clauses. Hence, the difficulty of transformations requires considering both source and target contexts. Experiments on formality transfer using GYAFC showed that our curriculum learning with edit distance improves the quality of paraphrase generation. Additionally, the proposed method improves the quality of difficult samples, which was not possible for previous methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 25, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.24.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "229-234"}, "authors": [{"authorId": "2124719261", "name": "Sora Kadotani"}, {"authorId": "1981103", "name": "Tomoyuki Kajiwara"}, {"authorId": "3043844", "name": "Yuki Arase"}, {"authorId": "48075831", "name": "Makoto Onizuka"}]}, {"paperId": "016dc75cb6666eb3e9394b68741690af57956e97", "externalIds": {"DBLP": "conf/acl/FicsorB21", "ACL": "2021.acl-srw.25", "DOI": "10.18653/v1/2021.acl-srw.25", "CorpusId": 237331887}, "corpusId": 237331887, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/016dc75cb6666eb3e9394b68741690af57956e97", "title": "Changing the Basis of Contextual Representations with Explicit Semantics", "abstract": "The application of transformer-based contextual representations has became a de facto solution for solving complex NLP tasks. Despite their successes, such representations are arguably opaque as their latent dimensions are not directly interpretable. To alleviate this limitation of contextual representations, we devise such an algorithm where the output representation expresses human-interpretable information of each dimension. We achieve this by constructing a transformation matrix based on the semantic content of the embedding space and predefined semantic categories using Hellinger distance. We evaluate our inferred representations on supersense prediction task. Our experiments reveal that the interpretable nature of transformed contextual representations makes it possible to accurately predict the supersense category of a word by simply looking for its transformed coordinate with the largest coefficient. We quantify the effects of our proposed transformation when applied over traditional dense contextual embeddings. We additionally investigate and report consistent improvements for the integration of sparse contextual word representations into our proposed algorithm.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 67, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "235-247"}, "authors": [{"authorId": "1930943199", "name": "Tam\u00e1s Ficsor"}, {"authorId": "2866858", "name": "G\u00e1bor Berend"}]}, {"paperId": "1e33044678e4883f16f4b02e9ddcb875340485ec", "externalIds": {"ACL": "2021.acl-srw.26", "DBLP": "conf/acl/MilkowskiGKKGK21", "DOI": "10.18653/v1/2021.acl-srw.26", "CorpusId": 236768402}, "corpusId": 236768402, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1e33044678e4883f16f4b02e9ddcb875340485ec", "title": "Personal Bias in Prediction of Emotions Elicited by Textual Opinions", "abstract": "Analysis of emotions elicited by opinions, comments, or articles commonly exploits annotated corpora, in which the labels assigned to documents average the views of all annotators, or represent a majority decision. The models trained on such data are effective at identifying the general views of the population. However, their usefulness for predicting the emotions evoked by the textual content in a particular individual is limited. In this paper, we present a study performed on a dataset containing 7,000 opinions, each annotated by about 50 people with two dimensions: valence, arousal, and with intensity of eight emotions from Plutchik\u2019s model. Our study showed that individual responses often significantly differed from the mean. Therefore, we proposed a novel measure to estimate this effect \u2013 Personal Emotional Bias (PEB). We also developed a new BERT-based transformer architecture to predict emotions from an individual human perspective. We found PEB a major factor for improving the quality of personalized reasoning. Both the method and measure may boost the quality of content recommendation systems and personalized solutions that protect users from hate speech or unwanted content, which are highly subjective in nature.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 52, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.26.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "248-259"}, "authors": [{"authorId": "1413772109", "name": "P. Milkowski"}, {"authorId": "2120757466", "name": "Marcin Gruza"}, {"authorId": "2007286374", "name": "Kamil Kanclerz"}, {"authorId": "1724788", "name": "Przemyslaw Kazienko"}, {"authorId": "2122115995", "name": "Damian Grimling"}, {"authorId": "2905929", "name": "Jan Koco\u0144"}]}, {"paperId": "4690eb050572a279f94560b6bbdccaae577b45f5", "externalIds": {"MAG": "3185537506", "ACL": "2021.acl-srw.27", "DBLP": "conf/acl/Zhu21a", "DOI": "10.18653/v1/2021.acl-srw.27", "CorpusId": 237331564}, "corpusId": 237331564, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4690eb050572a279f94560b6bbdccaae577b45f5", "title": "MVP-BERT: Multi-Vocab Pre-training for Chinese BERT", "abstract": "Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary (vocab) for these Chinese PLMs remains to be the one provided by Google Chinese BERT (CITATION), which is based on Chinese characters (chars). Second, the masked language model pre-training is based on a single vocab, limiting its downstream task performances. In this work, we first experimentally demonstrate that building a vocab via Chinese word segmentation (CWS) guided sub-word tokenization (SGT) can improve the performances of Chinese PLMs. Then we propose two versions of multi-vocab pre-training (MVP), Hi-MVP and AL-MVP, to improve the models\u2019 expressiveness. Experiments show that: (a) MVP training strategies improve PLMs\u2019 downstream performances, especially it can improve the PLM\u2019s performances on span-level tasks; (b) our AL-MVP outperforms the recent AMBERT (CITATION) after large-scale pre-training, and it is more robust against adversarial attacks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 33, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.27.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "260-269"}, "authors": [{"authorId": "2152348673", "name": "Wei Zhu"}]}, {"paperId": "5090628f2f034d0df837ea533bf25ac721c2041d", "externalIds": {"DBLP": "conf/acl/PraneshFSV21", "ACL": "2021.acl-srw.28", "DOI": "10.18653/v1/2021.acl-srw.28", "CorpusId": 237250087}, "corpusId": 237250087, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5090628f2f034d0df837ea533bf25ac721c2041d", "title": "CMTA: COVID-19 Misinformation Multilingual Analysis on Twitter", "abstract": "The internet has actually come to be an essential resource of health knowledge for individuals around the world in the present situation of the coronavirus condition pandemic(COVID-19). During pandemic situations, myths, sensationalism, rumours and misinformation, generated intentionally or unintentionally, spread rapidly through social networks. Twitter is one of these popular social networks people use to share COVID-19 related news, information, and thoughts that reflect their perception and opinion about the pandemic. Evaluation of tweets for recognizing misinformation can create beneficial understanding to review the top quality and also the readability of online information concerning the COVID-19. This paper presents a multilingual COVID-19 related tweet analysis method, CMTA, that uses BERT, a deep learning model for multilingual tweet misinformation detection and classification. CMTA extracts features from multilingual textual data, which is then categorized into specific information classes. Classification is done by a Dense-CNN model trained on tweets manually annotated into information classes (i.e., \u2018false\u2019, \u2018partly false\u2019, \u2018misleading\u2019). The paper presents an analysis of multilingual tweets from February to June, showing the distribution type of information spread across different languages. To access the performance of the CMTA multilingual model, we performed a comparative analysis of 8 monolingual model and CMTA for the misinformation detection task. The results show that our proposed CMTA model has surpassed various monolingual models which consolidated the fact that through transfer learning a multilingual framework could be developed.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.28.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": null, "journal": {"pages": "270-283"}, "authors": [{"authorId": "2122679612", "name": "R. Pranesh"}, {"authorId": "2124507011", "name": "Mehrdad Farokhenajd"}, {"authorId": "2008203279", "name": "Ambesh Shekhar"}, {"authorId": "1393643717", "name": "Genoveva Vargas-Solar"}]}, {"paperId": "9b0d5db75ead8014cd4bca6d1bbd2ae551081fb4", "externalIds": {"DBLP": "conf/acl/YangLYLP21", "ACL": "2021.acl-srw.29", "DOI": "10.18653/v1/2021.acl-srw.29", "CorpusId": 237332911, "PubMed": "36340582"}, "corpusId": 237332911, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9b0d5db75ead8014cd4bca6d1bbd2ae551081fb4", "title": "Predicting pragmatic discourse features in the language of adults with autism spectrum disorder", "abstract": "Individuals with autism spectrum disorder (ASD) experience difficulties in social aspects of communication, but the linguistic characteristics associated with deficits in discourse and pragmatic expression are often difficult to precisely identify and quantify. We are currently collecting a corpus of transcribed natural conversations produced in an experimental setting in which participants with and without ASD complete a number of collaborative tasks with their neurotypical peers. Using this dyadic conversational data, we investigate three pragmatic features \u2013 politeness, uncertainty, and informativeness \u2013 and present a dataset of utterances annotated for each of these features on a three-point scale. We then introduce ongoing work in developing and training neural models to automatically predict these features, with the goal of identifying the same between-groups differences that are observed using manual annotations. We find the best performing model for all three features is a feed-forward neural network trained with BERT embeddings. Our models yield higher accuracy than ones used in previous approaches for deriving these features, with F1 exceeding 0.82 for all three pragmatic features.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 32, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"name": "Proceedings of the conference. Association for Computational Linguistics. Meeting", "pages": "\n          284-291\n        ", "volume": "2021"}, "authors": [{"authorId": "2124741889", "name": "Christine Yang"}, {"authorId": "2124904525", "name": "Duanchen Liu"}, {"authorId": "2149535739", "name": "Qingyun Yang"}, {"authorId": "2109308254", "name": "Zoey Liu"}, {"authorId": "113057658", "name": "Emily Prudhommeaux"}]}, {"paperId": "8e1d376229b0bc6896eb50896761854a63b596ab", "externalIds": {"ACL": "2021.acl-srw.30", "DBLP": "conf/acl/GuptaBNK21", "DOI": "10.18653/v1/2021.acl-srw.30", "CorpusId": 236772881}, "corpusId": 236772881, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8e1d376229b0bc6896eb50896761854a63b596ab", "title": "SumPubMed: Summarization Dataset of PubMed Scientific Articles", "abstract": "Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SumPubMed , using scientific articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and informativeness on SumPubMed . SumPubMed is challenging because (a) the summary is distributed throughout the text (not-localized on top), and (b) it contains rare domain-specific scientific terms. We observe that seq2seq models that adequately summarize news articles struggle to summarize SumPubMed . Thus, SumPubMed opens new avenues for the future improvement of models as well as the development of new evaluation metrics.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 23, "citationCount": 24, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.30.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "292-303"}, "authors": [{"authorId": "46346053", "name": "Vivek Gupta"}, {"authorId": "103882825", "name": "Prerna Bharti"}, {"authorId": "35296383", "name": "Pegah Nokhiz"}, {"authorId": "2376013", "name": "H. Karnick"}]}, {"paperId": "347918ae216667b1d1b4d4075eda616ab25afb9c", "externalIds": {"ACL": "2021.acl-srw.31", "DBLP": "conf/acl/AboufoulMGLS21", "DOI": "10.18653/v1/2021.acl-srw.31", "CorpusId": 237331130}, "corpusId": 237331130, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/347918ae216667b1d1b4d4075eda616ab25afb9c", "title": "A Case Study of Analysis of Construals in Language on Social Media Surrounding a Crisis Event", "abstract": "The events that took place at the Unite the Right rally held in Charlottesville, Virginia on August 11-12, 2017 caused intense reaction on social media from users across the political spectrum. We present a novel application of psycholinguistics - specifically, construal level theory - to analyze the language on social media around this event of social import through topic models. We find that including psycholinguistic measures of concreteness as covariates in topic models can lead to informed analysis of the language surrounding an event of political import.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "304-309"}, "authors": [{"authorId": "2124725435", "name": "Lolo Aboufoul"}, {"authorId": "73124063", "name": "Khyati Mahajan"}, {"authorId": "3034920", "name": "T. Gallicano"}, {"authorId": "2882053", "name": "Sara M. Levens"}, {"authorId": "145102721", "name": "Samira Shaikh"}]}, {"paperId": "485530d40fc69d967a7be739522c616891fc0b74", "externalIds": {"DBLP": "conf/acl/DementievaP21", "ACL": "2021.acl-srw.32", "DOI": "10.18653/v1/2021.acl-srw.32", "CorpusId": 237332109}, "corpusId": 237332109, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/485530d40fc69d967a7be739522c616891fc0b74", "title": "Cross-lingual Evidence Improves Monolingual Fake News Detection", "abstract": "Misleading information spreads on the Internet at an incredible speed, which can lead to irreparable consequences in some cases. Therefore, it is becoming essential to develop fake news detection technologies. While substantial work has been done in this direction, one of the limitations of the current approaches is that these models are focused only on one language and do not use multilingual information. In this work, we propose a new technique based on cross-lingual evidence (CE) that can be used for fake news detection and improve existing approaches. The hypothesis of the usage of cross-lingual evidence as a feature for fake news detection is confirmed, firstly, by manual experiment based on a set of known true and fake news. Besides, we compared our fake news classification system based on the proposed feature with several strong baselines on two multi-domain datasets of general-topic news and one newly fake COVID-19 news dataset showing that combining cross-lingual evidence with strong baselines such as RoBERTa yields significant improvements in fake news detection.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.32.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "310-320"}, "authors": [{"authorId": "2027664710", "name": "D. Dementieva"}, {"authorId": "2027664756", "name": "A. Panchenko"}]}, {"paperId": "8547edec7c87ab01972762656ee6294dcbf03e52", "externalIds": {"ACL": "2021.acl-srw.33", "DBLP": "conf/acl/HaradaW21", "DOI": "10.18653/v1/2021.acl-srw.33", "CorpusId": 237332926}, "corpusId": 237332926, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8547edec7c87ab01972762656ee6294dcbf03e52", "title": "Neural Machine Translation with Synchronous Latent Phrase Structure", "abstract": "It is reported that grammatical information is useful for machine translation (MT) task. However, the annotation of grammatical information requires the highly human resources. Furthermore, it is not trivial to adapt grammatical information to MT since grammatical annotation usually adapts tokenization standards which might not be suitable to capture the relation of two languages, and the use of sub-word tokenization, e.g., Byte-Pair-Encoding, to alleviate out-of-vocabulary problem might not be compatible with those annotations. In this work, we propose two methods to explicitly incorporate grammatical information without supervising annotation; first, latent phrase structure is induced in an unsupervised fashion from a multi-head attention mechanism; second, the induced phrase structures in encoder and decoder are synchronized so that they are compatible with each other using constraints during training. We demonstrate that our approach produces better performance and explainability in two tasks, translation and alignment tasks without extra resources. Although we could not obtain the high quality phrase structure in constituency parsing when evaluated monolingually, we find that the induced phrase structures enhance the explainability of translation through the synchronization constraint.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 30, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "321-330"}, "authors": [{"authorId": "2432263", "name": "Shintaro Harada"}, {"authorId": "2110694221", "name": "Taro Watanabe"}]}, {"paperId": "adfc71c6d5090f3f8eb4396d99449d43532db8a8", "externalIds": {"DBLP": "conf/acl/IwataWN21", "ACL": "2021.acl-srw.34", "DOI": "10.18653/v1/2021.acl-srw.34", "CorpusId": 237332892}, "corpusId": 237332892, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/adfc71c6d5090f3f8eb4396d99449d43532db8a8", "title": "Zero Pronouns Identification based on Span prediction", "abstract": "The presence of zero-pronoun (ZP) greatly affects the downstream tasks of NLP in pro-drop languages such as Japanese and Chinese. To tackle the problem, the previous works identified ZPs as sequence labeling on the word sequence or the linearlized tree nodes of the input. We propose a novel approach to ZP identification by casting it as a query-based argument span prediction task. Given a predicate as a query, our model predicts the omission with ZP. In the experiments, our model surpassed the sequence labeling baseline.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "331-336"}, "authors": [{"authorId": "144564614", "name": "Seizi Iwata"}, {"authorId": "2110694221", "name": "Taro Watanabe"}, {"authorId": "2364073", "name": "M. Nagata"}]}, {"paperId": "4ca1d88e196612d6a4bd6d18cdfd508ebc6edcc3", "externalIds": {"ACL": "2021.acl-srw.35", "DBLP": "conf/acl/VazquezCCT21", "DOI": "10.18653/v1/2021.acl-srw.35", "CorpusId": 237302466}, "corpusId": 237302466, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4ca1d88e196612d6a4bd6d18cdfd508ebc6edcc3", "title": "On the differences between BERT and MT encoder spaces and how to address them in translation tasks", "abstract": "Various studies show that pretrained language models such as BERT cannot straightforwardly replace encoders in neural machine translation despite their enormous success in other tasks. This is even more astonishing considering the similarities between the architectures. This paper sheds some light on the embedding spaces they create, using average cosine similarity, contextuality metrics and measures for representational similarity for comparison, revealing that BERT and NMT encoder representations look significantly different from one another. In order to address this issue, we propose a supervised transformation from one into the other using explicit alignment and fine-tuning. Our results demonstrate the need for such a transformation to improve the applicability of BERT in MT.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "337-347"}, "authors": [{"authorId": "119942558", "name": "Ra\u00fal V\u00e1zquez"}, {"authorId": "3244157", "name": "H. \u00c7elikkanat"}, {"authorId": "2219854", "name": "Mathias Creutz"}, {"authorId": "143675545", "name": "J. Tiedemann"}]}, {"paperId": "bfd5b3ed81567f072f8516ef4c11111ea3adf423", "externalIds": {"ACL": "2021.acl-srw.36", "MAG": "3185537577", "DBLP": "conf/acl/DeguchiTN21", "DOI": "10.18653/v1/2021.acl-srw.36", "CorpusId": 237332208}, "corpusId": 237332208, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bfd5b3ed81567f072f8516ef4c11111ea3adf423", "title": "Synchronous Syntactic Attention for Transformer Neural Machine Translation", "abstract": "This paper proposes a novel attention mechanism for Transformer Neural Machine Translation, \u201cSynchronous Syntactic Attention,\u201d inspired by synchronous dependency grammars. The mechanism synchronizes source-side and target-side syntactic self-attentions by minimizing the difference between target-side self-attentions and the source-side self-attentions mapped by the encoder-decoder attention matrix. The experiments show that the proposed method improves the translation performance on WMT14 En-De, WMT16 En-Ro, and ASPEC Ja-En (up to +0.38 points in BLEU).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 0, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-srw.36.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "348-355"}, "authors": [{"authorId": "2059923851", "name": "Hiroyuki Deguchi"}, {"authorId": "1888638", "name": "Akihiro Tamura"}, {"authorId": "2067242699", "name": "Takashi Ninomiya"}]}, {"paperId": "9e54e3848697c49f4ea76c5998a3ae3820671bab", "externalIds": {"DBLP": "conf/acl/ThayaparanVF21", "ACL": "2021.findings-acl.1", "DOI": "10.18653/v1/2021.findings-acl.1", "CorpusId": 235386485}, "corpusId": 235386485, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9e54e3848697c49f4ea76c5998a3ae3820671bab", "title": "Explainable Inference Over Grounding-Abstract Chains for Science Questions", "abstract": "We propose an explainable inference approach for science questions by reasoning on grounding and abstract inference chains. This paper frames question answering as a natural language abductive reasoning problem, constructing plausible explanations for each candidate answer and then selecting the candidate with the best explanation as the \ufb01nal answer. Our method, ExplanationLP , elicits explanations by constructing a weighted graph of relevant facts for each candidate answer and employs a linear programming formalism designed to select the optimal subgraph of explanatory facts. The graphs\u2019 weighting function is composed of a set of parameters targeting relevance, cohesion and diversity, which we \ufb01ne-tune for answer selection via Bayesian Optimisation. We carry out our experiments on the WorldTree and ARC-Challenge datasets to empirically demonstrate the following contribu-tions: (1) ExplanationLP obtains strong performance when compared to transformer-based and multi-hop approaches despite having a signi\ufb01cantly lower number of parameters; (2) We show that our model is able to generate plausible explanations for answer prediction; (3) Our model demonstrates better robustness towards semantic drift when compared to transformer-based and multi-hop approaches.", "venue": "Findings", "year": 2021, "referenceCount": 42, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1-12"}, "authors": [{"authorId": "102669988", "name": "Mokanarangan Thayaparan"}, {"authorId": "34102057", "name": "Marco Valentino"}, {"authorId": "145528474", "name": "A. Freitas"}]}, {"paperId": "400bbf81e1a30ca35ccb39bef2d25a3c2c28c532", "externalIds": {"DBLP": "journals/corr/abs-2106-11740", "ACL": "2021.findings-acl.2", "ArXiv": "2106.11740", "DOI": "10.18653/v1/2021.findings-acl.2", "CorpusId": 235593024}, "corpusId": 235593024, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/400bbf81e1a30ca35ccb39bef2d25a3c2c28c532", "title": "LV-BERT: Exploiting Layer Variety for BERT", "abstract": "Modern pre-trained language models are mostly built upon backbones stacking self-attention and feed-forward layers in an interleaved order. In this paper, beyond this stereotyped layer pattern, we aim to improve pre-trained models by exploiting layer variety from two aspects: the layer type set and the layer order. Specifically, besides the original self-attention and feed-forward layers, we introduce convolution into the layer type set, which is experimentally found beneficial to pre-trained models. Furthermore, beyond the original interleaved order, we explore more layer orders to discover more powerful architectures. However, the introduced layer variety leads to a large architecture space of more than billions of candidates, while training a single candidate model from scratch already requires huge computation cost, making it not affordable to search such a space by directly training large amounts of candidate models. To solve this problem, we first pre-train a supernet from which the weights of all candidate models can be inherited, and then adopt an evolutionary algorithm guided by pre-training accuracy to find the optimal architecture. Extensive experiments show that LV-BERT model obtained by our method outperforms BERT and its variants on various downstream tasks. For example, LV-BERT-small achieves 79.8 on the GLUE testing set, 1.8 higher than the strong baseline ELECTRA-small.", "venue": "Findings", "year": 2021, "referenceCount": 88, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.2.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-22", "journal": {"name": "ArXiv", "volume": "abs/2106.11740"}, "authors": [{"authorId": "23476952", "name": "Weihao Yu"}, {"authorId": "145062296", "name": "Zihang Jiang"}, {"authorId": "117872156", "name": "Fei Chen"}, {"authorId": "3298532", "name": "Qibin Hou"}, {"authorId": "1698982", "name": "Jiashi Feng"}]}, {"paperId": "53bb3924925503986948bd3872efecf77f795a6a", "externalIds": {"DBLP": "journals/corr/abs-2012-02353", "ACL": "2021.findings-acl.3", "MAG": "3111162508", "ArXiv": "2012.02353", "DOI": "10.18653/v1/2021.findings-acl.3", "CorpusId": 227305529}, "corpusId": 227305529, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/53bb3924925503986948bd3872efecf77f795a6a", "title": "Few-Shot Event Detection with Prototypical Amortized Conditional Random Field", "abstract": "Event Detection, a fundamental task of Information Extraction, tends to struggle when it needs to recognize novel event types with a few samples, i.e. Few-Shot Event Detection (FSED). Previous identify-then-classify paradigm attempts to solve this problem in the pipeline manner but ignores the trigger discrepancy between event types, thus suffering from the error propagation. In this paper, we present a novel unified joint model which converts the task to a few-shot tagging problem with a double-part tagging scheme. To this end, we first design the Prototypical Amortized Conditional Random Field (PA-CRF) to model the label dependency in the few-shot scenario, which builds prototypical amortization networks to approximate the transition scores between labels based on the label prototypes. Then Gaussian distribution is introduced for the modeling of the transition scores in PA-CRF to alleviate the uncertain estimation resulting from insufficient data. We conduct experiments on the benchmark dataset FewEvent and the experimental results show that the tagging based methods are better than existing pipeline and joint learning methods. In addition, the proposed PA-CRF achieves the best results on the public dataset.", "venue": "Findings", "year": 2020, "referenceCount": 32, "citationCount": 37, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.3.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-04", "journal": {"pages": "28-40"}, "authors": [{"authorId": "2024380701", "name": "Xin Cong"}, {"authorId": "2111851118", "name": "Shiyao Cui"}, {"authorId": "48613402", "name": "Yu Bowen"}, {"authorId": "2079682", "name": "Tingwen Liu"}, {"authorId": "2115721575", "name": "Yubin Wang"}, {"authorId": "1864982637", "name": "Bin Wang"}]}, {"paperId": "e3c45dde86be18463b89e4880c715e84befe929e", "externalIds": {"ACL": "2021.findings-acl.4", "DBLP": "conf/acl/AzevedodDZ21", "DOI": "10.18653/v1/2021.findings-acl.4", "CorpusId": 236478124}, "corpusId": 236478124, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e3c45dde86be18463b89e4880c715e84befe929e", "title": "LUX (Linguistic aspects Under eXamination): Discourse Analysis for Automatic Fake News Classification", "abstract": "The democratization/decentralization of both the production and consumption of information has resulted in a subjective and often misleading depiction of facts known as Fake News - a phenomenon that is effectively shap-ing the perception of reality for many individ-uals. Manual fact-checking is time-consuming and cannot scale and although automatic fact-checking, vis a vis machine learning holds promise, it is signi\ufb01cantly hindered by a de\ufb01cit of suitable training data. We present both a novel dataset, VERITAS(VERIfying Textual Aspects), a collection of fact-checked claims, containing their original documents and LUX(Language Under eXamination), a text classi\ufb01er that makes use of an extensive linguistic analysis to infer the likelihood of the input being a piece of fake-news.", "venue": "Findings", "year": 2021, "referenceCount": 66, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.4.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "41-56"}, "authors": [{"authorId": "143817327", "name": "Lucas Azevedo"}, {"authorId": "1393699478", "name": "M. d\u2019Aquin"}, {"authorId": "2064935590", "name": "Brian Davis"}, {"authorId": "34936544", "name": "Manel Zarrouk"}]}, {"paperId": "73395837a22eb00ce4106be3653460377bed7725", "externalIds": {"ArXiv": "2105.13496", "ACL": "2021.findings-acl.5", "DBLP": "journals/corr/abs-2105-13496", "DOI": "10.18653/v1/2021.findings-acl.5", "CorpusId": 235247949}, "corpusId": 235247949, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/73395837a22eb00ce4106be3653460377bed7725", "title": "Diagnosing Transformers in Task-Oriented Semantic Parsing", "abstract": "Modern task-oriented semantic parsing approaches typically use seq2seq transformers to map textual utterances to semantic frames comprised of intents and slots. While these models are empirically strong, their specific strengths and weaknesses have largely remained unexplored. In this work, we study BART and XLM-R, two state-of-the-art parsers, across both monolingual and multilingual settings. Our experiments yield several key results: transformer-based parsers struggle not only with disambiguating intents/slots, but surprisingly also with producing syntactically-valid frames. Though pre-training imbues transformers with syntactic inductive biases, we find the ambiguity of copying utterance spans into frames often leads to tree invalidity, indicating span extraction is a major bottleneck for current parsers. However, as a silver lining, we show transformer-based parsers give sufficient indicators for whether a frame is likely to be correct or incorrect, making them easier to deploy in production settings.", "venue": "Findings", "year": 2021, "referenceCount": 15, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.5.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-27", "journal": {"name": "ArXiv", "volume": "abs/2105.13496"}, "authors": [{"authorId": "120777041", "name": "Shrey Desai"}, {"authorId": "2057450366", "name": "Ahmed Aly"}]}, {"paperId": "bce036f82d266c9781d2cbf64efc190679d4769e", "externalIds": {"ACL": "2021.findings-acl.6", "DBLP": "conf/acl/TuYLLGYY21", "DOI": "10.18653/v1/2021.findings-acl.6", "CorpusId": 236477908}, "corpusId": 236477908, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/bce036f82d266c9781d2cbf64efc190679d4769e", "title": "Semantic Relation-aware Difference Representation Learning for Change Captioning", "abstract": "Change captioning is to describe the difference in a pair of images with a natural language sentence. In this task, the distractors, such as the illumination or viewpoint change, bring the huge challenges about learning the difference representation. In this paper, we propose a semantic relation-aware difference representation learning network to explicitly learn the difference representation in the existence of distractors. Speci\ufb01cally, we introduce a self-semantic relation embedding block to explore the underlying changed objects and design a cross-semantic relation measuring block to localize the real change and learn the discriminative difference representation. Besides, relying on the POS of words, we devise an attention-based visual switch to dynamically use visual information for caption generation. Extensive experiments show that our method achieves the state-of-the-art performances on CLEVR-Change and Spot-the-Diff datasets 1 .", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.6.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "63-73"}, "authors": [{"authorId": "2054288292", "name": "Yu-Ming Tu"}, {"authorId": "145690248", "name": "Ting Yao"}, {"authorId": "73596205", "name": "Liang Li"}, {"authorId": "2105729435", "name": "Jiedong Lou"}, {"authorId": "2409659", "name": "Shengxiang Gao"}, {"authorId": "1760581", "name": "Zhengtao Yu"}, {"authorId": "7590116", "name": "C. Yan"}]}, {"paperId": "07f4920a4a120a4a8644804904d0f5cc215b8ff8", "externalIds": {"DBLP": "conf/acl/LiuJKLT21", "ArXiv": "2105.02778", "ACL": "2021.findings-acl.7", "DOI": "10.18653/v1/2021.findings-acl.7", "CorpusId": 233864681}, "corpusId": 233864681, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/07f4920a4a120a4a8644804904d0f5cc215b8ff8", "title": "The Authors Matter: Understanding and Mitigating Implicit Bias in Deep Text Classification", "abstract": "It is evident that deep text classification models trained on human data could be biased. In particular, they produce biased outcomes for texts that explicitly include identity terms of certain demographic groups. We refer to this type of bias as explicit bias, which has been extensively studied. However, deep text classification models can also produce biased outcomes for texts written by authors of certain demographic groups. We refer to such bias as implicit bias of which we still have a rather limited understanding. In this paper, we first demonstrate that implicit bias exists in different text classification tasks for different demographic groups. Then, we build a learning-based interpretation method to deepen our knowledge of implicit bias. Specifically, we verify that classifiers learn to make predictions based on language features that are related to the demographic attributes of the authors. Next, we propose a framework Debiased-TC to train deep text classifiers to make predictions on the right features and consequently mitigate implicit bias. We conduct extensive experiments on three real-world datasets. The results show that the text classification models trained under our proposed framework outperform traditional models significantly in terms of fairness, and also slightly in terms of classification performance.", "venue": "Findings", "year": 2021, "referenceCount": 48, "citationCount": 18, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.7.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-06", "journal": {"pages": "74-85"}, "authors": [{"authorId": "2143856455", "name": "Haochen Liu"}, {"authorId": "144767914", "name": "Wei Jin"}, {"authorId": "1596725015", "name": "Hamid Karimi"}, {"authorId": "2117940912", "name": "Zitao Liu"}, {"authorId": "1736632", "name": "Jiliang Tang"}]}, {"paperId": "d3fcae983f6fa3536b67542c17ae8cc1ba825490", "externalIds": {"DBLP": "conf/acl/ZhangYSXLG21", "ACL": "2021.findings-acl.8", "DOI": "10.18653/v1/2021.findings-acl.8", "CorpusId": 236477597}, "corpusId": 236477597, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d3fcae983f6fa3536b67542c17ae8cc1ba825490", "title": "From What to Why: Improving Relation Extraction with Rationale Graph", "abstract": "Which type of information affects the existing neural relation extraction (RE) models to make correct decisions is an important question. In this paper, we observe that entity type and trigger are the most indicative information for RE in each instance. Moreover, these indicative clues are always constrained to co-occur with specific relations at the corpus level. Motivated by this, we propose a novel RAtionale Graph (RAG) to organize such co-occurrence constraints among entity types, triggers and relations in a holistic graph view. By introducing two subtasks of entity type prediction and trigger labeling, we build the connection between each instance and RAG, and then leverage relevant global co-occurrence knowledge stored in the graph to improve the performance of neural RE models. Extensive experimental results indicate that our method outperforms strong baselines significantly and achieves state-ofthe-art performance on the document-level and sentence-level RE benchmarks.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.8.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "86-95"}, "authors": [{"authorId": "122542861", "name": "Zhenyu Zhang"}, {"authorId": "48613402", "name": "Yu Bowen"}, {"authorId": "2269366", "name": "Xiaobo Shu"}, {"authorId": "145470644", "name": "Mengge Xue"}, {"authorId": "2079682", "name": "Tingwen Liu"}, {"authorId": "2148933499", "name": "Li Guo"}]}, {"paperId": "348af8247f81d56cae1d9b00a69e242056b171e9", "externalIds": {"DBLP": "journals/corr/abs-2107-09622", "ArXiv": "2107.09622", "ACL": "2021.findings-acl.9", "DOI": "10.18653/v1/2021.findings-acl.9", "CorpusId": 236134371}, "corpusId": 236134371, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/348af8247f81d56cae1d9b00a69e242056b171e9", "title": "More Parameters? No Thanks!", "abstract": "This work studies the long-standing problems of model capacity and negative interference in multilingual neural machine translation MNMT. We use network pruning techniques and observe that pruning 50-70% of the parameters from a trained MNMT model results only in a 0.29-1.98 drop in the BLEU score. Suggesting that there exist large redundancies even in MNMT models. These observations motivate us to use the redundant parameters and counter the interference problem efficiently. We propose a novel adaptation strategy, where we iteratively prune and retrain the redundant parameters of an MNMT to improve bilingual representations while retaining the multilinguality. Negative interference severely affects high resource languages, and our method alleviates it without any additional adapter modules. Hence, we call it parameter-free adaptation strategy, paving way for the efficient adaptation of MNMT. We demonstrate the effectiveness of our method on a 9 language MNMT trained on TED talks, and report an average improvement of +1.36 BLEU on high resource pairs. Code will be released here.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.9.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-20", "journal": {"name": "ArXiv", "volume": "abs/2107.09622"}, "authors": [{"authorId": "153177135", "name": "Zeeshan Khan"}, {"authorId": "2034181156", "name": "Kartheek Akella"}, {"authorId": "145460361", "name": "Vinay P. Namboodiri"}, {"authorId": "1694502", "name": "C. V. Jawahar"}]}, {"paperId": "577d44a10b424a55165a6bf4839bafce2c695302", "externalIds": {"ArXiv": "2106.01077", "ACL": "2021.findings-acl.10", "DBLP": "conf/acl/YanakaMI21", "DOI": "10.18653/v1/2021.findings-acl.10", "CorpusId": 235294010}, "corpusId": 235294010, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/577d44a10b424a55165a6bf4839bafce2c695302", "title": "SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics", "abstract": "Recently, deep neural networks (DNNs) have achieved great success in semantically challenging NLP tasks, yet it remains unclear whether DNN models can capture compositional meanings, those aspects of meaning that have been long studied in formal semantics. To investigate this issue, we propose a Systematic Generalization testbed based on Natural language Semantics (SyGNS), whose challenge is to map natural language sentences to multiple forms of scoped meaning representations, designed to account for various semantic phenomena. Using SyGNS, we test whether neural networks can systematically parse sentences involving novel combinations of logical expressions such as quantifiers and negation. Experiments show that Transformer and GRU models can generalize to unseen combinations of quantifiers, negations, and modifiers that are similar to given training instances in form, but not to the others. We also find that the generalization performance to unseen combinations is better when the form of meaning representations is simpler. The data and code for SyGNS are publicly available at https://github.com/verypluming/SyGNS.", "venue": "Findings", "year": 2021, "referenceCount": 63, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.10.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "103-119"}, "authors": [{"authorId": "3486313", "name": "Hitomi Yanaka"}, {"authorId": "2106670", "name": "K. Mineshima"}, {"authorId": "3040648", "name": "Kentaro Inui"}]}, {"paperId": "1784fa1b5ac0b9f9fefe5e0508f91033f5952177", "externalIds": {"ACL": "2021.findings-acl.11", "DBLP": "journals/corr/abs-2012-15833", "ArXiv": "2012.15833", "DOI": "10.18653/v1/2021.findings-acl.11", "CorpusId": 229923438}, "corpusId": 229923438, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1784fa1b5ac0b9f9fefe5e0508f91033f5952177", "title": "Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade", "abstract": "Fully non-autoregressive neural machine translation (NAT) is proposed to simultaneously predict tokens with single forward of neural networks, which significantly reduces the inference latency at the expense of quality drop compared to the Transformer baseline. In this work, we target on closing the performance gap while maintaining the latency advantage. We first inspect the fundamental issues of fully NAT models, and adopt dependency reduction in the learning space of output tokens as the basic guidance. Then, we revisit methods in four different aspects that have been proven effective for improving NAT models, and carefully combine these techniques with necessary modifications. Our extensive experiments on three translation benchmarks show that the proposed system achieves the new state-of-the-art results for fully NAT models, and obtains comparable performance with the autoregressive and iterative NAT systems. For instance, one of the proposed models achieves 27.49 BLEU points on WMT14 En-De with approximately 16.5X speed up at inference time.", "venue": "Findings", "year": 2020, "referenceCount": 63, "citationCount": 92, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.11.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-31", "journal": {"pages": "120-133"}, "authors": [{"authorId": "3016273", "name": "Jiatao Gu"}, {"authorId": "145771502", "name": "X. Kong"}]}, {"paperId": "1c2b4dac511e37333948d0a2686755ea0ed24ce0", "externalIds": {"ACL": "2021.findings-acl.12", "ArXiv": "2106.01625", "DBLP": "conf/acl/ZhuB21", "DOI": "10.18653/v1/2021.findings-acl.12", "CorpusId": 235313392}, "corpusId": 235313392, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1c2b4dac511e37333948d0a2686755ea0ed24ce0", "title": "Generate, Prune, Select: A Pipeline for Counterspeech Generation against Online Hate Speech", "abstract": "Countermeasures to effectively fight the ever increasing hate speech online without blocking freedom of speech is of great social interest. Natural Language Generation (NLG), is uniquely capable of developing scalable solutions. However, off-the-shelf NLG methods are primarily sequence-to-sequence neural models and they are limited in that they generate commonplace, repetitive and safe responses regardless of the hate speech (e.g.,\"Please refrain from using such language.\") or irrelevant responses, making them ineffective for de-escalating hateful conversations. In this paper, we design a three-module pipeline approach to effectively improve the diversity and relevance. Our proposed pipeline first generates various counterspeech candidates by a generative model to promote diversity, then filters the ungrammatical ones using a BERT model, and finally selects the most relevant counterspeech response using a novel retrieval-based method. Extensive Experiments on three representative datasets demonstrate the efficacy of our approach in generating diverse and relevant counterspeech.", "venue": "Findings", "year": 2021, "referenceCount": 69, "citationCount": 22, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.12.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01625"}, "authors": [{"authorId": "2111092344", "name": "Wanzheng Zhu"}, {"authorId": "145355558", "name": "S. Bhat"}]}, {"paperId": "246fd63312287e3b629d6baa49c8842123d5a7a9", "externalIds": {"ArXiv": "2105.04201", "DBLP": "conf/acl/JiaoGNJLN21", "ACL": "2021.findings-acl.13", "DOI": "10.18653/v1/2021.findings-acl.13", "CorpusId": 234334701}, "corpusId": 234334701, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/246fd63312287e3b629d6baa49c8842123d5a7a9", "title": "REPT: Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training", "abstract": "Pre-trained Language Models (PLMs) have achieved great success on Machine Reading Comprehension (MRC) over the past few years. Although the general language representation learned from large-scale corpora does benefit MRC, the poor support in evidence extraction which requires reasoning across multiple sentences hinders PLMs from further advancing MRC. To bridge the gap between general PLMs and MRC, we present REPT, a REtrieval-based Pre-Training approach. In particular, we introduce two self-supervised tasks to strengthen evidence extraction during pre-training, which is further inherited by downstream MRC tasks through the consistent retrieval operation and model architecture. To evaluate our proposed method, we conduct extensive experiments on five MRC datasets that require collecting evidence from and reasoning across multiple sentences. Experimental results demonstrate the effectiveness of our pre-training approach. Moreover, further analysis shows that our approach is able to enhance the capacity of evidence extraction without explicit supervision.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 10, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.13.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-10", "journal": {"pages": "150-163"}, "authors": [{"authorId": "1689176705", "name": "Fangkai Jiao"}, {"authorId": "1390575046", "name": "Yangyang Guo"}, {"authorId": "10680347", "name": "Yilin Niu"}, {"authorId": "144642000", "name": "Feng Ji"}, {"authorId": "2144382531", "name": "Feng-Lin Li"}, {"authorId": "143982887", "name": "Liqiang Nie"}]}, {"paperId": "5cd842abb03293e5a3d5023a71f4fc0133b01ae8", "externalIds": {"DBLP": "journals/corr/abs-2107-01583", "ACL": "2021.findings-acl.14", "ArXiv": "2107.01583", "DOI": "10.18653/v1/2021.findings-acl.14", "CorpusId": 235732095}, "corpusId": 235732095, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5cd842abb03293e5a3d5023a71f4fc0133b01ae8", "title": "CasEE: A Joint Learning Framework with Cascade Decoding for Overlapping Event Extraction", "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Most existing methods assume that events appear in sentences without overlaps, which are not applicable to the complicated overlapping event extraction. This work systematically studies the realistic event overlapping problem, where a word may serve as triggers with several types or arguments with different roles. To tackle the above problem, we propose a novel joint learning framework with cascade decoding for overlapping event extraction, termed as CasEE. Particularly, CasEE sequentially performs type detection, trigger extraction and argument extraction, where the overlapped targets are extracted separately conditioned on the specific former prediction. All the subtasks are jointly learned in a framework to capture dependencies among the subtasks. The evaluation on a public event extraction benchmark FewFC demonstrates that CasEE achieves significant improvements on overlapping event extraction over previous competitive methods.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 33, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.14.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-04", "journal": {"pages": "164-174"}, "authors": [{"authorId": "2054250919", "name": "Jiawei Sheng"}, {"authorId": "144019293", "name": "Shu Guo"}, {"authorId": "48613402", "name": "Yu Bowen"}, {"authorId": "2117126771", "name": "Qian Li"}, {"authorId": "1491243898", "name": "Yiming Hei"}, {"authorId": "2108740151", "name": "Lihong Wang"}, {"authorId": "2079682", "name": "Tingwen Liu"}, {"authorId": "46485352", "name": "Hongbo Xu"}]}, {"paperId": "a9ee79cda5108f6e927dea5dafa197c9aaf3463d", "externalIds": {"ACL": "2021.findings-acl.15", "DBLP": "conf/acl/WuLM21", "DOI": "10.18653/v1/2021.findings-acl.15", "CorpusId": 236478173}, "corpusId": 236478173, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/a9ee79cda5108f6e927dea5dafa197c9aaf3463d", "title": "Discovering Topics in Long-tailed Corpora with Causal Intervention", "abstract": "Topic models are effective in capturing the latent semantics of large-scale textual data while existing methods are normally designed and evaluated on balanced corpora. However, it contradicts the fact that general corpora in our world are naturally long-tailed, and the longtailed bias can highly impair the topic modeling performance. Therefore, in this paper, we propose a causal inference framework to explain and overcome the issues of topic modeling on long-tailed corpora. In a neat and elegant way, causal intervention is applied in training to take out the influence brought by the long-tailed bias. Extensive experiments on manually constructed and naturally collected datasets demonstrate that our model can mitigate the bias effect, greatly improve topic quality and better discover the hidden semantics on the tail.", "venue": "Findings", "year": 2021, "referenceCount": 46, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "175-185"}, "authors": [{"authorId": "13901559", "name": "Xiaobao Wu"}, {"authorId": "2109333498", "name": "Chunping Li"}, {"authorId": "2666898", "name": "Yishu Miao"}]}, {"paperId": "6f5908d26a5c6928b414a71cdaaf5019668d77eb", "externalIds": {"ArXiv": "2106.00055", "ACL": "2021.findings-acl.16", "DBLP": "conf/acl/BottSW21", "DOI": "10.18653/v1/2021.findings-acl.16", "CorpusId": 235266005}, "corpusId": 235266005, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6f5908d26a5c6928b414a71cdaaf5019668d77eb", "title": "More than just Frequency? Demasking Unsupervised Hypernymy Prediction Methods", "abstract": "This paper presents a comparison of unsupervised methods of hypernymy prediction (i.e., to predict which word in a pair of words such as fish-cod is the hypernym and which the hyponym). Most importantly, we demonstrate across datasets for English and for German that the predictions of three methods (WeedsPrec, invCL, SLQS Row) strongly overlap and are highly correlated with frequency-based predictions. In contrast, the second-order method SLQS shows an overall lower accuracy but makes correct predictions where the others go wrong. Our study once more confirms the general need to check the frequency bias of a computational method in order to identify frequency-(un)related effects.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.16.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-31", "journal": {"pages": "186-192"}, "authors": [{"authorId": "119025687", "name": "Thomas Bott"}, {"authorId": "3449121", "name": "Dominik Schlechtweg"}, {"authorId": "7965906", "name": "Sabine Schulte im Walde"}]}, {"paperId": "e4a38435a08da7af34e801494c318a9ebb699e10", "externalIds": {"DBLP": "conf/acl/ChenWG21", "ACL": "2021.findings-acl.17", "ArXiv": "2012.14919", "DOI": "10.18653/v1/2021.findings-acl.17", "CorpusId": 235303723}, "corpusId": 235303723, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e4a38435a08da7af34e801494c318a9ebb699e10", "title": "WikiTableT: A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections", "abstract": "Datasets for data-to-text generation typically focus either on multi-domain, single-sentence generation or on single-domain, long-form generation. In this work, we cast generating Wikipedia sections as a data-to-text generation task and create a large-scale dataset, WikiTableT, that pairs Wikipedia sections with their corresponding tabular data and various metadata. WikiTableT contains millions of instances, covering a broad range of topics, as well as a variety of flavors of generation tasks with different levels of flexibility. We benchmark several training and decoding strategies on WikiTableT. Our qualitative analysis shows that the best approaches can generate fluent and high quality texts but they struggle with coherence and factuality, showing the potential for our dataset to inspire future work on long-form generation.", "venue": "Findings", "year": 2020, "referenceCount": 63, "citationCount": 20, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.17.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-29", "journal": {"pages": "193-209"}, "authors": [{"authorId": "46221498", "name": "Mingda Chen"}, {"authorId": "2844243", "name": "Sam Wiseman"}, {"authorId": "1700980", "name": "Kevin Gimpel"}]}, {"paperId": "8c4f89a9ac30cf94186916be1bfaa02dbfb3600d", "externalIds": {"ArXiv": "2105.14220", "DBLP": "journals/corr/abs-2105-14220", "ACL": "2021.findings-acl.18", "DOI": "10.18653/v1/2021.findings-acl.18", "CorpusId": 235254617}, "corpusId": 235254617, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8c4f89a9ac30cf94186916be1bfaa02dbfb3600d", "title": "CoDesc: A Large Code\u2013Description Parallel Dataset", "abstract": "Translation between natural language and source code can help software development by enabling developers to comprehend, ideate, search, and write computer programs in natural language. Despite growing interest from the industry and the research community, this task is often difficult due to the lack of large standard datasets suitable for training deep neural models, standard noise removal methods, and evaluation benchmarks. This leaves researchers to collect new small-scale datasets, resulting in inconsistencies across published works. In this study, we present CoDesc -- a large parallel dataset composed of 4.2 million Java methods and natural language descriptions. With extensive analysis, we identify and remove prevailing noise patterns from the dataset. We demonstrate the proficiency of CoDesc in two complementary tasks for code-description pairs: code summarization and code search. We show that the dataset helps improve code search by up to 22\\% and achieves the new state-of-the-art in code summarization. Furthermore, we show CoDesc's effectiveness in pre-training--fine-tuning setup, opening possibilities in building pretrained language models for Java. To facilitate future research, we release the dataset, a data processing tool, and a benchmark at \\url{https://github.com/csebuetnlp/CoDesc}.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.18.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-29", "journal": {"pages": "210-218"}, "authors": [{"authorId": "2078332622", "name": "Masum Hasan"}, {"authorId": "2077591349", "name": "Tanveer Muttaqueen"}, {"authorId": "2077591505", "name": "Abdullah Al Ishtiaq"}, {"authorId": "2077591291", "name": "Kazi Sajeed Mehrab"}, {"authorId": "2077592775", "name": "Md. Mahim Anjum Haque"}, {"authorId": "1400373232", "name": "Tahmid Hasan"}, {"authorId": "38123220", "name": "Wasi Uddin Ahmad"}, {"authorId": "1873031", "name": "Anindya Iqbal"}, {"authorId": "2046603", "name": "Rifat Shahriyar"}]}, {"paperId": "14ee04939eae5610d5d6141ad953021967ab2de5", "externalIds": {"DBLP": "conf/acl/CaiZWW21", "ACL": "2021.findings-acl.19", "DOI": "10.18653/v1/2021.findings-acl.19", "CorpusId": 236477378}, "corpusId": 236477378, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/14ee04939eae5610d5d6141ad953021967ab2de5", "title": "Deep Cognitive Reasoning Network for Multi-hop Question Answering over Knowledge Graphs", "abstract": "Knowledge Graphs (KGs) provide human knowledge with nodes and edges being entities and relations among them, respectively. Multihop question answering over KGs\u2014which aims to find answer entities of given questions through reasoning paths in KGs\u2014has attracted great attention from both academia and industry recently. However, this task remains challenging, as it requires to accurately identify answers in a large candidate entity set, of which the size grows exponentially with the number of reasoning hops. To tackle this problem, we propose a novel Deep Cognitive Reasoning Network (DCRN), which is inspired by the dual process theory in cognitive science. Specifically, DCRN consists of two phases\u2014the unconscious phase and the conscious phase. The unconscious phase first retrieves informative evidence from candidate entities by leveraging their semantic information. Then, the conscious phase accurately identifies answers by performing sequential reasoning according to the graph structure on the retrieved evidence. Experiments demonstrate that DCRN significantly outperforms state-of-the-art methods on benchmark datasets.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 9, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.19.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "219-229"}, "authors": [{"authorId": "2115669924", "name": "Jianyu Cai"}, {"authorId": "122542593", "name": "Zhanqiu Zhang"}, {"authorId": "144864333", "name": "Feng Wu"}, {"authorId": "2146041754", "name": "Jie Wang"}]}, {"paperId": "be35fbde5080831694244a3fe5cfbdf7af8e8734", "externalIds": {"ACL": "2021.findings-acl.20", "ArXiv": "2109.08475", "DBLP": "conf/acl/ChenCMLZ21", "DOI": "10.18653/v1/2021.findings-acl.20", "CorpusId": 236478170}, "corpusId": 236478170, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/be35fbde5080831694244a3fe5cfbdf7af8e8734", "title": "GoG: Relation-aware Graph-over-Graph Network for Visual Dialog", "abstract": "Visual dialog, which aims to hold a meaningful conversation with humans about a given image, is a challenging task that requires models to reason the complex dependencies among visual content, dialog history, and current questions. Graph neural networks are recently applied to model the implicit relations between objects in an image or dialog. However, they neglect the importance of 1) coreference relations among dialog history and dependency relations between words for the question representation; and 2) the representation of the image based on the fully represented question. Therefore, we propose a novel relation-aware graph-over-graph network (GoG) for visual dialog. Specifically, GoG consists of three sequential graphs: 1) H-Graph, which aims to capture coreference relations among dialog history; 2) History-aware Q-Graph, which aims to fully understand the question through capturing dependency relations between words based on coreference resolution on the dialog history; and 3) Question-aware I-Graph, which aims to capture the relations between objects in an image based on fully question representation. As an additional feature representation module, we add GoG to the existing visual dialogue model. Experimental results show that our model outperforms the strong baseline in both generative and discriminative settings by a significant margin.", "venue": "Findings", "year": 2021, "referenceCount": 59, "citationCount": 26, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.20.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-17", "journal": {"pages": "230-243"}, "authors": [{"authorId": null, "name": "Feilong Chen"}, {"authorId": "30917866", "name": "Xiuyi Chen"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "50492525", "name": "Peng Li"}, {"authorId": "2108485135", "name": "Jie Zhou"}]}, {"paperId": "e6b252ad22486c10b1b288e0a5e1ad468690be70", "externalIds": {"DBLP": "journals/corr/abs-2105-12410", "ArXiv": "2105.12410", "ACL": "2021.findings-acl.21", "DOI": "10.18653/v1/2021.findings-acl.21", "CorpusId": 235195590}, "corpusId": 235195590, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e6b252ad22486c10b1b288e0a5e1ad468690be70", "title": "Joint Optimization of Tokenization and Downstream Model", "abstract": "Since traditional tokenizers are isolated from a downstream task and model, they cannot output an appropriate tokenization depending on the task and model, although recent studies imply that the appropriate tokenization improves the performance. In this paper, we propose a novel method to find an appropriate tokenization to a given downstream model by jointly optimizing a tokenizer and the model. The proposed method has no restriction except for using loss values computed by the downstream model to train the tokenizer, and thus, we can apply the proposed method to any NLP task. Moreover, the proposed method can be used to explore the appropriate tokenization for an already trained model as post-processing. Therefore, the proposed method is applicable to various situations. We evaluated whether our method contributes to improving performance on text classification in three languages and machine translation in eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations.", "venue": "Findings", "year": 2021, "referenceCount": 38, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.21.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-26", "journal": {"name": "ArXiv", "volume": "abs/2105.12410"}, "authors": [{"authorId": "114789302", "name": "Tatsuya Hiraoka"}, {"authorId": "33544449", "name": "Sho Takase"}, {"authorId": "33658817", "name": "Kei Uchiumi"}, {"authorId": "46649024", "name": "Atsushi Keyaki"}, {"authorId": "1764004", "name": "Naoaki Okazaki"}]}, {"paperId": "83216b431d72cb5dcf852dfb6bc1fa60bb6dc55d", "externalIds": {"DBLP": "conf/acl/ZhangLHS21", "ACL": "2021.findings-acl.22", "DOI": "10.18653/v1/2021.findings-acl.22", "CorpusId": 236477936}, "corpusId": 236477936, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/83216b431d72cb5dcf852dfb6bc1fa60bb6dc55d", "title": "How does Attention Affect the Model?", "abstract": "The attention layer has become a prevalent component in improving the effectiveness of neural network models for NLP tasks. Figuring out why attention is effective and its interpretability has attracted a widespread deliberation. Current studies mostly investigate the effect of attention mechanism based on the attention distribution it generates with one single neural network structure. However they do not consider the changes in semantic capability of different components in the model due to the attention mechanism, which can vary across different network structures. In this paper, we propose a comprehensive analytical framework that exploits a convex hull representation of sequence semantics in an n-dimensional Semantic Euclidean Space and defines a series of indicators to capture the impact of attention on sequence semantics. Through a series of experiments on various NLP tasks and three representative recurrent units, we analyze why and how attention benefits the semantic capacity of different types of recurrent neural networks based on the indicators defined in the proposed framework.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "256-268"}, "authors": [{"authorId": "145107889", "name": "Chen Zhang"}, {"authorId": "2108273524", "name": "Qiuchi Li"}, {"authorId": "145014097", "name": "L. Hua"}, {"authorId": "48437245", "name": "D. Song"}]}, {"paperId": "8644924c73e9654b2899fddb9ed265424f33b363", "externalIds": {"ACL": "2021.findings-acl.23", "ArXiv": "2106.06965", "DBLP": "journals/corr/abs-2106-06965", "DOI": "10.18653/v1/2021.findings-acl.23", "CorpusId": 235422047}, "corpusId": 235422047, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8644924c73e9654b2899fddb9ed265424f33b363", "title": "Contrastive Attention for Automatic Chest X-ray Report Generation", "abstract": "Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into several existing models can boost their performance across most metrics. In addition, according to the analysis, the CA model can help existing models better attend to the abnormal regions and provide more accurate descriptions which are crucial for an interpretable diagnosis. Specifically, we achieve the state-of-the-art results on the two public datasets.", "venue": "Findings", "year": 2021, "referenceCount": 69, "citationCount": 60, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.23.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-13", "journal": {"pages": "269-280"}, "authors": [{"authorId": "1927674", "name": "Fenglin Liu"}, {"authorId": "1896799084", "name": "Changchang Yin"}, {"authorId": "144620586", "name": "Xian Wu"}, {"authorId": "36263371", "name": "Shen Ge"}, {"authorId": "2154286366", "name": "Ping Zhang"}, {"authorId": "11774802", "name": "Xu Sun"}]}, {"paperId": "87fa897b3d4a8ec1aa9a6ccdc342fbb9c96bcef7", "externalIds": {"DBLP": "journals/corr/abs-2108-02359", "ArXiv": "2108.02359", "ACL": "2021.findings-acl.24", "DOI": "10.18653/v1/2021.findings-acl.24", "CorpusId": 236478262}, "corpusId": 236478262, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/87fa897b3d4a8ec1aa9a6ccdc342fbb9c96bcef7", "title": "O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning", "abstract": "Video captioning combines video understanding and language generation. Different from image captioning that describes a static image with details of almost every object, video captioning usually considers a sequence of frames and biases towards focused objects, e.g., the objects that stay in focus regardless of the changing background. Therefore, detecting and properly accommodating focused objects is critical in video captioning. To enforce the description of focused objects and achieve controllable video captioning, we propose an Object-Oriented Non-Autoregressive approach (O2NA), which performs caption generation in three steps: 1) identify the focused objects and predict their locations in the target caption; 2) generate the related attribute words and relation words of these focused objects to form a draft caption; and 3) combine video information to refine the draft caption to a fluent final caption. Since the focused objects are generated and located ahead of other words, it is difficult to apply the word-by-word autoregressive generation process; instead, we adopt a non-autoregressive approach. The experiments on two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness of O2NA, which achieves results competitive with the state-of-the-arts but with both higher diversity and higher inference speed.", "venue": "Findings", "year": 2021, "referenceCount": 70, "citationCount": 20, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.24.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-05", "journal": {"pages": "281-292"}, "authors": [{"authorId": "1927674", "name": "Fenglin Liu"}, {"authorId": "19169659", "name": "Xuancheng Ren"}, {"authorId": "144620586", "name": "Xian Wu"}, {"authorId": "2115355581", "name": "Bang Yang"}, {"authorId": "36263371", "name": "Shen Ge"}, {"authorId": "11774802", "name": "Xu Sun"}]}, {"paperId": "95f8cf3dd2cd1d050d6b155e3c056f0e14f496b7", "externalIds": {"DBLP": "conf/acl/SrinivasanD21", "ACL": "2021.findings-acl.25", "DOI": "10.18653/v1/2021.findings-acl.25", "CorpusId": 236478135}, "corpusId": 236478135, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/95f8cf3dd2cd1d050d6b155e3c056f0e14f496b7", "title": "Better Chinese Sentence Segmentation with Reinforcement Learning", "abstract": "A long-standing challenge in Chinese\u2013English machine translation is that sentence boundaries are ambiguous in Chinese orthography, but inferring good splits is necessary for obtaining high quality translations. To solve this, we use reinforcement learning to train a segmentation policy that splits Chinese texts into segments that can be independently translated so as to maximise the overall translation quality. We compare to a variety of segmentation strategies and find that our approach improves the baseline BLEU score on the WMT2020 Chinese\u2013English news translation task by +0.3 BLEU overall and improves the score on input segments that contain more than 60 words by +3 BLEU.", "venue": "Findings", "year": 2021, "referenceCount": 27, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "293-302"}, "authors": [{"authorId": "2059763226", "name": "S. Srinivasan"}, {"authorId": "1745899", "name": "Chris Dyer"}]}, {"paperId": "6d2773a788067dcce7ac6a82019649528d341b4e", "externalIds": {"DBLP": "conf/acl/MinixhoferGI21", "ACL": "2021.findings-acl.26", "ArXiv": "2105.03791", "DOI": "10.18653/v1/2021.findings-acl.26", "CorpusId": 234339478}, "corpusId": 234339478, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6d2773a788067dcce7ac6a82019649528d341b4e", "title": "Enhancing Transformers with Gradient Boosted Decision Trees for NLI Fine-Tuning", "abstract": "Transfer learning has become the dominant paradigm for many natural language processing tasks. In addition to models being pretrained on large datasets, they can be further trained on intermediate (supervised) tasks that are similar to the target task. For small Natural Language Inference (NLI) datasets, language modelling is typically followed by pretraining on a large (labelled) NLI dataset before \ufb01ne-tuning with each NLI subtask. In this work, we explore Gradient Boosted Decision Trees (GBDTs) as an alternative to the commonly used Multi-Layer Perceptron (MLP) classi\ufb01cation head. GBDTs have de-sirable properties such as good performance on dense, numerical features and are effective where the ratio of the number of samples w.r.t the number of features is low. We then introduce FreeGBDT, a method of \ufb01tting a GBDT head on the features computed during \ufb01ne-tuning to increase performance without additional computation by the neural network. We demonstrate the effectiveness of our method on several NLI datasets using a strong baseline model (RoBERTa-large with MNLI pretrain-ing). The FreeGBDT shows a consistent improvement over the MLP classi\ufb01cation head.", "venue": "Findings", "year": 2021, "referenceCount": 60, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.26.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-08", "journal": {"pages": "303-313"}, "authors": [{"authorId": "2090357303", "name": "Benjamin Minixhofer"}, {"authorId": "22168669", "name": "Milan Gritta"}, {"authorId": "2676143", "name": "Ignacio Iacobacci"}]}, {"paperId": "008f036b32e9b118b78334553e9bc51176341543", "externalIds": {"DBLP": "conf/acl/NamyslBK21", "ArXiv": "2105.11872", "ACL": "2021.findings-acl.27", "DOI": "10.18653/v1/2021.findings-acl.27", "CorpusId": 235187329}, "corpusId": 235187329, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/008f036b32e9b118b78334553e9bc51176341543", "title": "Empirical Error Modeling Improves Robustness of Noisy Neural Sequence Labeling", "abstract": "Despite recent advances, standard sequence labeling systems often fail when processing noisy user-generated text or consuming the output of an Optical Character Recognition (OCR) process. In this paper, we improve the noise-aware training method by proposing an empirical error generation approach that employs a sequence-to-sequence model trained to perform translation from error-free to erroneous text. Using an OCR engine, we generated a large parallel text corpus for training and produced several real-world noisy sequence labeling benchmarks for evaluation. Moreover, to overcome the data sparsity problem that exacerbates in the case of imperfect textual input, we learned noisy language model-based embeddings. Our approach outperformed the baseline noise generation and error correction techniques on the erroneous sequence labeling data sets. To facilitate future research on robustness, we make our code, embeddings, and data conversion scripts publicly available.", "venue": "Findings", "year": 2021, "referenceCount": 53, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.27.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-25", "journal": {"pages": "314-329"}, "authors": [{"authorId": "134442417", "name": "Marcin Namysl"}, {"authorId": "1699019", "name": "Sven Behnke"}, {"authorId": "152162939", "name": "Joachim Kohler"}]}, {"paperId": "20d0564fd3fdbc24f266ca2076826a2271c3ea08", "externalIds": {"MAG": "3091226465", "ACL": "2021.findings-acl.28", "DBLP": "conf/acl/HwangYPYS21", "ArXiv": "2005.00642", "DOI": "10.18653/v1/2021.findings-acl.28", "CorpusId": 222103842}, "corpusId": 222103842, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/20d0564fd3fdbc24f266ca2076826a2271c3ea08", "title": "Spatial Dependency Parsing for Semi-Structured Document Information Extraction", "abstract": "Information Extraction (IE) for semi-structured document images is often approached as a sequence tagging problem by classifying each recognized input token into one of the IOB (Inside, Outside, and Beginning) categories. However, such problem setup has two inherent limitations that (1) it cannot easily handle complex spatial relationships and (2) it is not suitable for highly structured information, which are nevertheless frequently observed in real-world document images. To tackle these issues, we first formulate the IE task as spatial dependency parsing problem that focuses on the relationship among text segment nodes in the documents. Under this setup, we then propose SPADE (SPAtial DEpendency parser) that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner. We evaluate it on various kinds of documents such as receipts, name cards, forms, and invoices, and show that it achieves a similar or better performance compared to strong baselines including BERT-based IOB taggger, with up to 37.7% improvement.", "venue": "Findings", "year": 2020, "referenceCount": 37, "citationCount": 56, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.28.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-05-01", "journal": {"pages": "330-343"}, "authors": [{"authorId": "2054135353", "name": "Wonseok Hwang"}, {"authorId": "49841374", "name": "Jinyeong Yim"}, {"authorId": "7882243", "name": "Seunghyun Park"}, {"authorId": "16110760", "name": "Sohee Yang"}, {"authorId": "4418074", "name": "Minjoon Seo"}]}, {"paperId": "0558e6575cbed16a63761a906bbaf91c7843a78d", "externalIds": {"DBLP": "journals/corr/abs-2101-00294", "ACL": "2021.findings-acl.29", "DOI": "10.18653/v1/2021.findings-acl.29", "CorpusId": 230435683}, "corpusId": 230435683, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0558e6575cbed16a63761a906bbaf91c7843a78d", "title": "Reader-Guided Passage Reranking for Open-Domain Question Answering", "abstract": "Current open-domain question answering (QA) systems often follow a Retriever-Reader (R2) architecture, where the retriever \ufb01rst re-trieves relevant passages and the reader then reads the retrieved passages to form an answer. In this paper, we propose a simple and effective passage reranking method, R eader-gu IDE d R eranker (R IDER ), which does not involve any training and reranks the retrieved passages solely based on the top predictions of the reader before reranking. We show that R IDER , despite its simplicity, achieves 10 to 20 absolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) score gains without re\ufb01ning the retriever or reader. In particular, R IDER achieves 48.3 EM on the Natural Questions dataset and 66.4 on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are used as the reader input.", "venue": "Findings", "year": 2021, "referenceCount": 27, "citationCount": 24, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.29.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"name": "ArXiv", "volume": "abs/2101.00294"}, "authors": [{"authorId": "3375249", "name": "Yuning Mao"}, {"authorId": "2107782398", "name": "Pengcheng He"}, {"authorId": "2108860856", "name": "Xiaodong Liu"}, {"authorId": "1752875", "name": "Yelong Shen"}, {"authorId": "1800422", "name": "Jianfeng Gao"}, {"authorId": "153034701", "name": "Jiawei Han"}, {"authorId": "2109136147", "name": "Weizhu Chen"}]}, {"paperId": "366c40349b2813c4eb281b4747f5210f99a4d62e", "externalIds": {"DBLP": "conf/acl/ZhouRLSL21", "ACL": "2021.findings-acl.30", "DOI": "10.18653/v1/2021.findings-acl.30", "CorpusId": 236478143}, "corpusId": 236478143, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/366c40349b2813c4eb281b4747f5210f99a4d62e", "title": "Entity-Aware Abstractive Multi-Document Summarization", "abstract": ",", "venue": "Findings", "year": 2021, "referenceCount": 56, "citationCount": 18, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.30.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "351-362"}, "authors": [{"authorId": null, "name": "Hao Zhou"}, {"authorId": "2053308860", "name": "Weidong Ren"}, {"authorId": "150112803", "name": "Gongshen Liu"}, {"authorId": "153253583", "name": "Bo Su"}, {"authorId": "143844110", "name": "Wei Lu"}]}, {"paperId": "c30849528d4d75c514a5de7e01c0df25c6040803", "externalIds": {"DBLP": "journals/corr/abs-2106-00316", "ACL": "2021.findings-acl.31", "ArXiv": "2106.00316", "DOI": "10.18653/v1/2021.findings-acl.31", "CorpusId": 235265906}, "corpusId": 235265906, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c30849528d4d75c514a5de7e01c0df25c6040803", "title": "LenAtten: An Effective Length Controlling Unit For Text Summarization", "abstract": "Fixed length summarization aims at generating summaries with a preset number of words or characters. Most recent researches incorporate length information with word embeddings as the input to the recurrent decoding unit, causing a compromise between length controllability and summary quality. In this work, we present an effective length controlling unit Length Attention (LenAtten) to break this trade-off. Experimental results show that LenAtten not only brings improvements in length controllability and ROGUE scores but also has great generalization ability. In the task of generating a summary with the target length, our model is 732 times better than the best-performing length controllable summarizer in length controllability on the CNN/Daily Mail dataset.", "venue": "Findings", "year": 2021, "referenceCount": 22, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.31.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-01", "journal": {"pages": "363-370"}, "authors": [{"authorId": "2116680019", "name": "Zhongyi Yu"}, {"authorId": "7806992", "name": "Zhenghao Wu"}, {"authorId": null, "name": "Hao Zheng"}, {"authorId": "8666193", "name": "Zhe Xuanyuan"}, {"authorId": "2058160997", "name": "Jefferson Fong"}, {"authorId": "2239271", "name": "Weifeng Su"}]}, {"paperId": "c782d1f9ac3ead6f7b0952a4d1f9143e978f6b8f", "externalIds": {"ArXiv": "2105.02472", "DBLP": "journals/corr/abs-2105-02472", "ACL": "2021.findings-acl.32", "DOI": "10.18653/v1/2021.findings-acl.32", "CorpusId": 233864633}, "corpusId": 233864633, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c782d1f9ac3ead6f7b0952a4d1f9143e978f6b8f", "title": "XeroAlign: Zero-shot cross-lingual transformer alignment", "abstract": "The introduction of pretrained cross-lingual language models brought decisive improvements to multilingual NLP tasks. However, the lack of labelled task data necessitates a variety of methods aiming to close the gap to high-resource languages. Zero-shot methods in particular, often use translated task data as a training signal to bridge the performance gap between the source and target language(s). We introduce XeroAlign, a simple method for task-specific alignment of cross-lingual pretrained transformers such as XLM-R. XeroAlign uses translated task data to encourage the model to generate similar sentence embeddings for different languages. The XeroAligned XLM-R, called XLM-RA, shows strong improvements over the baseline models to achieve state-of-the-art zero-shot results on three multilingual natural language understanding tasks. XLM-RA's text classification accuracy exceeds that of XLM-R trained with labelled data and performs on par with state-of-the-art models on a cross-lingual adversarial paraphrasing task.", "venue": "Findings", "year": 2021, "referenceCount": 45, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.32.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-06", "journal": {"name": "ArXiv", "volume": "abs/2105.02472"}, "authors": [{"authorId": "22168669", "name": "Milan Gritta"}, {"authorId": "2676143", "name": "Ignacio Iacobacci"}]}, {"paperId": "23a812dde149021bab611ca4395279b9d41031f1", "externalIds": {"ACL": "2021.findings-acl.33", "DBLP": "conf/acl/Vera21", "DOI": "10.18653/v1/2021.findings-acl.33", "CorpusId": 236477847}, "corpusId": 236477847, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/23a812dde149021bab611ca4395279b9d41031f1", "title": "Using Word Embeddings to Analyze Teacher Evaluations: An Application to a Filipino Education Non-Profit Organization", "abstract": "Analysis of teacher evaluations is crucial to the development of robust educational programs, particularly through the validation of desirable qualities being reflected on in the text. This research applies Natural Language Processing techniques on a real-world dataset from a Filipino education non-profit to explore insights from analyzing evaluations written by Teacher Fellows who assess their own progress. Prior to this research, only qualitative assessment had been conducted on the text. Inspired by the use of word embedding similarities to capture semantic alignment, we utilize GloVe embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of Teacher Fellows and upholding the organization\u2019s Vision and Mission. As Fellows\u2019 quantitative ratings improved, so too did their demonstration of competency in the text. Further, Teacher Fellow language was consistent with the organization\u2019s Vision and Mission. This research therefore showcases the possibilities of NLP in education, improving our understanding of Teacher Fellow evaluations, which can lead to advances in program operations and education efforts.", "venue": "Findings", "year": 2021, "referenceCount": 12, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Education", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "382-389"}, "authors": [{"authorId": "2051972631", "name": "Francesca Vera"}]}, {"paperId": "97cbc8a78ad588931d7adfe319b4c68f3d167461", "externalIds": {"ACL": "2021.findings-acl.34", "DBLP": "journals/corr/abs-2105-08393", "ArXiv": "2105.08393", "DOI": "10.18653/v1/2021.findings-acl.34", "CorpusId": 234762987}, "corpusId": 234762987, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/97cbc8a78ad588931d7adfe319b4c68f3d167461", "title": "Relation Classification with Entity Type Restriction", "abstract": "Relation classification aims to predict a relation between two entities in a sentence. The existing methods regard all relations as the candidate relations for the two entities in a sentence. These methods neglect the restrictions on candidate relations by entity types, which leads to some inappropriate relations being candidate relations. In this paper, we propose a novel paradigm, RElation Classification with ENtity Type restriction (RECENT), which exploits entity types to restrict candidate relations. Specially, the mutual restrictions of relations and entity types are formalized and introduced into relation classification. Besides, the proposed paradigm, RECENT, is model-agnostic. Based on two representative models GCN and SpanBERT respectively, RECENT_GCN and RECENT_SpanBERT are trained in RECENT. Experimental results on a standard dataset indicate that RECENT improves the performance of GCN and SpanBERT by 6.9 and 4.4 F1 points, respectively. Especially, RECENT_SpanBERT achieves a new state-of-the-art on TACRED.", "venue": "Findings", "year": 2021, "referenceCount": 19, "citationCount": 46, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.34.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-18", "journal": {"pages": "390-395"}, "authors": [{"authorId": "1409427665", "name": "Shengfei Lyu"}, {"authorId": "2145302755", "name": "Huanhuan Chen"}]}, {"paperId": "9f863fcb229e762a95b8bb0e4a89d7aeeb3d8640", "externalIds": {"DBLP": "journals/corr/abs-2105-08476", "ArXiv": "2105.08476", "ACL": "2021.findings-acl.35", "DOI": "10.18653/v1/2021.findings-acl.35", "CorpusId": 234763248}, "corpusId": 234763248, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9f863fcb229e762a95b8bb0e4a89d7aeeb3d8640", "title": "Link Prediction on N-ary Relational Facts: A Graph-based Approach", "abstract": "Link prediction on knowledge graphs (KGs) is a key research topic. Previous work mainly focused on binary relations, paying less attention to higher-arity relations although they are ubiquitous in real-world KGs. This paper considers link prediction upon n-ary relational facts and proposes a graph-based approach to this task. The key to our approach is to represent the n-ary structure of a fact as a small heterogeneous graph, and model this graph with edge-biased fully-connected attention. The fully-connected attention captures universal inter-vertex interactions, while with edge-aware attentive biases to particularly encode the graph structure and its heterogeneity. In this fashion, our approach fully models global and local dependencies in each n-ary fact, and hence can more effectively capture associations therein. Extensive evaluation verifies the effectiveness and superiority of our approach. It performs substantially and consistently better than current state-of-the-art across a variety of n-ary relational benchmarks. Our code is publicly available.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 26, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.35.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-18", "journal": {"name": "ArXiv", "volume": "abs/2105.08476"}, "authors": [{"authorId": "143906199", "name": "Quan Wang"}, {"authorId": "144270731", "name": "Haifeng Wang"}, {"authorId": "8020700", "name": "Yajuan Lyu"}, {"authorId": "2116512598", "name": "Yong Zhu"}]}, {"paperId": "5fe78eb0f142902237df11cb67c455787a759172", "externalIds": {"ACL": "2021.findings-acl.36", "DBLP": "journals/corr/abs-2011-11928", "ArXiv": "2011.11928", "MAG": "3110048753", "DOI": "10.18653/v1/2021.findings-acl.36", "CorpusId": 227151534}, "corpusId": 227151534, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5fe78eb0f142902237df11cb67c455787a759172", "title": "GLGE: A New General Language Generation Evaluation Benchmark", "abstract": "Multi-task benchmarks such as GLUE and SuperGLUE have driven great progress of pretraining and transfer learning in Natural Language Processing (NLP). These benchmarks mostly focus on a range of Natural Language Understanding (NLU) tasks, without considering the Natural Language Generation (NLG) models. In this paper, we present the General Language Generation Evaluation (GLGE), a new multi-task benchmark for evaluating the generalization capabilities of NLG models across eight language generation tasks. For each task, we continue to design three subtasks in terms of task difficulty (GLGE-Easy, GLGE-Medium, and GLGE-Hard). This introduces 24 subtasks to comprehensively compare model performance. To encourage research on pretraining and transfer learning on NLG models, we make GLGE publicly available and build a leaderboard with strong baselines including MASS, BART, and ProphetNet\\footnote{The source code and dataset will be publicly available at this https URL.", "venue": "Findings", "year": 2020, "referenceCount": 55, "citationCount": 53, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.36.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-11-24", "journal": {"name": "ArXiv", "volume": "abs/2011.11928"}, "authors": [{"authorId": "7415571", "name": "Dayiheng Liu"}, {"authorId": "145967727", "name": "Yu Yan"}, {"authorId": "2171182", "name": "Yeyun Gong"}, {"authorId": "15629561", "name": "Weizhen Qi"}, {"authorId": "2119077859", "name": "Hang Zhang"}, {"authorId": "49097406", "name": "Jian Jiao"}, {"authorId": "2109136147", "name": "Weizhu Chen"}, {"authorId": "2089772402", "name": "Jie Fu"}, {"authorId": "24962156", "name": "Linjun Shou"}, {"authorId": "50175330", "name": "Ming Gong"}, {"authorId": "2108818254", "name": "Pengcheng Wang"}, {"authorId": "2108323525", "name": "Jiusheng Chen"}, {"authorId": "71790825", "name": "Daxin Jiang"}, {"authorId": "2053666", "name": "Jiancheng Lv"}, {"authorId": "2124601065", "name": "Ruofei Zhang"}, {"authorId": "2110027452", "name": "Winnie Wu"}, {"authorId": "92660691", "name": "Ming Zhou"}, {"authorId": "46429989", "name": "Nan Duan"}]}, {"paperId": "59c0076b3d814588e320820b95563965733d1875", "externalIds": {"ACL": "2021.findings-acl.37", "DBLP": "journals/corr/abs-2008-11869", "ArXiv": "2008.11869", "MAG": "3081031588", "DOI": "10.18653/v1/2021.findings-acl.37", "CorpusId": 221341000}, "corpusId": 221341000, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/59c0076b3d814588e320820b95563965733d1875", "title": "AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization", "abstract": "Pre-trained language models such as BERT have exhibited remarkable performances in many tasks in natural language understanding (NLU). The tokens in the models are usually fine-grained in the sense that for languages like English they are words or sub-words and for languages like Chinese they are characters. In English, for example, there are multi-word expressions which form natural lexical units and thus the use of coarse-grained tokenization also appears to be reasonable. In fact, both fine-grained and coarse-grained tokenizations have advantages and disadvantages for learning of pre-trained language models. In this paper, we propose a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained and coarse-grained tokenizations. For English, AMBERT takes both the sequence of words (fine-grained tokens) and the sequence of phrases (coarse-grained tokens) as input after tokenization, employs one encoder for processing the sequence of words and the other encoder for processing the sequence of the phrases, utilizes shared parameters between the two encoders, and finally creates a sequence of contextualized representations of the words and a sequence of contextualized representations of the phrases. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE. The results show that AMBERT outperforms the existing best performing models in almost all cases, particularly the improvements are significant for Chinese.", "venue": "Findings", "year": 2020, "referenceCount": 48, "citationCount": 38, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.37.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-08-27", "journal": {"pages": "421-435"}, "authors": [{"authorId": "47957145", "name": "Xinsong Zhang"}, {"authorId": "2145571830", "name": "Hang Li"}]}, {"paperId": "2da676d2b029fc96823c288b4d208d38f1fee96c", "externalIds": {"ArXiv": "2109.08478", "DBLP": "conf/acl/ChenMCLZ21", "ACL": "2021.findings-acl.38", "DOI": "10.18653/v1/2021.findings-acl.38", "CorpusId": 236478107}, "corpusId": 236478107, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/2da676d2b029fc96823c288b4d208d38f1fee96c", "title": "Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation", "abstract": "Visual dialogue is a challenging task since it needs to answer a series of coherent questions on the basis of understanding the visual environment. Previous studies focus on the implicit exploration of multimodal co-reference by implicitly attending to spatial image features or object-level image features but neglect the importance of locating the objects explicitly in the visual content, which is associated with entities in the textual content. Therefore, in this paper we propose a {\\bf M}ultimodal {\\bf I}ncremental {\\bf T}ransformer with {\\bf V}isual {\\bf G}rounding, named MITVG, which consists of two key parts: visual grounding and multimodal incremental transformer. Visual grounding aims to explicitly locate related objects in the image guided by textual entities, which helps the model exclude the visual content that does not need attention. On the basis of visual grounding, the multimodal incremental transformer encodes the multi-turn dialogue history combined with visual scene step by step according to the order of the dialogue and then generates a contextually and visually coherent response. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the proposed model, which achieves comparable performance.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 17, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.38.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-17", "journal": {"pages": "436-446"}, "authors": [{"authorId": null, "name": "Feilong Chen"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "30917866", "name": "Xiuyi Chen"}, {"authorId": "50492525", "name": "Peng Li"}, {"authorId": "2108485135", "name": "Jie Zhou"}]}, {"paperId": "273990598a6929bdc7c885f25bacfc6ebf51995e", "externalIds": {"DBLP": "conf/acl/LiYQY21", "ArXiv": "2106.02317", "ACL": "2021.findings-acl.39", "DOI": "10.18653/v1/2021.findings-acl.39", "CorpusId": 235352777}, "corpusId": 235352777, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/273990598a6929bdc7c885f25bacfc6ebf51995e", "title": "Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory", "abstract": "Dialogue policy learning, a subtask that determines the content of system response generation and then the degree of task completion, is essential for task-oriented dialogue systems. However, the unbalanced distribution of system actions in dialogue datasets often causes difficulty in learning to generate desired actions and responses. In this paper, we propose a retrieve-and-memorize framework to enhance the learning of system actions. Specially, we first design a neural context-aware retrieval module to retrieve multiple candidate system actions from the training set given a dialogue context. Then, we propose a memory-augmented multi-decoder network to generate the system actions conditioned on the candidate actions, which allows the network to adaptively select key information in the candidate actions and ignore noises. We conduct experiments on the large-scale multi-domain task-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1. Experimental results show that our method achieves competitive performance among several state-of-the-art models in the context-to-response generation task.", "venue": "Findings", "year": 2021, "referenceCount": 28, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"pages": "447-459"}, "authors": [{"authorId": "2135329729", "name": "Yunhao Li"}, {"authorId": "2116591052", "name": "Yunyi Yang"}, {"authorId": "38472218", "name": "Xiaojun Quan"}, {"authorId": "2155521332", "name": "Jianxing Yu"}]}, {"paperId": "cf5e670a79847d9be0eb185fb372d99d30d4d98f", "externalIds": {"DBLP": "journals/corr/abs-2106-13474", "ACL": "2021.findings-acl.40", "ArXiv": "2106.13474", "DOI": "10.18653/v1/2021.findings-acl.40", "CorpusId": 235652233}, "corpusId": 235652233, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/cf5e670a79847d9be0eb185fb372d99d30d4d98f", "title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains", "abstract": "Large pre-trained models have achieved great success in many natural language processing tasks. However, when they are applied in specific domains, these models suffer from domain shift and bring challenges in fine-tuning and online serving for latency and capacity constraints. In this paper, we present a general approach to developing small, fast and effective pre-trained models for specific domains. This is achieved by adapting the off-the-shelf general pre-trained models and performing task-agnostic knowledge distillation in target domains. Specifically, we propose domain-specific vocabulary expansion in the adaptation stage and employ corpus level occurrence probability to choose the size of incremental vocabulary automatically. Then we systematically explore different strategies to compress the large pre-trained models for specific domains. We conduct our experiments in the biomedical and computer science domain. The experimental results demonstrate that our approach achieves better performance over the BERT BASE model in domain-specific tasks while 3.3x smaller and 5.1x faster than BERT BASE. The code and pre-trained models are available at https://aka.ms/adalm.", "venue": "Findings", "year": 2021, "referenceCount": 22, "citationCount": 24, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.40.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-25", "journal": {"pages": "460-470"}, "authors": [{"authorId": "4841460", "name": "Yunzhi Yao"}, {"authorId": "3110003", "name": "Shaohan Huang"}, {"authorId": "51456429", "name": "Wenhui Wang"}, {"authorId": "145307652", "name": "Li Dong"}, {"authorId": "49807919", "name": "Furu Wei"}]}, {"paperId": "7dc43f7339e636ba49891732e3f20b3b377dfd78", "externalIds": {"ACL": "2021.findings-acl.41", "DBLP": "conf/acl/HanBC21", "DOI": "10.18653/v1/2021.findings-acl.41", "CorpusId": 236477648}, "corpusId": 236477648, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7dc43f7339e636ba49891732e3f20b3b377dfd78", "title": "Decoupling Adversarial Training for Fair NLP", "abstract": "Adversarial debiasing can help to learn fairer models. Previous work has assumed that both main task labels and protected attributes are available in the dataset. However, protected labels are often unavailable, or only available in limited numbers. In this paper, we propose a training strategy which needs only a small volume of protected labels in adversarial training, incorporating an estimation method to transfer private-labelled instances from one dataset to another. We demonstrate the in-and cross-domain effectiveness of our method through a range of experiments.", "venue": "Findings", "year": 2021, "referenceCount": 15, "citationCount": 18, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.41.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "471-477"}, "authors": [{"authorId": "2110982198", "name": "Xudong Han"}, {"authorId": "2059620776", "name": "Tim Baldwin"}, {"authorId": "1630460898", "name": "Trevor Cohn"}]}, {"paperId": "266bd8542c87be1030d578638dc1b4e793b5a091", "externalIds": {"DBLP": "conf/acl/GabrielCJCG21", "ACL": "2021.findings-acl.42", "ArXiv": "2010.12834", "DOI": "10.18653/v1/2021.findings-acl.42", "CorpusId": 225067529}, "corpusId": 225067529, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/266bd8542c87be1030d578638dc1b4e793b5a091", "title": "GO FIGURE: A Meta Evaluation of Factuality in Summarization", "abstract": "While neural language models can generate text with remarkable fluency and coherence, controlling for factual correctness in generation remains an open research question. This major discrepancy between the surface-level fluency and the content-level correctness of neural generation has motivated a new line of research that seeks automatic metrics for evaluating the factuality of machine text. In this paper, we introduce GO FIGURE, a meta-evaluation framework for evaluating factuality evaluation metrics. We propose five necessary and intuitive conditions to evaluate factuality metrics on diagnostic factuality data across three different summarization tasks. Our benchmark analysis on ten factuality metrics reveals that our meta-evaluation framework provides a robust and efficient evaluation that is extensible to multiple types of factual consistency and standard generation metrics, including QA metrics. It also reveals that while QA metrics generally improve over standard metrics that measure factuality across domains, performance is highly dependent on the way in which questions are generated.", "venue": "Findings", "year": 2020, "referenceCount": 39, "citationCount": 69, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.42.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-10-24", "journal": {"pages": "478-487"}, "authors": [{"authorId": "119902504", "name": "Saadia Gabriel"}, {"authorId": "1709797", "name": "Asli Celikyilmaz"}, {"authorId": "144598922", "name": "Rahul Jha"}, {"authorId": "1699545", "name": "Yejin Choi"}, {"authorId": "1800422", "name": "Jianfeng Gao"}]}, {"paperId": "727673f05b65bc842701f69a8c2d8c09b9042f99", "externalIds": {"ACL": "2021.findings-acl.43", "DBLP": "conf/acl/AhmedCWNLD21", "DOI": "10.18653/v1/2021.findings-acl.43", "CorpusId": 234797920}, "corpusId": 234797920, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/727673f05b65bc842701f69a8c2d8c09b9042f99", "title": "DNN-driven Gradual Machine Learning for Aspect-term Sentiment Analysis", "abstract": "Recent work has shown that Aspect-Term Sentiment Analysis (ATSA) can be performed by Gradual Machine Learning (GML), which begins with some automatically labeled easy instances, and then gradually labels more challenging instances by iterative factor graph inference without manual intervention. As a non-i.i.d learning paradigm, GML leverages shared features between labeled and unlabeled instances for knowledge conveyance. However, the existing GML solution extracts sentiment features based on pre-specified lexicons, which are usually inaccurate and incomplete and thus lead to inadequate knowledge conveyance. In this paper, we propose a Deep Neural Network (DNN) driven GML approach for ATSA, which exploits the power of DNN in feature representation for gradual learning. It first uses an unsupervised neural network to cluster the automatically extracted features by their sentiment orientation. Then, it models the clustered features as factors to enable implicit knowledge conveyance for gradual inference in a factor graph. To leverage labeled training data, we also present a hybrid solution that fulfills gradual learning by fusing the influence of supervised DNN predictions and implicit knowledge conveyance in a unified factor graph. Finally, we empirically evaluate the performance of the proposed approach on real benchmark data. Our extensive experiments have shown that the proposed approach consistently achieves the state-of-the-art performance across all the test datasets in both unsupervised and supervised settings and the improvement margins are considerable.", "venue": "Findings", "year": 2021, "referenceCount": 28, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.43.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "488-497"}, "authors": [{"authorId": "2115208866", "name": "Murtadha Ahmed"}, {"authorId": "2109396478", "name": "Qun Chen"}, {"authorId": "2136909001", "name": "Yanyan Wang"}, {"authorId": "40896166", "name": "Youcef Nafa"}, {"authorId": "3216007", "name": "Zhanhuai Li"}, {"authorId": "15069771", "name": "Tianyi Duan"}]}, {"paperId": "db899c6191e25818091d4bab1c2da88689554e7f", "externalIds": {"DBLP": "journals/corr/abs-2109-01754", "ArXiv": "2109.01754", "ACL": "2021.findings-acl.44", "DOI": "10.18653/v1/2021.findings-acl.44", "CorpusId": 236477472}, "corpusId": 236477472, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/db899c6191e25818091d4bab1c2da88689554e7f", "title": "Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models", "abstract": "Large-scale conversational assistants like Alexa, Siri, Cortana and Google Assistant process every utterance using multiple models for domain, intent and named entity recognition. Given the decoupled nature of model development and large traffic volumes, it is extremely difficult to identify utterances processed erroneously by such systems. We address this challenge to detect domain classification errors using offline Transformer models. We combine utterance encodings from a RoBERTa model with the Nbest hypothesis produced by the production system. We then fine-tune end-to-end in a multitask setting using a small dataset of humanannotated utterances with domain classification errors. We tested our approach for detecting misclassifications from one domain that accounts for <0.5% of the traffic in a large-scale conversational AI system. Our approach achieves an F1 score of 30% outperforming a biLSTM baseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this further by 2.2% to 32.2% by ensembling multiple models.", "venue": "Findings", "year": 2021, "referenceCount": 17, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.44.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-04", "journal": {"name": "ArXiv", "volume": "abs/2109.01754"}, "authors": [{"authorId": "147887099", "name": "Rakesh Chada"}, {"authorId": "49824581", "name": "P. Natarajan"}, {"authorId": "2121339589", "name": "Darshan Fofadiya"}, {"authorId": "2121359655", "name": "Prathap Ramachandra"}]}, {"paperId": "26c6815536cfe65f12f61a39b239fc31bc3668c3", "externalIds": {"ACL": "2021.findings-acl.45", "DBLP": "conf/acl/ChoiSKS21", "ArXiv": "2105.05601", "DOI": "10.18653/v1/2021.findings-acl.45", "CorpusId": 236478377}, "corpusId": 236478377, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/26c6815536cfe65f12f61a39b239fc31bc3668c3", "title": "OutFlip: Generating Examples for Unknown Intent Detection with Natural Language Attack", "abstract": "Out-of-domain (OOD) input detection is vital in a task-oriented dialogue system since the acceptance of unsupported inputs could lead to an incorrect response of the system. This paper proposes OutFlip, a method to generate out-of-domain samples using only in-domain training dataset automatically. A white-box natural language attack method HotFlip is revised to generate out-of-domain samples instead of adversarial examples. Our evaluation results showed that integrating OutFlip-generated out-of-domain samples into the training dataset could significantly improve an intent classification model's out-of-domain detection performance.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.45.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-12", "journal": {"name": "ArXiv", "volume": "abs/2105.05601"}, "authors": [{"authorId": "2091774270", "name": "Donghyun Choi"}, {"authorId": "2067943846", "name": "M. Shin"}, {"authorId": "2210474389", "name": "EungGyun Kim"}, {"authorId": "2149548470", "name": "Dong Ryeol Shin"}]}, {"paperId": "291133a657498920451481d3bf784ebbafda8d6e", "externalIds": {"ArXiv": "2105.14517", "DBLP": "conf/acl/ChenTQLLXL21", "ACL": "2021.findings-acl.46", "DOI": "10.18653/v1/2021.findings-acl.46", "CorpusId": 235253782}, "corpusId": 235253782, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/291133a657498920451481d3bf784ebbafda8d6e", "title": "GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning", "abstract": "Automatic math problem solving has recently attracted increasing attention as a long-standing AI benchmark. In this paper, we focus on solving geometric problems, which requires a comprehensive understanding of textual descriptions, visual diagrams, and theorem knowledge. However, the existing methods were highly dependent on handcraft rules and were merely evaluated on small-scale datasets. Therefore, we propose a Geometric Question Answering dataset GeoQA, containing 4,998 geometric problems with corresponding annotated programs, which illustrate the solving process of the given problems. Compared with another publicly available dataset GeoS, GeoQA is 25 times larger, in which the program annotations can provide a practical testbed for future research on explicit and explainable numerical reasoning. Moreover, we introduce a Neural Geometric Solver (NGS) to address geometric problems by comprehensively parsing multimodal information and generating interpretable programs. We further add multiple self-supervised auxiliary tasks on NGS to enhance cross-modal semantic representation. Extensive experiments on GeoQA validate the effectiveness of our proposed NGS and auxiliary tasks. However, the results are still significantly lower than human performance, which leaves large room for future research. Our benchmark and code are released at https://github.com/chen-judge/GeoQA .", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 29, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.46.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-30", "journal": {"name": "ArXiv", "volume": "abs/2105.14517"}, "authors": [{"authorId": "2108234021", "name": "Jiaqi Chen"}, {"authorId": "66239746", "name": "Jianheng Tang"}, {"authorId": "9102638", "name": "Jinghui Qin"}, {"authorId": "40250403", "name": "Xiaodan Liang"}, {"authorId": "1391190009", "name": "Lingbo Liu"}, {"authorId": "143977260", "name": "E. Xing"}, {"authorId": "1737218", "name": "Liang Lin"}]}, {"paperId": "0df00857af8c2d2c17b368a8008b965243322924", "externalIds": {"ArXiv": "2106.01709", "DBLP": "conf/acl/ZengWC21", "ACL": "2021.findings-acl.47", "DOI": "10.18653/v1/2021.findings-acl.47", "CorpusId": 235313883}, "corpusId": 235313883, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0df00857af8c2d2c17b368a8008b965243322924", "title": "SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction", "abstract": "Document-level relation extraction has attracted much attention in recent years. It is usually formulated as a classification problem that predicts relations for all entity pairs in the document. However, previous works indiscriminately represent intra- and inter-sentential relations in the same way, confounding the different patterns for predicting them. Besides, they create a document graph and use paths between entities on the graph as clues for logical reasoning. However, not all entity pairs can be connected with a path and have the correct logical reasoning paths in their graph. Thus many cases of logical reasoning cannot be covered. This paper proposes an effective architecture, SIRE, to represent intra- and inter-sentential relations in different ways. We design a new and straightforward form of logical reasoning module that can cover more logical reasoning chains. Experiments on the public datasets show SIRE outperforms the previous state-of-the-art methods. Further analysis shows that our predictions are reliable and explainable. Our code is available at https://github.com/DreamInvoker/SIRE.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 39, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.47.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01709"}, "authors": [{"authorId": "48486877", "name": "Shuang Zeng"}, {"authorId": "3028245", "name": "Yuting Wu"}, {"authorId": "39488576", "name": "Baobao Chang"}]}, {"paperId": "4a7184a8e7a3a944c20c971d9e4523166c290fbc", "externalIds": {"MAG": "3166665573", "DBLP": "journals/corr/abs-2106-00459", "ACL": "2021.findings-acl.48", "ArXiv": "2106.00459", "DOI": "10.18653/v1/2021.findings-acl.48", "CorpusId": 235266074}, "corpusId": 235266074, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4a7184a8e7a3a944c20c971d9e4523166c290fbc", "title": "KGPool: Dynamic Knowledge Graph Context Selection for Relation Extraction", "abstract": "We present a novel method for relation extraction (RE) from a single sentence, mapping the sentence and two given entities to a canonical fact in a knowledge graph (KG). Especially in this presumed sentential RE setting, the context of a single sentence is often sparse. This paper introduces the KGPool method to address this sparsity, dynamically expanding the context with additional facts from the KG. It learns the representation of these facts (entity alias, entity descriptions, etc.) using neural methods, supplementing the sentential context. Unlike existing methods that statically use all expanded facts, KGPool conditions this expansion on the sentence. We study the efficacy of KGPool by evaluating it with different neural models and KGs (Wikidata and NYT Freebase). Our experimental evaluation on standard datasets shows that by feeding the KGPool representation into a Graph Neural Network, the overall method is significantly more accurate than state-of-the-art methods.", "venue": "Findings", "year": 2021, "referenceCount": 43, "citationCount": 21, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.48.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-01", "journal": {"name": "ArXiv", "volume": "abs/2106.00459"}, "authors": [{"authorId": "1380871402", "name": "Abhishek Nadgeri"}, {"authorId": "46182569", "name": "Anson Bastos"}, {"authorId": "145447998", "name": "Kuldeep Singh"}, {"authorId": "30523699", "name": "I. Mulang'"}, {"authorId": "1727527", "name": "Johannes Hoffart"}, {"authorId": "2956693", "name": "Saeedeh Shekarpour"}, {"authorId": "1714061", "name": "V. Saraswat"}]}, {"paperId": "c1a84ed0c78bb7e8d063f35802313b1f701beffe", "externalIds": {"ACL": "2021.findings-acl.49", "DBLP": "conf/acl/FeiWRLJ21", "DOI": "10.18653/v1/2021.findings-acl.49", "CorpusId": 236478299}, "corpusId": 236478299, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c1a84ed0c78bb7e8d063f35802313b1f701beffe", "title": "Better Combine Them Together! Integrating Syntactic Constituency and Dependency Representations for Semantic Role Labeling", "abstract": "Structural syntax knowledge has been proven effective for semantic role labeling (SRL), while existing works mostly use only one singleton syntax, such as either syntactic dependency or constituency tree. In this paper, we explore the integration of heterogeneous syntactic representations for SRL. We \ufb01rst consider a TreeLSTM-based integration, collaboratively learning the phrasal boundaries from the constituency and the semantic relations from dependency. We further introduce a label-aware GCN solution for simultaneously modeling the syntactic edges and labels. Experimental results demonstrate that by effectively combining the heterogeneous syntactic representations, our methods yield task improvements on both span-based and dependency-based SRL. Also our system achieves new state-of-the-art SRL performances, meanwhile bringing explainable task improvements.", "venue": "Findings", "year": 2021, "referenceCount": 55, "citationCount": 36, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.49.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "549-559"}, "authors": [{"authorId": "46959445", "name": "Hao Fei"}, {"authorId": "1957924118", "name": "Shengqiong Wu"}, {"authorId": "3350168", "name": "Yafeng Ren"}, {"authorId": "2109530930", "name": "Fei Li"}, {"authorId": "1719916", "name": "D. Ji"}]}, {"paperId": "b70d7de2690137986aff7895b763d3cc69c92766", "externalIds": {"DBLP": "conf/acl/SuVBWC21", "ACL": "2021.findings-acl.50", "DOI": "10.18653/v1/2021.findings-acl.50", "CorpusId": 236477430}, "corpusId": 236477430, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b70d7de2690137986aff7895b763d3cc69c92766", "title": "Keep the Primary, Rewrite the Secondary: A Two-Stage Approach for Paraphrase Generation", "abstract": "Paraphrase generation is an important and challenging NLG problem. In this work, we propose a new Identi\ufb01cation-then-Aggregation (IA) framework to tackle this task. In the identi\ufb01cation step, the input tokens are sorted into two groups by a novel Primary/Secondary Identi\ufb01cation (PSI) algorithm. In the aggregation step, these groups are separately encoded, before being aggregated by a custom designed decoder, which autoregressively generates the paraphrased sentence. In extensive experiments on two benchmark datasets, we demonstrate that our model outperforms previous studies by a notable margin. We also show that the proposed approach can generate paraphrases in an interpretable and controllable way.", "venue": "Findings", "year": 2021, "referenceCount": 27, "citationCount": 14, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.50.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "560-569"}, "authors": [{"authorId": "50087162", "name": "Yixuan Su"}, {"authorId": "92480907", "name": "David Vandyke"}, {"authorId": "2105669418", "name": "Simon Baker"}, {"authorId": "2152546690", "name": "Yan Wang"}, {"authorId": "50638196", "name": "Nigel Collier"}]}, {"paperId": "f7bd23eaf95aad340fc87caaa5674bb4f1930184", "externalIds": {"DBLP": "conf/acl/MaSA21", "ArXiv": "2105.12932", "ACL": "2021.findings-acl.51", "DOI": "10.18653/v1/2021.findings-acl.51", "CorpusId": 235212057}, "corpusId": 235212057, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f7bd23eaf95aad340fc87caaa5674bb4f1930184", "title": "Contrastive Fine-tuning Improves Robustness for Neural Rankers", "abstract": "The performance of state-of-the-art neural rankers can deteriorate substantially when exposed to noisy inputs or applied to a new domain. In this paper, we present a novel method for fine-tuning neural rankers that can significantly improve their robustness to out-of-domain data and query perturbations. Specifically, a contrastive loss that compares data points in the representation space is combined with the standard ranking loss during fine-tuning. We use relevance labels to denote similar/dissimilar pairs, which allows the model to learn the underlying matching semantics across different query-document pairs and leads to improved robustness. In experiments with four passage ranking datasets, the proposed contrastive fine-tuning method obtains improvements on robustness to query reformulations, noise perturbations, and zero-shot transfer for both BERT and BART based rankers. Additionally, our experiments show that contrastive fine-tuning outperforms data augmentation for robustifying neural rankers.", "venue": "Findings", "year": 2021, "referenceCount": 55, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.51.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-27", "journal": {"pages": "570-582"}, "authors": [{"authorId": "47646605", "name": "Xiaofei Ma"}, {"authorId": "1790831", "name": "C. D. Santos"}, {"authorId": "2112031035", "name": "Andrew O. Arnold"}]}, {"paperId": "791f31b7ea4976f71dfd63783f6c12def8fbebcc", "externalIds": {"ACL": "2021.findings-acl.52", "DBLP": "journals/corr/abs-2010-09828", "MAG": "3093497039", "ArXiv": "2010.09828", "DOI": "10.18653/v1/2021.findings-acl.52", "CorpusId": 224803418}, "corpusId": 224803418, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/791f31b7ea4976f71dfd63783f6c12def8fbebcc", "title": "Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking", "abstract": "Cross-language entity linking grounds mentions in multiple languages to a single-language knowledge base. We propose a neural ranking architecture for this task that uses multilingual BERT representations of the mention and the context in a neural network. We find that the multilingual ability of BERT leads to robust performance in monolingual and multilingual settings. Furthermore, we explore zero-shot language transfer and find surprisingly robust performance. We investigate the zero-shot degradation and find that it can be partially mitigated by a proposed auxiliary training objective, but that the remaining error can best be attributed to domain shift rather than language transfer.", "venue": "Findings", "year": 2020, "referenceCount": 32, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.52.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-10-19", "journal": {"name": "ArXiv", "volume": "abs/2010.09828"}, "authors": [{"authorId": "28298566", "name": "Elliot Schumacher"}, {"authorId": "145510552", "name": "J. Mayfield"}, {"authorId": "1782853", "name": "Mark Dredze"}]}, {"paperId": "3eeedb6651a629a105c1185ada862e2cad7a0522", "externalIds": {"DBLP": "conf/acl/LalCMB21", "ACL": "2021.findings-acl.53", "ArXiv": "2106.06132", "DOI": "10.18653/v1/2021.findings-acl.53", "CorpusId": 235417184}, "corpusId": 235417184, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/3eeedb6651a629a105c1185ada862e2cad7a0522", "title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives", "abstract": "Answering questions about why characters perform certain actions is central to understanding and reasoning about narratives. Despite recent progress in QA, it is not clear if existing models have the ability to answer\"why\"questions that may require commonsense knowledge external to the input narrative. In this work, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more than 30k questions and free-form answers concerning why characters in short narratives perform the actions described. For a third of this dataset, the answers are not present within the narrative. Given the limitations of automated evaluation for this task, we also present a systematized human evaluation interface for this dataset. Our evaluation of state-of-the-art models show that they are far below human performance on answering such questions. They are especially worse on questions whose answers are external to the narrative, thus providing a challenge for future QA and narrative understanding research.", "venue": "Findings", "year": 2021, "referenceCount": 58, "citationCount": 21, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.53.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-11", "journal": {"name": "ArXiv", "volume": "abs/2106.06132"}, "authors": [{"authorId": "27604402", "name": "Yash Kumar Lal"}, {"authorId": "1729918", "name": "Nathanael Chambers"}, {"authorId": "1797655", "name": "R. Mooney"}, {"authorId": "35217367", "name": "Niranjan Balasubramanian"}]}, {"paperId": "b7a64a22f69a17ea0b007f7748580f7641b55cb6", "externalIds": {"ACL": "2021.findings-acl.54", "DBLP": "conf/acl/ShusterUDSW21", "DOI": "10.18653/v1/2021.findings-acl.54", "CorpusId": 236478290}, "corpusId": 236478290, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b7a64a22f69a17ea0b007f7748580f7641b55cb6", "title": "Dialogue in the Wild: Learning from a Deployed Role-Playing Game with Humans and Bots", "abstract": "Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more ef\ufb01cient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.54.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "611-624"}, "authors": [{"authorId": "35752280", "name": "Kurt Shuster"}, {"authorId": "39219656", "name": "Jack Urbanek"}, {"authorId": "31461304", "name": "Emily Dinan"}, {"authorId": "3149531", "name": "Arthur Szlam"}, {"authorId": "145183709", "name": "J. Weston"}]}, {"paperId": "6179ebc6e8d535513a71e9db9faabdbdcf96a075", "externalIds": {"ACL": "2021.findings-acl.55", "DBLP": "conf/acl/CasanovaGCSGSLC21", "DOI": "10.18653/v1/2021.findings-acl.55", "CorpusId": 236477782}, "corpusId": 236477782, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6179ebc6e8d535513a71e9db9faabdbdcf96a075", "title": "Deep Learning against COVID-19: Respiratory Insufficiency Detection in Brazilian Portuguese Speech", "abstract": "Respiratory insufficiency is a symptom that requires hospitalization. This work investigates whether it is possible to detect this condition by analyzing patient\u2019s speech samples; the analysis was performed on data collected during the first wave of the COVID-19 pandemic in 2020, and thus limited to respiratory insufficiency in COVID-19 patients. For that, a dataset was created consisting of speech emissions of both COVID-19 patients affected by respiratory insufficiency and a control group. This dataset was used to build a Convolution Neural Network to detect respiratory insufficiency using speech emission MFCC representations. Methodologically, dealing with background noise was a challenge, so we also collected background noise from COVID-19 wards where patients were located. Due to the difficulty in filtering noise without eliminating crucial information, noise samples were injected in the control group data to prevent bias. Moreover, we investigated (i) two approaches to address the duration variance of audios, and (ii) the ideal number of noise samples to inject in both patients and the control group to prevent bias and overfitting. The techniques developed reached 91.66% accuracy. Thus we validated the project\u2019s Leading Hypothesis, namely that it is possible to detect respiratory insufficiency in speech utterances, under real-life environmental conditions; we believe our results justify further enquiries into the use of automated speech analysis to support health professionals in triage procedures.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.55.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "625-633"}, "authors": [{"authorId": "1504370087", "name": "Edresson Casanova"}, {"authorId": "2097647981", "name": "L. Gris"}, {"authorId": "2219676670", "name": "Augusto Camargo Neto"}, {"authorId": "2151128193", "name": "Daniel Peixoto Pinto da Silva"}, {"authorId": "2121361268", "name": "M. Gazzola"}, {"authorId": "144348241", "name": "E. Sabino"}, {"authorId": "47251670", "name": "A. Levin"}, {"authorId": "35181405", "name": "Arnaldo Candido"}, {"authorId": "1718246", "name": "S. Alu\u00edsio"}, {"authorId": "52195533", "name": "M. Finger"}]}, {"paperId": "9da6e38f5124b3bb55839e0ad42e1ddcc50f840b", "externalIds": {"MAG": "3023532425", "ACL": "2021.findings-acl.56", "DBLP": "conf/acl/SiYCMLW21", "ArXiv": "2004.14004", "DOI": "10.18653/v1/2021.findings-acl.56", "CorpusId": 216641718}, "corpusId": 216641718, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9da6e38f5124b3bb55839e0ad42e1ddcc50f840b", "title": "Benchmarking Robustness of Machine Reading Comprehension Models", "abstract": "Machine Reading Comprehension (MRC) is an important testbed for evaluating models' natural language understanding (NLU) ability. There has been rapid progress in this area, with new models achieving impressive performance on various MRC benchmarks. However, most of these benchmarks only evaluate models on in-domain test sets without considering their robustness under test-time perturbations. To fill this important gap, we construct AdvRACE (Adversarial RACE), a new model-agnostic benchmark for evaluating the robustness of MRC models under six different types of test-time perturbations, including our novel superimposed attack and distractor construction attack. We show that current state-of-the-art (SOTA) models are vulnerable to these simple black-box attacks. Our benchmark is constructed automatically based on the existing RACE benchmark, and thus the construction pipeline can be easily adopted by other tasks and datasets. We will release the data and source codes to facilitate future work. We hope that our work will encourage more research on improving the robustness of MRC and other NLU models.", "venue": "Findings", "year": 2020, "referenceCount": 43, "citationCount": 35, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.56.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-04-29", "journal": {"pages": "634-644"}, "authors": [{"authorId": "152358188", "name": "Chenglei Si"}, {"authorId": "48599077", "name": "Ziqing Yang"}, {"authorId": "3043830", "name": "Yiming Cui"}, {"authorId": "145910725", "name": "Wentao Ma"}, {"authorId": "40282288", "name": "Ting Liu"}, {"authorId": "2108620507", "name": "Shijin Wang"}]}, {"paperId": "154b27e548db8a9998b867971794e0615322b0d1", "externalIds": {"DBLP": "conf/acl/LiZLXC21", "ACL": "2021.findings-acl.57", "ArXiv": "2012.15150", "DOI": "10.18653/v1/2021.findings-acl.57", "CorpusId": 229923035}, "corpusId": 229923035, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/154b27e548db8a9998b867971794e0615322b0d1", "title": "Improving BERT with Syntax-aware Local Attention", "abstract": "Pre-trained Transformer-based neural language models, such as BERT, have achieved remarkable results on varieties of NLP tasks. Recent works have shown that attention-based models can benefit from more focused attention over local regions. Most of them restrict the attention scope within a linear span, or confine to certain tasks such as machine translation and question answering. In this paper, we propose a syntax-aware local attention, where the attention scopes are restrained based on the distances in the syntactic structure. The proposed syntax-aware local attention can be integrated with pretrained language models, such as BERT, to render the model to focus on syntactically relevant words. We conduct experiments on various single-sentence benchmarks, including sentence classification and sequence labeling tasks. Experimental results show consistent gains over BERT on all benchmark datasets. The extensive studies verify that our model achieves better performance owing to more focused attention over syntactically relevant words.", "venue": "Findings", "year": 2020, "referenceCount": 23, "citationCount": 33, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.57.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-30", "journal": {"pages": "645-653"}, "authors": [{"authorId": "2017913223", "name": "Zhongli Li"}, {"authorId": "50656875", "name": "Qingyu Zhou"}, {"authorId": "2150359436", "name": "Chao Li"}, {"authorId": "2087261268", "name": "Ke Xu"}, {"authorId": "2154235", "name": "Yunbo Cao"}]}, {"paperId": "65bb703c67822be2f201d2b1fcc1681e580c0861", "externalIds": {"ArXiv": "2107.05866", "ACL": "2021.findings-acl.58", "DBLP": "conf/acl/PengZYMCWXWL21", "DOI": "10.18653/v1/2021.findings-acl.58", "CorpusId": 235829168}, "corpusId": 235829168, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/65bb703c67822be2f201d2b1fcc1681e580c0861", "title": "A Dialogue-based Information Extraction System for Medical Insurance Assessment", "abstract": "In the Chinese medical insurance industry, the assessor's role is essential and requires significant efforts to converse with the claimant. This is a highly professional job that involves many parts, such as identifying personal information, collecting related evidence, and making a final insurance report. Due to the coronavirus (COVID-19) pandemic, the previous offline insurance assessment has to be conducted online. However, for the junior assessor often lacking practical experience, it is not easy to quickly handle such a complex online procedure, yet this is important as the insurance company needs to decide how much compensation the claimant should receive based on the assessor's feedback. In order to promote assessors' work efficiency and speed up the overall procedure, in this paper, we propose a dialogue-based information extraction system that integrates advanced NLP technologies for medical insurance assessment. With the assistance of our system, the average time cost of the procedure is reduced from 55 minutes to 35 minutes, and the total human resources cost is saved 30% compared with the previous offline procedure. Until now, the system has already served thousands of online claim cases. \u00a9 2021 Association for Computational Linguistics", "venue": "Findings", "year": 2021, "referenceCount": 25, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.58.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-13", "journal": {"pages": "654-663"}, "authors": [{"authorId": "51250063", "name": "Shuang Peng"}, {"authorId": "2112494753", "name": "Mengdi Zhou"}, {"authorId": "1830455541", "name": "Minghui Yang"}, {"authorId": "2013337", "name": "Haitao Mi"}, {"authorId": "2105817", "name": "Shaosheng Cao"}, {"authorId": "1515550185", "name": "Zujie Wen"}, {"authorId": "2119248868", "name": "Teng Xu"}, {"authorId": "2120853132", "name": "Hongbin Wang"}, {"authorId": "2150801362", "name": "Lei Liu"}]}, {"paperId": "9578cd3422eba0d2ce7fccd4b93c190c0b318efd", "externalIds": {"DBLP": "journals/corr/abs-2106-02399", "ArXiv": "2106.02399", "ACL": "2021.findings-acl.59", "DOI": "10.18653/v1/2021.findings-acl.59", "CorpusId": 235352906}, "corpusId": 235352906, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9578cd3422eba0d2ce7fccd4b93c190c0b318efd", "title": "Prediction or Comparison: Toward Interpretable Qualitative Reasoning", "abstract": "Qualitative relationships illustrate how changing one property (e.g., moving velocity) affects another (e.g., kinetic energy) and constitutes a considerable portion of textual knowledge. Current approaches use either semantic parsers to transform natural language inputs into logical expressions or a\"black-box\"model to solve them in one step. The former has a limited application range, while the latter lacks interpretability. In this work, we categorize qualitative reasoning tasks into two types: prediction and comparison. In particular, we adopt neural network modules trained in an end-to-end manner to simulate the two reasoning processes. Experiments on two qualitative reasoning question answering datasets, QuaRTz and QuaRel, show our methods' effectiveness and generalization capability, and the intermediate outputs provided by the modules make the reasoning process interpretable.", "venue": "Findings", "year": 2021, "referenceCount": 28, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.59.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02399"}, "authors": [{"authorId": "1380343020", "name": "Mucheng Ren"}, {"authorId": "4590286", "name": "Heyan Huang"}, {"authorId": "145644809", "name": "Yang Gao"}]}, {"paperId": "02c687f366e8d5e622dc39d967995495bdb99bd8", "externalIds": {"ACL": "2021.findings-acl.60", "DBLP": "conf/acl/LiGFZW21", "DOI": "10.18653/v1/2021.findings-acl.60", "CorpusId": 236478215}, "corpusId": 236478215, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/02c687f366e8d5e622dc39d967995495bdb99bd8", "title": "Boundary Detection with BERT for Span-level Emotion Cause Analysis", "abstract": "Emotion cause analysis (ECA) has been an emerging topic in natural language processing, which aims to identify the reasons behind a certain emotion expressed in the text. Most ECA methods intend to identify the clause which contains the cause of a given emotion, but such clause-level ECA (CECA) can be am-biguous and imprecise. In this paper, we aim at span-level ECA (SECA) by detecting the precise boundaries of text spans conveying ac-curate emotion causes from the given context. We formulate this task as sequence labeling and position identi\ufb01cation problems and design two neural methods to solve them. Experiments on two benchmark ECA datasets show that the proposed methods substantially outperform the existing ECA models 1 .", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 17, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.60.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "676-682"}, "authors": [{"authorId": "50079896", "name": "Xiangju Li"}, {"authorId": "145816335", "name": "Wei Gao"}, {"authorId": "144588144", "name": "Shi Feng"}, {"authorId": "2108463824", "name": "Yifei Zhang"}, {"authorId": "2111226672", "name": "Daling Wang"}]}, {"paperId": "07b95736960731b49b6ce5aa0b29f10bd0586a6d", "externalIds": {"DBLP": "conf/acl/CuiCWZ21", "ACL": "2021.findings-acl.61", "ArXiv": "2008.03945", "DOI": "10.18653/v1/2021.findings-acl.61", "CorpusId": 235436397}, "corpusId": 235436397, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/07b95736960731b49b6ce5aa0b29f10bd0586a6d", "title": "On Commonsense Cues in BERT for Solving Commonsense Tasks", "abstract": "BERT has been used for solving commonsense tasks such as CommonsenseQA. While prior research has found that BERT does contain commonsense information to some extent, there has been work showing that pre-trained models can rely on spurious associations (e.g., data bias) rather than key cues in solving sentiment classification and other problems. We quantitatively investigate the presence of structural commonsense cues in BERT when solving commonsense tasks, and the importance of such cues for the model prediction. Using two different measures, we find that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy.", "venue": "Findings", "year": 2020, "referenceCount": 51, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.61.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-08-10", "journal": {"pages": "683-693"}, "authors": [{"authorId": "152496687", "name": "Leyang Cui"}, {"authorId": "2110844331", "name": "Sijie Cheng"}, {"authorId": "2142240763", "name": "Yu Wu"}, {"authorId": "1591125925", "name": "Yue Zhang"}]}, {"paperId": "9651c3f83b9310829622305f5316443253861fba", "externalIds": {"DBLP": "conf/acl/SeonwooLKHO21", "ACL": "2021.findings-acl.62", "ArXiv": "2106.09983", "DOI": "10.18653/v1/2021.findings-acl.62", "CorpusId": 235485084}, "corpusId": 235485084, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9651c3f83b9310829622305f5316443253861fba", "title": "Weakly Supervised Pre-Training for Multi-Hop Retriever", "abstract": "In multi-hop QA, answering complex questions entails iterative document retrieval for finding the missing entity of the question. The main steps of this process are sub-question detection, document retrieval for the sub-question, and generation of a new query for the final document retrieval. However, building a dataset that contains complex questions with sub-questions and their corresponding documents requires costly human annotation. To address the issue, we propose a new method for weakly supervised multi-hop retriever pre-training without human efforts. Our method includes 1) a pre-training task for generating vector representations of complex questions, 2) a scalable data generation method that produces the nested structure of question and sub-question as weak supervision for pre-training, and 3) a pre-training model structure based on dense encoders. We conduct experiments to compare the performance of our pre-trained retriever with several state-of-the-art models on end-to-end multi-hop QA as well as document retrieval. The experimental results show that our pre-trained retriever is effective and also robust on limited data and computational resources.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.62.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-18", "journal": {"pages": "694-704"}, "authors": [{"authorId": "80450954", "name": "Yeon Seonwoo"}, {"authorId": "3226948", "name": "Sang-Woo Lee"}, {"authorId": "2680879", "name": "Ji-Hoon Kim"}, {"authorId": "2577039", "name": "Jung-Woo Ha"}, {"authorId": "2463290", "name": "Alice H. Oh"}]}, {"paperId": "cff464e5dfe16f48559bf6353da311dc8d13a2ed", "externalIds": {"ArXiv": "2107.10747", "DBLP": "conf/acl/LiNK21", "ACL": "2021.findings-acl.63", "DOI": "10.18653/v1/2021.findings-acl.63", "CorpusId": 236170987}, "corpusId": 236170987, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/cff464e5dfe16f48559bf6353da311dc8d13a2ed", "title": "Meet The Truth: Leverage Objective Facts and Subjective Views for Interpretable Rumor Detection", "abstract": "Existing rumor detection strategies typically provide detection labels while ignoring their explanation. Nonetheless, providing pieces of evidence to explain why a suspicious tweet is rumor is essential. As such, a novel model, LOSIRD, was proposed in this paper. First, LOSIRD mines appropriate evidence sentences and classifies them by automatically checking the veracity of the relationship of the given claim and its evidence from about 5 million Wikipedia documents. LOSIRD then automatically constructs two heterogeneous graph objects to simulate the propagation layout of the tweets and code the relationship of evidence. Finally, a graphSAGE processing component is used in LOSIRD to provide the label and evidence. To the best of our knowledge, we are the first one who combines objective facts and subjective views to verify rumor. The experimental results on two real-world Twitter datasets showed that our model exhibited the best performance in the early rumor detection task and its rumor detection performance outperformed other baseline and state-of-the-art models. Moreover, we confirmed that both objective information and subjective information are fundamental clues for rumor detection.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.63.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-21", "journal": {"name": "ArXiv", "volume": "abs/2107.10747"}, "authors": [{"authorId": "2109400844", "name": "Jiawen Li"}, {"authorId": "2028953122", "name": "Shiwen Ni"}, {"authorId": "1738550", "name": "Hung-Yu kao"}]}, {"paperId": "b92abf442f5cc4971556b157b8e14d17fd600f4b", "externalIds": {"DBLP": "journals/corr/abs-2105-12306", "ArXiv": "2105.12306", "ACL": "2021.findings-acl.64", "DOI": "10.18653/v1/2021.findings-acl.64", "CorpusId": 235196067}, "corpusId": 235196067, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b92abf442f5cc4971556b157b8e14d17fd600f4b", "title": "Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking", "abstract": "Chinese Spell Checking (CSC) aims to detect and correct erroneous characters for user-generated text in the Chinese language. Most of the Chinese spelling errors are misused semantically, phonetically or graphically similar characters. Previous attempts noticed this phenomenon and try to use the similarity for this task. However, these methods use either heuristics or handcrafted confusion sets to predict the correct character. In this paper, we propose a Chinese spell checker called ReaLiSe, by directly leveraging the multimodal information of the Chinese characters. The ReaLiSe model tackles the CSC task by (1) capturing the semantic, phonetic and graphic information of the input characters, and (2) selectively mixing the information in these modalities to predict the correct output. Experiments on the SIGHAN benchmarks show that the proposed model outperforms strong baselines by a large margin.", "venue": "Findings", "year": 2021, "referenceCount": 52, "citationCount": 52, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.64.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-26", "journal": {"pages": "716-728"}, "authors": [{"authorId": "152337253", "name": "Heng-Da Xu"}, {"authorId": "2017913223", "name": "Zhongli Li"}, {"authorId": "50656875", "name": "Qingyu Zhou"}, {"authorId": "2150359436", "name": "Chao Li"}, {"authorId": "2117415732", "name": "Zizhen Wang"}, {"authorId": "2154235", "name": "Yunbo Cao"}, {"authorId": "4590286", "name": "Heyan Huang"}, {"authorId": "134880677", "name": "Xian-Ling Mao"}]}, {"paperId": "82581fe6523ee22021123aed76c4312e3d291c52", "externalIds": {"ACL": "2021.findings-acl.65", "DBLP": "conf/acl/WangW21", "DOI": "10.18653/v1/2021.findings-acl.65", "CorpusId": 236477631}, "corpusId": 236477631, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/82581fe6523ee22021123aed76c4312e3d291c52", "title": "TransSum: Translating Aspect and Sentiment Embeddings for Self-Supervised Opinion Summarization", "abstract": "In this paper, we propose a novel self-supervised opinion summarization framework TransSum, which models opinion summaries as translations operating on the low-dimensional aspect and sentiment embedding spaces. Speci\ufb01cally, we propose two contrastive objectives to learn the crucial aspect and sentiment embeddings of reviews, by taking advantage of the intra-and inter-group invariances that have not been considered in previous studies. Furthermore, these embeddings can be used to reduce opinion redundancy and construct highly relevant reviews-summary pairs to train a supervised multi-input opinion summarization model. Experimental results on three different domains show that TransSum outperforms several strong baselines in generating informative, relevant and low-redundant summaries, unveiling the effectiveness of our approach.", "venue": "Findings", "year": 2021, "referenceCount": 58, "citationCount": 12, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.65.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "journal": {"pages": "729-742"}, "authors": [{"authorId": "2133843744", "name": "Ke Wang"}, {"authorId": "117908148", "name": "Xiaojun Wan"}]}, {"paperId": "b7ef3452894180224542c23a647ceed839d829f2", "externalIds": {"ACL": "2021.findings-acl.66", "DBLP": "conf/acl/TuJLSHDM21", "DOI": "10.18653/v1/2021.findings-acl.66", "CorpusId": 236478064}, "corpusId": 236478064, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b7ef3452894180224542c23a647ceed839d829f2", "title": "Hashing based Efficient Inference for Image-Text Matching", "abstract": "August 1\u20136, 2021. \u00a92021 Association for Computational Linguistics 743 Hashing based Efficient Inference for Image-Text Matching Rong-Cheng Tu\u2020, Lei Ji \u2021\u00a7\u00b6\u2217 , Huaishao Luo\u2016, Botian Shio, Heyan Huang\u2020, Nan Duan\u00b6 and Xian-Ling Mao\u2020 \u2020School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China, Beijing, China \u2021Institute of Computing Technology, CAS, Beijing, China \u00a7University of Chinese Academy of Sciences, Beijing, China \u00b6Microsoft Research Asia, Beijing, China \u2016Southwest Jiaotong University, Chengdu, China oShanghai AI lab, Shanghai, China \u2020{turongcheng, hhy63, maoxl}@bit.edu.cn, \u00b6{leiji,nanduan}@microsoft.com Abstract", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.66.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "743-752"}, "authors": [{"authorId": "47434142", "name": "Rong-Cheng Tu"}, {"authorId": "144906579", "name": "Lei Ji"}, {"authorId": "35347136", "name": "Huaishao Luo"}, {"authorId": "119700639", "name": "Botian Shi"}, {"authorId": "4590286", "name": "Heyan Huang"}, {"authorId": "2072609829", "name": "Nan Duan"}, {"authorId": "134880677", "name": "Xian-Ling Mao"}]}, {"paperId": "1ea54f567c222619754417229d1eafebaa892bd5", "externalIds": {"ACL": "2021.findings-acl.67", "DBLP": "conf/acl/BernardyEM21", "DOI": "10.18653/v1/2021.findings-acl.67", "CorpusId": 236477449}, "corpusId": 236477449, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1ea54f567c222619754417229d1eafebaa892bd5", "title": "Can the Transformer Learn Nested Recursion with Symbol Masking?", "abstract": "We investigate if, given a simple symbol masking strategy, self-attention models are capable of learning nested structures and generalise over their depth. We do so in the simplest setting possible, namely languages consisting of nested parentheses of several kinds. We use encoder-only models, which we train to predict randomly masked symbols, in a BERTlike fashion. We find that the accuracy is well above random baseline, with accuracy consistently above 50% both when increasing nesting depth and distances between training and testing. However, we find that the predictions made correspond to a simple parenthesis counting strategy, rather than a push-down automaton. This suggests that self-attention models are not suitable for tasks which require generalisation to more complex instances of recursive structures than those found in the training set.", "venue": "Findings", "year": 2021, "referenceCount": 10, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "753-760"}, "authors": [{"authorId": "144701616", "name": "Jean-Philippe Bernardy"}, {"authorId": "20493598", "name": "Adam Ek"}, {"authorId": "17150827", "name": "Vladislav Maraev"}]}, {"paperId": "3097f7d9a2696d4ec06999dee4a50052c01453a4", "externalIds": {"DBLP": "journals/corr/abs-2105-04837", "ArXiv": "2105.04837", "ACL": "2021.findings-acl.68", "DOI": "10.18653/v1/2021.findings-acl.68", "CorpusId": 234357794}, "corpusId": 234357794, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/3097f7d9a2696d4ec06999dee4a50052c01453a4", "title": "Rationalization through Concepts", "abstract": "Automated predictions require explanations to be interpretable by humans. One type of explanation is a rationale, i.e., a selection of input features such as relevant text snippets from which the model computes the outcome. However, a single overall selection does not provide a complete explanation, e.g., weighing several aspects for decisions. To this end, we present a novel self-interpretable model called ConRAT. Inspired by how human explanations for high-level decisions are often based on key concepts, ConRAT extracts a set of text snippets as concepts and infers which ones are described in the document. Then, it explains the outcome with a linear aggregation of concepts. Two regularizers drive ConRAT to build interpretable concepts. In addition, we propose two techniques to boost the rationale and predictive performance further. Experiments on both single- and multi-aspect sentiment classification tasks show that ConRAT is the first to generate concepts that align with human rationalization while using only the overall label. Further, it outperforms state-of-the-art methods trained on each aspect label independently.", "venue": "Findings", "year": 2021, "referenceCount": 51, "citationCount": 14, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.68.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-11", "journal": {"name": "ArXiv", "volume": "abs/2105.04837"}, "authors": [{"authorId": "26399699", "name": "Diego Antognini"}, {"authorId": "1735128", "name": "B. Faltings"}]}, {"paperId": "a53806d486c42d2eb7ba57d8f2c81e5b2a51479c", "externalIds": {"ACL": "2021.findings-acl.69", "DBLP": "journals/corr/abs-2105-08481", "ArXiv": "2105.08481", "DOI": "10.18653/v1/2021.findings-acl.69", "CorpusId": 234762800}, "corpusId": 234762800, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/a53806d486c42d2eb7ba57d8f2c81e5b2a51479c", "title": "Parallel Attention Network with Sequence Matching for Video Grounding", "abstract": "Given a video, video grounding aims to retrieve a temporal moment that semantically corresponds to a language query. In this work, we propose a Parallel Attention Network with Sequence matching (SeqPAN) to address the challenges in this task: multi-modal representation learning, and target moment boundary prediction. We design a self-guided parallel attention module to effectively capture self-modal contexts and cross-modal attentive information between video and text. Inspired by sequence labeling tasks in natural language processing, we split the ground truth moment into begin, inside, and end regions. We then propose a sequence matching strategy to guide start/end boundary predictions using region labels. Experimental results on three datasets show that SeqPAN is superior to state-of-the-art methods. Furthermore, the effectiveness of the self-guided parallel attention module and the sequence matching module is verified.", "venue": "Findings", "year": 2021, "referenceCount": 78, "citationCount": 19, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.69.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-18", "journal": {"pages": "776-790"}, "authors": [{"authorId": "2144615395", "name": "Hao Zhang"}, {"authorId": "1735962", "name": "Aixin Sun"}, {"authorId": "1664805530", "name": "Wei Jing"}, {"authorId": "1786516", "name": "Liangli Zhen"}, {"authorId": "10638646", "name": "Joey Tianyi Zhou"}, {"authorId": "2092816365", "name": "R. Goh"}]}, {"paperId": "79604e29340a8336003d1e8a348083b6249cc754", "externalIds": {"DBLP": "journals/corr/abs-2106-05630", "ACL": "2021.findings-acl.70", "ArXiv": "2106.05630", "DOI": "10.18653/v1/2021.findings-acl.70", "CorpusId": 235391043}, "corpusId": 235391043, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/79604e29340a8336003d1e8a348083b6249cc754", "title": "MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training", "abstract": "Symbolic music understanding, which refers to the understanding of music from the symbolic data (e.g., MIDI format, but not audio), covers many music applications such as genre classification, emotion classification, and music pieces matching. While good music representations are beneficial for these applications, the lack of training data hinders representation learning. Inspired by the success of pre-training models in natural language processing, in this paper, we develop MusicBERT, a large-scale pre-trained model for music understanding. To this end, we construct a large-scale symbolic music corpus that contains more than 1 million music songs. Since symbolic music contains more structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), simply adopting the pre-training techniques from NLP to symbolic music only brings marginal gains. Therefore, we design several mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to enhance pre-training with symbolic music data. Experiments demonstrate the advantages of MusicBERT on four music understanding tasks, including melody completion, accompaniment suggestion, genre classification, and style classification. Ablation studies also verify the effectiveness of our designs of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 58, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.70.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-10", "journal": {"pages": "791-800"}, "authors": [{"authorId": "2109759226", "name": "Mingliang Zeng"}, {"authorId": "48391466", "name": "Xu Tan"}, {"authorId": "2151038979", "name": "Rui Wang"}, {"authorId": "1613055688", "name": "Zeqian Ju"}, {"authorId": "143826491", "name": "Tao Qin"}, {"authorId": "2110264337", "name": "Tie-Yan Liu"}]}, {"paperId": "3877fed12c8b3c422c74eaed01f1b8357ca3338b", "externalIds": {"DBLP": "journals/corr/abs-2106-01478", "ArXiv": "2106.01478", "ACL": "2021.findings-acl.71", "DOI": "10.18653/v1/2021.findings-acl.71", "CorpusId": 235313819}, "corpusId": 235313819, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/3877fed12c8b3c422c74eaed01f1b8357ca3338b", "title": "Evaluating the Efficacy of Summarization Evaluation across Languages", "abstract": "While automatic summarization evaluation methods developed for English are routinely applied to other languages, this is the first attempt to systematically quantify their panlinguistic efficacy. We take a summarization corpus for eight different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.71.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01478"}, "authors": [{"authorId": "2789148", "name": "Fajri Koto"}, {"authorId": "1800564", "name": "Jey Han Lau"}, {"authorId": "145465286", "name": "Timothy Baldwin"}]}, {"paperId": "e6d03bf7d9b961dff94e71c46086f09791531b7b", "externalIds": {"ArXiv": "2105.08316", "DBLP": "journals/corr/abs-2105-08316", "ACL": "2021.findings-acl.72", "DOI": "10.18653/v1/2021.findings-acl.72", "CorpusId": 234763067}, "corpusId": 234763067, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e6d03bf7d9b961dff94e71c46086f09791531b7b", "title": "CoMAE: A Multi-factor Hierarchical Framework for Empathetic Response Generation", "abstract": "The capacity of empathy is crucial to the success of open-domain dialog systems. Due to its nature of multi-dimensionality, there are various factors that relate to empathy expression, such as communication mechanism, dialog act and emotion. However, existing methods for empathetic response generation usually either consider only one empathy factor or ignore the hierarchical relationships between different factors, leading to a weak ability of empathy modeling. In this paper, we propose a multi-factor hierarchical framework, CoMAE, for empathetic response generation, which models the above three key factors of empathy expression in a hierarchical way. We show experimentally that our CoMAE-based model can generate more empathetic responses than previous methods. We also highlight the importance of hierarchical modeling of different factors through both the empirical analysis on a real-life corpus and the extensive experiments. Our codes and used data are available at https://github.com/chujiezheng/CoMAE.", "venue": "Findings", "year": 2021, "referenceCount": 48, "citationCount": 40, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.72.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-18", "journal": {"pages": "813-824"}, "authors": [{"authorId": "146452866", "name": "Chujie Zheng"}, {"authorId": "2144386054", "name": "Yong Liu"}, {"authorId": "2154941685", "name": "Wei Chen"}, {"authorId": "2093918659", "name": "Yongcai Leng"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "73c8cee29fd57097288a6919116b5c8a448f3030", "externalIds": {"ArXiv": "2106.04847", "DBLP": "journals/corr/abs-2106-04847", "ACL": "2021.findings-acl.73", "DOI": "10.18653/v1/2021.findings-acl.73", "CorpusId": 235377418}, "corpusId": 235377418, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/73c8cee29fd57097288a6919116b5c8a448f3030", "title": "UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction", "abstract": "Keyphrase Prediction (KP) task aims at predicting several keyphrases that can summarize the main idea of the given document. Mainstream KP methods can be categorized into purely generative approaches and integrated models with extraction and generation. However, these methods either ignore the diversity among keyphrases or only weakly capture the relation across tasks implicitly. In this paper, we propose UniKeyphrase, a novel end-to-end learning framework that jointly learns to extract and generate keyphrases. In UniKeyphrase, stacked relation layer and bag-of-words constraint are proposed to fully exploit the latent semantic relation between extraction and generation in the view of model structure and training process, respectively. Experiments on KP benchmarks demonstrate that our joint approach outperforms mainstream methods by a large margin.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.73.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-09", "journal": {"pages": "825-835"}, "authors": [{"authorId": "46477486", "name": "Huanqin Wu"}, {"authorId": "39059473", "name": "Wei Liu"}, {"authorId": "2151531864", "name": "Lei Li"}, {"authorId": "2108884094", "name": "Dan Nie"}, {"authorId": "2118213045", "name": "Tao Chen"}, {"authorId": "1884418505", "name": "Feng Zhang"}, {"authorId": "2145384984", "name": "Di Wang"}]}, {"paperId": "56446cb1da48cbe6e19e5051ed80c3861021e5ba", "externalIds": {"DBLP": "journals/corr/abs-2012-05628", "ACL": "2021.findings-acl.74", "ArXiv": "2012.05628", "MAG": "3112784227", "DOI": "10.18653/v1/2021.findings-acl.74", "CorpusId": 228083868}, "corpusId": 228083868, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/56446cb1da48cbe6e19e5051ed80c3861021e5ba", "title": "As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages", "abstract": "Large generative language models have been very successful for English, but other languages lag behind due to data and computational limitations. We propose a method that may overcome these problems by adapting existing pre-trained language models to new languages. Specifically, we describe the adaptation of English GPT-2 to Italian and Dutch by retraining lexical embeddings without tuning the Transformer layers. As a result, we obtain lexical embeddings for Italian and Dutch that are aligned with the original English lexical embeddings and induce a bilingual lexicon from this alignment. Additionally, we show how to scale up complexity by transforming relearned lexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This method minimises the amount of training and prevents losing information during adaptation that was learned by GPT-2. English GPT-2 models with relearned lexical embeddings can generate realistic sentences in Italian and Dutch, but on average these sentences are still identifiable as artificial by humans. Based on perplexity scores and human judgements, we find that generated sentences become more realistic with some additional full model finetuning, especially for Dutch. For Italian, we see that they are evaluated on par with sentences generated by a GPT-2 model fully trained from scratch. Our work can be conceived as a blueprint for training GPT-2s for other languages, and we provide a 'recipe' to do so.", "venue": "Findings", "year": 2020, "referenceCount": 42, "citationCount": 34, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.74.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-10", "journal": {"name": "ArXiv", "volume": "abs/2012.05628"}, "authors": [{"authorId": "144611157", "name": "Wietse de Vries"}, {"authorId": "2742475", "name": "M. Nissim"}]}, {"paperId": "37dd6bb2fde54ce667f189ac259b586f3afe2d7c", "externalIds": {"ACL": "2021.findings-acl.75", "DBLP": "conf/acl/FourrierBS21", "DOI": "10.18653/v1/2021.findings-acl.75", "CorpusId": 236478204}, "corpusId": 236478204, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/37dd6bb2fde54ce667f189ac259b586f3afe2d7c", "title": "Can Cognate Prediction Be Modelled as a Low-Resource Machine Translation Task?", "abstract": "Cognate prediction is the task of generating, in a given language, the likely cognates of words in a related language, where cognates are words in related languages that have evolved from a common ancestor word. It is a task for which little data exists and which can aid linguists in the discovery of previously undiscovered relations. Previous work has applied machine translation (MT) techniques to this task, based on the tasks\u2019 similarities, without, however, studying their numerous differences or optimising architectural choices and hyper-parameters. In this paper, we investigate whether cognate prediction can benefit from insights from low-resource MT. We first compare statistical MT (SMT) and neural MT (NMT) architectures in a bilingual setup. We then study the impact of employing data augmentation techniques commonly seen to give gains in low-resource MT: monolingual pretraining, backtranslation and multilinguality. Our experiments on several Romance languages show that cognate prediction behaves only to a certain extent like a standard lowresource MT task. In particular, MT architectures, both statistical and neural, can be successfully used for the task, but using supplementary monolingual data is not always as beneficial as using additional language data, contrarily to what is observed for MT.", "venue": "Findings", "year": 2021, "referenceCount": 57, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.75.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "847-861"}, "authors": [{"authorId": "2080941785", "name": "Cl\u00e9mentine Fourrier"}, {"authorId": "48983885", "name": "Rachel Bawden"}, {"authorId": "68990982", "name": "Beno\u00eet Sagot"}]}, {"paperId": "0e0199c71361471d4517f4af86890b308f564ad5", "externalIds": {"ArXiv": "2105.14002", "ACL": "2021.findings-acl.76", "DBLP": "journals/corr/abs-2105-14002", "DOI": "10.18653/v1/2021.findings-acl.76", "CorpusId": 235247884}, "corpusId": 235247884, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0e0199c71361471d4517f4af86890b308f564ad5", "title": "What if This Modified That? Syntactic Interventions with Counterfactual Embeddings", "abstract": "Neural language models exhibit impressive performance on a variety of tasks, but their internal reasoning may be difficult to understand. Prior art aims to uncover meaningful properties within model representations via probes, but it is unclear how faithfully such probes portray information that the models actually use. To overcome such limitations, we propose a technique, inspired by causal analysis, for generating counterfactual embeddings within models. In experiments testing our technique, we produce evidence that suggests some BERT-based models use a tree-distance-like representation of syntax in downstream prediction tasks.", "venue": "Findings", "year": 2021, "referenceCount": 24, "citationCount": 29, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.76.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-28", "journal": {"pages": "862-875"}, "authors": [{"authorId": "48228222", "name": "Mycal Tucker"}, {"authorId": "1483502658", "name": "Peng Qian"}, {"authorId": "50007746", "name": "R. Levy"}]}, {"paperId": "7ca6c7af591be3ea45eb1fc63043eaa7afd22c7a", "externalIds": {"DBLP": "journals/corr/abs-2107-13662", "ACL": "2021.findings-acl.77", "ArXiv": "2107.13662", "DOI": "10.18653/v1/2021.findings-acl.77", "CorpusId": 236478176}, "corpusId": 236478176, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7ca6c7af591be3ea45eb1fc63043eaa7afd22c7a", "title": "Investigating Text Simplification Evaluation", "abstract": "Modern text simplification (TS) heavily relies on the availability of gold standard data to build machine learning models. However, existing studies show that parallel TS corpora contain inaccurate simplifications and incorrect alignments. Additionally, evaluation is usually performed by using metrics such as BLEU or SARI to compare system output to the gold standard. A major limitation is that these metrics do not match human judgements and the performance on different datasets and linguistic phenomena vary greatly. Furthermore, our research shows that the test and training subsets of parallel datasets differ significantly. In this work, we investigate existing TS corpora, providing new insights that will motivate the improvement of existing state-of-the-art TS evaluation methods. Our contributions include the analysis of TS corpora based on existing modifications used for simplification and an empirical study on TS models performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build more robust TS models.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 7, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.77.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-28", "journal": {"name": "ArXiv", "volume": "abs/2107.13662"}, "authors": [{"authorId": "2121361476", "name": "Laura V\u00e1squez-Rodr\u00edguez"}, {"authorId": "2895959", "name": "M. Shardlow"}, {"authorId": "1984182", "name": "Piotr Przyby\u0142a"}, {"authorId": "1881965", "name": "S. Ananiadou"}]}, {"paperId": "6597d61bdb531051678c773526758a6dc113b9ce", "externalIds": {"ArXiv": "2106.00969", "DBLP": "journals/corr/abs-2106-00969", "ACL": "2021.findings-acl.78", "DOI": "10.18653/v1/2021.findings-acl.78", "CorpusId": 235293697}, "corpusId": 235293697, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6597d61bdb531051678c773526758a6dc113b9ce", "title": "COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences", "abstract": "Commonsense reasoning is intuitive for humans but has been a long-term challenge for artificial intelligence (AI). Recent advancements in pretrained language models have shown promising results on several commonsense benchmark datasets. However, the reliability and comprehensiveness of these benchmarks towards assessing model's commonsense reasoning ability remains unclear. To this end, we introduce a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs. We propose a pairwise accuracy metric to reliably measure an agent's ability to perform commonsense reasoning over a given situation. The dataset is crowdsourced and enhanced with an adversarial model-in-the-loop setup to incentivize challenging samples. To facilitate a systematic analysis of commonsense capabilities, we design our dataset along the dimensions of knowledge domains, reasoning scenarios and numeracy. Experimental results demonstrate that our strongest baseline (UnifiedQA-3B), after fine-tuning, achieves ~71% standard accuracy and ~51% pairwise accuracy, well below human performance (~95% for both metrics). The dataset is available at https://github.com/PlusLabNLP/Com2Sense.", "venue": "Findings", "year": 2021, "referenceCount": 50, "citationCount": 27, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.78.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "883-898"}, "authors": [{"authorId": "2108410562", "name": "Shikhar Singh"}, {"authorId": "2059844121", "name": "Nuan Wen"}, {"authorId": "2118739951", "name": "Yu Hou"}, {"authorId": "1805993128", "name": "Pegah Alipoormolabashi"}, {"authorId": "2015467", "name": "Te-Lin Wu"}, {"authorId": "2378954", "name": "Xuezhe Ma"}, {"authorId": "3157053", "name": "Nanyun Peng"}]}, {"paperId": "473b70bb3c531b2d740fa6d652956e2733b53243", "externalIds": {"ArXiv": "2106.11783", "DBLP": "conf/acl/ChungTG21", "ACL": "2021.findings-acl.79", "DOI": "10.18653/v1/2021.findings-acl.79", "CorpusId": 235593406}, "corpusId": 235593406, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/473b70bb3c531b2d740fa6d652956e2733b53243", "title": "Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech", "abstract": "Tackling online hatred using informed textual responses - called counter narratives - has been brought under the spotlight recently. Accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. Still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. Moreover, these models can create plausible but not necessarily true arguments. In this paper we present the first complete knowledge-bound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. Together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings.", "venue": "Findings", "year": 2021, "referenceCount": 59, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.79.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-22", "journal": {"pages": "899-914"}, "authors": [{"authorId": "3365740", "name": "Yi-Ling Chung"}, {"authorId": "2034636", "name": "Serra Sinem Tekiro\u011flu"}, {"authorId": "1912357", "name": "Marco Guerini"}]}, {"paperId": "660732e9f04063c4e7f15dc83670033d261e8aaa", "externalIds": {"ArXiv": "2004.14454", "ACL": "2021.findings-acl.80", "DBLP": "journals/corr/abs-2004-14454", "MAG": "3022992164", "DOI": "10.18653/v1/2021.findings-acl.80", "CorpusId": 216868486}, "corpusId": 216868486, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/660732e9f04063c4e7f15dc83670033d261e8aaa", "title": "SOLID: A Large-Scale Semi-Supervised Dataset for Offensive Language Identification", "abstract": "The use of offensive language is a major problem in social media which has led to an abundance of research in detecting content such as hate speech, cyberbulling, and cyber-aggression. There have been several attempts to consolidate and categorize these efforts. Recently, the OLID dataset used at SemEval-2019 proposed a hierarchical three-level annotation taxonomy which addresses different types of offensive language as well as important information such as the target of such content. The categorization provides meaningful and important information for understanding offensive language. However, the OLID dataset is limited in size, especially for some of the low-level categories, which included only a few hundred instances, thus making it challenging to train robust deep learning models. Here, we address this limitation by creating the largest available dataset for this task, SOLID. SOLID contains over nine million English tweets labeled in a semi-supervised manner. We further demonstrate experimentally that using SOLID along with OLID yields improved performance on the OLID test set for two different models, especially for the lower levels of the taxonomy. Finally, we perform analysis of the models' performance on easy and hard examples of offensive language using data annotated in a semi-supervised way.", "venue": "Findings", "year": 2020, "referenceCount": 52, "citationCount": 125, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.80.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-04-29", "journal": {"name": "ArXiv", "volume": "abs/2004.14454"}, "authors": [{"authorId": "144063596", "name": "Sara Rosenthal"}, {"authorId": "145676297", "name": "Pepa Atanasova"}, {"authorId": "48764610", "name": "Georgi Karadzhov"}, {"authorId": "145130358", "name": "Marcos Zampieri"}, {"authorId": "1683562", "name": "Preslav Nakov"}]}, {"paperId": "6914a7997ff4be207fa7b3472a9c5879abaec646", "externalIds": {"DBLP": "journals/corr/abs-2012-11747", "ACL": "2021.findings-acl.81", "ArXiv": "2012.11747", "DOI": "10.18653/v1/2021.findings-acl.81", "CorpusId": 229376913}, "corpusId": 229376913, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6914a7997ff4be207fa7b3472a9c5879abaec646", "title": "RealFormer: Transformer Likes Residual Attention", "abstract": "Transformer is the backbone of modern NLP models. In this paper, we propose RealFormer, a simple and generic technique to create Residual Attention Layer Transformer networks that significantly outperform the canonical Transformer and its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked Language Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA, Natural Questions, and OpenKP. We also observe empirically that RealFormer stabilizes training and leads to models with sparser attention. Source code and pre-trained checkpoints for RealFormer can be found at https://github.com/google-research/google-research/tree/master/realformer.", "venue": "Findings", "year": 2020, "referenceCount": 41, "citationCount": 66, "influentialCitationCount": 12, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.81.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-21", "journal": {"pages": "929-943"}, "authors": [{"authorId": "2933399", "name": "Ruining He"}, {"authorId": "101210026", "name": "Anirudh Ravula"}, {"authorId": "3015985", "name": "Bhargav Kanagal"}, {"authorId": "1643737606", "name": "J. Ainslie"}]}, {"paperId": "b37afeb5301445546bca3d7293044a77b1a8f25f", "externalIds": {"DBLP": "journals/corr/abs-2012-15793", "ACL": "2021.findings-acl.82", "ArXiv": "2012.15793", "DOI": "10.18653/v1/2021.findings-acl.82", "CorpusId": 229923446}, "corpusId": 229923446, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b37afeb5301445546bca3d7293044a77b1a8f25f", "title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation", "abstract": "Generating text from structured inputs, such as meaning representations or RDF triples, has often involved the use of specialized graph-encoding neural networks. However, recent applications of pretrained transformers to linearizations of graph inputs have yielded state-of-the-art generation results on graph-to-text tasks. Here, we explore the ability of these linearized models to encode local graph structures, in particular their invariance to the graph linearization strategy and their ability to reconstruct corrupted inputs. Our findings motivate solutions to enrich the quality of models' implicit graph encodings via scaffolding. Namely, we use graph-denoising objectives implemented in a multi-task text-to-text framework. We find that these denoising scaffolds lead to substantial improvements in downstream generation in low-resource settings.", "venue": "Findings", "year": 2020, "referenceCount": 48, "citationCount": 20, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.82.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-31", "journal": {"name": "ArXiv", "volume": "abs/2012.15793"}, "authors": [{"authorId": "49462969", "name": "Alexander Miserlis Hoyle"}, {"authorId": "3451494", "name": "Ana Marasovi\u0107"}, {"authorId": "144365875", "name": "Noah A. Smith"}]}, {"paperId": "2c81da90eed8ff8cfb4301c7ed9a3f2cf2b4ea0d", "externalIds": {"DBLP": "journals/corr/abs-2012-15263", "ACL": "2021.findings-acl.83", "ArXiv": "2012.15263", "DOI": "10.18653/v1/2021.findings-acl.83", "CorpusId": 229923364}, "corpusId": 229923364, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/2c81da90eed8ff8cfb4301c7ed9a3f2cf2b4ea0d", "title": "Predicting cross-linguistic adjective order with information gain", "abstract": "Languages vary in their placement of multiple adjectives before, after, or surrounding the noun, but they typically exhibit strong intra-language tendencies on the relative order of those adjectives (e.g., the preference for `big blue box' in English, `grande bo\\^{i}te bleue' in French, and `alsund\\={u}q al'azraq alkab\\={\\i}r' in Arabic). We advance a new quantitative account of adjective order across typologically-distinct languages based on maximizing information gain. Our model addresses the left-right asymmetry of French-type ANA sequences with the same approach as AAN and NAA orderings, without appeal to other mechanisms. We find that, across 32 languages, the preferred order of adjectives largely mirrors an efficient algorithm of maximizing information gain.", "venue": "Findings", "year": 2020, "referenceCount": 50, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.83.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-30", "journal": {"pages": "957-967"}, "authors": [{"authorId": "119054303", "name": "William Dyer"}, {"authorId": "2585394", "name": "Richard Futrell"}, {"authorId": "2109308254", "name": "Zoey Liu"}, {"authorId": "2040994", "name": "Gregory Scontras"}]}, {"paperId": "63d8426ba1f51a8525dd19fd8ec92934ec71aea5", "externalIds": {"ACL": "2021.findings-acl.84", "ArXiv": "2105.03075", "DBLP": "journals/corr/abs-2105-03075", "DOI": "10.18653/v1/2021.findings-acl.84", "CorpusId": 234093015}, "corpusId": 234093015, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/63d8426ba1f51a8525dd19fd8ec92934ec71aea5", "title": "A Survey of Data Augmentation Approaches for NLP", "abstract": "Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP", "venue": "Findings", "year": 2021, "referenceCount": 194, "citationCount": 445, "influentialCitationCount": 45, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.84.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-05-07", "journal": {"pages": "968-988"}, "authors": [{"authorId": "152913678", "name": "Steven Y. Feng"}, {"authorId": "3375999", "name": "Varun Gangal"}, {"authorId": "144026731", "name": "Jason Wei"}, {"authorId": "123607932", "name": "Sarath Chandar"}, {"authorId": "1918441", "name": "Soroush Vosoughi"}, {"authorId": "1706595", "name": "T. Mitamura"}, {"authorId": "144547315", "name": "E. Hovy"}]}, {"paperId": "476afc913d63f3ba1882f6419f718984379b2380", "externalIds": {"DBLP": "journals/corr/abs-2106-01024", "ACL": "2021.findings-acl.85", "ArXiv": "2106.01024", "DOI": "10.18653/v1/2021.findings-acl.85", "CorpusId": 235293903}, "corpusId": 235293903, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/476afc913d63f3ba1882f6419f718984379b2380", "title": "Why Machine Reading Comprehension Models Learn Shortcuts?", "abstract": "Recent studies report that many machine reading comprehension (MRC) models can perform closely to or even better than humans on benchmark datasets. However, existing works indicate that many MRC models may learn shortcuts to outwit these benchmarks, but the performance is unsatisfactory in real-world applications. In this work, we attempt to explore, instead of the expected comprehension skills, why these models learn the shortcuts. Based on the observation that a large portion of questions in current datasets have shortcut solutions, we argue that larger proportion of shortcut questions in training data make models rely on shortcut tricks excessively. To investigate this hypothesis, we carefully design two synthetic datasets with annotations that indicate whether a question can be answered using shortcut solutions. We further propose two new methods to quantitatively analyze the learning difficulty regarding shortcut and challenging questions, and revealing the inherent learning mechanism behind the different performance between the two kinds of questions. A thorough empirical analysis shows that MRC models tend to learn shortcut questions earlier than challenging questions, and the high proportions of shortcut questions in training sets hinder models from exploring the sophisticated reasoning skills in the later stage of training.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 32, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.85.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "989-1002"}, "authors": [{"authorId": "7827757", "name": "Yuxuan Lai"}, {"authorId": "2111574159", "name": "Chen Zhang"}, {"authorId": "2115387922", "name": "Yansong Feng"}, {"authorId": "2007771781", "name": "Quzhe Huang"}, {"authorId": "9072379", "name": "Dongyan Zhao"}]}, {"paperId": "9c3efccaabab034634629364758d15518423386d", "externalIds": {"ACL": "2021.findings-acl.86", "DBLP": "conf/acl/LimkonchotiwatP21", "DOI": "10.18653/v1/2021.findings-acl.86", "CorpusId": 236477541}, "corpusId": 236477541, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9c3efccaabab034634629364758d15518423386d", "title": "Handling Cross- and Out-of-Domain Samples in Thai Word Segmentation", "abstract": "While word segmentation is a solved problem in many languages, it is still a challenge in continuous-script or low-resource languages. Like other NLP tasks, word segmentation is domain-dependent, which can be a challenge in low-resource languages like Thai and Urdu since there can be domains with insufficient data. This investigation proposes a new solution to adapt an existing domaingeneric model to a target domain, as well as a data augmentation technique to combat the low-resource problems. In addition to domain adaptation, we also propose a framework to handle out-of-domain inputs using an ensemble of domain-specific models called MultiDomain Ensemble (MDE). To assess the effectiveness of the proposed solutions, we conducted extensive experiments on domain adaptation and out-of-domain scenarios. Moreover, we also proposed a multiple task dataset for Thai text processing, including word segmentation. For domain adaptation, we compared our solution to the state-of-the-art Thai word segmentation (TWS) method and obtained improvements from 93.47% to 98.48% at the character level and 84.03% to 96.75% at the word level. For out-of-domain scenarios, our MDE method significantly outperformed the state-of-the-art TWS and multi-criteria methods. Furthermore, to demonstrate our method\u2019s generalizability, we also applied our MDE framework to other languages, namely Chinese, Japanese, and Urdu, and obtained improvements similar to Thai\u2019s.", "venue": "Findings", "year": 2021, "referenceCount": 54, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.86.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1003-1016"}, "authors": [{"authorId": "1596821065", "name": "Peerat Limkonchotiwat"}, {"authorId": "2008191108", "name": "Wannaphong Phatthiyaphaibun"}, {"authorId": "34674471", "name": "Raheem Sarwar"}, {"authorId": "1819128", "name": "E. Chuangsuwanich"}, {"authorId": "2304090", "name": "Sarana Nutanong"}]}, {"paperId": "e6f195cde91ab408f750525dfb3554ac2432b790", "externalIds": {"DBLP": "conf/acl/WuHGS21", "ACL": "2021.findings-acl.87", "ArXiv": "2101.00130", "DOI": "10.18653/v1/2021.findings-acl.87", "CorpusId": 230437484}, "corpusId": 230437484, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e6f195cde91ab408f750525dfb3554ac2432b790", "title": "Sensei: Self-Supervised Sensor Name Segmentation", "abstract": "A sensor name, typically an alphanumeric string, encodes the key context (e.g., function and location) of a sensor needed for deploying smart building applications. Sensor names, however, are curated in a building vendor-specific manner using different structures and vocabularies that are often esoteric. They thus require tremendous manual effort to annotate on a per-building basis; even to just segment these sensor names into meaningful chunks. In this paper, we propose a fully automated self-supervised framework, Sensei, which can learn to segment sensor names without any human annotation. Specifically, we employ a neural language model to capture the underlying sensor naming structure and then induce self-supervision based on information from the language model to build the segmentation model. Extensive experiments on five real-world buildings comprising thousands of sensors demonstrate the superiority of Sensei over baseline methods.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.87.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-01-01", "journal": {"pages": "1017-1027"}, "authors": [{"authorId": "2110410087", "name": "Jiaman Wu"}, {"authorId": "2505157", "name": "Dezhi Hong"}, {"authorId": "2110343779", "name": "Rajesh K. Gupta"}, {"authorId": "2884976", "name": "Jingbo Shang"}]}, {"paperId": "d43f96523af43615950659561a8a9ccbccf812e1", "externalIds": {"ACL": "2021.findings-acl.88", "DBLP": "conf/acl/MaYLZ21", "DOI": "10.18653/v1/2021.findings-acl.88", "CorpusId": 236477946}, "corpusId": 236477946, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d43f96523af43615950659561a8a9ccbccf812e1", "title": "Frustratingly Simple Few-Shot Slot Tagging", "abstract": "We propose a simple and effective few-shot model for slot tagging. Recent work shows that it is promising to extend standard few-shot classi\ufb01cation methods to sequence labeling with CRF-speci\ufb01c augmentations. Such methods show strengths in encoding slot name semantics and slot dependencies. However, we \ufb01nd these strengths can be obtained by a much simpler method, which casts slot tagging into machine reading comprehension (MRC). We \ufb01ne-tune a standard BERT-based MRC model with a mixture of source domain and (few-shot) target domain data. Such simple method outperforms state-of-the-art methods by a large margin on the SNIPS dataset.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 7, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.88.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1028-1033"}, "authors": [{"authorId": "34831725", "name": "Jianqiang Ma"}, {"authorId": "2110058238", "name": "Zeyu Yan"}, {"authorId": "2145421503", "name": "Chang Li"}, {"authorId": "2145955950", "name": "Yang Zhang"}]}, {"paperId": "6fe24352834ee2b77d15f7d9af7aaa1a896598bc", "externalIds": {"MAG": "3093187610", "ACL": "2021.findings-acl.89", "DBLP": "journals/corr/abs-2010-06975", "ArXiv": "2010.06975", "DOI": "10.18653/v1/2021.findings-acl.89", "CorpusId": 222341619}, "corpusId": 222341619, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6fe24352834ee2b77d15f7d9af7aaa1a896598bc", "title": "Medical Code Assignment with Gated Convolution and Note-Code Interaction", "abstract": "Medical code assignment from clinical text is a fundamental task in clinical information system management. As medical notes are typically lengthy and the medical coding system's code space is large, this task is a long-standing challenge. Recent work applies deep neural network models to encode the medical notes and assign medical codes to clinical documents. However, these methods are still ineffective as they do not fully encode and capture the lengthy and rich semantic information of medical notes nor explicitly exploit the interactions between the notes and codes. We propose a novel method, gated convolutional neural networks, and a note-code interaction (GatedCNN-NCI), for automatic medical code assignment to overcome these challenges. Our methods capture the rich semantic information of the lengthy clinical text for better representation by utilizing embedding injection and gated information propagation in the medical note encoding module. With a novel note-code interaction design and a graph message passing mechanism, we explicitly capture the underlying dependency between notes and codes, enabling effective code prediction. A weight sharing scheme is further designed to decrease the number of trainable parameters. Empirical experiments on real-world clinical datasets show that our proposed model outperforms state-of-the-art models in most cases, and our model size is on par with light-weighted baselines.", "venue": "Findings", "year": 2020, "referenceCount": 41, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.89.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-10-14", "journal": {"pages": "1034-1043"}, "authors": [{"authorId": "51394448", "name": "Shaoxiong Ji"}, {"authorId": "2585415", "name": "Shirui Pan"}, {"authorId": "2848857", "name": "P. Marttinen"}]}, {"paperId": "8f894c51cae3f5e4067b41b139cf1e9ba5598a4a", "externalIds": {"DBLP": "journals/corr/abs-2105-11776", "ACL": "2021.findings-acl.90", "ArXiv": "2105.11776", "DOI": "10.18653/v1/2021.findings-acl.90", "CorpusId": 235187342}, "corpusId": 235187342, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8f894c51cae3f5e4067b41b139cf1e9ba5598a4a", "title": "Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering", "abstract": "Knowledge retrieval and reasoning are two key stages in multi-hop question answering (QA) at web scale. Existing approaches suffer from low confidence when retrieving evidence facts to fill the knowledge gap and lack transparent reasoning process. In this paper, we propose a new framework to exploit more valid facts while obtaining explainability for multi-hop QA by dynamically constructing a semantic graph and reasoning over it. We employ Abstract Meaning Representation (AMR) as semantic graph representation. Our framework contains three new ideas: (a) {\\tt AMR-SG}, an AMR-based Semantic Graph, constructed by candidate fact AMRs to uncover any hop relations among question, answer and multiple facts. (b) A novel path-based fact analytics approach exploiting {\\tt AMR-SG} to extract active facts from a large fact pool to answer questions. (c) A fact-level relation modeling leveraging graph convolution network (GCN) to guide the reasoning process. Results on two scientific multi-hop QA datasets show that we can surpass recent approaches including those using additional knowledge graphs while maintaining high explainability on OpenBookQA and achieve a new state-of-the-art result on ARC-Challenge in a computationally practicable setting.", "venue": "Findings", "year": 2021, "referenceCount": 60, "citationCount": 25, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.90.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-25", "journal": {"name": "ArXiv", "volume": "abs/2105.11776"}, "authors": [{"authorId": "2218345185", "name": "Weiwen Xu"}, {"authorId": "2111118042", "name": "Huihui Zhang"}, {"authorId": "1724421", "name": "Deng Cai"}, {"authorId": "144594306", "name": "Wai Lam"}]}, {"paperId": "f9ce79b7e238be2e5ca228672181169bb5bc3029", "externalIds": {"ACL": "2021.findings-acl.91", "DBLP": "journals/corr/abs-2106-02228", "ArXiv": "2106.02228", "DOI": "10.18653/v1/2021.findings-acl.91", "CorpusId": 235352840}, "corpusId": 235352840, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f9ce79b7e238be2e5ca228672181169bb5bc3029", "title": "Addressing Inquiries about History: An Efficient and Practical Framework for Evaluating Open-domain Chatbot Consistency", "abstract": "A good open-domain chatbot should avoid presenting contradictory responses about facts or opinions in a conversational session, known as its consistency capacity. However, evaluating the consistency capacity of a chatbot is still challenging. Employing human judges to interact with chatbots on purpose to check their capacities is costly and low-efficient, and difficult to get rid of subjective bias. In this paper, we propose the Addressing Inquiries about History (AIH), an efficient and practical framework for the consistency evaluation. At the conversation stage, AIH attempts to address appropriate inquiries about the dialogue history to induce the chatbot to redeclare the historical facts or opinions. We carry out the conversation between chatbots, which is more efficient than the human-bot interaction and can also alleviate the subjective bias. In this way, we manage to rapidly obtain a dialog session that contains responses with high contradiction possibilities. At the contradiction recognition stage, we can either employ human judges or a natural language inference (NLI) model to recognize whether the answers to the inquiries are contradictory with history. Finally, we are able to rank chatbots according to the contradiction statistics. Experiments on open-domain chatbots show that our approach can efficiently and reliably assess the consistency capacity of chatbots and achieve a high ranking correlation with the human evaluation. We release the framework and hope to help improve the consistency capacity of chatbots. \\footnote{\\url{https://github.com/ictnlp/AIH}}", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.91.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"pages": "1057-1067"}, "authors": [{"authorId": "2109965103", "name": "Zekang Li"}, {"authorId": "2108970018", "name": "Jinchao Zhang"}, {"authorId": "2066415714", "name": "Zhengcong Fei"}, {"authorId": "49771779", "name": "Yang Feng"}, {"authorId": "49178343", "name": "Jie Zhou"}]}, {"paperId": "4043c666017b4e843f5f3485ded1100962f4ed6d", "externalIds": {"ArXiv": "2105.04840", "ACL": "2021.findings-acl.92", "DBLP": "journals/corr/abs-2105-04840", "DOI": "10.18653/v1/2021.findings-acl.92", "CorpusId": 234357798}, "corpusId": 234357798, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4043c666017b4e843f5f3485ded1100962f4ed6d", "title": "Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation", "abstract": "We study the possibilities of building a non-autoregressive speech-to-text translation model using connectionist temporal classification (CTC), and use CTC-based automatic speech recognition as an auxiliary task to improve the performance. CTC's success on translation is counter-intuitive due to its monotonicity assumption, so we analyze its reordering capability. Kendall's tau distance is introduced as the quantitative metric, and gradient-based visualization provides an intuitive way to take a closer look into the model. Our analysis shows that transformer encoders have the ability to change the word order and points out the future research direction that worth being explored more on non-autoregressive speech translation.", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 17, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.92.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-11", "journal": {"pages": "1068-1077"}, "authors": [{"authorId": "7146976", "name": "Shun-Po Chuang"}, {"authorId": "2475831", "name": "Yung-Sung Chuang"}, {"authorId": "2109693770", "name": "Chih-Chiang Chang"}, {"authorId": "1706104", "name": "Hung-yi Lee"}]}, {"paperId": "77f9b54da444f49b2436712abe932627f16cbf95", "externalIds": {"ACL": "2021.findings-acl.93", "DBLP": "conf/acl/WuZZ21", "ArXiv": "2012.14710", "DOI": "10.18653/v1/2021.findings-acl.93", "CorpusId": 235266324}, "corpusId": 235266324, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/77f9b54da444f49b2436712abe932627f16cbf95", "title": "Code Summarization with Structure-induced Transformer", "abstract": "Code summarization (CS) is becoming a promising area in recent language understanding, which aims to generate sensible human language automatically for programming language in the format of source code, serving in the most convenience of programmer developing. It is well known that programming languages are highly structured. Thus previous works attempt to apply structure-based traversal (SBT) or non-sequential models like Tree-LSTM and graph neural network (GNN) to learn structural program semantics. However, it is surprising that incorporating SBT into advanced encoder like Transformer instead of LSTM has been shown no performance gain, which lets GNN become the only rest means modeling such necessary structural clue in source code. To release such inconvenience, we propose structure-induced Transformer, which encodes sequential code inputs with multi-view structural clues in terms of a newly-proposed structure-induced self-attention mechanism. Extensive experiments show that our proposed structure-induced Transformer helps achieve new state-of-the-art results on benchmarks.", "venue": "Findings", "year": 2020, "referenceCount": 54, "citationCount": 38, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.93.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-29", "journal": {"pages": "1078-1090"}, "authors": [{"authorId": "2120430544", "name": "Hongqi Wu"}, {"authorId": "153716230", "name": "Hai Zhao"}, {"authorId": "2156053262", "name": "Min Zhang"}]}, {"paperId": "b7a7260270f1c596bd82ead3fa73d2ac2c4c59bf", "externalIds": {"DBLP": "conf/acl/LiuZHXZ21", "ACL": "2021.findings-acl.94", "DOI": "10.18653/v1/2021.findings-acl.94", "CorpusId": 236478347}, "corpusId": 236478347, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b7a7260270f1c596bd82ead3fa73d2ac2c4c59bf", "title": "Scheduled Dialog Policy Learning: An Automatic Curriculum Learning Framework for Task-oriented Dialog System", "abstract": "In reinforcement learning (RL) based task-oriented dialogue systems, users act as the environment and the agent learns the policy by interacting with users. However, due to the subjectivity of different users, the complexity of user-generated training conversations varies greatly, which leads to different dif\ufb01culties for the agent to learn. Therefore, it is necessary for modeling dialogue complexity and make a reasonable learning schedule for ef\ufb01ciently training the agent. Towards that, we propose Scheduled Dialog Policy Learning, an automatic curriculum learning framework for joint-ing curriculum learning and policy optimiza-tion in the task-oriented dialog system. To our best knowledge, it is the \ufb01rst RL framework that improves dialogue policy learning by scheduling its learning process. Speci\ufb01-cally, we introduce an automatic measurement to evaluate the dialogue complexity, and based on this automatic measurement, we train the dialog agent from easy dialogues to complex ones. Experiments demonstrate that our ap-proach can be applied to the task-oriented dialogue policy learning and outperforms the pre-vious state-of-the-art model, which increases 9.6% and 10.0% in the accuracy on the dialog success rate, respectively on the MultiWoz and Movie-Ticket Booking datasets.", "venue": "Findings", "year": 2021, "referenceCount": 0, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.94.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1091-1102"}, "authors": [{"authorId": "48641700", "name": "Sihong Liu"}, {"authorId": "2108970018", "name": "Jinchao Zhang"}, {"authorId": "2058349088", "name": "Keqing He"}, {"authorId": "1753096", "name": "Weiran Xu"}, {"authorId": "2108485135", "name": "Jie Zhou"}]}, {"paperId": "ef6832885f9f2fb7fe9e14e4b54499e54508e88e", "externalIds": {"ACL": "2021.findings-acl.95", "DBLP": "conf/acl/GonzalezBFMJI21", "DOI": "10.18653/v1/2021.findings-acl.95", "CorpusId": 236478213}, "corpusId": 236478213, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ef6832885f9f2fb7fe9e14e4b54499e54508e88e", "title": "Do Explanations Help Users Detect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual Explanations", "abstract": "While research on explaining predictions of open-domain QA systems (ODQA) is gaining momentum, most works do not evaluate whether these explanations improve user trust. Furthermore, many users interact with ODQA using voice-assistants, yet prior works exclusively focus on visual displays, risking (as we also show) incorrectly extrapolating the effectiveness of explanations across modalities. To better understand the effectiveness of ODQA explanations strategies in the wild, we conduct user studies that measure whether explanations help users correctly decide when to accept or reject an ODQA system\u2019s answer. Unlike prior work, we control for explanation modality, i.e., whether they are communicated to users through a spoken or visual interface, and contrast effectiveness across modalities. We show that explanations derived from retrieved evidence can outperform strong baselines across modalities but the best explanation strategy varies with the modality. We show common failure cases of current explanations, emphasize end-to-end evaluation of explanations, and caution against evaluating them in proxy modalities that differ from deployment.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1103-1116"}, "authors": [{"authorId": "1453616756", "name": "Ana Valeria Gonzalez"}, {"authorId": "33340656", "name": "Gagan Bansal"}, {"authorId": "144270981", "name": "Angela Fan"}, {"authorId": "2121361882", "name": "Yashar Mehdad"}, {"authorId": "3422908", "name": "Robin Jia"}, {"authorId": "1900163", "name": "Srini Iyer"}]}, {"paperId": "af051c87cecca64c2de4ad9110608f7579766653", "externalIds": {"ACL": "2021.findings-acl.96", "ArXiv": "2105.07688", "DBLP": "conf/acl/XiangZCCLZ21", "DOI": "10.18653/v1/2021.findings-acl.96", "CorpusId": 234741971}, "corpusId": 234741971, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/af051c87cecca64c2de4ad9110608f7579766653", "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding", "abstract": "Semantic embedding has been widely investigated for aligning knowledge graph (KG) entities. Current methods have explored and utilized the graph structure, the entity names and attributes, but ignore the ontology (or ontological schema) which contains critical meta information such as classes and their membership relationships with entities. In this paper, we propose an ontology-guided entity alignment method named OntoEA, where both KGs and their ontologies are jointly embedded, and the class hierarchy and the class disjointness are utilized to avoid false mappings. Extensive experiments on seven public and industrial benchmarks have demonstrated the state-of-the-art performance of OntoEA and the effectiveness of the ontologies.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 25, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.96.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-17", "journal": {"pages": "1117-1128"}, "authors": [{"authorId": "51178762", "name": "Yuejia Xiang"}, {"authorId": "2030976630", "name": "Ziheng Zhang"}, {"authorId": "1731892", "name": "Jiaoyan Chen"}, {"authorId": "2145307827", "name": "Xi Chen"}, {"authorId": "148431025", "name": "Zhenxi Lin"}, {"authorId": "2145273405", "name": "Yefeng Zheng"}]}, {"paperId": "6d00b1024298e5b64ee873028385f7bb4396b05d", "externalIds": {"DBLP": "journals/corr/abs-2107-06516", "ACL": "2021.findings-acl.97", "ArXiv": "2107.06516", "DOI": "10.18653/v1/2021.findings-acl.97", "CorpusId": 235829155}, "corpusId": 235829155, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6d00b1024298e5b64ee873028385f7bb4396b05d", "title": "Learning Algebraic Recombination for Compositional Generalization", "abstract": "Neural sequence models exhibit limited compositional generalization ability in semantic parsing tasks. Compositional generalization requires algebraic recombination, i.e., dynamically recombining structured expressions in a recursive manner. However, most previous studies mainly concentrate on recombining lexical units, which is an important but not sufficient part of algebraic recombination. In this paper, we propose LeAR, an end-to-end neural model to learn algebraic recombination for compositional generalization. The key insight is to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic recombination. Specifically, we learn two modules jointly: a Composer for producing latent syntax, and an Interpreter for assigning semantic operations. Experiments on two realistic and comprehensive compositional generalization benchmarks demonstrate the effectiveness of our model. The source code is publicly available at https://github.com/microsoft/ContextualSP.", "venue": "Findings", "year": 2021, "referenceCount": 60, "citationCount": 30, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.97.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-14", "journal": {"name": "ArXiv", "volume": "abs/2107.06516"}, "authors": [{"authorId": "2107899830", "name": "Chenyao Liu"}, {"authorId": "2119217081", "name": "Shengnan An"}, {"authorId": "2284174", "name": "Zeqi Lin"}, {"authorId": "1409707585", "name": "Qian Liu"}, {"authorId": null, "name": "Bei Chen"}, {"authorId": "153249455", "name": "Jian-Guang Lou"}, {"authorId": "40650846", "name": "L. Wen"}, {"authorId": "2144620206", "name": "Nanning Zheng"}, {"authorId": "1485159990", "name": "Dongmei Zhang"}]}, {"paperId": "776a49616c84d52e8fff9911c561e3bac90910eb", "externalIds": {"DBLP": "conf/acl/PhamBMN21", "ArXiv": "2012.15180", "ACL": "2021.findings-acl.98", "DOI": "10.18653/v1/2021.findings-acl.98", "CorpusId": 229923132}, "corpusId": 229923132, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/776a49616c84d52e8fff9911c561e3bac90910eb", "title": "Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?", "abstract": "Do state-of-the-art natural language understanding models care about word order - one of the most important characteristics of a sequence? Not always! We found 75% to 90% of the correct predictions of BERT-based classifiers, trained on many GLUE tasks, remain constant after input words are randomly shuffled. Despite BERT embeddings are famously contextual, the contribution of each individual word to downstream tasks is almost unchanged even after the word's context is shuffled. BERT-based models are able to exploit superficial cues (e.g. the sentiment of keywords in sentiment analysis; or the word-wise similarity between sequence-pair inputs in natural language inference) to make correct decisions when tokens are arranged in random orders. Encouraging classifiers to capture word order information improves the performance on most GLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE tasks are not challenging machines to understand the meaning of a sentence.", "venue": "Findings", "year": 2020, "referenceCount": 71, "citationCount": 84, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.98.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-30", "journal": {"name": "ArXiv", "volume": "abs/2012.15180"}, "authors": [{"authorId": "2042922511", "name": "Thang M. Pham"}, {"authorId": "145262461", "name": "Trung Bui"}, {"authorId": "2712573", "name": "Long Mai"}, {"authorId": "151414531", "name": "Anh M Nguyen"}]}, {"paperId": "21ad14109f25210d55af3e2ea68d7c60462ddbe2", "externalIds": {"DBLP": "conf/acl/LuBSMCWH21", "ArXiv": "2106.00957", "ACL": "2021.findings-acl.99", "DOI": "10.18653/v1/2021.findings-acl.99", "CorpusId": 235294081}, "corpusId": 235294081, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/21ad14109f25210d55af3e2ea68d7c60462ddbe2", "title": "RevCore: Review-Augmented Conversational Recommendation", "abstract": "Existing conversational recommendation (CR) systems usually suffer from insufficient item information when conducted on short dialogue history and unfamiliar items. Incorporating external information (e.g., reviews) is a potential solution to alleviate this problem. Given that reviews often provide a rich and detailed user experience on different interests, they are potential ideal resources for providing high-quality recommendations within an informative conversation. In this paper, we design a novel end-to-end framework, namely, Review-augmented Conversational Recommender (RevCore), where reviews are seamlessly incorporated to enrich item information and assist in generating both coherent and informative responses. In detail, we extract sentiment-consistent reviews, perform review-enriched and entity-based recommendations for item suggestions, as well as use a review-attentive encoder-decoder for response generation. Experimental results demonstrate the superiority of our approach in yielding better performance on both recommendation and conversation responding.", "venue": "Findings", "year": 2021, "referenceCount": 55, "citationCount": 36, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.99.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.00957"}, "authors": [{"authorId": "2140044892", "name": "Yu Lu"}, {"authorId": "3299718", "name": "Junwei Bao"}, {"authorId": "1922182598", "name": "Yan Song"}, {"authorId": "2116609374", "name": "Zichen Ma"}, {"authorId": "2106614719", "name": "Shuguang Cui"}, {"authorId": "2115860568", "name": "Youzheng Wu"}, {"authorId": "144137069", "name": "Xiaodong He"}]}, {"paperId": "0098123efc851b67137c1028f7bac8d8bffbc8fd", "externalIds": {"DBLP": "conf/acl/LiuYZGZL21", "ArXiv": "2109.10540", "ACL": "2021.findings-acl.100", "DOI": "10.18653/v1/2021.findings-acl.100", "CorpusId": 236478283}, "corpusId": 236478283, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0098123efc851b67137c1028f7bac8d8bffbc8fd", "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing", "abstract": "Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasing-then-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%.", "venue": "Findings", "year": 2021, "referenceCount": 56, "citationCount": 25, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.100.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-22", "journal": {"name": "ArXiv", "volume": "abs/2109.10540"}, "authors": [{"authorId": "1409707585", "name": "Qian Liu"}, {"authorId": "2111181262", "name": "Dejian Yang"}, {"authorId": null, "name": "Jiahui Zhang"}, {"authorId": "2148899355", "name": "Jiaqi Guo"}, {"authorId": "2118870582", "name": "Bin Zhou"}, {"authorId": "153249455", "name": "Jian-Guang Lou"}]}, {"paperId": "c8d6174672f6b139759bed1c0a7d87166fa503b7", "externalIds": {"DBLP": "journals/corr/abs-2106-03103", "ACL": "2021.findings-acl.101", "ArXiv": "2106.03103", "DOI": "10.18653/v1/2021.findings-acl.101", "CorpusId": 235358926}, "corpusId": 235358926, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c8d6174672f6b139759bed1c0a7d87166fa503b7", "title": "Enhancing Label Correlation Feedback in Multi-Label Text Classification via Multi-Task Learning", "abstract": "In multi-label text classification (MLTC), each given document is associated with a set of correlated labels. To capture label correlations, previous classifier-chain and sequence-to-sequence models transform MLTC to a sequence prediction task. However, they tend to suffer from label order dependency, label combination over-fitting and error propagation problems. To address these problems, we introduce a novel approach with multi-task learning to enhance label correlation feedback. We first utilize a joint embedding (JE) mechanism to obtain the text and label representation simultaneously. In MLTC task, a document-label cross attention (CA) mechanism is adopted to generate a more discriminative document representation. Furthermore, we propose two auxiliary label co-occurrence prediction tasks to enhance label correlation learning: 1) Pairwise Label Co-occurrence Prediction (PLCP), and 2) Conditional Label Co-occurrence Prediction (CLCP). Experimental results on AAPD and RCV1-V2 datasets show that our method outperforms competitive baselines by a large margin. We analyze low-frequency label performance, label dependency, label combination diversity and coverage speed to show the effectiveness of our proposed method on label correlation learning.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 23, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.101.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-06", "journal": {"pages": "1190-1200"}, "authors": [{"authorId": "2108890077", "name": "Ximing Zhang"}, {"authorId": "2108007406", "name": "Qian-Wen Zhang"}, {"authorId": "2119231778", "name": "Zhao Yan"}, {"authorId": "2152936487", "name": "Ruifang Liu"}, {"authorId": "2154235", "name": "Yunbo Cao"}]}, {"paperId": "7dab194e7a49213f2bb5bf694dfbaf24976730d9", "externalIds": {"DBLP": "conf/acl/XuZXLZH21", "ACL": "2021.findings-acl.102", "ArXiv": "2012.04808", "DOI": "10.18653/v1/2021.findings-acl.102", "CorpusId": 235303708}, "corpusId": 235303708, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7dab194e7a49213f2bb5bf694dfbaf24976730d9", "title": "Fusing Context Into Knowledge Graph for Commonsense Question Answering", "abstract": "Commonsense question answering (QA) requires a model to grasp commonsense and factual knowledge to answer questions about world events. Many prior methods couple language modeling with knowledge graphs (KG). However, although a KG contains rich structural information, it lacks the context to provide a more precise understanding of the concepts. This creates a gap when fusing knowledge graphs into language modeling, especially when there is insufficient labeled data. Thus, we propose to employ external entity descriptions to provide contextual information for knowledge understanding. We retrieve descriptions of related concepts from Wiktionary and feed them as additional input to pre-trained language models. The resulting model achieves state-of-the-art result in the CommonsenseQA dataset and the best result among non-generative models in OpenBookQA.", "venue": "Findings", "year": 2020, "referenceCount": 30, "citationCount": 41, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.102.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-09", "journal": {"pages": "1201-1207"}, "authors": [{"authorId": "2110197273", "name": "Yichong Xu"}, {"authorId": "1456009348", "name": "Chenguang Zhu"}, {"authorId": "8233965", "name": "Ruochen Xu"}, {"authorId": "39798499", "name": "Yang Liu"}, {"authorId": "48262024", "name": "Michael Zeng"}, {"authorId": "144531812", "name": "Xuedong Huang"}]}, {"paperId": "d2caa1cbea3629ada3039089cbd69b5570da2aef", "externalIds": {"DBLP": "conf/acl/ZouYW21", "ACL": "2021.findings-acl.103", "DOI": "10.18653/v1/2021.findings-acl.103", "CorpusId": 236477395}, "corpusId": 236477395, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d2caa1cbea3629ada3039089cbd69b5570da2aef", "title": "Unsupervised Energy-based Adversarial Domain Adaptation for Cross-domain Text Classification", "abstract": "Transferring knowledge from a label-rich domain (source domain) to a label-scarce domain (target domain) for pervasive cross-domain Text Classi\ufb01cation (TC) is a non-trivial task. To overcome this issue, we propose EADA, a novel unsupervised energy-based adversarial domain adaptation framework. First, a deep pre-trained language model (e.g. RoBERTa) is leveraged as a shared feature extractor that maps the text sequences from both source and target domains to a feature space. Since the source features maintain good feature discrim-inability because of the full supervised training, we design a method that encourages target features towards the source ones via adversarial learning. An autoencoder is designed as an energy function that focuses on reconstructing source feature embeddings, while the feature extractor aims to generate source-like target feature embeddings to deceive the autoencoder. In this manner, the target feature embeddings become domain-invariant and inherit great discriminability. Extensive experiments on multi-domain sentiment classi\ufb01cation (Amazon review dataset) and Yes/No question-answering classi\ufb01cation (BoolQ and MARCO dataset) are conducted. The experimental results validate that EADA largely alleviates the domain discrepancy while maintaining excellent discriminability and achieves state-of-the-art cross-domain TC performance.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 16, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.103.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "journal": {"pages": "1208-1218"}, "authors": [{"authorId": "145791406", "name": "Han Zou"}, {"authorId": "2109722597", "name": "Jianfei Yang"}, {"authorId": "2154602151", "name": "Xiaojian Wu"}]}, {"paperId": "1afa3ab80abda57920b8d456a6513e6f01cc82e7", "externalIds": {"DBLP": "conf/acl/KockV21", "ACL": "2021.findings-acl.104", "DOI": "10.18653/v1/2021.findings-acl.104", "CorpusId": 236478233}, "corpusId": 236478233, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1afa3ab80abda57920b8d456a6513e6f01cc82e7", "title": "Survival text regression for time-to-event prediction in conversations", "abstract": "Time-to-event prediction tasks are common in conversation modelling, for applications such as predicting the length of a conversation or when a user will stop contributing to a platform. Despite the fact that it is natural to frame such predictions as regression tasks, recent work has modelled them as classification tasks, determining whether the time-to-event is greater than a pre-determined cut-off point. While this allows for the application of classification models which are well studied in NLP, it imposes a formulation that is contrived, as well as less informative. In this paper, we explore how to handle time-to-event forecasting in conversations as regression tasks. We focus on a family of regression techniques known as survival regression, which are commonly used in the context of healthcare and reliability engineering. We adapt these models to time-to-event prediction in conversations, using linguistic markers as features. On three datasets, we demonstrate that they outperform commonly considered text regression methods and comparable classification models.", "venue": "Findings", "year": 2021, "referenceCount": 27, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1219-1229"}, "authors": [{"authorId": "2047358683", "name": "Christine de Kock"}, {"authorId": "2064056928", "name": "Andreas Vlachos"}]}, {"paperId": "e88ea5108f4bb9c596ee359994fcc94158c3b101", "externalIds": {"DBLP": "conf/acl/ChenCMLZ21a", "ACL": "2021.findings-acl.105", "DOI": "10.18653/v1/2021.findings-acl.105", "CorpusId": 236477776}, "corpusId": 236477776, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e88ea5108f4bb9c596ee359994fcc94158c3b101", "title": "Unsupervised Knowledge Selection for Dialogue Generation", "abstract": "Knowledge selection is an important and challenging task which could provide the appropriate knowledge for informative dialogue generation. However, the needed gold knowledge label is dif\ufb01cult to collect in reality. In this paper, we study knowledge selection for dialogue generation in the unsupervised scenario and propose a novel Distilled Distant Supervision Loss (DDSL) to supervise knowledge selection when the gold knowledge label is unknown. Speci\ufb01cally, we \ufb01rst obtain an oracle knowledge label via distant supervision and then leverage knowledge distillation to alleviate the noisy labeling problem of distant supervision. Furthermore, we propose a pretraining-\ufb01netuning strategy to deal with the mismatch knowledge selection problem that models tend to select the mismatched knowledge for dialogue generation in the unsupervised setting and will cause the degeneration of knowledge-aware decoder. Experiments on two knowledge-grounded dialogue datasets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines. 1", "venue": "Findings", "year": 2021, "referenceCount": 74, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.105.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1230-1244"}, "authors": [{"authorId": "30917866", "name": "Xiuyi Chen"}, {"authorId": null, "name": "Feilong Chen"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "50492525", "name": "Peng Li"}, {"authorId": "2108485135", "name": "Jie Zhou"}]}, {"paperId": "302a691914b1e000ba260f88e6859d1b0ae35557", "externalIds": {"ACL": "2021.findings-acl.106", "DBLP": "conf/acl/PontiASRS21", "ArXiv": "2106.01051", "DOI": "10.18653/v1/2021.findings-acl.106", "CorpusId": 235294280}, "corpusId": 235294280, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/302a691914b1e000ba260f88e6859d1b0ae35557", "title": "Minimax and Neyman\u2013Pearson Meta-Learning for Outlier Languages", "abstract": "Model-agnostic meta-learning (MAML) has been recently put forth as a strategy to learn resource-poor languages in a sample-efficient fashion. Nevertheless, the properties of these languages are often not well represented by those available during training. Hence, we argue that the i.i.d. assumption ingrained in MAML makes it ill-suited for cross-lingual NLP. In fact, under a decision-theoretic framework, MAML can be interpreted as minimising the expected risk across training languages (with a uniform prior), which is known as Bayes criterion. To increase its robustness to outlier languages, we create two variants of MAML based on alternative criteria: Minimax MAML reduces the maximum risk across languages, while Neyman\u2013Pearson MAML constrains the risk in each language to a maximum threshold. Both criteria constitute fully differentiable two-player games. In light of this, we propose a new adaptive optimiser solving for a local approximation to their Nash equilibrium. We evaluate both model variants on two popular NLP tasks, part-of-speech tagging and question answering. We report gains for their average and minimum performance across low-resource languages in zeroand few-shot settings, compared to joint multisource transfer and vanilla MAML. The code for our experiments is available at https:// github.com/rahular/robust-maml.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.106.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "1245-1260"}, "authors": [{"authorId": "3381663", "name": "E. Ponti"}, {"authorId": "19509693", "name": "Rahul Aralikatte"}, {"authorId": "36921665", "name": "Disha Shrivastava"}, {"authorId": "145732771", "name": "Siva Reddy"}, {"authorId": "1700187", "name": "Anders S\u00f8gaard"}]}, {"paperId": "1949df8dc876e2e1919640cd03242a832b3bfcb2", "externalIds": {"DBLP": "conf/acl/DongBLHBCC21", "ACL": "2021.findings-acl.107", "ArXiv": "2101.00371", "DOI": "10.18653/v1/2021.findings-acl.107", "CorpusId": 236477371}, "corpusId": 236477371, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1949df8dc876e2e1919640cd03242a832b3bfcb2", "title": "On-the-Fly Attention Modulation for Neural Generation", "abstract": "Despite considerable advancements with deep neural language models (LMs), neural text generation still suffers from degeneration: the generated text is repetitive, generic, self-contradictory, and often lacks commonsense. Our analyses on sentence-level attention patterns in LMs reveal that neural degeneration may be associated with insufficient learning of task-specific characteristics by the attention mechanism. This finding motivates on-the-fly attention modulation -- a simple but effective method that enables the injection of priors into attention computation during inference. Automatic and human evaluation results on three text generation benchmarks demonstrate that attention modulation helps LMs generate text with enhanced fluency, creativity, and commonsense reasoning, in addition to significantly reduce sentence-level repetition.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.107.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-01-02", "journal": {"pages": "1261-1274"}, "authors": [{"authorId": "49265991", "name": "Yue Dong"}, {"authorId": "1857797", "name": "Chandra Bhagavatula"}, {"authorId": "50085131", "name": "Ximing Lu"}, {"authorId": "2012510", "name": "Jena D. Hwang"}, {"authorId": "8536286", "name": "A. Bosselut"}, {"authorId": "3159752", "name": "J. Cheung"}, {"authorId": "1699545", "name": "Yejin Choi"}]}, {"paperId": "b670b00ecc6531201f944c90227257bb0b3574f8", "externalIds": {"DBLP": "conf/acl/BaranowskiH21", "ACL": "2021.findings-acl.108", "DOI": "10.18653/v1/2021.findings-acl.108", "CorpusId": 236477930}, "corpusId": 236477930, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b670b00ecc6531201f944c90227257bb0b3574f8", "title": "Grammar-Constrained Neural Semantic Parsing with LR Parsers", "abstract": "Target meaning representations for semantic parsing tasks are often based on programming or query languages, such as SQL, and can be formalized by a context-free grammar. Assuming a priori knowledge of the target domain, such grammars can be exploited to enforce syntactical constraints when predicting logical forms. To that end, we assess how syntactical parsers can be integrated into modern encoder-decoder frameworks. Specifically, we implement an attentional SEQ2SEQ model that uses an LR parser to maintain syntactically valid sequences throughout the decoding procedure. Compared to other approaches to grammar-guided decoding that modify the underlying neural network architecture or attempt to derive full parse trees, our approach is conceptually simpler, adds less computational overhead during inference and integrates seamlessly with current SEQ2SEQ frameworks. We present preliminary evaluation results against a recurrent SEQ2SEQ baseline on GEOQUERY and ATIS and demonstrate improved performance while enforcing grammatical constraints.", "venue": "Findings", "year": 2021, "referenceCount": 15, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.108.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1275-1279"}, "authors": [{"authorId": "51416313", "name": "Artur Baranowski"}, {"authorId": "1688405", "name": "N. Hochgeschwender"}]}, {"paperId": "9f4d1924211b72e77257328ded3980b32786e75d", "externalIds": {"DBLP": "conf/acl/SuWC21", "ACL": "2021.findings-acl.109", "DOI": "10.18653/v1/2021.findings-acl.109", "CorpusId": 236477867}, "corpusId": 236477867, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9f4d1924211b72e77257328ded3980b32786e75d", "title": "Enhanced Metaphor Detection via Incorporation of External Knowledge Based on Linguistic Theories", "abstract": "Use of external knowledge is an important and effective method applied widely in metaphor detection. Although existing knowledge-based methods perform well, when leveraging external knowledge, they take little consideration on linguistic theories of metaphor detection. Based on Metaphor Identification Procedure (MIP) and Select Preference Violation (SPV), directly using examples and definitions of words from the Oxford Dictionary1, we propose two BERT-based models for metaphor detection: ExampleBERT and DefinitionBERT. Experimental results show that our methods achieve state-of-the-art performance on two established metaphor datasets. Furthermore, we show that our DefinitionBERT is highly interpretable.", "venue": "Findings", "year": 2021, "referenceCount": 27, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.109.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1280-1287"}, "authors": [{"authorId": "143698008", "name": "Chang Su"}, {"authorId": "2150371235", "name": "Kechun Wu"}, {"authorId": "2109359287", "name": "Yijiang Chen"}]}, {"paperId": "72024eef4198f88130ba3b8823bdbda9168cc493", "externalIds": {"DBLP": "journals/corr/abs-2105-11018", "ACL": "2021.findings-acl.110", "ArXiv": "2105.11018", "DOI": "10.18653/v1/2021.findings-acl.110", "CorpusId": 235166857}, "corpusId": 235166857, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/72024eef4198f88130ba3b8823bdbda9168cc493", "title": "Controlling Text Edition by Changing Answers of Specific Questions", "abstract": "In this paper, we introduce the new task of controllable text edition, in which we take as input a long text, a question, and a target answer, and the output is a minimally modified text, so that it fits the target answer. This task is very important in many situations, such as changing some conditions, consequences, or properties in a legal document, or changing some key information of an event in a news text. This is very challenging, as it is hard to obtain a parallel corpus for training, and we need to first find all text positions that should be changed and then decide how to change them. We constructed the new dataset WikiBioCTE for this task based on the existing dataset WikiBio (originally created for table-to-text generation). We use WikiBioCTE for training, and manually labeled a test set for testing. We also propose novel evaluation metrics and a novel method for solving the new task. Experimental results on the test set show that our proposed method is a good fit for this novel NLP task.", "venue": "Findings", "year": 2021, "referenceCount": 49, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.110.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-23", "journal": {"pages": "1288-1299"}, "authors": [{"authorId": "39058310", "name": "Lei Sha"}, {"authorId": "15991062", "name": "Patrick Hohenecker"}, {"authorId": "1690572", "name": "Thomas Lukasiewicz"}]}, {"paperId": "c610aeef3a8da5150ff25d5e6bd8c2dd98dd4bee", "externalIds": {"DBLP": "conf/acl/TangZBLWZY21", "ACL": "2021.findings-acl.111", "DOI": "10.18653/v1/2021.findings-acl.111", "CorpusId": 236478113}, "corpusId": 236478113, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c610aeef3a8da5150ff25d5e6bd8c2dd98dd4bee", "title": "Grammar-Based Patches Generation for Automated Program Repair", "abstract": "Automated program repair (APR) aims to find an automatic solution to program language bugs without human intervention, and it can potentially reduce debugging costs and improve software quality. Conventional approaches adopt learning-based methods such as sequence-to-sequence models for the patches generation. However, they tend to ignore the code structure information and suffer from grammar and syntax errors. To consider the grammar and syntax information, in this paper, we propose a grammar-based ruleto-rule model, which regards the repair process as the transformation of grammar rules, and leverages two encoders modeling both the original token sequence and the grammar rules, enhanced with a new tree-based self-attention. Besides, to guarantee grammar correctness, we employ a grammatically restricted inference method to generate each grammar rule in a legally constrained sub-search-space considering the generated previous rules. Experimental evaluations on a Java dataset demonstrate that the proposed approach significantly outperforms the state-of-the-art baselines in terms of generated code accuracy.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 11, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.111.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1300-1305"}, "authors": [{"authorId": "2107222434", "name": "Yu Tang"}, {"authorId": "2135918679", "name": "Long Zhou"}, {"authorId": "37488446", "name": "Ambrosio Blanco"}, {"authorId": "2107983441", "name": "Shujie Liu"}, {"authorId": "49807919", "name": "Furu Wei"}, {"authorId": "92660691", "name": "Ming Zhou"}, {"authorId": "2105775", "name": "Muyun Yang"}]}, {"paperId": "1b5f62d474c2bdc21ec1c816f7c7353c0d735d63", "externalIds": {"DBLP": "journals/corr/abs-2105-09543", "ACL": "2021.findings-acl.112", "ArXiv": "2105.09543", "DOI": "10.18653/v1/2021.findings-acl.112", "CorpusId": 234790381}, "corpusId": 234790381, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1b5f62d474c2bdc21ec1c816f7c7353c0d735d63", "title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction", "abstract": "Distantly supervised (DS) relation extraction (RE) has attracted much attention in the past few years as it can utilize large-scale auto-labeled data. However, its evaluation has long been a problem: previous works either took costly and inconsistent methods to manually examine a small sample of model predictions, or directly test models on auto-labeled data -- which, by our check, produce as much as 53% wrong labels at the entity pair level in the popular NYT10 dataset. This problem has not only led to inaccurate evaluation, but also made it hard to understand where we are and what's left to improve in the research of DS-RE. To evaluate DS-RE models in a more credible way, we build manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20, and thoroughly evaluate several competitive models, especially the latest pre-trained ones. The experimental results show that the manual evaluation can indicate very different conclusions from automatic ones, especially some unexpected observations, e.g., pre-trained models can achieve dominating performance while being more susceptible to false-positives compared to previous methods. We hope that both our manual test sets and novel observations can help advance future DS-RE research.", "venue": "Findings", "year": 2021, "referenceCount": 51, "citationCount": 20, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.112.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-05-20", "journal": {"pages": "1306-1318"}, "authors": [{"authorId": "4800645", "name": "Tianyu Gao"}, {"authorId": "48506411", "name": "Xu Han"}, {"authorId": "2008212953", "name": "Keyue Qiu"}, {"authorId": "2115834034", "name": "Yuzhuo Bai"}, {"authorId": "2099588738", "name": "Zhiyu Xie"}, {"authorId": "2149202150", "name": "Yankai Lin"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "144326610", "name": "Peng Li"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "49178343", "name": "Jie Zhou"}]}, {"paperId": "5d1e996bb64a0d082c85010b9c78036db4da02e0", "externalIds": {"DBLP": "conf/acl/TanWJLLHZH21", "ACL": "2021.findings-acl.113", "DOI": "10.18653/v1/2021.findings-acl.113", "CorpusId": 236478211}, "corpusId": 236478211, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5d1e996bb64a0d082c85010b9c78036db4da02e0", "title": "GCRC: A New Challenging MRC Dataset from Gaokao Chinese for Explainable Evaluation", "abstract": "Recently, driven by numerous publicly available machine reading comprehension (MRC) datasets, MRC systems have made some progress. These datasets, however, have two major limitations: 1) the de\ufb01ned tasks are relatively simple, and 2) they do not provide explainable evaluation which is critical to objectively and comprehensively review the reasoning capabilities of current MRC systems. In this paper, we propose GCRC, a new dataset with challenging and high-quality multi-choice questions, collected from Gaokao Chinese (Chinese subject from the National College Entrance Examination of China). We have manually labelled three types of evidence to evaluate MRC systems\u2019 reasoning process: 1) sentence-level relevant supporting facts in an article required for answering a given question, 2) error reason of a distractor (i.e., an incorrect option) for explaining why a distractor should be eliminated, which is an important reasoning step for multi-choice questions, and 3) types of reasoning skills required for answering questions. Extensive experiments show that our proposed dataset is more challenging and very useful for identifying the limitations of existing MRC systems in an explainable way, facilitating researchers to develop novel machine learning and reasoning approaches to tackle this challenging research problem. 1", "venue": "Findings", "year": 2021, "referenceCount": 20, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "journal": {"pages": "1319-1330"}, "authors": [{"authorId": "3075017", "name": "Hongye Tan"}, {"authorId": "48631751", "name": "Xiaoyue Wang"}, {"authorId": "115560211", "name": "Yuri Ji"}, {"authorId": "2109380034", "name": "Ru Li"}, {"authorId": "2127399967", "name": "Xiaoli Li"}, {"authorId": "2111297694", "name": "Zhiwei Hu"}, {"authorId": "2140037138", "name": "Yunxiao Zhao"}, {"authorId": "2118234808", "name": "Xiaoqi Han"}]}, {"paperId": "d61387def94f4a21125b143c6db909466574f306", "externalIds": {"DBLP": "conf/acl/ZhangWR21", "ACL": "2021.findings-acl.114", "ArXiv": "2012.15243", "DOI": "10.18653/v1/2021.findings-acl.114", "CorpusId": 236477798}, "corpusId": 236477798, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d61387def94f4a21125b143c6db909466574f306", "title": "Zero-shot Label-Aware Event Trigger and Argument Classification", "abstract": "Identifying events and mapping them to pre-defined event types has long been an important natural language processing problem. Most previous work has been heavily relying on labor-intensive and domain-specific annotations while ignoring the semantic meaning contained in the labels of the event types. As a result, the learned models cannot effectively generalize to new domains, where new event types could be introduced. In this paper, we propose an unsupervised event extraction pipeline, which first identifies events with available tools (e.g., SRL) and then automatically maps them to pre-defined event types with our proposed unsupervised classification model. Rather than relying on annotated data, our model matches the semantics of identified events with those of event type labels. Specifically, we leverage pre-trained language models to contextually represent pre-defined types for both event triggers and arguments. After we map identified events to the target types via representation similarity, we use the event ontology (e.g., argument type\"Victim\"can only appear as the argument of event type\"Attack\") as global constraints to regularize the prediction. The proposed approach is shown to be very effective when tested on the ACE-2005 dataset, which has 33 trigger and 22 argument types. Without using any annotation, we successfully map 83% of the triggers and 54% of the arguments to the correct types, almost doubling the performance of previous zero-shot approaches.", "venue": "Findings", "year": 2020, "referenceCount": 33, "citationCount": 31, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.114.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-31", "journal": {"pages": "1331-1340"}, "authors": [{"authorId": "2111112132", "name": "Hongming Zhang"}, {"authorId": "34269118", "name": "Haoyu Wang"}, {"authorId": "144590225", "name": "D. Roth"}]}, {"paperId": "114cd7727c71a1eb28b491e63082405d0af8fac5", "externalIds": {"ACL": "2021.findings-acl.115", "DBLP": "conf/acl/ZhaoZXZLL21", "DOI": "10.18653/v1/2021.findings-acl.115", "CorpusId": 236477489}, "corpusId": 236477489, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/114cd7727c71a1eb28b491e63082405d0af8fac5", "title": "Incorporating Global Information in Local Attention for Knowledge Representation Learning", "abstract": "Graph Attention Networks (GATs) have proven a promising model that takes advantage of localized attention mechanism to perform knowledge representation learning (KRL) on graph-structure data, e.g., Knowledge Graphs (KGs). While such approaches model entities\u2019 local pairwise importance, they lack the capability to model global importance relative to other entities of KGs. This causes such models to miss critical information in tasks where global information is also a significant component for the task, such as in knowledge representation learning. To address the issue, we allow the proper incorporation of global information into the GAT family of models through the use of scaled entity importance, which is calculated by an attention-based global random walk algorithm. In the context of KRL, incorporating global information boosts performance significantly. Experimental results on KG entity prediction against the state-of-thearts sufficiently demonstrate the effectiveness of our proposed model.", "venue": "Findings", "year": 2021, "referenceCount": 76, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.115.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1341-1351"}, "authors": [{"authorId": "97522134", "name": "Yu Zhao"}, {"authorId": "2024855331", "name": "Hannah Zhou"}, {"authorId": "3360722", "name": "Ruobing Xie"}, {"authorId": "1799525", "name": "Fuzhen Zhuang"}, {"authorId": "2117895423", "name": "Qing Li"}, {"authorId": "40478933", "name": "Ji Liu"}]}, {"paperId": "70e22e27d0e533675e22ca976ca7b922b4a44a64", "externalIds": {"DBLP": "journals/corr/abs-2105-14210", "ArXiv": "2105.14210", "ACL": "2021.findings-acl.116", "DOI": "10.18653/v1/2021.findings-acl.116", "CorpusId": 235253817}, "corpusId": 235253817, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/70e22e27d0e533675e22ca976ca7b922b4a44a64", "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification", "abstract": "Aspect sentiment classification (ASC) aims at determining sentiments expressed towards different aspects in a sentence. While state-of-the-art ASC models have achieved remarkable performance, they are recently shown to suffer from the issue of robustness. Particularly in two common scenarios: when domains of test and training data are different (out-of-domain scenario) or test data is adversarially perturbed (adversarial scenario), ASC models may attend to irrelevant words and neglect opinion expressions that truly describe diverse aspects. To tackle the challenge, in this paper, we hypothesize that position bias (i.e., the words closer to a concerning aspect would carry a higher degree of importance) is crucial for building more robust ASC models by reducing the probability of mis-attending. Accordingly, we propose two mechanisms for capturing position bias, namely position-biased weight and position-biased dropout, which can be flexibly injected into existing models to enhance representations for classification. Experiments conducted on out-of-domain and adversarial datasets demonstrate that our proposed approaches largely improve the robustness and effectiveness of current models.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.116.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-29", "journal": {"name": "ArXiv", "volume": "abs/2105.14210"}, "authors": [{"authorId": "103560469", "name": "Fangjie Ma"}, {"authorId": "2111572928", "name": "Chen Zhang"}, {"authorId": "48437245", "name": "D. Song"}]}, {"paperId": "dc3864e841bf4d82a33b0ca303bf5fef256991d9", "externalIds": {"DBLP": "conf/acl/LiXLFRJ21", "ACL": "2021.findings-acl.117", "DOI": "10.18653/v1/2021.findings-acl.117", "CorpusId": 236477583}, "corpusId": 236477583, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/dc3864e841bf4d82a33b0ca303bf5fef256991d9", "title": "MRN: A Locally and Globally Mention-Based Reasoning Network for Document-Level Relation Extraction", "abstract": "Document-level relation extraction aims to detect the relations within one document, which is challenging since it requires complex reasoning using mentions, entities, local and global contexts. Few previous studies have distinguished local and global reasoning explicitly, which may be problematic because they play different roles in intraand inter-sentence relations. Moreover, the interactions between local and global contexts should be considered since they could help relation reasoning based on our observation. In this paper, we propose a novel mention-based reasoning (MRN) module based on explicitly and collaboratively local and global reasoning. Based on MRN, we design a co-predictor module to predict entity relations based on local and global entity and relation representations jointly. We evaluate our MRN model on three widelyused benchmark datasets, namely DocRED, CDR, and GDA. Experimental results show that our model outperforms previous state-ofthe-art models by a large margin.", "venue": "Findings", "year": 2021, "referenceCount": 39, "citationCount": 48, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.117.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1359-1370"}, "authors": [{"authorId": "2000217741", "name": "Jingye Li"}, {"authorId": "2113474255", "name": "Kang Xu"}, {"authorId": "2109530930", "name": "Fei Li"}, {"authorId": "46959445", "name": "Hao Fei"}, {"authorId": "3350168", "name": "Yafeng Ren"}, {"authorId": "1719916", "name": "D. Ji"}]}, {"paperId": "35a3237dae11d0269c721aa1acd101c281e470a8", "externalIds": {"ACL": "2021.findings-acl.118", "DBLP": "conf/acl/SongCCWS21", "DOI": "10.18653/v1/2021.findings-acl.118", "CorpusId": 236478142}, "corpusId": 236478142, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/35a3237dae11d0269c721aa1acd101c281e470a8", "title": "Adversary-Aware Rumor Detection", "abstract": "While social media becomes a primary source of news now, it also becomes more challenging for people to distinguish the rumors and non-rumors, which attracts malicious manip-ulation and may lead to public health harm or economic loss. Consequently, many rumor detection models have been proposed to automatically detect the rumors based on the contents and propagation path. However, most previous works are not aware of malicious attacks, e.g., framing. Therefore, we propose a novel rumor detection framework, Adversary-Aware Rumor Detection including Weighted-Edge Transformer-Graph Network and Position-aware Adversarial Response Generator, to improve the vulnerability of detection models. To the best of our knowledge, this is the \ufb01rst work that can generate the adversarial response with the consideration of the response position. Experimental results show that our model achieves the state-of-the-art on various rumor detection tasks by the proposed Weighted-Edge Transformer-Graph Network and can maintain the performance under the adversarial response attack after the adversarial learning by Position-aware Adversarial Response Generator. 1", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 20, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.118.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1371-1382"}, "authors": [{"authorId": "1491598459", "name": "Yun-Zhu Song"}, {"authorId": "2109293164", "name": "Yi-Syuan Chen"}, {"authorId": "2185273", "name": "Yi-Ting Chang"}, {"authorId": "2121358112", "name": "Shao-Yu Weng"}, {"authorId": "2426757", "name": "Hong-Han Shuai"}]}, {"paperId": "27c39dd62635791a0ec3c0c81c2690e7a9bd62ad", "externalIds": {"DBLP": "conf/acl/GuoZZNLLLT21", "ArXiv": "2108.00801", "ACL": "2021.findings-acl.119", "DOI": "10.18653/v1/2021.findings-acl.119", "CorpusId": 236477372}, "corpusId": 236477372, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/27c39dd62635791a0ec3c0c81c2690e7a9bd62ad", "title": "LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization", "abstract": "Language model pre-training based on large corpora has achieved tremendous success in terms of constructing enriched contextual representations and has led to significant performance gains on a diverse range of Natural Language Understanding (NLU) tasks. Despite the success, most current pre-trained language models, such as BERT, are trained based on single-grained tokenization, usually with fine-grained characters or sub-words, making it hard for them to learn the precise meaning of coarse-grained words and phrases. In this paper, we propose a simple yet effective pre-training method named LICHEE to efficiently incorporate multi-grained information of input text. Our method can be applied to various pre-trained language models and improve their representation capability. Extensive experiments conducted on CLUE and SuperGLUE demonstrate that our method achieves comprehensive improvements on a wide variety of NLU tasks in both Chinese and English with little extra inference cost incurred, and that our best ensemble model achieves the state-of-the-art performance on CLUE benchmark competition.", "venue": "Findings", "year": 2021, "referenceCount": 22, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.119.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-02", "journal": {"name": "ArXiv", "volume": "abs/2108.00801"}, "authors": [{"authorId": "2153297754", "name": "Weidong Guo"}, {"authorId": "50396672", "name": "Mingjun Zhao"}, {"authorId": "153824052", "name": "Lusheng Zhang"}, {"authorId": "2106506796", "name": "Di Niu"}, {"authorId": "2116741301", "name": "Jinwen Luo"}, {"authorId": "2125024057", "name": "Zhenhua Liu"}, {"authorId": "1692446232", "name": "Zhenyang Li"}, {"authorId": "2107705628", "name": "J. Tang"}]}, {"paperId": "01ed852322e718d2c44c4debc8a64631070fa6df", "externalIds": {"MAG": "3095962368", "ACL": "2021.findings-acl.120", "DBLP": "conf/acl/ZhouNGDGZG21", "ArXiv": "2011.02593", "DOI": "10.18653/v1/2021.findings-acl.120", "CorpusId": 226254579}, "corpusId": 226254579, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/01ed852322e718d2c44c4debc8a64631070fa6df", "title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation", "abstract": "Neural sequence models can generate highly fluent sentences but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input, which can cause a lack of trust in the model. To better assess the faithfulness of the machine outputs, we propose a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. We also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of our proposed approach -- we obtain an average F1 of around 0.6 across all the benchmark datasets and achieve significant improvements in sentence-level hallucination scoring compared to baseline methods. We also release our annotated data and code for future research at this https URL.", "venue": "Findings", "year": 2020, "referenceCount": 63, "citationCount": 102, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.120.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-11-05", "journal": {"name": "ArXiv", "volume": "abs/2011.02593"}, "authors": [{"authorId": "2384711", "name": "Chunting Zhou"}, {"authorId": "3016273", "name": "Jiatao Gu"}, {"authorId": "1700007", "name": "Mona T. Diab"}, {"authorId": "134578837", "name": "P. Guzm\u00e1n"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}, {"authorId": "2320509", "name": "Marjan Ghazvininejad"}]}, {"paperId": "4f03e69963b9649950ba29ae864a0de8c14f1f86", "externalIds": {"ACL": "2021.findings-acl.121", "DBLP": "journals/corr/abs-2002-01808", "MAG": "3005441132", "ArXiv": "2002.01808", "DOI": "10.18653/v1/2021.findings-acl.121", "CorpusId": 211031933}, "corpusId": 211031933, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4f03e69963b9649950ba29ae864a0de8c14f1f86", "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters", "abstract": "We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, they may suffer from catastrophic forgetting. To address this, we propose K-Adapter, which remains the original parameters of the pre-trained model fixed and supports continual knowledge infusion. Taking RoBERTa as the pre-trained model, K-Adapter has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus different adapters are efficiently trained in a distributed way. We inject two kinds of knowledge, including factual knowledge obtained from automatically aligned text-triplets on Wikipedia and Wikidata, and linguistic knowledge obtained from dependency parsing. Results on three knowledge-driven tasks (total six datasets) including relation classification, entity typing and question answering demonstrate that each adapter improves the performance, and the combination of both adapters brings further improvements. Probing experiments further indicate that K-Adapter captures richer factual and commonsense knowledge than RoBERTa.", "venue": "Findings", "year": 2020, "referenceCount": 53, "citationCount": 372, "influentialCitationCount": 58, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.121.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-02-05", "journal": {"pages": "1405-1418"}, "authors": [{"authorId": "29068663", "name": "Ruize Wang"}, {"authorId": "39483833", "name": "Duyu Tang"}, {"authorId": "46429989", "name": "Nan Duan"}, {"authorId": "2712533", "name": "Zhongyu Wei"}, {"authorId": "1790227", "name": "Xuanjing Huang"}, {"authorId": "22206677", "name": "Jianshu Ji"}, {"authorId": "3320836", "name": "Guihong Cao"}, {"authorId": "71790825", "name": "Daxin Jiang"}, {"authorId": "92660691", "name": "Ming Zhou"}]}, {"paperId": "74512e17ba6e9617260bf9a65503ffa22bc66e3a", "externalIds": {"DBLP": "conf/acl/GuoNWZX21", "ACL": "2021.findings-acl.122", "DOI": "10.18653/v1/2021.findings-acl.122", "CorpusId": 236477700}, "corpusId": 236477700, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/74512e17ba6e9617260bf9a65503ffa22bc66e3a", "title": "Global Attention Decoder for Chinese Spelling Error Correction", "abstract": "Recent progress has been made in using BERT framework for Chinese spelling error correction (CSC). However, most existing methods correct words based on local contextual information, without considering the in\ufb02uence of error words in sentences. Imposing attention on error contextual information could mislead and decrease the overall performance of CSC. To address this issue, we propose a G lobal A ttention D ecoder (GAD) approach for CSC. Speci\ufb01cally, the proposed method learns the global relationship of the potential correct input characters and the candidates of potential error characters. Rich global contextual information is obtained to alleviate the impact of the local error contextual information. In addition, a BERT with C onfusion set guided R eplacement S trategy (BERT CRS) is designed to narrow the gap between BERT and CSC. The candidates generated by BERT CRS covering the correct character with more than 99.9% probability. To demonstrate the effectiveness of our proposed framework, we test our method on three human-annotated datasets. The experimental results show that our approach outperforms all competitor models by a large margin of up to 6.2%, achieving state-of-the-art methods on all datasets.", "venue": "Findings", "year": 2021, "referenceCount": 24, "citationCount": 25, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.122.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1419-1428"}, "authors": [{"authorId": "2112598274", "name": "Zhao Guo"}, {"authorId": "2072724069", "name": "Yuan Ni"}, {"authorId": "2124619305", "name": "Keqiang Wang"}, {"authorId": "2152348673", "name": "Wei Zhu"}, {"authorId": "2052157756", "name": "G. Xie"}]}, {"paperId": "703f6adc2c80d3e9673ff90c64a41359ed99b4ab", "externalIds": {"ACL": "2021.findings-acl.123", "DBLP": "conf/acl/ChenHLWW21", "DOI": "10.18653/v1/2021.findings-acl.123", "CorpusId": 236478116}, "corpusId": 236478116, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/703f6adc2c80d3e9673ff90c64a41359ed99b4ab", "title": "Jointly Identifying Rhetoric and Implicit Emotions via Multi-Task Learning", "abstract": "Rhetorical implicit emotion identification is one of important and challenging tasks in natural language processing. We observe that each rhetoric may express certain evidence of semantic and syntactic patterns. Then, we design a gate mechanism based classification module to capture respective rhetorical representation and identify each rhetoric. Moreover, sentences carved with rhetoric tends to express emotions in subtle ways. We thus propose a new multi-task learning framework that can encode the categorical correlation between tasks to improve the performance of rhetoric and emotion identification problem. Experimental results validate the benefit of the proposed model over state-of-the-art baselines for rhetoric and emotion identification tasks. In addition, a new Chinese rhetorical implicit emotion dataset was constructed and will be released in this work.", "venue": "Findings", "year": 2021, "referenceCount": 19, "citationCount": 4, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.123.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1429-1434"}, "authors": [{"authorId": "2145230492", "name": "Xin Chen"}, {"authorId": "2754618", "name": "Zhen Hai"}, {"authorId": "49620984", "name": "Deyu Li"}, {"authorId": "2622027", "name": "Suge Wang"}, {"authorId": "2119265832", "name": "Di Wang"}]}, {"paperId": "84c018678e19d1508c5cd86f93ebdad62f0302a8", "externalIds": {"DBLP": "conf/acl/GhosalMMP21", "ACL": "2021.findings-acl.124", "DOI": "10.18653/v1/2021.findings-acl.124", "CorpusId": 236477584}, "corpusId": 236477584, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/84c018678e19d1508c5cd86f93ebdad62f0302a8", "title": "Exploring the Role of Context in Utterance-level Emotion, Act and Intent Classification in Conversations: An Empirical Study", "abstract": "the user inten-tion and background. In recent years, a number of context-aware approaches have been proposed for various utterance-level dialogue understanding tasks. In this paper, we explore and quantify the role of context for different aspects of a dialogue, namely emotion, dialogue act, and intent identi\ufb01cation, using state-of-the-art dialogue understanding methods as baselines. Speci\ufb01cally, we employ various perturbations to distort the context of a given utterance and study its impact on the different tasks and baselines. This provides us with insights into the fundamental context factors that have immediate implications on different aspects of a dialogue. Such insights may inspire more effective dialogue understanding models and provide support for future text generation approaches.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.124.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1435-1449"}, "authors": [{"authorId": "32528506", "name": "Deepanway Ghosal"}, {"authorId": "35122767", "name": "Navonil Majumder"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}, {"authorId": "1746416", "name": "Soujanya Poria"}]}, {"paperId": "648a3bd3647f062c0111799c5f262b4ac7db1391", "externalIds": {"DBLP": "conf/acl/AilemLQ21", "ArXiv": "2106.03730", "ACL": "2021.findings-acl.125", "DOI": "10.18653/v1/2021.findings-acl.125", "CorpusId": 235358574}, "corpusId": 235358574, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/648a3bd3647f062c0111799c5f262b4ac7db1391", "title": "Encouraging Neural Machine Translation to Satisfy Terminology Constraints", "abstract": "We present a new approach to encourage neural machine translation to satisfy lexical constraints. Our method acts at the training step and thereby avoiding the introduction of any extra computational overhead at inference step. The proposed method combines three main ingredients. The first one consists in augmenting the training data to specify the constraints. Intuitively, this encourages the model to learn a copy behavior when it encounters constraint terms. Compared to previous work, we use a simplified augmentation strategy without source factors. The second ingredient is constraint token masking, which makes it even easier for the model to learn the copy behavior and generalize better. The third one, is a modification of the standard cross entropy loss to bias the model towards assigning high probabilities to constraint words. Empirical results show that our method improves upon related baselines in terms of both BLEU score and the percentage of generated constraint terms.", "venue": "Findings", "year": 2021, "referenceCount": 19, "citationCount": 11, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.125.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-07", "journal": {"name": "ArXiv", "volume": "abs/2106.03730"}, "authors": [{"authorId": "3097148", "name": "Melissa Ailem"}, {"authorId": "2108372318", "name": "Jinghsu Liu"}, {"authorId": "1912522", "name": "Raheel Qader"}]}, {"paperId": "d22b109eb5089179f8bd48ef47513533890f6bf9", "externalIds": {"DBLP": "journals/corr/abs-2105-05727", "ArXiv": "2105.05727", "MAG": "3173753074", "ACL": "2021.findings-acl.126", "DOI": "10.18653/v1/2021.findings-acl.126", "CorpusId": 234469858}, "corpusId": 234469858, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d22b109eb5089179f8bd48ef47513533890f6bf9", "title": "BertGCN: Transductive Text Classification by Combining GNN and BERT", "abstract": "In this work, we propose BertGCN, a model that combines large scale pretraining and transductive learning for text classification. BertGCN constructs a heterogeneous graph over the dataset and represents documents as nodes using BERT representations. By jointly training the BERT and GCN modules within BertGCN, the proposed model is able to leverage the advantages of both worlds: large-scale pretraining which takes the advantage of the massive amount of raw data and transductive learning which jointly learns representations for both training data and unlabeled test data by propagating label influence through graph convolution. Experiments show that BertGCN achieves SOTA performances on a wide range of text classification datasets. Code is available at https://github.com/ZeroRin/BertGCN.", "venue": "Findings", "year": 2021, "referenceCount": 50, "citationCount": 118, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.126.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-12", "journal": {"name": "ArXiv", "volume": "abs/2105.05727"}, "authors": [{"authorId": "1993442141", "name": "Yuxiao Lin"}, {"authorId": "65844131", "name": "Yuxian Meng"}, {"authorId": "2109329406", "name": "Xiaofei Sun"}, {"authorId": "5439717", "name": "Qinghong Han"}, {"authorId": "33870528", "name": "Kun Kuang"}, {"authorId": "49298465", "name": "Jiwei Li"}, {"authorId": "144894837", "name": "Fei Wu"}]}, {"paperId": "0d8f28b641adeb67d5f5bb6a50d50d530e088039", "externalIds": {"DBLP": "journals/corr/abs-2107-05243", "ACL": "2021.findings-acl.127", "ArXiv": "2107.05243", "DOI": "10.18653/v1/2021.findings-acl.127", "CorpusId": 235795109}, "corpusId": 235795109, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0d8f28b641adeb67d5f5bb6a50d50d530e088039", "title": "Putting words into the system\u2019s mouth: A targeted attack on neural machine translation using monolingual data poisoning", "abstract": "Neural machine translation systems are known to be vulnerable to adversarial test inputs, however, as we show in this paper, these systems are also vulnerable to training attacks. Specifically, we propose a poisoning attack in which a malicious adversary inserts a small poisoned sample of monolingual text into the training set of a system trained using back-translation. This sample is designed to induce a specific, targeted translation behaviour, such as peddling misinformation. We present two methods for crafting poisoned examples, and show that only a tiny handful of instances, amounting to only 0.02% of the training set, is sufficient to enact a successful attack. We outline a defence method against said attacks, which partly ameliorates the problem. However, we stress that this is a blind-spot in modern NMT, demanding immediate attention.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 18, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.127.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-12", "journal": {"name": "ArXiv", "volume": "abs/2107.05243"}, "authors": [{"authorId": "2152811136", "name": "Jun Wang"}, {"authorId": "2115472755", "name": "Chang Xu"}, {"authorId": "144204682", "name": "Francisco Guzm\u00e1n"}, {"authorId": "1398503968", "name": "Ahmed El-Kishky"}, {"authorId": "1825565215", "name": "Yuqing Tang"}, {"authorId": "1868067", "name": "Benjamin I. P. Rubinstein"}, {"authorId": "143620680", "name": "Trevor Cohn"}]}, {"paperId": "0363e12da739e50bac2dc369be935c9b45d97512", "externalIds": {"DBLP": "journals/corr/abs-2106-03315", "ACL": "2021.findings-acl.128", "ArXiv": "2106.03315", "DOI": "10.18653/v1/2021.findings-acl.128", "CorpusId": 235358854}, "corpusId": 235358854, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0363e12da739e50bac2dc369be935c9b45d97512", "title": "Semantic and Syntactic Enhanced Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to extract triplets from sentences, where each triplet includes an entity, its associated sentiment, and the opinion span explaining the reason for the sentiment. Most existing research addresses this problem in a multi-stage pipeline manner, which neglects the mutual information between such three elements and has the problem of error propagation. In this paper, we propose a Semantic and Syntactic Enhanced aspect Sentiment triplet Extraction model (S3E2) to fully exploit the syntactic and semantic relationships between the triplet elements and jointly extract them. Specifically, we design a Graph-Sequence duel representation and modeling paradigm for the task of ASTE: we represent the semantic and syntactic relationships between word pairs in a sentence by graph and encode it by Graph Neural Networks (GNNs), as well as modeling the original sentence by LSTM to preserve the sequential information. Under this setting, we further apply a more efficient inference strategy for the extraction of triplets. Extensive evaluations on four benchmark datasets show that S3E2 significantly outperforms existing approaches, which proves our S3E2's superiority and flexibility in an end-to-end fashion.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 34, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.128.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-07", "journal": {"name": "ArXiv", "volume": "abs/2106.03315"}, "authors": [{"authorId": "2111333318", "name": "Zhexue Chen"}, {"authorId": "2115734676", "name": "Hong Huang"}, {"authorId": "7648396", "name": "Bang Liu"}, {"authorId": "1678835", "name": "Xuanhua Shi"}, {"authorId": "145914256", "name": "Hai Jin"}]}, {"paperId": "331102c42a340e7bcd9a4b063b9b1204f30f665f", "externalIds": {"DBLP": "conf/acl/ZhongTWYD21", "ACL": "2021.findings-acl.129", "DOI": "10.18653/v1/2021.findings-acl.129", "CorpusId": 236478014}, "corpusId": 236478014, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/331102c42a340e7bcd9a4b063b9b1204f30f665f", "title": "UserAdapter: Few-Shot User Learning in Sentiment Analysis", "abstract": "Adapting a model to a handful of personalized data is challenging, especially when it has gigantic parameters, such as a Transformerbased pretrained model. The standard way of fine-tuning all the parameters necessitates storing a huge model for each user. In this work, we introduce a lightweight approach dubbed UserAdapter, which clamps hundred millions of parameters of the Transformer model and optimizes a tiny user-specific vector. We take sentiment analysis as a test bed, and collect datasets of reviews from Yelp and IMDB respectively. Results show that, on both datasets, UserAdapter achieves better accuracy than the standard fine-tuned Transformerbased pre-trained model. More importantly, UserAdapter offers an efficient way to produce a personalized Transformer model with less than 0.5% parameters added for each user.", "venue": "Findings", "year": 2021, "referenceCount": 8, "citationCount": 12, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.129.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "journal": {"pages": "1484-1488"}, "authors": [{"authorId": "81970097", "name": "Wanjun Zhong"}, {"authorId": "39483833", "name": "Duyu Tang"}, {"authorId": "2815388", "name": "Jiahai Wang"}, {"authorId": "2111609925", "name": "Jian Yin"}, {"authorId": "46429989", "name": "Nan Duan"}]}, {"paperId": "db20a10ef5641a0d0e60584e4cc8430a9763d437", "externalIds": {"DBLP": "journals/corr/abs-2106-01702", "ArXiv": "2106.01702", "ACL": "2021.findings-acl.130", "DOI": "10.18653/v1/2021.findings-acl.130", "CorpusId": 235313818}, "corpusId": 235313818, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/db20a10ef5641a0d0e60584e4cc8430a9763d437", "title": "PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support", "abstract": "Great research interests have been attracted to devise AI services that are able to provide mental health support. However, the lack of corpora is a main obstacle to this research, particularly in Chinese language. In this paper, we propose PsyQA, a Chinese dataset of psychological health support in the form of question and answer pair. PsyQA is crawled from a Chinese mental health service platform, and contains 22K questions and 56K long and well-structured answers. Based on the psychological counseling theories, we annotate a portion of answer texts with typical strategies for providing support, and further present in-depth analysis of both lexical features and strategy patterns in the counseling answers. We also evaluate the performance of generating counseling answers with the generative pretrained models. Results show that utilizing strategies enhances the fluency and helpfulness of generated answers, but there is still a large space for future research.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 26, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.130.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01702"}, "authors": [{"authorId": "144990601", "name": "Hao Sun"}, {"authorId": "2108740041", "name": "Zhenru Lin"}, {"authorId": "146452866", "name": "Chujie Zheng"}, {"authorId": "50152447", "name": "Siyang Liu"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "71fab1ce3c66998ba681ab378484be77690327a9", "externalIds": {"DBLP": "conf/acl/LinWYLR21", "ACL": "2021.findings-acl.131", "ArXiv": "2101.00376", "DOI": "10.18653/v1/2021.findings-acl.131", "CorpusId": 235731975}, "corpusId": 235731975, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/71fab1ce3c66998ba681ab378484be77690327a9", "title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge", "abstract": "Question: I have five fingers but I am not alive. What am I? Answer: a glove. Answering such a riddle-style question is a challenging cognitive process, in that it requires complex commonsense reasoning abilities, an understanding of figurative language, and counterfactual reasoning skills, which are all important abilities for advanced natural language understanding (NLU). However, there are currently no dedicated datasets aiming to test these abilities. Herein, we present RiddleSense, a new multiple-choice question answering task, which comes with the first large dataset (5.7k examples) for answering riddle-style commonsense questions. We systematically evaluate a wide range of models over the challenge, and point out that there is a large gap between the best-supervised model and human performance -- suggesting intriguing future research in the direction of higher-order commonsense reasoning and linguistic creativity towards building advanced NLU systems.", "venue": "Findings", "year": 2021, "referenceCount": 47, "citationCount": 26, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.131.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-01-02", "journal": {"pages": "1504-1515"}, "authors": [{"authorId": "51583409", "name": "Bill Yuchen Lin"}, {"authorId": "2004501562", "name": "Ziyi Wu"}, {"authorId": "2108652034", "name": "Yichi Yang"}, {"authorId": "2115475530", "name": "Dong-Ho Lee"}, {"authorId": "1384550891", "name": "Xiang Ren"}]}, {"paperId": "622b17134bca8302305d5607cdeb6769b08b9fc1", "externalIds": {"DBLP": "conf/acl/BackKCLC21", "ACL": "2021.findings-acl.132", "DOI": "10.18653/v1/2021.findings-acl.132", "CorpusId": 236477924}, "corpusId": 236477924, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/622b17134bca8302305d5607cdeb6769b08b9fc1", "title": "Learning to Generate Questions by Learning to Recover Answer-containing Sentences", "abstract": "To train a question answering model based on machine reading comprehension (MRC), signi\ufb01cant effort is required to prepare annotated training data composed of questions and their answers from contexts. Recent research has focused on synthetically generating a question from a given context and an annotated (or generated) answer by training an additional generative model to augment the training data. In light of this research direction, we propose a novel pre-training approach that learns to generate contextually rich questions, by recover-ing answer-containing sentences. We evaluate our method against existing ones in terms of the quality of generated questions, and \ufb01ne-tuned MRC model accuracy after training on the data synthetically generated by our method. We consistently improve the question generation capability of existing models such as T5 and UniLM, and achieve state-of-the-art results on MS MARCO and NewsQA, and comparable results to the state-of-the-art on SQuAD. Additionally, the data synthetically generated by our approach is bene\ufb01cial for boosting up the downstream MRC accuracy across a wide range of datasets, such as SQuAD-v1.1, v2.0, KorQuAD and BioASQ, without any modi\ufb01cation to the existing MRC models. Furthermore, our method shines especially when a limited amount of pre-training or downstream MRC data is given.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.132.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1516-1529"}, "authors": [{"authorId": "51128138", "name": "Seohyun Back"}, {"authorId": "1576802314", "name": "Akhil Kedia"}, {"authorId": "1576802357", "name": "Sai Chetan Chinthakindi"}, {"authorId": "2110308676", "name": "Haejun Lee"}, {"authorId": "1795455", "name": "J. Choo"}]}, {"paperId": "48502b3988213b0d296c4b9d37577df8807b250d", "externalIds": {"ArXiv": "2106.02363", "DBLP": "journals/corr/abs-2106-02363", "ACL": "2021.findings-acl.133", "DOI": "10.18653/v1/2021.findings-acl.133", "CorpusId": 235352590}, "corpusId": 235352590, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/48502b3988213b0d296c4b9d37577df8807b250d", "title": "Learning Slice-Aware Representations with Mixture of Attentions", "abstract": "Real-world machine learning systems are achieving remarkable performance in terms of coarse-grained metrics like overall accuracy and F-1 score. However, model improvement and development often require fine-grained modeling on individual data subsets or slices, for instance, the data slices where the models have unsatisfactory results. In practice, it gives tangible values for developing such models that can pay extra attention to critical or interested slices while retaining the original overall performance. This work extends the recent slice-based learning (SBL)~\\cite{chen2019slice} with a mixture of attentions (MoA) to learn slice-aware dual attentive representations. We empirically show that the MoA approach outperforms the baseline method as well as the original SBL approach on monitored slices with two natural language understanding (NLU) tasks.", "venue": "Findings", "year": 2021, "referenceCount": 28, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.133.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02363"}, "authors": [{"authorId": "2144492662", "name": "Cheng Wang"}, {"authorId": "2108230831", "name": "Sungjin Lee"}, {"authorId": "2108106092", "name": "Sunghyun Park"}, {"authorId": "2145571947", "name": "Han Li"}, {"authorId": "49170818", "name": "Young-Bum Kim"}, {"authorId": "1705701", "name": "R. Sarikaya"}]}, {"paperId": "d986d3436ccdef0c1e450036dfc0ce47d6948f58", "externalIds": {"ACL": "2021.findings-acl.134", "DBLP": "journals/corr/abs-2106-04814", "ArXiv": "2106.04814", "DOI": "10.18653/v1/2021.findings-acl.134", "CorpusId": 235377367}, "corpusId": 235377367, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d986d3436ccdef0c1e450036dfc0ce47d6948f58", "title": "Making Better Use of Bilingual Information for Cross-Lingual AMR Parsing", "abstract": "Abstract Meaning Representation (AMR) is a rooted, labeled, acyclic graph representing the semantics of natural language. As previous works show, although AMR is designed for English at first, it can also represent semantics in other languages. However, they find that concepts in their predicted AMR graphs are less specific. We argue that the misprediction of concepts is due to the high relevance between English tokens and AMR concepts. In this work, we introduce bilingual input, namely the translated texts as well as non-English texts, in order to enable the model to predict more accurate concepts. Besides, we also introduce an auxiliary task, requiring the decoder to predict the English sequences at the same time. The auxiliary task can help the decoder understand what exactly the corresponding English tokens are. Our proposed cross-lingual AMR parser surpasses previous state-of-the-art parser by 10.6 points on Smatch F1 score. The ablation study also demonstrates the efficacy of our proposed modules.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 1, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.134.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-09", "journal": {"pages": "1537-1547"}, "authors": [{"authorId": "51127875", "name": "Yitao Cai"}, {"authorId": "3341018", "name": "Zhe-nan Lin"}, {"authorId": "145078589", "name": "Xiaojun Wan"}]}, {"paperId": "6133fa7026dbc659727b1554bc4ad167a3b1b315", "externalIds": {"ArXiv": "2109.01862", "ACL": "2021.findings-acl.135", "DBLP": "journals/corr/abs-2109-01862", "DOI": "10.18653/v1/2021.findings-acl.135", "CorpusId": 236477448}, "corpusId": 236477448, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6133fa7026dbc659727b1554bc4ad167a3b1b315", "title": "Pushing Paraphrase Away from Original Sentence: A Multi-Round Paraphrase Generation Approach", "abstract": "In recent years, neural paraphrase generation based on Seq2Seq has achieved superior performance, however, the generated paraphrase still has the problem of lack of diversity. In this paper, we focus on improving the diversity between the generated paraphrase and the original sentence, i.e., making generated paraphrase different from the original sentence as much as possible. We propose BTmPG (Back-Translation guided multi-round Paraphrase Generation), which leverages multi-round paraphrase generation to improve diversity and employs back-translation to preserve semantic information. We evaluate BTmPG on two benchmark datasets. Both automatic and human evaluation show BTmPG can improve the diversity of paraphrase while preserving the semantics of the original sentence.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 12, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.135.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-04", "journal": {"name": "ArXiv", "volume": "abs/2109.01862"}, "authors": [{"authorId": "3341018", "name": "Zhe-nan Lin"}, {"authorId": "145078589", "name": "Xiaojun Wan"}]}, {"paperId": "5e1621967c6a85bfa2dc0277a09bd0d2d9789e47", "externalIds": {"DBLP": "conf/acl/LiTZWYW21", "ArXiv": "2106.01623", "ACL": "2021.findings-acl.136", "DOI": "10.18653/v1/2021.findings-acl.136", "CorpusId": 235313702}, "corpusId": 235313702, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5e1621967c6a85bfa2dc0277a09bd0d2d9789e47", "title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models", "abstract": "This paper studies how to automatically generate a natural language text that describes the facts in knowledge graph (KG). Considering the few-shot setting, we leverage the excellent capacities of pretrained language models (PLMs) in language understanding and generation. We make three major technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our model on KG-to-text generation task. In particular, our model outperforms all comparison methods on both fully-supervised and few-shot settings. Our code and datasets are available at https://github.com/RUCAIBox/Few-Shot-KG2Text.", "venue": "Findings", "year": 2021, "referenceCount": 38, "citationCount": 28, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.136.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"pages": "1558-1568"}, "authors": [{"authorId": "2138220600", "name": "Junyi Li"}, {"authorId": "1997234792", "name": "Tianyi Tang"}, {"authorId": "2542603", "name": "Wayne Xin Zhao"}, {"authorId": "2115564913", "name": "Zhicheng Wei"}, {"authorId": "1677643972", "name": "N. Yuan"}, {"authorId": "153693432", "name": "Ji-rong Wen"}]}, {"paperId": "c5662edb2182b5e27eb73d1187c37db28c98fba6", "externalIds": {"ACL": "2021.findings-acl.137", "DBLP": "conf/acl/SiZQLWLS21", "ArXiv": "2012.15699", "DOI": "10.18653/v1/2021.findings-acl.137", "CorpusId": 236477723}, "corpusId": 236477723, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c5662edb2182b5e27eb73d1187c37db28c98fba6", "title": "Better Robustness by More Coverage: Adversarial and Mixup Data Augmentation for Robust Finetuning", "abstract": "Pretrained language models (PLMs) perform poorly under adversarial attacks. To improve the adversarial robustness, adversarial data augmentation (ADA) has been widely adopted to cover more search space of adversarial attacks by adding textual adversarial examples during training. However, the number of adversarial examples for text augmentation is still extremely insufficient due to the exponentially large attack search space. In this work, we propose a simple and effective method to cover a much larger proportion of the attack search space, called Adversarial and Mixup Data Augmentation (AMDA). Specifically, AMDA linearly interpolates the representations of pairs of training samples to form new virtual samples, which are more abundant and diverse than the discrete text adversarial examples in conventional ADA. Moreover, to fairly evaluate the robustness of different models, we adopt a challenging evaluation setup, which generates a new set of adversarial examples targeting each model. In text classification experiments of BERT and RoBERTa, AMDA achieves significant robustness gains under two strong adversarial attacks and alleviates the performance degradation of ADA on the clean data. Our code is available at: https://github.com/thunlp/MixADA .", "venue": "Findings", "year": 2020, "referenceCount": 36, "citationCount": 38, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.137.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-31", "journal": {"pages": "1569-1576"}, "authors": [{"authorId": "152358188", "name": "Chenglei Si"}, {"authorId": "2148904862", "name": "Zhengyan Zhang"}, {"authorId": "51466208", "name": "Fanchao Qi"}, {"authorId": "2141313179", "name": "Zhiyuan Liu"}, {"authorId": "2108738457", "name": "Yasheng Wang"}, {"authorId": "30738758", "name": "Qun Liu"}, {"authorId": "1753344", "name": "Maosong Sun"}]}, {"paperId": "f747527e455c118cbadd169479323f38032c0475", "externalIds": {"DBLP": "conf/acl/HuangCWGZH21", "ACL": "2021.findings-acl.138", "ArXiv": "2106.02210", "DOI": "10.18653/v1/2021.findings-acl.138", "CorpusId": 235352940}, "corpusId": 235352940, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f747527e455c118cbadd169479323f38032c0475", "title": "NAST: A Non-Autoregressive Generator with Word Alignment for Unsupervised Text Style Transfer", "abstract": "Autoregressive models have been widely used in unsupervised text style transfer. Despite their success, these models still suffer from the content preservation problem that they usually ignore part of the source sentence and generate some irrelevant words with strong styles. In this paper, we propose a Non-Autoregressive generator for unsupervised text Style Transfer (NAST), which alleviates the problem from two aspects. First, we observe that most words in the transferred sentence can be aligned with related words in the source sentence, so we explicitly model word alignments to suppress irrelevant words. Second, existing models trained with the cycle loss align sentences in two stylistic text spaces, which lacks fine-grained control at the word level. The proposed non-autoregressive generator focuses on the connections between aligned words, which learns the word-level transfer between styles. For experiments, we integrate the proposed generator into two base models and evaluate them on two style transfer tasks. The results show that NAST can significantly improve the overall performance and provide explainable word alignments. Moreover, the non-autoregressive generator achieves over 10x speedups at inference. Our codes are available at https://github.com/thu-coai/NAST.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 14, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.138.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"pages": "1577-1590"}, "authors": [{"authorId": "152159016", "name": "Fei Huang"}, {"authorId": "2117099036", "name": "Zikai Chen"}, {"authorId": null, "name": "Chen Henry Wu"}, {"authorId": "2107034399", "name": "Qihan Guo"}, {"authorId": "145213540", "name": "Xiaoyan Zhu"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "8d3076c38f56df22052567f4783c670d8e860f09", "externalIds": {"ArXiv": "2105.06041", "ACL": "2021.findings-acl.139", "DBLP": "conf/acl/GaoTPLH21", "DOI": "10.18653/v1/2021.findings-acl.139", "CorpusId": 234482518}, "corpusId": 234482518, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8d3076c38f56df22052567f4783c670d8e860f09", "title": "HyKnow: End-to-End Task-Oriented Dialog Modeling with Hybrid Knowledge Management", "abstract": "Task-oriented dialog (TOD) systems typically manage structured knowledge (e.g. ontologies and databases) to guide the goal-oriented conversations. However, they fall short of handling dialog turns grounded on unstructured knowledge (e.g. reviews and documents). In this paper, we formulate a task of modeling TOD grounded on both structured and unstructured knowledge. To address this task, we propose a TOD system with hybrid knowledge management, HyKnow. It extends the belief state to manage both structured and unstructured knowledge, and is the first end-to-end model that jointly optimizes dialog modeling grounded on these two kinds of knowledge. We conduct experiments on the modified version of MultiWOZ 2.1 dataset, where dialogs are grounded on hybrid knowledge. Experimental results show that HyKnow has strong end-to-end performance compared to existing TOD systems. It also outperforms the pipeline knowledge management schemes, with higher unstructured knowledge retrieval accuracy.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.139.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-05-13", "journal": {"pages": "1591-1602"}, "authors": [{"authorId": "1492154932", "name": "Silin Gao"}, {"authorId": "51055574", "name": "Ryuichi Takanobu"}, {"authorId": "145439284", "name": "Wei Peng"}, {"authorId": "1688015", "name": "Qun Liu"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "b5ea9f36777b719c24ccee3b67dca361e9c93daa", "externalIds": {"ACL": "2021.findings-acl.140", "DBLP": "conf/acl/ZhangMCXZ21", "ArXiv": "2107.10523", "DOI": "10.18653/v1/2021.findings-acl.140", "CorpusId": 236171300}, "corpusId": 236171300, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b5ea9f36777b719c24ccee3b67dca361e9c93daa", "title": "Target-oriented Fine-tuning for Zero-Resource Named Entity Recognition", "abstract": "Zero-resource named entity recognition (NER) severely suffers from data scarcity in a specific domain or language. Most studies on zero-resource NER transfer knowledge from various data by fine-tuning on different auxiliary tasks. However, how to properly select training data and fine-tuning tasks is still an open problem. In this paper, we tackle the problem by transferring knowledge from three aspects, i.e., domain, language and task, and strengthening connections among them. Specifically, we propose four practical guidelines to guide knowledge transfer and task fine-tuning. Based on these guidelines, we design a target-oriented fine-tuning (TOF) framework to exploit various data from three aspects in a unified training manner. Experimental results on six benchmarks show that our method yields consistent improvements over baselines in both cross-domain and cross-lingual scenarios. Particularly, we achieve new state-of-the-art performance on five benchmarks.", "venue": "Findings", "year": 2021, "referenceCount": 38, "citationCount": 7, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.140.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-22", "journal": {"pages": "1603-1615"}, "authors": [{"authorId": null, "name": "Ying Zhang"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "47559028", "name": "Yufeng Chen"}, {"authorId": "2310092", "name": "Jinan Xu"}, {"authorId": "2108485135", "name": "Jie Zhou"}]}, {"paperId": "a76c98c6814ce8de07707b81c18520af508b7184", "externalIds": {"ACL": "2021.findings-acl.141", "DBLP": "journals/corr/abs-2106-01452", "ArXiv": "2106.01452", "DOI": "10.18653/v1/2021.findings-acl.141", "CorpusId": 235313334}, "corpusId": 235313334, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/a76c98c6814ce8de07707b81c18520af508b7184", "title": "BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks", "abstract": "Adversarial attacks expose important blind spots of deep learning systems. While word- and sentence-level attack scenarios mostly deal with finding semantic paraphrases of the input that fool NLP models, character-level attacks typically insert typos into the input stream. It is commonly thought that these are easier to defend via spelling correction modules. In this work, we show that both a standard spellchecker and the approach of Pruthi et al. (2019), which trains to defend against insertions, deletions and swaps, perform poorly on the character-level benchmark recently proposed in Eger and Benz (2020) which includes more challenging attacks such as visual and phonetic perturbations and missing word segmentations. In contrast, we show that an untrained iterative approach which combines context-independent character-level information with context-dependent information from BERT's masked language modeling can perform on par with human crowd-workers from Amazon Mechanical Turk (AMT) supervised via 3-shot learning.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 14, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.141.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01452"}, "authors": [{"authorId": "2080125428", "name": "Y. Keller"}, {"authorId": "147106579", "name": "J. Mackensen"}, {"authorId": "2620186", "name": "Steffen Eger"}]}, {"paperId": "f9c21c55e5a9e805be83b054a96d54303ddd2a16", "externalIds": {"DBLP": "conf/acl/XieSZQD21", "ACL": "2021.findings-acl.142", "DOI": "10.18653/v1/2021.findings-acl.142", "CorpusId": 236477950}, "corpusId": 236477950, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f9c21c55e5a9e805be83b054a96d54303ddd2a16", "title": "Event Detection as Graph Parsing", "abstract": "Event detection is a fundamental task in information extraction. Most previous approaches typically view event detection as a triggerbased classification problem, focusing on using syntactic dependency structure or external knowledge to boost the classification performance. To overcome the inherent issues with existing trigger classification based models, we propose a novel approach to event detection by formulating it as a graph parsing problem, which can explicitly model the multiple event correlations and naturally utilize the rich information conveyed by event type and subtype. Furthermore, to cope with data sparsity, we employ a pretrained sequence-tosequence (seq2seq) model to transduce an input sentence into an accurate event graph without the need for trigger words. Extensive experimental results on the public ACE2005 dataset show that, our approach outperforms all previous state-of-the-art models for event detection by a large margin, obtaining an improvement of 4.2% F1 score. The result is very encouraging since we achieve this with a conceptually simple seq2seq model; moreover, by extending the graph structure, this proposed architecture can be flexibly applied to more information extraction problems for sentences.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 9, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.142.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1630-1640"}, "authors": [{"authorId": "2153624151", "name": "Jianye Xie"}, {"authorId": "2156232246", "name": "Haotong Sun"}, {"authorId": "3016489", "name": "Junsheng Zhou"}, {"authorId": "2901365", "name": "Weiguang Qu"}, {"authorId": "3035069", "name": "Xinyu Dai"}]}, {"paperId": "9b0c9749b8ef1d9e233a68a4270fb718a3d15d9a", "externalIds": {"DBLP": "conf/acl/HuZZZZ21", "ACL": "2021.findings-acl.143", "DOI": "10.18653/v1/2021.findings-acl.143", "CorpusId": 236477669}, "corpusId": 236477669, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9b0c9749b8ef1d9e233a68a4270fb718a3d15d9a", "title": "Toward Fully Exploiting Heterogeneous Corpus:A Decoupled Named Entity Recognition Model with Two-stage Training", "abstract": "Named Entity Recognition (NER) is a fundamental and widely used task in natural language processing (NLP), which is generally trained on the human-annotated corpus. However, data annotation is costly and time-consuming, which restricts its scale and further leads to the performance bottleneck of NER models. In reality, we can conveniently collect large-scale entity dictionaries and distantly supervised data. However, the collected dictionaries are lack of semantic context and the distantly supervised training instances contain large noise, which will bring uncertain effects to NER models when directly incorporated into the high-quality training set. To address the above issue, we propose a BERT-based decoupled NER model with two-stage training to appropriately take advantage of the heterogeneous corpus, including dictionaries, distantly supervised instances, and human-annotated instances. Our decoupled model consists of a Mention-BERT and a Context-BERT to respectively learn from the context-de\ufb01cient dictionaries and noised distantly supervised instances at the pre-training stage. At the uni\ufb01ed-training stage, the two BERTs are trained together on human-annotated data to predict the correct labels for candidate regions. Empirical studies on three Chinese NER datasets demon-strate that our method achieves signi\ufb01cant im-provements against several baselines, estab-lishing the new state-of-the-art performance.", "venue": "Findings", "year": 2021, "referenceCount": 46, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.143.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1641-1652"}, "authors": [{"authorId": "2112242846", "name": "Yun Hu"}, {"authorId": "2109414723", "name": "Yeshuang Zhu"}, {"authorId": "2108970018", "name": "Jinchao Zhang"}, {"authorId": "2153619515", "name": "Changwen Zheng"}, {"authorId": "2108485135", "name": "Jie Zhou"}]}, {"paperId": "b91157bd86212b565fa74229c9035db95560ae1a", "externalIds": {"DBLP": "conf/acl/XuCZ21", "ACL": "2021.findings-acl.144", "ArXiv": "2106.01562", "DOI": "10.18653/v1/2021.findings-acl.144", "CorpusId": 235313469}, "corpusId": 235313469, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b91157bd86212b565fa74229c9035db95560ae1a", "title": "Discriminative Reasoning for Document-level Relation Extraction", "abstract": "Document-level relation extraction (DocRE) models generally use graph networks to implicitly model the reasoning skill (i.e., pattern recognition, logical reasoning, coreference reasoning, etc.) related to the relation between one entity pair in a document. In this paper, we propose a novel discriminative reasoning framework to explicitly model the paths of these reasoning skills between each entity pair in this document. Thus, a discriminative reasoning network is designed to estimate the relation probability distribution of different reasoning paths based on the constructed graph and vectorized document contexts for each entity pair, thereby recognizing their relation. Experimental results show that our method outperforms the previous state-of-the-art performance on the large-scale DocRE dataset. The code is publicly available at https://github.com/xwjim/DRN.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 30, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.144.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01562"}, "authors": [{"authorId": "2110725401", "name": "Wang Xu"}, {"authorId": "2152954660", "name": "Kehai Chen"}, {"authorId": "145382463", "name": "T. Zhao"}]}, {"paperId": "34b709a6763cf1ed75216b5bc5f65ab0f0632ee5", "externalIds": {"ArXiv": "2107.12262", "DBLP": "conf/acl/HanFZQGZ21", "ACL": "2021.findings-acl.145", "DOI": "10.18653/v1/2021.findings-acl.145", "CorpusId": 236428542}, "corpusId": 236428542, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/34b709a6763cf1ed75216b5bc5f65ab0f0632ee5", "title": "Meta-Learning Adversarial Domain Adaptation Network for Few-Shot Text Classification", "abstract": "Meta-learning has emerged as a trending technique to tackle few-shot text classification and achieved state-of-the-art performance. However, existing solutions heavily rely on the exploitation of lexical features and their distributional signatures on training data, while neglecting to strengthen the model's ability to adapt to new tasks. In this paper, we propose a novel meta-learning framework integrated with an adversarial domain adaptation network, aiming to improve the adaptive ability of the model and generate high-quality text embedding for new classes. Extensive experiments are conducted on four benchmark datasets and our method demonstrates clear superiority over the state-of-the-art models in all the datasets. In particular, the accuracy of 1-shot and 5-shot classification on the dataset of 20 Newsgroups is boosted from 52.1% to 59.6%, and from 68.3% to 77.8%, respectively.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 28, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.145.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-26", "journal": {"name": "ArXiv", "volume": "abs/2107.12262"}, "authors": [{"authorId": "6071878", "name": "Chengcheng Han"}, {"authorId": "2153925626", "name": "Zeqiu Fan"}, {"authorId": "2109596982", "name": "Dongxiang Zhang"}, {"authorId": "2642333", "name": "Minghui Qiu"}, {"authorId": "46572901", "name": "Ming Gao"}, {"authorId": "145031580", "name": "Aoying Zhou"}]}, {"paperId": "7bd5e6b44bd9aac23499944ac00e63f7d80b996a", "externalIds": {"ACL": "2021.findings-acl.146", "DBLP": "conf/acl/VoronaPPC21", "DOI": "10.18653/v1/2021.findings-acl.146", "CorpusId": 236477735}, "corpusId": 236477735, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7bd5e6b44bd9aac23499944ac00e63f7d80b996a", "title": "Documents Representation via Generalized Coupled Tensor Chain with the Rotation Group constraint", "abstract": "Continuous representations of linguistic structures are an important part of modern natural language processing systems. Despite the diversity, most of the existing log-multilinear embedding models are organized under vector operations. However, these operations can not precisely represent the compositionality of natural language due to a lack of order-preserving properties. In this work, we focus on one of the promising alternatives based on the embedding of documents and words in the rotation group through the generalization of the coupled tensor chain decomposition to the exponential family of the probability distributions. In this model, documents and words are represented as matrices, and n-grams representations are combined from word representations by matrix multiplication. The proposed model is optimized via noise-contrastive estimation. We show empirically that capturing word order and higher-order word interactions allows our model to achieve the best results in several document classification benchmarks.", "venue": "Findings", "year": 2021, "referenceCount": 52, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1674-1684"}, "authors": [{"authorId": "5015109", "name": "I. Vorona"}, {"authorId": "9377326", "name": "A. Phan"}, {"authorId": "2027664756", "name": "A. Panchenko"}, {"authorId": "145683892", "name": "A. Cichocki"}]}, {"paperId": "21dabd3a989d578a32fb024732a2107e8da28581", "externalIds": {"ACL": "2021.findings-acl.147", "DBLP": "conf/acl/LiangWLL21", "DOI": "10.18653/v1/2021.findings-acl.147", "CorpusId": 236477774}, "corpusId": 236477774, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/21dabd3a989d578a32fb024732a2107e8da28581", "title": "Improving Unsupervised Extractive Summarization with Facet-Aware Modeling", "abstract": "Unsupervised extractive summarization aims to extract salient sentences from documents without labeled corpus. Existing methods are mostly graph-based by computing sentence centrality. These methods usually tend to select sentences within the same facet, however, which often leads to the facet bias problem especially when the document has multiple facets (i.e. long-document and multi-documents). To address this problem, we proposed a novel facet-aware centrality-based ranking model. We let the model pay more attention to different facets by introducing a sentence-document weight. The weight is added to the sentence centrality score. We evaluate our method on a wide range of summarization tasks that include 8 representative benchmark datasets. Experimental re-sults show that our method consistently out-performs strong baselines especially in long-and multi-document scenarios and even performs comparably to some supervised models. Extensive analyses con\ufb01rm that the performance gains come from alleviating the facet bias problem.", "venue": "Findings", "year": 2021, "referenceCount": 57, "citationCount": 26, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.147.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1685-1697"}, "authors": [{"authorId": "120436437", "name": "Xinnian Liang"}, {"authorId": "2362902", "name": "Shuangzhi Wu"}, {"authorId": "2112143809", "name": "Mu Li"}, {"authorId": "1707275", "name": "Zhoujun Li"}]}, {"paperId": "9715e184e28f205fe15f2718ea873e674e66b23c", "externalIds": {"ArXiv": "2109.06536", "ACL": "2021.findings-acl.148", "DBLP": "conf/acl/QiuZZ21", "DOI": "10.18653/v1/2021.findings-acl.148", "CorpusId": 236478053}, "corpusId": 236478053, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9715e184e28f205fe15f2718ea873e674e66b23c", "title": "Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder", "abstract": "Recent work has proposed several efficient approaches for generating gradient-based adversarial perturbations on embeddings and proved that the model's performance and robustness can be improved when they are trained with these contaminated embeddings. While they paid little attention to how to help the model to learn these adversarial samples more efficiently. In this work, we focus on enhancing the model's ability to defend gradient-based adversarial attack during the model's training process and propose two novel adversarial training approaches: (1) CARL narrows the original sample and its adversarial sample in the representation space while enlarging their distance from different labeled samples. (2) RAR forces the model to reconstruct the original sample from its adversarial representation. Experiments show that the proposed two approaches outperform strong baselines on various text classification datasets. Analysis experiments find that when using our approaches, the semantic representation of the input sentence won't be significantly affected by adversarial perturbations, and the model's performance drops less under adversarial attack. That is to say, our approaches can effectively improve the robustness of the model. Besides, RAR can also be used to generate text-form adversarial samples.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.148.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-14", "journal": {"pages": "1698-1707"}, "authors": [{"authorId": "2153278060", "name": "Yao Qiu"}, {"authorId": "2108970018", "name": "Jinchao Zhang"}, {"authorId": "2108485135", "name": "Jie Zhou"}]}, {"paperId": "52e645112a845fc8286879a2ec00ee433ebc5900", "externalIds": {"ACL": "2021.findings-acl.149", "DBLP": "conf/acl/LiYLX21", "DOI": "10.18653/v1/2021.findings-acl.149", "CorpusId": 236478212}, "corpusId": 236478212, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/52e645112a845fc8286879a2ec00ee433ebc5900", "title": "Multi-Granularity Contrasting for Cross-Lingual Pre-Training", "abstract": "Cross-lingual pre-training aims at providing effective prior representations for the inputs from multiple languages. With the modeling of bidirectional contexts, recently prevalent language modeling approaches such as XLM achieve better performance than traditional methods based on embedding alignment, which strives to assign similar vector representations to semantic-equivalent units. However, such approaches like XLM capture cross-lingual information based solely on shared BPE vocabulary, resulting in the absence of fine-grained supervision induced by embedding alignment. Inheriting the advantages of the above two paradigms, this work presents a multi-granularity contrasting framework, namely MGC, to learn languageuniversal representations. While predicting the masked words based on bidirectional contexts, the proposal also encodes semantic equivalents from different languages into similar representations to introduce more finegrained and explicit cross-lingual information. Two effective contrasting strategies are further proposed, which can be built upon semantic units of multiple granularities covering words, span, and sentences. Extensive experiments demonstrate that our approach can achieve significant performance gains in various downstream tasks, including machine translation and cross-lingual language understanding.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 3, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1708-1717"}, "authors": [{"authorId": "2117947438", "name": "Shicheng Li"}, {"authorId": "46709826", "name": "Pengcheng Yang"}, {"authorId": "2140495101", "name": "Fuli Luo"}, {"authorId": "145626731", "name": "Jun Xie"}]}, {"paperId": "86445cf8bb0712d21a4f07b752435702973f514b", "externalIds": {"DBLP": "conf/acl/HuangWZ21", "ACL": "2021.findings-acl.150", "DOI": "10.18653/v1/2021.findings-acl.150", "CorpusId": 236478013}, "corpusId": 236478013, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/86445cf8bb0712d21a4f07b752435702973f514b", "title": "A Comparison between Pre-training and Large-scale Back-translation for Neural Machine Translation", "abstract": "BERT has been studied as a promising technique to improve NMT. Given that BERT is based on the similar Transformer architecture to NMT and the current datasets for most MT tasks are rather large, how pre-training has managed to outperform standard Transformer NMT models is underestimated. We compare MT engines trained with pre-trained BERT and back-translation with incrementally larger amounts of data, implementing the two most widely-used monolingual paradigms. We analyze their strengths and weaknesses based on both standard automatic metrics and intrinsic test suites that comprise a large range of linguistic phenomena. Primarily, we find that 1) BERT has limited advantages compared with large-scale back-translation in accuracy and consistency on morphology and syntax; 2) BERT can boost the Transformer baseline in semantic and pragmatic tasks which involve intensive understanding; 3) pre-training on huge datasets may introduce inductive social bias thus affects translation fairness.", "venue": "Findings", "year": 2021, "referenceCount": 65, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.150.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1718-1732"}, "authors": [{"authorId": "2110409012", "name": "Dandan Huang"}, {"authorId": "2155023049", "name": "Kun Wang"}, {"authorId": "2145913925", "name": "Yue Zhang"}]}, {"paperId": "1c49e8e25f52449ae6a986cad9d6a1f8fc7d2e71", "externalIds": {"DBLP": "conf/acl/LuoHQ21", "ACL": "2021.findings-acl.151", "ArXiv": "2106.02327", "DOI": "10.18653/v1/2021.findings-acl.151", "CorpusId": 235352852}, "corpusId": 235352852, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1c49e8e25f52449ae6a986cad9d6a1f8fc7d2e71", "title": "Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene", "abstract": "The major paradigm of applying a pre-trained language model to downstream tasks is to fine-tune it on labeled task data, which often suffers instability and low performance when the labeled examples are scarce.~One way to alleviate this problem is to apply post-training on unlabeled task data before fine-tuning, adapting the pre-trained model to target domains by contrastive learning that considers either token-level or sequence-level similarity. Inspired by the success of sequence masking, we argue that both token-level and sequence-level similarities can be captured with a pair of masked sequences.~Therefore, we propose complementary random masking (CRM) to generate a pair of masked sequences from an input sequence for sequence-level contrastive learning and then develop contrastive masked language modeling (CMLM) for post-training to integrate both token-level and sequence-level contrastive learnings.~Empirical results show that CMLM surpasses several recent post-training methods in few-shot settings without the need for data augmentation.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.151.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02327"}, "authors": [{"authorId": "3120821", "name": "Ruikun Luo"}, {"authorId": "2115501143", "name": "Guanhuan Huang"}, {"authorId": "38472218", "name": "Xiaojun Quan"}]}, {"paperId": "ba0a55c06298eed1bc8dc2d855f064c834efc796", "externalIds": {"ACL": "2021.findings-acl.152", "DBLP": "conf/acl/XiongFWKO21", "DOI": "10.18653/v1/2021.findings-acl.152", "CorpusId": 236477943}, "corpusId": 236477943, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ba0a55c06298eed1bc8dc2d855f064c834efc796", "title": "Fusing Label Embedding into BERT: An Efficient Improvement for Text Classification", "abstract": "With pre-trained models, such as BERT, gaining more and more attention, plenty of research has been done to further promote their capabilities, from enhancing the experimental procedures (Sun et al., 2019) to improving the mathematical principles. In this paper, we propose a concise method for improving BERT\u2019s performance in text classification by utilizing a label embedding technique while keeping almost the same computational cost. Experimental results on six text classification benchmark datasets demonstrate its effectiveness.", "venue": "Findings", "year": 2021, "referenceCount": 48, "citationCount": 23, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.152.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1743-1750"}, "authors": [{"authorId": "2153657499", "name": "Yijin Xiong"}, {"authorId": "48260295", "name": "Yukun Feng"}, {"authorId": "1664776313", "name": "Hao Wu"}, {"authorId": "2300756", "name": "Hidetaka Kamigaito"}, {"authorId": "144859189", "name": "M. Okumura"}]}, {"paperId": "28d794bc8b8cf579745146b3fd3362942787dd10", "externalIds": {"DBLP": "journals/corr/abs-2004-13631", "ACL": "2021.findings-acl.153", "ArXiv": "2004.13631", "MAG": "3022668299", "DOI": "10.18653/v1/2021.findings-acl.153", "CorpusId": 216562512}, "corpusId": 216562512, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/28d794bc8b8cf579745146b3fd3362942787dd10", "title": "KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion", "abstract": "Knowledge graphs (KGs) contains an instance-level entity graph and an ontology-level concept graph. Recent studies reveal that jointly modeling of these two graphs could improve the understanding of each one. The completion processes on the concept graph and the entity graph can be further regarded as processes of knowledge abstraction and concretization. However, concept graphs in existing datasets are usually small and the links between concepts and entities are usually sparse, which cannot provide sufficient information for knowledge transfer between the two graphs. In this paper, we propose large-scale datasets extracted from Wikidata, which provide more size-balanced concept graphs and abundant cross-view links. Based on the datasets, we further propose a benchmark to test the ability of existing models on knowledge abstraction, concretization and completion (KACC). Our dataset is available at this https URL.", "venue": "Findings", "year": 2020, "referenceCount": 32, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.153.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-04-28", "journal": {"pages": "1751-1763"}, "authors": [{"authorId": "49640256", "name": "Jie Zhou"}, {"authorId": "48574888", "name": "Xin Lv"}, {"authorId": "2154170623", "name": "Cheng Yang"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "8549842", "name": "Juan-Zi Li"}, {"authorId": "1753344", "name": "Maosong Sun"}]}, {"paperId": "722058a002c8f2ee8b496073888095a37db5c83b", "externalIds": {"DBLP": "conf/acl/FangHP21", "ACL": "2021.findings-acl.154", "ArXiv": "2106.07346", "DOI": "10.18653/v1/2021.findings-acl.154", "CorpusId": 235422207}, "corpusId": 235422207, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/722058a002c8f2ee8b496073888095a37db5c83b", "title": "A Query-Driven Topic Model", "abstract": "Topic modeling is an unsupervised method for revealing the hidden semantic structure of a corpus. It has been increasingly widely adopted as a tool in the social sciences, including political science, digital humanities and sociological research in general. One desirable property of topic models is to allow users to find topics describing a specific aspect of the corpus. A possible solution is to incorporate domain-specific knowledge into topic modeling, but this requires a specification from domain experts. We propose a novel query-driven topic model that allows users to specify a simple query in words or phrases and return query-related topics, thus avoiding tedious work from domain experts. Our proposed approach is particularly attractive when the user-specified query has a low occurrence in a text corpus, making it difficult for traditional topic models built on word cooccurrence patterns to identify relevant topics. Experimental results demonstrate the effectiveness of our model in comparison with both classical topic models and neural topic models.", "venue": "Findings", "year": 2021, "referenceCount": 28, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.154.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-28", "journal": {"pages": "1764-1777"}, "authors": [{"authorId": "2072874946", "name": "Zheng Fang"}, {"authorId": "1390509967", "name": "Yulan He"}, {"authorId": "144723416", "name": "R. Procter"}]}, {"paperId": "7c799b7bd8c069c6feb7235345c97aa1f5330b84", "externalIds": {"DBLP": "journals/corr/abs-2105-05641", "ACL": "2021.findings-acl.155", "ArXiv": "2105.05641", "DOI": "10.18653/v1/2021.findings-acl.155", "CorpusId": 234469686}, "corpusId": 234469686, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7c799b7bd8c069c6feb7235345c97aa1f5330b84", "title": "How Reliable are Model Diagnostics?", "abstract": "In the pursuit of a deeper understanding of a model's behaviour, there is recent impetus for developing suites of probes aimed at diagnosing models beyond simple metrics like accuracy or BLEU. This paper takes a step back and asks an important and timely question: how reliable are these diagnostics in providing insight into models and training setups? We critically examine three recent diagnostic tests for pre-trained language models, and find that likelihood-based and representation-based model diagnostics are not yet as reliable as previously assumed. Based on our empirical findings, we also formulate recommendations for practitioners and researchers.", "venue": "Findings", "year": 2021, "referenceCount": 39, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.155.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-12", "journal": {"name": "ArXiv", "volume": "abs/2105.05641"}, "authors": [{"authorId": "1481714624", "name": "V. Aribandi"}, {"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "1680617", "name": "Donald Metzler"}]}, {"paperId": "f896b12d743b471b83c5f2a012e1dbe4932320c7", "externalIds": {"ACL": "2021.findings-acl.156", "DBLP": "conf/acl/WuFWCBPZW21", "DOI": "10.18653/v1/2021.findings-acl.156", "CorpusId": 236477463}, "corpusId": 236477463, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f896b12d743b471b83c5f2a012e1dbe4932320c7", "title": "Gaussian Process based Deep Dyna-Q approach for Dialogue Policy Learning", "abstract": "August 1\u20136, 2021. \u00a92021 Association for Computational Linguistics 1786 Gaussian Process based Deep Dyna-Q Approach for Dialogue Policy Learning Guanlin Wu 1,2,\u2217 Wenqi Fang 3,\u2217,\u2020 Ji Wang 1,\u2217 Jiang Cao 2 Weidong Bao 1 Yang Ping 2 Xiaomin Zhu 1 Zheng Wang 4 National University of Defense Technology, Changsha, China Academy of Military Science, Beijing, China Nanhu Laboratory, Jiaxing, China Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China {wuguanlin16,wangji,wdbao,xmzhu}@nudt.edu.cn ocean.py@163.com amscaojiang@126.com wqfang@nanhulab.ac.cn zheng.wang@siat.ac.cn Abstract", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.156.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Education", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1786-1795"}, "authors": [{"authorId": "20762573", "name": "Guanlin Wu"}, {"authorId": "1490936209", "name": "Wenqi Fang"}, {"authorId": "2110218064", "name": "Ji Wang"}, {"authorId": "2115872277", "name": "Jiang Cao"}, {"authorId": "3327011", "name": "Weidong Bao"}, {"authorId": "2106594845", "name": "Yang Ping"}, {"authorId": "1726302", "name": "Xiaomin Zhu"}, {"authorId": "11032852", "name": "Z. Wang"}]}, {"paperId": "91ad9c6345723717b9cf77727482195834620060", "externalIds": {"ArXiv": "2105.10912", "DBLP": "conf/acl/WrightA21", "ACL": "2021.findings-acl.157", "DOI": "10.18653/v1/2021.findings-acl.157", "CorpusId": 235166702}, "corpusId": 235166702, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/91ad9c6345723717b9cf77727482195834620060", "title": "CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding", "abstract": "Scientific document understanding is challenging as the data is highly domain specific and diverse. However, datasets for tasks with scientific text require expensive manual annotation and tend to be small and limited to only one or a few fields. At the same time, scientific documents contain many potential training signals, such as citations, which can be used to build large labelled datasets. Given this, we present an in-depth study of cite-worthiness detection in English, where a sentence is labelled for whether or not it cites an external source. To accomplish this, we introduce CiteWorth, a large, contextualized, rigorously cleaned labelled dataset for cite-worthiness detection built from a massive corpus of extracted plain-text scientific documents. We show that CiteWorth is high-quality, challenging, and suitable for studying problems such as domain adaptation. Our best performing cite-worthiness detection model is a paragraph-level contextualized sentence labelling model based on Longformer, exhibiting a 5 F1 point improvement over SciBERT which considers only individual sentences. Finally, we demonstrate that language model fine-tuning with cite-worthiness as a secondary task leads to improved performance on downstream scientific document understanding tasks.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.157.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-23", "journal": {"pages": "1796-1807"}, "authors": [{"authorId": "49753393", "name": "Dustin Wright"}, {"authorId": "1736067", "name": "Isabelle Augenstein"}]}, {"paperId": "3abdc83e17d1c93f04a6c4a60d0f8f0706269351", "externalIds": {"DBLP": "conf/acl/Plank21", "ACL": "2021.findings-acl.158", "DOI": "10.18653/v1/2021.findings-acl.158", "CorpusId": 236477962}, "corpusId": 236477962, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/3abdc83e17d1c93f04a6c4a60d0f8f0706269351", "title": "Cross-Lingual Cross-Domain Nested Named Entity Evaluation on English Web Texts", "abstract": "Named Entity Recognition (NER) is a key Natural Language Processing task. However, most existing work on NER targets flat named entities (NEs) and ignores the recognition of nested structures, where entities can be enclosed within other NEs. Moreover, evaluation of Nested Named Entity Recognition (NNER) across domains remains challenging, mainly due to the limited availability of datasets. To address these gaps, we present EWT-NNER, a dataset covering five web domains annotated for nested named entities on top of the English Web Treebank (EWT). We present the corpus and an empirical evaluation, including transfer results from German and Danish. EWTNNER is annotated for four major entity types, including suffixes for derivational entity markers and partial named entities, spanning a total of 12 classes. We envision the public release of EWT-NNER to encourage further research on nested NER, particularly on cross-lingual cross-domain evaluation.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1808-1815"}, "authors": [{"authorId": "2065013766", "name": "Barbara Plank"}]}, {"paperId": "dc9e3dc608f24e1c8dc9e48761373032384a8b60", "externalIds": {"DBLP": "journals/corr/abs-2105-11752", "ACL": "2021.findings-acl.159", "ArXiv": "2105.11752", "DOI": "10.18653/v1/2021.findings-acl.159", "CorpusId": 236478040}, "corpusId": 236478040, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/dc9e3dc608f24e1c8dc9e48761373032384a8b60", "title": "Counter-Argument Generation by Attacking Weak Premises", "abstract": "Text generation has received a lot of attention in computational argumentation research as of recent. A particularly challenging task is the generation of counter-arguments. So far, approaches primarily focus on rebutting a given conclusion, yet other ways to counter an argument exist. In this work, we go beyond previous research by exploring argument undermining, that is, countering an argument by attacking one of its premises. We hypothesize that identifying the argument's weak premises is key to effective countering. Accordingly, we propose a pipeline approach that first assesses the premises' strength and then generates a counter-argument targeting the weak ones. On the one hand, both manual and automatic evaluation proves the importance of identifying weak premises in counter-argument generation. On the other hand, when considering correctness and content richness, human annotators favored our approach over state-of-the-art counter-argument generation.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 10, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.159.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-25", "journal": {"pages": "1816-1827"}, "authors": [{"authorId": "2300829", "name": "Milad Alshomary"}, {"authorId": "18417916", "name": "S. Syed"}, {"authorId": "2121367673", "name": "Arkajit Dhar"}, {"authorId": "2058609780", "name": "Martin Potthast"}, {"authorId": "2626599", "name": "Henning Wachsmuth"}]}, {"paperId": "3ba0f3fb628569a4a6c0fcdcae0d7ff6542df618", "externalIds": {"DBLP": "journals/corr/abs-2106-08582", "ACL": "2021.findings-acl.160", "ArXiv": "2106.08582", "DOI": "10.18653/v1/2021.findings-acl.160", "CorpusId": 235446438}, "corpusId": 235446438, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/3ba0f3fb628569a4a6c0fcdcae0d7ff6542df618", "title": "Alternated Training with Synthetic and Authentic Data for Neural Machine Translation", "abstract": "While synthetic bilingual corpora have demonstrated their effectiveness in low-resource neural machine translation (NMT), adding more synthetic data often deteriorates translation performance. In this work, we propose alternated training with synthetic and authentic data for NMT. The basic idea is to alternate synthetic and authentic corpora iteratively during training. Compared with previous work, we introduce authentic data as guidance to prevent the training of NMT models from being disturbed by noisy synthetic data. Experiments on Chinese-English and German-English translation tasks show that our approach improves the performance over several strong baselines. We visualize the BLEU landscape to further investigate the role of authentic and synthetic data during alternated training. From the visualization, we find that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement.", "venue": "Findings", "year": 2021, "referenceCount": 21, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.160.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-16", "journal": {"name": "ArXiv", "volume": "abs/2106.08582"}, "authors": [{"authorId": "2112690595", "name": "Rui Jiao"}, {"authorId": "19343873", "name": "Zonghan Yang"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "2152797839", "name": "Yang Liu"}]}, {"paperId": "ce498651107588db67adcfbb5479bdb416f4de2f", "externalIds": {"ACL": "2021.findings-acl.161", "DBLP": "journals/corr/abs-2106-01760", "ArXiv": "2106.01760", "DOI": "10.18653/v1/2021.findings-acl.161", "CorpusId": 235313658}, "corpusId": 235313658, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ce498651107588db67adcfbb5479bdb416f4de2f", "title": "Template-Based Named Entity Recognition Using BART", "abstract": "There is a recent interest in investigating few-shot NER, where the low-resource target domain has different label sets compared with a resource-rich source domain. Existing methods use a similarity-based metric. However, they cannot make full use of knowledge transfer in NER model parameters. To address the issue, we propose a template-based method for NER, treating NER as a language model ranking problem in a sequence-to-sequence framework, where original sentences and statement templates filled by candidate named entity span are regarded as the source sequence and the target sequence, respectively. For inference, the model is required to classify each candidate span based on the corresponding template scores. Our experiments demonstrate that the proposed method achieves 92.55% F1 score on the CoNLL03 (rich-resource task), and significantly better than fine-tuning BERT 10.88%, 15.34%, and 11.73% F1 score on the MIT Movie, the MIT Restaurant, and the ATIS (low-resource task), respectively.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 193, "influentialCitationCount": 30, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.161.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"pages": "1835-1845"}, "authors": [{"authorId": "152496687", "name": "Leyang Cui"}, {"authorId": "2142240763", "name": "Yu Wu"}, {"authorId": "2150168584", "name": "Jian Liu"}, {"authorId": "1998967974", "name": "Sen Yang"}, {"authorId": "1591125925", "name": "Yue Zhang"}]}, {"paperId": "9e8c54f4bd1ad434f3ebde280e242a18c14a53c3", "externalIds": {"ACL": "2021.findings-acl.162", "DBLP": "conf/acl/ChouCJL21", "DOI": "10.18653/v1/2021.findings-acl.162", "CorpusId": 236478236}, "corpusId": 236478236, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9e8c54f4bd1ad434f3ebde280e242a18c14a53c3", "title": "\u201cDoes it Matter When I Think You Are Lying?\u201d Improving Deception Detection by Integrating Interlocutor\u2019s Judgements in Conversations", "abstract": "It is well known that human is not good at deception detection because of a natural inclination of truth-bias. However, during a conversation, when an interlocutor (interrogator) is being asked explicitly to assess whether his/her interacting partner (deceiver) is lying, this perceptual judgment depends highly on how the interrogator interprets the context of the conversation. While the deceptive behaviors can be difficult to model due to their heterogeneous manifestation, we hypothesize that this contextual information, i.e., whether the interlocutor trusts or distrusts what his/her partner is saying, provides an important condition in which the deceiver\u2019s deceptive behaviors are more consistently distinct. In this work, we propose a Judgmental-Enhanced Automatic Deception Detection Network (JEADDN) that explicitly considers interrogator\u2019s perceived truths-deceptions with three types of speechlanguage features (acoustic-prosodic, linguistic, and conversational temporal dynamics features) extracted during a conversation. We evaluate our framework on a large Mandarin Chinese Deception Dialog Database. The results show that the method significantly outperforms the current state-of-the-art approach without conditioning on the judgements of interrogators on this database. We further demonstrate that the behaviors of interrogators are important in detecting deception when the interrogators distrust the deceivers. Finally, with the late fusion of audio, text, and turntaking dynamics (TTD) features, we obtain promising results of 87.27% and 94.18% accuracy under the conditions that the interrogators trust and distrust the deceivers in deception detection which improves 7.27% and 13.57% than the model without considering the judgements of interlocutor respectively.", "venue": "Findings", "year": 2021, "referenceCount": 53, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1846-1860"}, "authors": [{"authorId": "35725460", "name": "Huang-Cheng Chou"}, {"authorId": "26632228", "name": "Woan-Shiuan Chien"}, {"authorId": "50270386", "name": "Da-Cheng Juan"}, {"authorId": "2467369", "name": "Chi-Chun Lee"}]}, {"paperId": "66cb883bfba8600740a043c8c119ef541ec07b73", "externalIds": {"DBLP": "conf/acl/TangKY21", "ACL": "2021.findings-acl.163", "DOI": "10.18653/v1/2021.findings-acl.163", "CorpusId": 236478147}, "corpusId": 236478147, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/66cb883bfba8600740a043c8c119ef541ec07b73", "title": "High-Quality Dialogue Diversification by Intermittent Short Extension Ensembles", "abstract": "Many task-oriented dialogue systems use deep reinforcement learning (DRL) to learn policies that respond to the user appropriately and complete the tasks successfully. Training DRL agents with diverse dialogue trajectories prepare them well for rare user requests and unseen situations. One effective diversification method is to let the agent interact with a diverse set of learned user models. However, trajectories created by these artificial user models may contain generation errors, which can quickly propagate into the agent\u2019s policy. It is thus important to control the quality of the diversification and resist the noise. In this paper, we propose a novel dialogue diversification method for task-oriented dialogue systems trained in simulators. Our method, Intermittent Short Extension Ensemble (I-SEE),1 constrains the intensity to interact with an ensemble of diverse user models and effectively controls the quality of the diversification. Evaluations on the Multiwoz dataset show that ISEE successfully boosts the performance of several state-of-the-art DRL dialogue agents.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.163.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1861-1872"}, "authors": [{"authorId": "47347567", "name": "Zhiwen Tang"}, {"authorId": "50359017", "name": "Hrishikesh Kulkarni"}, {"authorId": "2109540870", "name": "Grace Hui Yang"}]}, {"paperId": "94205c0e603b0359f8fc7b6db3e7bb0b6b33ca6c", "externalIds": {"ACL": "2021.findings-acl.164", "DBLP": "conf/acl/WangSMW21", "MAG": "3177327077", "DOI": "10.18653/v1/2021.findings-acl.164", "CorpusId": 236478021}, "corpusId": 236478021, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/94205c0e603b0359f8fc7b6db3e7bb0b6b33ca6c", "title": "Structured Refinement for Sequential Labeling", "abstract": "Filtering target-irrelevant information through hierarchically refining hidden states has been demonstrated to be effective for obtaining informative representations. However, previous work simply relies on locally normalized attention without considering possible labels at other time steps, the capacity for modeling long-term dependency relations is thus limited. In this paper, we propose to extend previous work with globally normalized attention, e.g., structured attention, to leverage structural information for more effective representation refinement. We also propose two implementation tricks to accelerate CRF computation and an initialization trick for Chinese character embeddings to further improve performance. We provide extensive experimental results on various datasets to show the effectiveness and efficiency of our proposed method.", "venue": "Findings", "year": 2021, "referenceCount": 54, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-01", "journal": {"pages": "1873-1884"}, "authors": [{"authorId": "2107920627", "name": "Yiran Wang"}, {"authorId": "73467110", "name": "Hiroyuki Shindo"}, {"authorId": "1681502", "name": "Yuji Matsumoto"}, {"authorId": "2110694221", "name": "Taro Watanabe"}]}, {"paperId": "751e2acb19c2b4e4023795e4d8973471a79ab8d2", "externalIds": {"DBLP": "conf/acl/MondalHJ21", "DOI": "10.18653/v1/2021.findings-acl.165", "CorpusId": 260435123}, "corpusId": 260435123, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/751e2acb19c2b4e4023795e4d8973471a79ab8d2", "title": "End-to-End Construction of NLP Knowledge Graph", "abstract": null, "venue": "ACL/IJCNLP", "year": 2021, "referenceCount": 0, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.165.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1885-1895"}, "authors": [{"authorId": "9377739", "name": "Ishani Mondal"}, {"authorId": "150348519", "name": "Yufang Hou"}, {"authorId": "2078553", "name": "Charles Jochim"}]}, {"paperId": "69d45dfa4d5049671df9e9214264fbd57d99951e", "externalIds": {"DBLP": "journals/corr/abs-2106-05903", "ACL": "2021.findings-acl.166", "ArXiv": "2106.05903", "DOI": "10.18653/v1/2021.findings-acl.166", "CorpusId": 235390735}, "corpusId": 235390735, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/69d45dfa4d5049671df9e9214264fbd57d99951e", "title": "Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for Multimodal Hate", "abstract": "Accurate detection and classification of online hate is a difficult task. Implicit hate is particularly challenging as such content tends to have unusual syntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This problem is heightened with multimodal content, such as memes (combinations of text and images), as they are often harder to decipher than unimodal content (e.g., text alone). This paper evaluates the role of semantic and multimodal context for detecting implicit and explicit hate. We show that both text- and visual- enrichment improves model performance, with the multimodal model (0.771) outperforming other models' F1 scores (0.544, 0.737, and 0.754). While the unimodal-text context-aware (transformer) model was the most accurate on the subtask of implicit hate detection, the multimodal model outperformed it overall because of a lower propensity towards false positives. We find that all models perform better on content with full annotator agreement and that multimodal models are best at classifying the content where annotators disagree. To conduct these investigations, we undertook high-quality annotation of a sample of 5,000 multimodal entries. Tweets were annotated for primary category, modality, and strategy. We make this corpus, along with the codebook, code, and final model, freely available.", "venue": "Findings", "year": 2021, "referenceCount": 82, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.166.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-10", "journal": {"pages": "1896-1907"}, "authors": [{"authorId": "1686287970", "name": "Austin Botelho"}, {"authorId": "2737827", "name": "Bertie Vidgen"}, {"authorId": "1741886127", "name": "Scott A. Hale"}]}, {"paperId": "f9f5830e5d292edc28a94dd4b6439f47ba6c038c", "externalIds": {"ACL": "2021.findings-acl.167", "DBLP": "conf/acl/UbanCD21", "DOI": "10.18653/v1/2021.findings-acl.167", "CorpusId": 236477326}, "corpusId": 236477326, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f9f5830e5d292edc28a94dd4b6439f47ba6c038c", "title": "Studying the Evolution of Scientific Topics and their Relationships", "abstract": "We propose a study of the development of scienti\ufb01c topics through time, as well as the relations between them within the scienti\ufb01c \ufb01eld of computational linguistics and across sub\ufb01elds. We use topic modeling to analyze scienti\ufb01c texts published in the ACL Anthology, and introduce a categorization of topics in our \ufb01eld into 3 types: tasks, algorithms, and data. In order to understand how topics emerge, evolve, and gradually disappear over time, we analyze the evolution of these topics across time through several case studies. We further include in our analysis papers published in NeurIPS, and try to understand whether there was any in\ufb02uence between topics in this conference focused on neural methods and computational linguistics conferences, as well as measure the divergence over time between conferences in terms of the topics approached. We additionally look at the relationships between topics, categorizing them into types of competing or cooperating topics.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.167.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1908-1922"}, "authors": [{"authorId": "2009863", "name": "Ana Sabina Uban"}, {"authorId": "1690656", "name": "Cornelia Caragea"}, {"authorId": "2467676", "name": "Liviu P. Dinu"}]}, {"paperId": "faf201efd2b860d1f93b091387f6c031bc31eb15", "externalIds": {"DBLP": "conf/acl/GhaddarLRR21", "ACL": "2021.findings-acl.168", "ArXiv": "2109.02071", "DOI": "10.18653/v1/2021.findings-acl.168", "CorpusId": 236477370}, "corpusId": 236477370, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/faf201efd2b860d1f93b091387f6c031bc31eb15", "title": "End-to-End Self-Debiasing Framework for Robust NLU Training", "abstract": "Existing Natural Language Understanding (NLU) models have been shown to incorporate dataset biases leading to strong performance on in-distribution (ID) test sets but poor performance on out-of-distribution (OOD) ones. We introduce a simple yet effective debiasing framework whereby the shallow representations of the main model are used to derive a bias model and both models are trained simultaneously. We demonstrate on three well studied NLU tasks that despite its simplicity, our method leads to competitive OOD results. It significantly outperforms other debiasing approaches on two tasks, while still delivering high in-distribution performance.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 21, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.168.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-05", "journal": {"pages": "1923-1929"}, "authors": [{"authorId": "3448973", "name": "Abbas Ghaddar"}, {"authorId": "1724343955", "name": "P. Langlais"}, {"authorId": "1924511", "name": "Mehdi Rezagholizadeh"}, {"authorId": "2064509318", "name": "Ahmad Rashid"}]}, {"paperId": "e3fc48e01eb6e89a793b8f981b5836beb525fd99", "externalIds": {"ACL": "2021.findings-acl.169", "DBLP": "conf/acl/ThakurCHEB21", "DOI": "10.18653/v1/2021.findings-acl.169", "CorpusId": 236477424}, "corpusId": 236477424, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e3fc48e01eb6e89a793b8f981b5836beb525fd99", "title": "A Mixed-Method Design Approach for Empirically Based Selection of Unbiased Data Annotators", "abstract": "Implicit bias embedded in the annotated data is by far the greatest impediment in the effectual use of supervised machine learning models in tasks involving race, ethics, and geopolitical polarization. For societal good and demonstrable positive impact on wider society, it is paramount to carefully select data annotators and rigorously validate the annotation process. Current approaches to selecting annotators are not sufficiently grounded in scientific principles and are limited at the policy-guidance level, thereby rendering them unusable for machine learning practitioners. This work proposes a new approach based on the mixed-methods design that is functional, adaptable, and simpler to implement in selecting unbiased annotators for any machine learning problem. By demonstrating it on a realworld geopolitical problem, we also identified and ranked key inane profile characteristics towards an empirically-based selection of unbiased data annotators.", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1930-1938"}, "authors": [{"authorId": "81226050", "name": "Gautam Thakur"}, {"authorId": "30281690", "name": "Janna Caspersen"}, {"authorId": "1839605", "name": "Drahomira Herrmannova"}, {"authorId": "2121346027", "name": "Bryan Eaton"}, {"authorId": "2121330107", "name": "Jordan Burdette"}]}, {"paperId": "43ca608abb757aaf31f0d43a719d7331fa3e4e00", "externalIds": {"DBLP": "conf/acl/VishnubhotlaHR21", "ACL": "2021.findings-acl.170", "DOI": "10.18653/v1/2021.findings-acl.170", "CorpusId": 236478313}, "corpusId": 236478313, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/43ca608abb757aaf31f0d43a719d7331fa3e4e00", "title": "An Evaluation of Disentangled Representation Learning for Texts", "abstract": ",", "venue": "Findings", "year": 2021, "referenceCount": 22, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1939-1951"}, "authors": [{"authorId": "51172231", "name": "Krishnapriya Vishnubhotla"}, {"authorId": "145036961", "name": "Graeme Hirst"}, {"authorId": "2479037", "name": "Frank Rudzicz"}]}, {"paperId": "05561128b2d98753775de9cb12a84ac1fd03a794", "externalIds": {"DBLP": "conf/acl/VerlindenZDDD21", "ACL": "2021.findings-acl.171", "ArXiv": "2107.02286", "DOI": "10.18653/v1/2021.findings-acl.171", "CorpusId": 235742760}, "corpusId": 235742760, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/05561128b2d98753775de9cb12a84ac1fd03a794", "title": "Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution", "abstract": "We consider a joint information extraction (IE) model, solving named entity recognition, coreference resolution and relation extraction jointly over the whole document. In particular, we study how to inject information from a knowledge base (KB) in such IE model, based on unsupervised entity linking. The used KB entity representations are learned from either (i) hyperlinked text documents (Wikipedia), or (ii) a knowledge graph (Wikidata), and appear complementary in raising IE performance. Representations of corresponding entity linking (EL) candidates are added to text span representations of the input document, and we experiment with (i) taking a weighted average of the EL candidate representations based on their prior (in Wikipedia), and (ii) using an attention scheme over the EL candidate list. Results demonstrate an increase of up to 5% F1-score for the evaluated IE tasks on two datasets. Despite a strong performance of the prior-based model, our quantitative and qualitative analysis reveals the advantage of using the attention-based approach.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.171.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-05", "journal": {"name": "ArXiv", "volume": "abs/2107.02286"}, "authors": [{"authorId": "46476168", "name": "S. Verlinden"}, {"authorId": "46214177", "name": "Klim Zaporojets"}, {"authorId": "2630759", "name": "Johannes Deleu"}, {"authorId": "1388296896", "name": "Thomas Demeester"}, {"authorId": "2067842103", "name": "Chris Develder"}]}, {"paperId": "2c33f4aa30a4d8f708db79eef5c9e2a0ce84695b", "externalIds": {"DBLP": "conf/acl/ZhangGC21", "ArXiv": "2106.01494", "ACL": "2021.findings-acl.172", "DOI": "10.18653/v1/2021.findings-acl.172", "CorpusId": 235313893}, "corpusId": 235313893, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/2c33f4aa30a4d8f708db79eef5c9e2a0ce84695b", "title": "Knowing More About Questions Can Help: Improving Calibration in Question Answering", "abstract": "We study calibration in question answering, estimating whether model correctly predicts answer for each question. Unlike prior work which mainly rely on the model's confidence score, our calibrator incorporates information about the input example (e.g., question and the evidence context). Together with data augmentation via back translation, our simple approach achieves 5-10% gains in calibration accuracy on reading comprehension benchmarks. Furthermore, we present the first calibration study in the open retrieval setting, comparing the calibration accuracy of retrieval-based span prediction models and answer generation models. Here again, our approach shows consistent gains over calibrators relying on the model confidence. Our simple and efficient calibrator can be easily adapted to many tasks and model architectures, showing robust gains in all settings.", "venue": "Findings", "year": 2021, "referenceCount": 58, "citationCount": 41, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.172.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01494"}, "authors": [{"authorId": "2107944048", "name": "Shujian Zhang"}, {"authorId": "29777869", "name": "Chengyue Gong"}, {"authorId": "2890423", "name": "Eunsol Choi"}]}, {"paperId": "095c38aa15b2fad08daa567aab316145a331e76c", "externalIds": {"DBLP": "conf/acl/WanLDSZ21", "ACL": "2021.findings-acl.173", "DOI": "10.18653/v1/2021.findings-acl.173", "CorpusId": 236477820}, "corpusId": 236477820, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/095c38aa15b2fad08daa567aab316145a331e76c", "title": "Enhancing Metaphor Detection by Gloss-based Interpretations", "abstract": "This paper focuses on utilizing metaphor interpretation to enhance metaphor detection. Considering that existing approaches to metaphor interpretation are limited by ambiguous meanings of the metaphorical substitute words, this paper proposes a novel interpretation mechanism that utilizes glosses to interpret metaphorical words. Since there is no dataset annotated for both metaphor detection and metaphor interpretation, we enhance three datasets TroFi, VUA, and PSUCMC from the field of metaphor detection with gloss annotations. Accordingly, we develop a model for jointly conducting metaphor detection and gloss-based interpretation (named MDGIJoint for short). Experimental results demonstrate that MDGI-Joint outperforms state-ofthe-art models on all the three enhanced datasets and that gloss-based metaphor interpretation benefits metaphor detection.1", "venue": "Findings", "year": 2021, "referenceCount": 57, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.173.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "1971-1981"}, "authors": [{"authorId": "2105531615", "name": "Hai Wan"}, {"authorId": "48208642", "name": "Jinxia Lin"}, {"authorId": "2849559", "name": "Jianfeng Du"}, {"authorId": "2065248786", "name": "Dawei Shen"}, {"authorId": "2109066249", "name": "Manrong Zhang"}]}, {"paperId": "304f93ebf38fcb7726e89157fadb5527ba16a1b7", "externalIds": {"DBLP": "journals/corr/abs-2106-00877", "ArXiv": "2106.00877", "ACL": "2021.findings-acl.174", "DOI": "10.18653/v1/2021.findings-acl.174", "CorpusId": 235294229}, "corpusId": 235294229, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/304f93ebf38fcb7726e89157fadb5527ba16a1b7", "title": "Evaluating Word Embeddings with Categorical Modularity", "abstract": "We introduce categorical modularity, a novel low-resource intrinsic metric to evaluate word embedding quality. Categorical modularity is a graph modularity metric based on the $k$-nearest neighbor graph constructed with embedding vectors of words from a fixed set of semantic categories, in which the goal is to measure the proportion of words that have nearest neighbors within the same categories. We use a core set of 500 words belonging to 59 neurobiologically motivated semantic categories in 29 languages and analyze three word embedding models per language (FastText, MUSE, and subs2vec). We find moderate to strong positive correlations between categorical modularity and performance on the monolingual tasks of sentiment analysis and word similarity calculation and on the cross-lingual task of bilingual lexicon induction both to and from English. Overall, we suggest that categorical modularity provides non-trivial predictive information about downstream task performance, with breakdowns of correlations by model suggesting some meta-predictive properties about semantic information loss as well.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.174.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.00877"}, "authors": [{"authorId": "102481070", "name": "S\u00edlvia Casacuberta"}, {"authorId": "2106627578", "name": "Karina Halevy"}, {"authorId": "6894443", "name": "Dami\u00e1n E. Blasi"}]}, {"paperId": "7597deda4ff66e8aa068b7f4da993fd6c2e8b4c6", "externalIds": {"ArXiv": "2106.01451", "DBLP": "conf/acl/MartinezNBRSG21", "ACL": "2021.findings-acl.175", "DOI": "10.18653/v1/2021.findings-acl.175", "CorpusId": 235313804}, "corpusId": 235313804, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7597deda4ff66e8aa068b7f4da993fd6c2e8b4c6", "title": "Attention-based Contextual Language Model Adaptation for Speech Recognition", "abstract": "Language modeling (LM) for automatic speech recognition (ASR) does not usually incorporate utterance level contextual information. For some domains like voice assistants, however, additional context, such as the time at which an utterance was spoken, provides a rich input signal. We introduce an attention mechanism for training neural speech recognition language models on both text and non-linguistic contextual data. When applied to a large de-identified dataset of utterances collected by a popular voice assistant platform, our method reduces perplexity by 7.0% relative over a standard LM that does not incorporate contextual information. When evaluated on utterances extracted from the long tail of the dataset, our method improves perplexity by 9.0% relative over a standard LM and by over 2.8% relative when compared to a state-of-the-art model for contextual LM.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.175.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "1994-2003"}, "authors": [{"authorId": "144584594", "name": "Richard Diehl Martinez"}, {"authorId": "2757384", "name": "Scott Novotney"}, {"authorId": "2575459", "name": "I. Bulyko"}, {"authorId": "3070896", "name": "A. Rastrow"}, {"authorId": "1762744", "name": "A. Stolcke"}, {"authorId": "36881920", "name": "Ankur Gandhe"}]}, {"paperId": "8a740e79efe29be8154c91becc6277b699b1c766", "externalIds": {"ACL": "2021.findings-acl.176", "DBLP": "conf/acl/BaruahCN21", "DOI": "10.18653/v1/2021.findings-acl.176", "CorpusId": 236478028}, "corpusId": 236478028, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8a740e79efe29be8154c91becc6277b699b1c766", "title": "Annotation and Evaluation of Coreference Resolution in Screenplays", "abstract": "Screenplays refer to characters using different names, pronouns, and nominal expressions. We need to resolve these mentions to the correct referent character for better story understanding and holistic research in computational narratology. Coreference resolution of character mentions in screenplays becomes challenging because of the large document lengths, unique structural features like scene headers, interleaving of action and speech passages, and reliance on the accompanying video. In this work, we first adapt widelyused annotation guidelines to address domainspecific issues in screenplays. We develop an automatic screenplay parser to extract the structural information and design coreference rules based upon the structure. Our model exploits these structural features and outperforms a benchmark coreference model on the screenplay coreference resolution task.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.176.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2004-2010"}, "authors": [{"authorId": "18081634", "name": "Sabyasachee Baruah"}, {"authorId": "32585792", "name": "Sandeep Nallan Chakravarthula"}, {"authorId": "152434613", "name": "Shrikanth S. Narayanan"}]}, {"paperId": "04a5ab102b5b4b0cbdc5dd5850a8fe925757e988", "externalIds": {"ACL": "2021.findings-acl.177", "DBLP": "conf/acl/WangGDJ21", "DOI": "10.18653/v1/2021.findings-acl.177", "CorpusId": 236477587}, "corpusId": 236477587, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/04a5ab102b5b4b0cbdc5dd5850a8fe925757e988", "title": "Exploring Cross-Lingual Transfer Learning with Unsupervised Machine Translation", "abstract": "In Natural Language Understanding (NLU), to facilitate Cross-Lingual Transfer Learning (CLTL), especially CLTL between distant languages, we integrate CLTL with Machine Translation (MT), and thereby propose a novel CLTL model named Translation Aided Language Learner (TALL). TALL is constructed as a standard transformer, where the encoder is a pre-trained multilingual language model. The training of TALL includes an MT-oriented pre-training and an NLU-oriented fine-tuning. To make use of unannotated data, we implement the recently proposed Unsupervised Machine Translation (UMT) technique in the MToriented pre-training of TALL. The experimental results show that the application of UMT enables TALL to consistently achieve better CLTL performance than our baseline model, which is the pre-trained multilingual language model serving as the encoder of TALL, without using more annotated data, and the performance gain is relatively prominent in the case of distant languages.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2011-2020"}, "authors": [{"authorId": "37779809", "name": "Chao Wang"}, {"authorId": "3025472", "name": "Judith Gaspers"}, {"authorId": "2973433", "name": "Q. Do"}, {"authorId": "36357862", "name": "Hui Jiang"}]}, {"paperId": "ffa8f901372d67bd6e4b4f8c0f89f264fcdddce7", "externalIds": {"ACL": "2021.findings-acl.178", "DBLP": "conf/acl/YanoU21", "DOI": "10.18653/v1/2021.findings-acl.178", "CorpusId": 236477623}, "corpusId": 236477623, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ffa8f901372d67bd6e4b4f8c0f89f264fcdddce7", "title": "Pipeline Signed Japanese Translation Focusing on a Post-positional Particle Complement and Conjugation in a Low-resource Setting", "abstract": "Because sign language is a visual language, the translation of it into spoken language is typically performed through an intermediate representation called gloss notation. In sign language, function words, such as particles and determiners, are not explicitly expressed, and there is little or no concept of morphological inflection in sign language. Therefore, gloss notation does not include such linguistic constructs. Because of these factors, we argue that sign language translation is effectively processed by taking advantage of the similarities and differences between sign language and its spoken counterpart. We thus propose a pipeline translation method that clearly focuses on the difference between spoken Japanese and signed Japanese written in gloss notation. Specifically, our method first uses statistical machine translation (SMT) to map glosses to corresponding spoken language words. We then use three transformer-based seq2seq models trained using a large out-ofdomain monolingual Japanese corpus to complement postpositional particles and estimate conjugations for the verbs, adjectives, and auxiliary verbs in the first translation. We apply the seq2seq models in sequence until the translation converges. Our experimental results show that the proposed method performs robustly on the low-resource corpus and is +4.4/+4.9 points above the SMT baseline for BLEU-3/4.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2021-2032"}, "authors": [{"authorId": "1861002", "name": "Ken Yano"}, {"authorId": "1785513", "name": "A. Utsumi"}]}, {"paperId": "88f5743491bdf7368ec50a4340a629069dd64adb", "externalIds": {"DBLP": "journals/corr/abs-2012-15814", "ACL": "2021.findings-acl.179", "ArXiv": "2012.15814", "DOI": "10.18653/v1/2021.findings-acl.179", "CorpusId": 229924024}, "corpusId": 229924024, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/88f5743491bdf7368ec50a4340a629069dd64adb", "title": "Language-Mediated, Object-Centric Representation Learning", "abstract": "We present Language-mediated, Object-centric Representation Learning (LORL), a paradigm for learning disentangled, object-centric scene representations from vision and language. LORL builds upon recent advances in unsupervised object discovery and segmentation, notably MONet and Slot Attention. While these algorithms learn an object-centric representation just by reconstructing the input image, LORL enables them to further learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. These object-centric concepts derived from language facilitate the learning of object-centric representations. LORL can be integrated with various unsupervised object discovery algorithms that are language-agnostic. Experiments show that the integration of LORL consistently improves the performance of unsupervised object discovery methods on two datasets via the help of language. We also show that concepts learned by LORL, in conjunction with object discovery methods, aid downstream tasks such as referring expression comprehension.", "venue": "Findings", "year": 2020, "referenceCount": 48, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.179.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-31", "journal": {"pages": "2033-2046"}, "authors": [{"authorId": "22222520", "name": "Ruocheng Wang"}, {"authorId": "13589371", "name": "Jiayuan Mao"}, {"authorId": "1831199", "name": "S. Gershman"}, {"authorId": "3045089", "name": "Jiajun Wu"}]}, {"paperId": "2d09646cb71e088d18d57a6915868a583a1c927d", "externalIds": {"DBLP": "conf/acl/ViegasA21", "ACL": "2021.findings-acl.180", "MAG": "3173852075", "DOI": "10.18653/v1/2021.findings-acl.180", "CorpusId": 236478291}, "corpusId": 236478291, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/2d09646cb71e088d18d57a6915868a583a1c927d", "title": "Entheos: A Multimodal Dataset for Studying Enthusiasm", "abstract": "Enthusiasm plays an important role in engaging communication. It enables speakers to be distinguished and remembered, creating an emotional bond that inspires and motivates their addressees to act, listen, and coordinate (Bettencourt et al., 1983). Although people can easily identify enthusiasm, this is a rather difficult task for machines due to the lack of resources and models that can help them understand or generate enthusiastic behavior. We introduce Entheos, the first multimodal dataset for studying enthusiasm composed of video, audio, and text. We present several baseline models and an ablation study using different features, showing the importance of pitch, loudness, and discourse relation parsing in distinguishing enthusiastic communication.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-01", "journal": {"pages": "2047-2060"}, "authors": [{"authorId": "46591029", "name": "Carla Viegas"}, {"authorId": "2715920", "name": "Malihe Alikhani"}]}, {"paperId": "fd074eaa6b2441347238df28462b5db39666c327", "externalIds": {"ACL": "2021.findings-acl.181", "DBLP": "conf/acl/DoP21", "DOI": "10.18653/v1/2021.findings-acl.181", "CorpusId": 236478307}, "corpusId": 236478307, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/fd074eaa6b2441347238df28462b5db39666c327", "title": "Are Rotten Apples Edible? Challenging Commonsense Inference Ability with Exceptions", "abstract": "Previous studies have argued that pre-trained language models encode commonsense relational knowledge (e.g. that apples are edible ). However, simultaneous work has revealed that such models are often insensitive to context, even ignoring overt contextual cues such as negations. In this paper, we investigate whether masked language models (the BERT family) can move beyond naive associative biases (e.g., apple \u2192 edible ) when the context warrants (e.g. ranking inedible higher when presented with the information that the apple is rotten ). We introduce the W INO V ENTI procedure, which adversarially exploits generic associations in masked language models to create model-speci\ufb01c Winograd-style entailment schemas. Using our constructed W INO V ENTI challenges set of over 2 , 000 schemas, we show that language models in the BERT family experience a steep drop in performance on prompts that require them to pick answers which require reasoning about context (e.g., from 89 . 8% to 18 . 4% for BERT LARGE ). We present evidence that language models exhibit different associative biases, suggesting a need for future work in developing and analyzing frameworks similar to W INO V ENTI that are tuned to model-speci\ufb01c weaknesses.", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 12, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2061-2073"}, "authors": [{"authorId": "2218042", "name": "Nam Do"}, {"authorId": "2949185", "name": "Ellie Pavlick"}]}, {"paperId": "e764dee4e50db01d77976e8f313fc092fc0eba85", "externalIds": {"DBLP": "conf/acl/ZhengQFZZ21", "ACL": "2021.findings-acl.182", "DOI": "10.18653/v1/2021.findings-acl.182", "CorpusId": 236477641}, "corpusId": 236477641, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e764dee4e50db01d77976e8f313fc092fc0eba85", "title": "GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning", "abstract": "Understanding what we genuinely mean instead of what we literally say in conversations is challenging for both humans and machines; yet, this direction is mostly left untouched in modern open-ended dialogue systems. To fill in this gap, we present a grammar-based dialogue dataset, GRICE, designed to bring implicature into pragmatic reasoning in the context of conversations. Our design of GRICE also incorporates other essential aspects of modern dialogue modeling (e.g., coreference). The entire dataset is systematically generated using a hierarchical grammar model, such that each dialogue context has intricate implicatures and is temporally consistent. We further present two tasks, the implicature recovery task followed by the pragmatic reasoning task in conversation, to evaluate the model\u2019s reasoning capability. In experiments, we adopt baselines that claimed to have pragmatics reasoning capability; the results show a significant performance gap between baseline methods and human performance. After integrating a simple module that explicitly reasons about implicature, the model shows an overall performance boost in conversational reasoning. These observations demonstrate the significance of implicature recovery for open-ended dialogue reasoning and call for future research in conversational implicature and conversational reasoning.", "venue": "Findings", "year": 2021, "referenceCount": 70, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2074-2085"}, "authors": [{"authorId": "49774254", "name": "Zilong Zheng"}, {"authorId": "27883493", "name": "Shuwen Qiu"}, {"authorId": "2087083220", "name": "Lifeng Fan"}, {"authorId": "2118318556", "name": "Yixin Zhu"}, {"authorId": "145380991", "name": "Song-Chun Zhu"}]}, {"paperId": "644d4ffd908698e4fafa0b1320c185a3e2c94495", "externalIds": {"ACL": "2021.findings-acl.183", "DBLP": "conf/acl/Colon-Hernandez21", "ArXiv": "2108.12941", "DOI": "10.18653/v1/2021.findings-acl.183", "CorpusId": 236477477}, "corpusId": 236477477, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/644d4ffd908698e4fafa0b1320c185a3e2c94495", "title": "RetroGAN: A Cyclic Post-Specialization System for Improving Out-of-Knowledge and Rare Word Representations", "abstract": "Retrofitting is a technique used to move word vectors closer together or further apart in their space to reflect their relationships in a Knowledge Base (KB). However, retrofitting only works on concepts that are present in that KB. RetroGAN uses a pair of Generative Adversarial Networks (GANs) to learn a one-to-one mapping between concepts and their retrofitted counterparts. It applies that mapping (post-specializes) to handle concepts that do not appear in the original KB in a manner similar to how some natural language systems handle out-of-vocabulary entries. We test our system on three word-similarity benchmarks and a downstream sentence simplification task and achieve the state of the art (CARD-660). Altogether, our results demonstrate our system's effectiveness for out-of-knowledge and rare word generalization.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.183.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-30", "journal": {"pages": "2086-2095"}, "authors": [{"authorId": "1410565307", "name": "Pedro Colon-Hernandez"}, {"authorId": "2047523627", "name": "Yida Xin"}, {"authorId": "145507148", "name": "H. Lieberman"}, {"authorId": "2232845", "name": "Catherine Havasi"}, {"authorId": "2065304843", "name": "C. Breazeal"}, {"authorId": "2065786106", "name": "Peter Chin"}]}, {"paperId": "ae5294d6e32bf1af730101db30708cabc942f48f", "externalIds": {"DBLP": "conf/acl/LuoXGSM21", "ACL": "2021.findings-acl.184", "DOI": "10.18653/v1/2021.findings-acl.184", "CorpusId": 236478271}, "corpusId": 236478271, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ae5294d6e32bf1af730101db30708cabc942f48f", "title": "Fusion: Towards Automated ICD Coding via Feature Compression", "abstract": "ICD coding aims to automatically assign International Classification of Diseases (ICD) codes from unstructured clinical notes or discharge summaries, which saves human labor and reduces errors. Although several studies are proposed to solve this challenging task, none distinguishes the importance of different phrases with a word window. Intuitively, informative phrases should be more useful for the prediction. This paper proposes a feature compressed ICD coding model named Fusion to address this issue. In particular, we propose an attentive soft-pooling approach to compress the sparse and redundant word representations into informative and dense ones as local features. Besides, we use the key-query attention mechanism for modeling the inner relations among local features to generate the global features, which are further used to predict ICD codes. Experiments on two widely used datasets demonstrate that Fusion outperforms baselines. However, on the MIMIC-III Full dataset, we find that none of the state-ofthe-art approaches significantly perform better than others. Thus, automated ICD coding is still a challenging task.", "venue": "Findings", "year": 2021, "referenceCount": 22, "citationCount": 14, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.184.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2096-2101"}, "authors": [{"authorId": "48300263", "name": "Junyu Luo"}, {"authorId": "145781464", "name": "Cao Xiao"}, {"authorId": "28331874", "name": "Lucas Glass"}, {"authorId": "1738536", "name": "Jimeng Sun"}, {"authorId": "2068198592", "name": "Fenglong Ma"}]}, {"paperId": "c86f268c3f37749595baad4f2474edf668645756", "externalIds": {"DBLP": "conf/acl/WuGBZD21", "ACL": "2021.findings-acl.185", "ArXiv": "2106.07192", "DOI": "10.18653/v1/2021.findings-acl.185", "CorpusId": 235422493}, "corpusId": 235422493, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c86f268c3f37749595baad4f2474edf668645756", "title": "Automatic Document Sketching: Generating Drafts from Analogous Texts", "abstract": "The advent of large pre-trained language models has made it possible to make high-quality predictions on how to add or change a sentence in a document. However, the high branching factor inherent to text generation impedes the ability of even the strongest language models to offer useful editing suggestions at a more global or document level. We introduce a new task, document sketching, which involves generating entire draft documents for the writer to review and revise. These drafts are built from sets of documents that overlap in form - sharing large segments of potentially reusable text - while diverging in content. To support this task, we introduce a Wikipedia-based dataset of analogous documents and investigate the application of weakly supervised methods, including use of a transformer-based mixture of experts, together with reinforcement learning. We report experiments using automated and human evaluation methods and discuss relative merits of these models.", "venue": "Findings", "year": 2021, "referenceCount": 56, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.185.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-06-14", "journal": {"name": "ArXiv", "volume": "abs/2106.07192"}, "authors": [{"authorId": "7806955", "name": "Zeqiu Wu"}, {"authorId": "1947267", "name": "Michel Galley"}, {"authorId": "3125776", "name": "Chris Brockett"}, {"authorId": "48378494", "name": "Yizhe Zhang"}, {"authorId": "66648221", "name": "Bill Dolan"}]}, {"paperId": "803750b376603d1aa4b8512a18a865b0adbdc32c", "externalIds": {"DBLP": "conf/acl/ZhouML21", "ACL": "2021.findings-acl.186", "ArXiv": "2105.12825", "DOI": "10.18653/v1/2021.findings-acl.186", "CorpusId": 235212271}, "corpusId": 235212271, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/803750b376603d1aa4b8512a18a865b0adbdc32c", "title": "Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading", "abstract": "In this paper, we introduce an event-driven trading strategy that predicts stock movements by detecting corporate events from news articles. Unlike existing models that utilize textual features (e.g., bag-of-words) and sentiments to directly make stock predictions, we consider corporate events as the driving force behind stock movements and aim to profit from the temporary stock mispricing that may occur when corporate events take place. The core of the proposed strategy is a bi-level event detection model. The low-level event detector identifies events' existences from each token, while the high-level event detector incorporates the entire article's representation and the low-level detected results to discover events at the article-level. We also develop an elaborately-annotated dataset EDT for corporate event detection and news-based stock prediction benchmark. EDT includes 9721 news articles with token-level event labels as well as 303893 news articles with minute-level timestamps and comprehensive stock price labels. Experiments on EDT indicate that the proposed strategy outperforms all the baselines in winning rate, excess returns over the market, and the average return on each transaction.", "venue": "Findings", "year": 2021, "referenceCount": 24, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.186.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Economics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Economics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-26", "journal": {"pages": "2114-2124"}, "authors": [{"authorId": "50251585", "name": "Zhihan Zhou"}, {"authorId": "2109635281", "name": "Li-Qian Ma"}, {"authorId": "2118960667", "name": "Han Liu"}]}, {"paperId": "182c36a784f768da6676a258813b94cc74a67be8", "externalIds": {"DBLP": "conf/acl/KohitaWKCTM21", "ACL": "2021.findings-acl.187", "DOI": "10.18653/v1/2021.findings-acl.187", "CorpusId": 236477578}, "corpusId": 236477578, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/182c36a784f768da6676a258813b94cc74a67be8", "title": "Language-based General Action Template for Reinforcement Learning Agents", "abstract": "Prior knowledge plays a critical role in decision-making, and humans preserve such knowledge in the form of natural language (NL). To emulate real-world decision-making, artificial agents should incorporate such generic knowledge into their decisionmaking framework through NL. However, since policy learning with NL-based action representation is intractable due to NL\u2019s combinatorial complexity, previous studies have limited agents\u2019 expressive power to only a specific environment, which sacrificed the generalization ability to other environments. This paper proposes a new environmentagnostic action framework, the languagebased general action template (L-GAT). We design action templates on the basis of general semantic schemes (FrameNet, VerbNet, and WordNet), facilitating the agent in finding a plausible action in a given state by using prior knowledge while covering broader types of actions in a general manner. Our experiment using 18 text-based games showed that our proposed L-GAT agent which uses the same actions across games, achieved a performance competitive with agents that rely on gamespecific actions. We have published the code at https://github.com/kohilin/lgat.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2125-2139"}, "authors": [{"authorId": "22312240", "name": "Ryosuke Kohita"}, {"authorId": "41036308", "name": "Akifumi Wachi"}, {"authorId": "40433860", "name": "Daiki Kimura"}, {"authorId": "34597365", "name": "Subhajit Chaudhury"}, {"authorId": "3305985", "name": "Michiaki Tatsubori"}, {"authorId": "1688057", "name": "Asim Munawar"}]}, {"paperId": "b5b006dc558cb7fbd532d67e989173b536e8ac80", "externalIds": {"DBLP": "conf/acl/WangBHDW21", "ArXiv": "2012.15828", "ACL": "2021.findings-acl.188", "DOI": "10.18653/v1/2021.findings-acl.188", "CorpusId": 229923069}, "corpusId": 229923069, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b5b006dc558cb7fbd532d67e989173b536e8ac80", "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers", "abstract": "We generalize deep self-attention distillation in MiniLM (Wang et al., 2020) by only using self-attention relation distillation for task-agnostic compression of pretrained Transformers. In particular, we define multi-head self-attention relations as scaled dot-product between the pairs of query, key, and value vectors within each self-attention module. Then we employ the above relational knowledge to train the student model. Besides its simplicity and unified principle, more favorably, there is no restriction in terms of the number of student's attention heads, while most previous work has to guarantee the same head number between teacher and student. Moreover, the fine-grained self-attention relations tend to fully exploit the interaction knowledge learned by Transformer. In addition, we thoroughly examine the layer selection strategy for teacher models, rather than just relying on the last layer as in MiniLM. We conduct extensive experiments on compressing both monolingual and multilingual pretrained models. Experimental results demonstrate that our models distilled from base-size and large-size teachers (BERT, RoBERTa and XLM-R) outperform the state-of-the-art.", "venue": "Findings", "year": 2020, "referenceCount": 52, "citationCount": 117, "influentialCitationCount": 28, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.188.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-31", "journal": {"pages": "2140-2151"}, "authors": [{"authorId": "51456429", "name": "Wenhui Wang"}, {"authorId": "10699417", "name": "Hangbo Bao"}, {"authorId": "3110003", "name": "Shaohan Huang"}, {"authorId": "145307652", "name": "Li Dong"}, {"authorId": "49807919", "name": "Furu Wei"}]}, {"paperId": "fd375925c8fc8e932e6c8356b2c776cd81f69d37", "externalIds": {"DBLP": "conf/acl/ZhouWCHH21", "ACL": "2021.findings-acl.189", "DOI": "10.18653/v1/2021.findings-acl.189", "CorpusId": 236477986}, "corpusId": 236477986, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/fd375925c8fc8e932e6c8356b2c776cd81f69d37", "title": "Attending via both Fine-tuning and Compressing", "abstract": "Though being a primary trend for enhancing interpretability of neural networks, attention mechanism\u2019s reliability and validity are still under debate. In this paper, we try to purify attention scores to obtain a more faithful explanation of downstream models. Specifically, we propose a framework consisting of a learner and a compressor, which performs finetuning and compressing iteratively to enhance the performance and interpretability of the attention mechanism. The learner focuses on learning better text representations to achieve good decisions by fine-tuning, while the compressor aims to perform compressions over the representations to retain the most useful clues for explanations with a Variational information bottleneck ATtention (VAT) mechanism. Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2152-2161"}, "authors": [{"authorId": "2119617269", "name": "Jie Zhou"}, {"authorId": "3174675", "name": "Yuanbin Wu"}, {"authorId": "1585496097", "name": "Qin Chen"}, {"authorId": "1790227", "name": "Xuanjing Huang"}, {"authorId": "2148951260", "name": "Liang He"}]}, {"paperId": "47ea802a1fed0f735a9a61a7c9b63062f02a5f5d", "externalIds": {"DBLP": "journals/corr/abs-2106-01654", "ArXiv": "2106.01654", "ACL": "2021.findings-acl.190", "DOI": "10.18653/v1/2021.findings-acl.190", "CorpusId": 235313618}, "corpusId": 235313618, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/47ea802a1fed0f735a9a61a7c9b63062f02a5f5d", "title": "Improving Event Causality Identification via Self-Supervised Representation Learning on External Causal Statement", "abstract": "Current models for event causality identification (ECI) mainly adopt a supervised framework, which heavily rely on labeled data for training. Unfortunately, the scale of current annotated datasets is relatively limited, which cannot provide sufficient support for models to capture useful indicators from causal statements, especially for handing those new, unseen cases. To alleviate this problem, we propose a novel approach, shortly named CauSeRL, which leverages external causal statements for event causality identification. First of all, we design a self-supervised framework to learn context-specific causal patterns from external causal statements. Then, we adopt a contrastive transfer strategy to incorporate the learned context-specific causal patterns into the target ECI model. Experimental results show that our method significantly outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.0 and +3.4 points on F1 value respectively).", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 26, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.190.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"pages": "2162-2172"}, "authors": [{"authorId": "87696608", "name": "Xinyu Zuo"}, {"authorId": "49776272", "name": "Pengfei Cao"}, {"authorId": "152829071", "name": "Yubo Chen"}, {"authorId": "77397868", "name": "Kang Liu"}, {"authorId": "11447228", "name": "Jun Zhao"}, {"authorId": "49161576", "name": "Weihua Peng"}, {"authorId": "2145264600", "name": "Yuguang Chen"}]}, {"paperId": "4097ae9aca6444fd7536bfbed1e62560521b70d3", "externalIds": {"ArXiv": "2108.06027", "DBLP": "conf/acl/RenLQLZSWWW21", "ACL": "2021.findings-acl.191", "DOI": "10.18653/v1/2021.findings-acl.191", "CorpusId": 236477844}, "corpusId": 236477844, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4097ae9aca6444fd7536bfbed1e62560521b70d3", "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval", "abstract": "Recently, dense passage retrieval has become a mainstream approach to finding relevant information in various natural language processing tasks. A number of studies have been devoted to improving the widely adopted dual-encoder architecture. However, most of the previous studies only consider query-centric similarity relation when learning the dual-encoder retriever. In order to capture more comprehensive similarity relations, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. To implement our approach, we make three major technical contributions by introducing formal formulations of the two kinds of similarity relations, generating high-quality pseudo labeled data via knowledge distillation, and designing an effective two-stage training procedure that incorporates passage-centric similarity relation constraint. Extensive experiments show that our approach significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions datasets.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 62, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.191.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-13", "journal": {"name": "ArXiv", "volume": "abs/2108.06027"}, "authors": [{"authorId": "1708171825", "name": "Ruiyang Ren"}, {"authorId": "28128416", "name": "Shangwen Lv"}, {"authorId": "51281403", "name": "Yingqi Qu"}, {"authorId": "46700619", "name": "Jing Liu"}, {"authorId": "2542603", "name": "Wayne Xin Zhao"}, {"authorId": "40430110", "name": "Qiaoqiao She"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "144270731", "name": "Haifeng Wang"}, {"authorId": "153693432", "name": "Ji-rong Wen"}]}, {"paperId": "0d8bf9ca6e58ab9f34bc9a24a82884073d0153ea", "externalIds": {"DBLP": "conf/acl/VasilyevB21", "ACL": "2021.findings-acl.192", "ArXiv": "2012.14602", "DOI": "10.18653/v1/2021.findings-acl.192", "CorpusId": 229924264}, "corpusId": 229924264, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0d8bf9ca6e58ab9f34bc9a24a82884073d0153ea", "title": "Is Human Scoring the Best Criteria for Summary Evaluation?", "abstract": "Normally, summary quality measures are compared with quality scores produced by human annotators. A higher correlation with human scores is considered to be a fair indicator of a better measure. We discuss observations that cast doubt on this view. We attempt to show a possibility of an alternative indicator. Given a family of measures, we explore a criterion of selecting the best measure not relying on correlations with human scores. Our observations for the BLANC family of measures suggest that the criterion is universal across very different styles of summaries.", "venue": "Findings", "year": 2020, "referenceCount": 31, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.192.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-29", "journal": {"name": "ArXiv", "volume": "abs/2012.14602"}, "authors": [{"authorId": "2753049", "name": "Oleg V. Vasilyev"}, {"authorId": "1995240446", "name": "John Bohannon"}]}, {"paperId": "e3d8f27f124cdf495834798ffcc5507005ceab6d", "externalIds": {"ArXiv": "2105.02573", "DBLP": "conf/acl/XiangLCLLL21", "ACL": "2021.findings-acl.193", "DOI": "10.18653/v1/2021.findings-acl.193", "CorpusId": 233864494}, "corpusId": 233864494, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e3d8f27f124cdf495834798ffcc5507005ceab6d", "title": "Assessing Dialogue Systems with Distribution Distances", "abstract": "An important aspect of developing dialogue systems is how to evaluate and compare the performance of different systems. Existing automatic evaluation metrics are based on turn-level quality evaluation and use average scores for system-level comparison. In this paper, we propose to measure the performance of a dialogue system by computing the distribution-wise distance between its generated conversations and real-world conversations. Specifically, two distribution-wise metrics, FBD and PRD, are developed and evaluated. Experiments on several dialogue corpora show that our proposed metrics correlate better with human judgments than existing metrics.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.193.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-06", "journal": {"name": "ArXiv", "volume": "abs/2105.02573"}, "authors": [{"authorId": "2057477005", "name": "Jiannan Xiang"}, {"authorId": "2144475418", "name": "Yahui Liu"}, {"authorId": "1724421", "name": "Deng Cai"}, {"authorId": "91956362", "name": "Huayang Li"}, {"authorId": "1862782", "name": "Defu Lian"}, {"authorId": "2978364", "name": "Lemao Liu"}]}, {"paperId": "37d3a46448e87d7abcb287472baf886c65783ee4", "externalIds": {"ArXiv": "2106.06689", "ACL": "2021.findings-acl.194", "DBLP": "journals/corr/abs-2106-06689", "DOI": "10.18653/v1/2021.findings-acl.194", "CorpusId": 235422434}, "corpusId": 235422434, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/37d3a46448e87d7abcb287472baf886c65783ee4", "title": "Neural Combinatory Constituency Parsing", "abstract": "We propose two fast neural combinatory models for constituency parsing: binary and multi-branching. Our models decompose the bottom-up parsing process into 1) classification of tags, labels, and binary orientations or chunks and 2) vector composition based on the computed orientations or chunks. These models have theoretical sub-quadratic complexity and empirical linear complexity. The binary model achieves an F1 score of 92.54 on Penn Treebank, speeding at 1327.2 sents/sec. Both the models with XLNet provide near state-of-the-art accuracies for English. Syntactic branching tendency and headedness of a language are observed during the training and inference processes for Penn Treebank, Chinese Treebank, and Keyaki Treebank (Japanese).", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.194.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-12", "journal": {"name": "ArXiv", "volume": "abs/2106.06689"}, "authors": [{"authorId": "2144174417", "name": "Zhousi Chen"}, {"authorId": "2108283190", "name": "Longtu Zhang"}, {"authorId": "2111877206", "name": "Aizhan Imankulova"}, {"authorId": "2936411", "name": "Mamoru Komachi"}]}, {"paperId": "8b8f7c580bb94ace0676be7a5c424b27b1194913", "externalIds": {"DBLP": "conf/acl/HanWJL21", "ACL": "2021.findings-acl.195", "ArXiv": "2105.03095", "DOI": "10.18653/v1/2021.findings-acl.195", "CorpusId": 234093787}, "corpusId": 234093787, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8b8f7c580bb94ace0676be7a5c424b27b1194913", "title": "Learning Shared Semantic Space for Speech-to-Text Translation", "abstract": "Having numerous potential applications and great impact, end-to-end speech translation (ST) has long been treated as an independent task, failing to fully draw strength from the rapid advances of its sibling - text machine translation (MT). With text and audio inputs represented differently, the modality gap has rendered MT data and its end-to-end models incompatible with their ST counterparts. In observation of this obstacle, we propose to bridge this representation gap with Chimera. By projecting audio and text features to a common semantic representation, Chimera unifies MT and ST tasks and boosts the performance on ST benchmarks, MuST-C and Augmented Librispeech, to a new state-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE, improving the SOTA by a +1.9 BLEU margin. Further experimental analyses demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities. Code, data, and resources are available at https://github.com/Glaciohound/Chimera-ST.", "venue": "Findings", "year": 2021, "referenceCount": 65, "citationCount": 48, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.195.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-07", "journal": {"pages": "2214-2225"}, "authors": [{"authorId": "2118642562", "name": "Chi Han"}, {"authorId": "50468534", "name": "Mingxuan Wang"}, {"authorId": "144016781", "name": "Heng Ji"}, {"authorId": "143900005", "name": "Lei Li"}]}, {"paperId": "f50abcae0477351e9d2814547158d348ebf59bcb", "externalIds": {"DBLP": "journals/corr/abs-2106-03046", "ACL": "2021.findings-acl.196", "ArXiv": "2106.03046", "DOI": "10.18653/v1/2021.findings-acl.196", "CorpusId": 235358557}, "corpusId": 235358557, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f50abcae0477351e9d2814547158d348ebf59bcb", "title": "Empowering Language Understanding with Counterfactual Reasoning", "abstract": "Present language understanding methods have demonstrated extraordinary ability of recognizing patterns in texts via machine learning. However, existing methods indiscriminately use the recognized patterns in the testing phase that is inherently different from us humans who have counterfactual thinking, e.g., to scrutinize for the hard testing samples. Inspired by this, we propose a Counterfactual Reasoning Model, which mimics the counterfactual thinking by learning from few counterfactual samples. In particular, we devise a generation module to generate representative counterfactual samples for each factual sample, and a retrospective module to retrospect the model prediction by comparing the counterfactual and factual samples. Extensive experiments on sentiment analysis (SA) and natural language inference (NLI) validate the effectiveness of our method.", "venue": "Findings", "year": 2021, "referenceCount": 43, "citationCount": 22, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.196.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-06", "journal": {"name": "ArXiv", "volume": "abs/2106.03046"}, "authors": [{"authorId": "2163400298", "name": "Fuli Feng"}, {"authorId": "2570260", "name": "Jizhi Zhang"}, {"authorId": "7792071", "name": "Xiangnan He"}, {"authorId": "2119078220", "name": "Hanwang Zhang"}, {"authorId": "143779329", "name": "Tat-seng Chua"}]}, {"paperId": "5f5929efc9bb806f0dfa1c8af38a5e49a40d922d", "externalIds": {"ACL": "2021.findings-acl.197", "DBLP": "journals/corr/abs-2008-10327", "ArXiv": "2008.10327", "MAG": "3080138651", "DOI": "10.18653/v1/2021.findings-acl.197", "CorpusId": 221265975}, "corpusId": 221265975, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5f5929efc9bb806f0dfa1c8af38a5e49a40d922d", "title": "Knowledge-Empowered Representation Learning for Chinese Medical Reading Comprehension: Task, Model and Resources", "abstract": "Machine Reading Comprehension (MRC) aims to extract answers to questions given a passage. It has been widely studied recently, especially in open domains. However, few efforts have been made on closed-domain MRC, mainly due to the lack of large-scale training data. In this paper, we introduce a multi-target MRC task for the medical domain, whose goal is to predict answers to medical questions and the corresponding support sentences from medical information sources simultaneously, in order to ensure the high reliability of medical knowledge serving. A high-quality dataset is manually constructed for the purpose, named Multi-task Chinese Medical MRC dataset (CMedMRC), with detailed analysis conducted. We further propose the Chinese medical BERT model for the task (CMedBERT), which fuses medical knowledge into pre-trained language models by the dynamic fusion mechanism of heterogeneous features and the multi-task learning strategy. Experiments show that CMedBERT consistently outperforms strong baselines by fusing context-aware and knowledge-aware token representations.", "venue": "Findings", "year": 2020, "referenceCount": 50, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.197.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-08-24", "journal": {"pages": "2237-2249"}, "authors": [{"authorId": "2146342371", "name": "Taolin Zhang"}, {"authorId": "121899912", "name": "Chengyu Wang"}, {"authorId": "2642333", "name": "Minghui Qiu"}, {"authorId": "2821227", "name": "Bite Yang"}, {"authorId": "143644849", "name": "Xiaofeng He"}, {"authorId": "2078113", "name": "Jun Huang"}]}, {"paperId": "c70b9276d4e4a27f2218bc3e921e9fc3ffd18f14", "externalIds": {"MAG": "3173712076", "DBLP": "conf/acl/ZhangPZWHSWW21", "ACL": "2021.findings-acl.198", "DOI": "10.18653/v1/2021.findings-acl.198", "CorpusId": 236478088}, "corpusId": 236478088, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c70b9276d4e4a27f2218bc3e921e9fc3ffd18f14", "title": "Correcting Chinese Spelling Errors with Phonetic Pre-training", "abstract": "Chinese spelling correction (CSC) is an important yet challenging task. Existing state-of-the-art methods either only use a pre-trained language model or incorporate phonological information as external knowledge. In this paper, we propose a novel end-to-end CSC model that integrates phonetic features into language model by leveraging the powerful pre-training and \ufb01ne-tuning method. Instead of conventionally masking words with a special token in training language model, we replace words with phonetic features and their sound-alike words. We further propose an adaptive weighted objective to jointly train error detection and correction in a uni\ufb01ed framework. Experimental results show that our model achieves signi\ufb01cant improvements on SIGHAN datasets and outperforms the previous state-of-the-art methods.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 45, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.198.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-01", "journal": {"pages": "2250-2261"}, "authors": [{"authorId": "49775095", "name": "Ruiqing Zhang"}, {"authorId": "2054086618", "name": "Chao Pang"}, {"authorId": "30750818", "name": "Chuanqiang Zhang"}, {"authorId": "104463827", "name": "Shuohuan Wang"}, {"authorId": "37985966", "name": "Zhongjun He"}, {"authorId": "2117103617", "name": "Yu Sun"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "144270731", "name": "Haifeng Wang"}]}, {"paperId": "aaef74b96d40ba33a21f553117e7b962b1ce00ba", "externalIds": {"ACL": "2021.findings-acl.199", "DBLP": "conf/acl/WangYCXW21", "DOI": "10.18653/v1/2021.findings-acl.199", "CorpusId": 236477876}, "corpusId": 236477876, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/aaef74b96d40ba33a21f553117e7b962b1ce00ba", "title": "Multi-Lingual Question Generation with Language Agnostic Language Model", "abstract": "Question generation is the task of generating coherent and relevant question given context paragraph. Recently, with the development of large-scale question answering datasets such as SQuAD, the English question generation has been rapidly developed. However, for other languages such as Chinese, the available training data is limited, which hinders the development of question generation in the corresponding language. To investigate the multi-lingual question generation, in this paper, we develop a language-agnostic language model, which learns the shared representation from several languages in a single architecture. We propose an adversarial training objective to encourage the model to learn both language-specific and language-independent information. We utilize abundant monolingual text to improve the multi-lingual question generation via pre-training. With the languageagnostic language model, we achieve significant improvement in multi-lingual question generation over five languages. In addition, we propose a large-scale Chinese question generation dataset containing more than 220k human-generated questions to benefit the multi-lingual question generation research.", "venue": "Findings", "year": 2021, "referenceCount": 47, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2262-2272"}, "authors": [{"authorId": "46270259", "name": "Bingning Wang"}, {"authorId": "48577275", "name": "Ting Yao"}, {"authorId": "11070957", "name": "Weipeng Chen"}, {"authorId": "2774294", "name": "Jingfang Xu"}, {"authorId": "2108387103", "name": "Xiaochuan Wang"}]}, {"paperId": "70cd8680ef7fa1ac476a901ab94d16e310fb790e", "externalIds": {"DBLP": "conf/acl/XingW21", "ACL": "2021.findings-acl.200", "DOI": "10.18653/v1/2021.findings-acl.200", "CorpusId": 236477566}, "corpusId": 236477566, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/70cd8680ef7fa1ac476a901ab94d16e310fb790e", "title": "Structure-Aware Pre-Training for Table-to-Text Generation", "abstract": "Table-to-text generation is a subtask of data-to-text generation which aims to generate nal-tural language text based on input table. Pre-training techniques have achieved great success on table-to-text generation. However, the pre-trained models used in previous works are typically trained on free-form natural language text while the input of table-to-text task is structured table. In this paper, we propose STTP, a pre-trained model that is trained with tables and their contexts. The STTP model can understand the structured input table and generate \ufb02uent text. Experiments on two datasets show the ef\ufb01cacy of our model.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2273-2278"}, "authors": [{"authorId": "143989287", "name": "Xinyu Xing"}, {"authorId": "117908148", "name": "Xiaojun Wan"}]}, {"paperId": "52b1cf563d1368f72e82b91b0349a7012a746f4f", "externalIds": {"ACL": "2021.findings-acl.201", "ArXiv": "2105.14668", "DBLP": "conf/acl/YuE21", "DOI": "10.18653/v1/2021.findings-acl.201", "CorpusId": 235254510}, "corpusId": 235254510, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/52b1cf563d1368f72e82b91b0349a7012a746f4f", "title": "On the Interplay Between Fine-tuning and Composition in Transformers", "abstract": "Pre-trained transformer language models have shown remarkable performance on a variety of NLP tasks. However, recent research has suggested that phrase-level representations in these models reflect heavy influences of lexical content, but lack evidence of sophisticated, compositional phrase information. Here we investigate the impact of fine-tuning on the capacity of contextualized embeddings to capture phrase meaning information beyond lexical content. Specifically, we fine-tune models on an adversarial paraphrase classification task with high lexical overlap, and on a sentiment classification task. After fine-tuning, we analyze phrasal representations in controlled settings following prior work. We find that fine-tuning largely fails to benefit compositionality in these representations, though training on sentiment yields a small, localized benefit for certain models. In follow-up analyses, we identify confounding cues in the paraphrase dataset that may explain the lack of composition benefits from that task, and we discuss potential factors underlying the localized benefits from sentiment training.", "venue": "Findings", "year": 2021, "referenceCount": 66, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.201.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-31", "journal": {"pages": "2279-2293"}, "authors": [{"authorId": "2414467", "name": "Lang-Chi Yu"}, {"authorId": "37907837", "name": "Allyson Ettinger"}]}, {"paperId": "4e7891e7f3d9380614cb6b0152a7129ed5f4d1d3", "externalIds": {"ACL": "2021.findings-acl.202", "DBLP": "conf/acl/QinLCR21", "DOI": "10.18653/v1/2021.findings-acl.202", "CorpusId": 236477586}, "corpusId": 236477586, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4e7891e7f3d9380614cb6b0152a7129ed5f4d1d3", "title": "Lifelong Learning of Topics and Domain-Specific Word Embeddings", "abstract": "Lifelong topic models mainly focus on indomain text streams in which each chunk only contains documents from a single domain. To overcome data diversity of the in-domain corpus, most of the existing methods exploit the information from limited sources in a separate and heuristic manner. In this study, we develop a lifelong collaborative model (LCM) based on non-negative matrix factorization to accurately learn topics and domain-specific word embeddings. LCM particularly investigates: (1) developing a knowledge graph based on the semantic relationships among words in the lifelong learning process, so as to accumulate global context information discovered by topic models and local context information reflected by context word embeddings from previous domains, and (2) developing a subword graph based on byte pair encoding and pairwise word relationships to exploit subword information of words in the current in-domain corpus. To the best of our knowledge, we are the first to collaboratively learn topics and word embeddings via lifelong learning. Experiments on real-world in-domain text streams validate the effectiveness of our method.", "venue": "Findings", "year": 2021, "referenceCount": 51, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2294-2309"}, "authors": [{"authorId": "2116159181", "name": "Xiaorui Qin"}, {"authorId": "2140021021", "name": "Yuyin Lu"}, {"authorId": "2109122533", "name": "Yufu Chen"}, {"authorId": "36300013", "name": "Yanghui Rao"}]}, {"paperId": "63506de8d7f16369e8e73dadff1d0675ca89f5c1", "externalIds": {"DBLP": "conf/acl/YuanWZZJ21", "ACL": "2021.findings-acl.203", "DOI": "10.18653/v1/2021.findings-acl.203", "CorpusId": 236477425}, "corpusId": 236477425, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/63506de8d7f16369e8e73dadff1d0675ca89f5c1", "title": "Leveraging Argumentation Knowledge Graph for Interactive Argument Pair Identification", "abstract": "Interactive argument pair identi\ufb01cation is essential in the context of dialogical argumentation mining. Existing research treats it as a problem of sentence matching and largely relies on textual information to compute the similarities. However, the interaction of opinions usually involves the background of the topic and requires reasoning of knowledge, which is beyond textual information. In this paper, we propose to leverage external knowledge to enhance the identi\ufb01cation of interactive argu-ment pairs. We construct the argumentation knowledge graph from the discussion thread of the target topic in the online forum. The interaction between the original argument and the reply is then represented as the path of concepts in the knowledge graph. In practice, we utilize Graph Convolutional Network (GCN) to learn the concept representation in the knowledge graph and use a Transformer-based encoder to learn the representation of paths. Finally, an information alignment network is employed to capture the interaction of textual information of conceptual information (both entity-level and path-level). Experiment results indicate that our model achieves state-of-the-art performance in the benchmark dataset. Further analysis demonstrates the effectiveness of our model for enforcing knowledge reasoning through paths in the knowledge graph.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2310-2319"}, "authors": [{"authorId": "46685232", "name": "Jianda Yuan"}, {"authorId": "2118602528", "name": "Zhongyu Wei"}, {"authorId": "2110997601", "name": "Donghua Zhao"}, {"authorId": "1409702669", "name": "Qi Zhang"}, {"authorId": "2115484286", "name": "Changjiang Jiang"}]}, {"paperId": "167841e8d0a0a58ad7e92de8e9babdff057e4735", "externalIds": {"DBLP": "conf/acl/LiC21", "ACL": "2021.findings-acl.204", "DOI": "10.18653/v1/2021.findings-acl.204", "CorpusId": 236478122}, "corpusId": 236478122, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/167841e8d0a0a58ad7e92de8e9babdff057e4735", "title": "A Multi-Task Learning Framework for Multi-Target Stance Detection", "abstract": "Multi-target stance detection aims to identify the stance taken toward a pair of different targets from the same text, and typically, there are multiple target pairs per dataset. Existing works generally train one model for each target pair. However, they fail to learn target-speci\ufb01c representations and are prone to over\ufb01tting. In this paper, we propose a new training strategy under the multi-task learning setting by training one model on all target pairs, which helps the model learn more universal representations and alleviate over\ufb01tting. Moreover, in order to extract more accurate target-speci\ufb01c representations, we propose a multi-task learning network which can jointly train our model with a stance (dis)agreement detection task that is designed to identify agreement and disagreement between stances in paired texts. Experimental results demonstrate that our proposed model outperforms the best-performing baseline by 12.39% in macro-averaged F1-score. Our resources are publicly available on GitHub. 1", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.204.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2320-2326"}, "authors": [{"authorId": "50820271", "name": "Yingjie Li"}, {"authorId": "1690656", "name": "Cornelia Caragea"}]}, {"paperId": "938d5a00caa0a5951e1d6d6879b3f904b9741b13", "externalIds": {"ArXiv": "2107.10427", "DBLP": "conf/acl/LiuMCXZ21", "ACL": "2021.findings-acl.205", "DOI": "10.18653/v1/2021.findings-acl.205", "CorpusId": 236171371}, "corpusId": 236171371, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/938d5a00caa0a5951e1d6d6879b3f904b9741b13", "title": "Confidence-Aware Scheduled Sampling for Neural Machine Translation", "abstract": "Scheduled sampling is an effective method to alleviate the exposure bias problem of neural machine translation. It simulates the inference scene by randomly replacing ground-truth target input tokens with predicted ones during training. Despite its success, its critical schedule strategies are merely based on training steps, ignoring the real-time model competence, which limits its potential performance and convergence speed. To address this issue, we propose confidence-aware scheduled sampling. Specifically, we quantify real-time model competence by the confidence of model predictions, based on which we design fine-grained schedule strategies. In this way, the model is exactly exposed to predicted tokens for high-confidence positions and still ground-truth tokens for low-confidence positions. Moreover, we observe vanilla scheduled sampling suffers from degenerating into the original teacher forcing mode since most predicted tokens are the same as ground-truth tokens. Therefore, under the above confidence-aware strategy, we further expose more noisy tokens (e.g., wordy and incorrect word order) instead of predicted ones for high-confidence token positions. We evaluate our approach on the Transformer and conduct experiments on large-scale WMT 2014 English-German, WMT 2014 English-French, and WMT 2019 Chinese-English. Results show that our approach significantly outperforms the Transformer and vanilla scheduled sampling on both translation quality and convergence speed.", "venue": "Findings", "year": 2021, "referenceCount": 43, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.205.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-22", "journal": {"pages": "2327-2337"}, "authors": [{"authorId": "2108060455", "name": "Yijin Liu"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "47559028", "name": "Yufeng Chen"}, {"authorId": "2310092", "name": "Jinan Xu"}, {"authorId": "2108485135", "name": "Jie Zhou"}]}, {"paperId": "92492f73a2d2afac5ff18ad186fc83444c98991d", "externalIds": {"ACL": "2021.findings-acl.206", "DBLP": "conf/acl/ZhangWYZ21", "DOI": "10.18653/v1/2021.findings-acl.206", "CorpusId": 236478333}, "corpusId": 236478333, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/92492f73a2d2afac5ff18ad186fc83444c98991d", "title": "MA-BERT: Learning Representation by Incorporating Multi-Attribute Knowledge in Transformers", "abstract": "Incorporating attribute information such as user and product features into deep neural networks has been shown to be useful in sentiment analysis. Previous works typically ac-complished this in two ways: concatenating multiple attributes to word/text representation or treating them as a bias to adjust attention distribution. To leverage the advantages of both methods, this paper proposes a multi-attribute BERT (MA-BERT) to incorporate external attribute knowledge. The proposed method has two advantages. First, it applies multi-attribute transformer (MA-Transformer) encoders to incorporate multiple attributes into both input representation and attention distribution. Second, the MA-Transformer is implemented as a universal layer and stacked on a BERT-based model such that it can be initialized from a pre-trained checkpoint and \ufb01ne-tuned for the downstream applications without extra pre-training costs. Experiments on three benchmark datasets show that the proposed method outperformed pre-trained BERT models and other methods incorporating external attribute knowledge.", "venue": "Findings", "year": 2021, "referenceCount": 25, "citationCount": 10, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.206.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2338-2343"}, "authors": [{"authorId": "2118130372", "name": "Youjia Zhang"}, {"authorId": "2143717068", "name": "Jin Wang"}, {"authorId": "1756567", "name": "Liang-Chih Yu"}, {"authorId": "2144497782", "name": "Xuejie Zhang"}]}, {"paperId": "5dec0a3d01bb36074ae7dd1965ffe14b8f11f755", "externalIds": {"ACL": "2021.findings-acl.207", "DBLP": "conf/acl/WangCTCZL21", "DOI": "10.18653/v1/2021.findings-acl.207", "CorpusId": 236477710}, "corpusId": 236477710, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5dec0a3d01bb36074ae7dd1965ffe14b8f11f755", "title": "A Closer Look into the Robustness of Neural Dependency Parsers Using Better Adversarial Examples", "abstract": "Previous work on adversarial attacks on dependency parsers has mostly focused on attack methods, as opposed to the quality of adversarial examples, which in previous work has been relatively low. To address this gap, we propose a method to generate high-quality adversarial examples with a higher number of candidate generators and stricter filters, and then verify their quality using automatic and human evaluations. We perform analysis with different parsing models and observe that: (i) injecting words not used in the training stage is an effective attack strategy; (ii) adversarial examples generated against a parser strongly depend on the parser model, the token embeddings, and even the specific instantiation of the model (i.e., a random seed). We use these insights to improve the robustness of English parsing models, relying on adversarial training and model ensembling.1", "venue": "Findings", "year": 2021, "referenceCount": 24, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.207.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2344-2354"}, "authors": [{"authorId": "2115829412", "name": "Yuxuan Wang"}, {"authorId": "2256319", "name": "Wanxiang Che"}, {"authorId": "144889265", "name": "Ivan Titov"}, {"authorId": "40146204", "name": "Shay B. Cohen"}, {"authorId": "1622081834", "name": "Zhilin Lei"}, {"authorId": "40282288", "name": "Ting Liu"}]}, {"paperId": "7c235d03b34794ae9d421778a74252c559f7f419", "externalIds": {"ACL": "2021.findings-acl.208", "DBLP": "conf/acl/LiSSNIC21", "DOI": "10.18653/v1/2021.findings-acl.208", "CorpusId": 236477909}, "corpusId": 236477909, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7c235d03b34794ae9d421778a74252c559f7f419", "title": "P-Stance: A Large Dataset for Stance Detection in Political Domain", "abstract": "Stance detection determines whether the author of a text is in favor of, against or neutral to a speci\ufb01c target and provides valuable insights into important events such as presidential election. However, progress on stance detection has been hampered by the absence of large annotated datasets. In this paper, we present P-S TANCE , a large stance detection dataset in the political domain, which contains 21,574 labeled tweets. We provide a detailed description of the newly created dataset and develop deep learning models on it. Our best model achieves a macro-average F1-score of 80.53%, which we improve further by using semi-supervised learning. Moreover, our P-S TANCE dataset can facilitate research in the \ufb01elds of cross-domain stance detection such as cross-target stance detection where a classi\ufb01er is adapted from a different but related target. We publicly release our dataset and code. 1", "venue": "Findings", "year": 2021, "referenceCount": 44, "citationCount": 32, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.208.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2355-2365"}, "authors": [{"authorId": "50820271", "name": "Yingjie Li"}, {"authorId": "2008183567", "name": "Tiberiu Sosea"}, {"authorId": "9408003", "name": "Aditya Sawant"}, {"authorId": "2121326030", "name": "A. Nair"}, {"authorId": "1697366", "name": "D. Inkpen"}, {"authorId": "1690656", "name": "Cornelia Caragea"}]}, {"paperId": "a6842f8d70f599fdd74617632dc89de071f0a94b", "externalIds": {"DBLP": "conf/acl/ChenCW21", "ACL": "2021.findings-acl.209", "DOI": "10.18653/v1/2021.findings-acl.209", "CorpusId": 236478079}, "corpusId": 236478079, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/a6842f8d70f599fdd74617632dc89de071f0a94b", "title": "WIND: Weighting Instances Differentially for Model-Agnostic Domain Adaptation", "abstract": "Domain Adaptation is a fundamental problem in machine learning and natural language processing. In this paper, we study the domain adaptation problem from the perspective of instance weighting. Conventional instance weighting approaches cannot learn the weights which make the model generalize well in target domain. To tackle this problem, inspired by meta-learning, we formulate the domain adaptation problem as a bi-level optimization problem, and propose a novel differentiable modelagnostic instance weighting algorithm. Our proposed approach can automatically learn the instance weights instead of using manually designed weighting metrics. To reduce the computational complexity, we adopt the secondorder approximation technique during training. Experimental results1 on three different NLP tasks (Sentiment Classification, Neural Machine Translation and Relation Extraction) illustrate the efficacy of our proposed method.", "venue": "Findings", "year": 2021, "referenceCount": 50, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.209.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2366-2376"}, "authors": [{"authorId": "145671740", "name": "Xiang Chen"}, {"authorId": "2112823372", "name": "Yue Cao"}, {"authorId": "117908148", "name": "Xiaojun Wan"}]}, {"paperId": "1249cce1cf2dd9c0f1d2daa261c0164d89ffec90", "externalIds": {"DBLP": "conf/acl/DongZSKL21", "ArXiv": "2105.04271", "ACL": "2021.findings-acl.210", "DOI": "10.18653/v1/2021.findings-acl.210", "CorpusId": 234342151}, "corpusId": 234342151, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1249cce1cf2dd9c0f1d2daa261c0164d89ffec90", "title": "DocOIE: A Document-level Context-Aware Dataset for OpenIE", "abstract": "Open Information Extraction (OpenIE) aims to extract structured relational tuples (subject, relation, object) from sentences and plays critical roles for many downstream NLP applications. Existing solutions perform extraction at sentence level, without referring to any additional contextual information. In reality, however, a sentence typically exists as part of a document rather than standalone; we often need to access relevant contextual information around the sentence before we can accurately interpret it. As there is no document-level context-aware OpenIE dataset available, we manually annotate 800 sentences from 80 documents in two domains (Healthcare and Transportation) to form a DocOIE dataset for evaluation. In addition, we propose DocIE, a novel document-level context-aware OpenIE model. Our experimental results based on DocIE demonstrate that incorporating document-level context is helpful in improving OpenIE performance. Both DocOIE dataset and DocIE model are released for public.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.210.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-10", "journal": {"name": "ArXiv", "volume": "abs/2105.04271"}, "authors": [{"authorId": "2090408218", "name": "Kuicai Dong"}, {"authorId": "2109819833", "name": "Yilin Zhao"}, {"authorId": "1735962", "name": "Aixin Sun"}, {"authorId": "2109200350", "name": "Jung-jae Kim"}, {"authorId": "2127399967", "name": "Xiaoli Li"}]}, {"paperId": "b05576978f3ff0eb5e84c71b24eb7283c91c5544", "externalIds": {"ACL": "2021.findings-acl.211", "DBLP": "conf/acl/LaiNKN21", "DOI": "10.18653/v1/2021.findings-acl.211", "CorpusId": 236477882}, "corpusId": 236477882, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b05576978f3ff0eb5e84c71b24eb7283c91c5544", "title": "Event Extraction from Historical Texts: A New Dataset for Black Rebellions", "abstract": "Understanding historical events is necessary for the study of contemporary society, culture, and politics. In this work, we focus on the event extraction task (EE) to detect event trigger words and their arguments in a novel domain of historical texts. In particular, we introduce a new EE dataset for a corpus of nineteenth-century African American newspapers. Our goal is to study the discourse of slave and non-slave African diaspora rebellions published in the periodical press in this period. Our dataset features 5 entity types, 12 event types, and 6 argument roles that concern slavery and black movements between the eighteenth and nineteenth centuries. Historical newspapers present many challenges for existing EE systems, including the evolution of meanings of words and the extensive use of religious discourse in newspapers from this era. Our experiments with current state-ofthe-art EE systems and BERT models demonstrate their poor performance over historical texts and call for more robust research efforts in this area.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 18, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.211.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2390-2400"}, "authors": [{"authorId": "1405279380", "name": "Viet Dac Lai"}, {"authorId": "1789308", "name": "Minh Le Nguyen"}, {"authorId": "117566555", "name": "Heidi Kaufman"}, {"authorId": "1811211", "name": "Thien Huu Nguyen"}]}, {"paperId": "b223ddf2189174ac2c0837d316c9b5636b4291d8", "externalIds": {"DBLP": "journals/corr/abs-2105-12682", "ACL": "2021.findings-acl.212", "ArXiv": "2105.12682", "DOI": "10.18653/v1/2021.findings-acl.212", "CorpusId": 235195583}, "corpusId": 235195583, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b223ddf2189174ac2c0837d316c9b5636b4291d8", "title": "Zero-shot Medical Entity Retrieval without Annotation: Learning From Rich Knowledge Graph Semantics", "abstract": "Medical entity retrieval is an integral component for understanding and communicating information across various health systems. Current approaches tend to work well on specific medical domains but generalize poorly to unseen sub-specialties. This is of increasing concern under a public health crisis as new medical conditions and drug treatments come to light frequently. Zero-shot retrieval is challenging due to the high degree of ambiguity and variability in medical corpora, making it difficult to build an accurate similarity measure between mentions and concepts. Medical knowledge graphs (KG), however, contain rich semantics including large numbers of synonyms as well as its curated graphical structures. To take advantage of this valuable information, we propose a suite of learning tasks designed for training efficient zero-shot entity retrieval models. Without requiring any human annotation, our knowledge graph enriched architecture significantly outperforms common zero-shot benchmarks including BM25 and Clinical BERT with 7% to 30% higher recall across multiple major medical ontologies, such as UMLS, SNOMED, and ICD-10.", "venue": "Findings", "year": 2021, "referenceCount": 15, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.212.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-26", "journal": {"pages": "2401-2405"}, "authors": [{"authorId": "1491551635", "name": "Luyang Kong"}, {"authorId": "80135268", "name": "C. Winestock"}, {"authorId": "50339091", "name": "Parminder Bhatia"}]}, {"paperId": "5bc32a8f9a6245ae7f0761d539074dc7623ff61e", "externalIds": {"ACL": "2021.findings-acl.213", "DBLP": "conf/acl/WeldHLZWGLPH21", "ArXiv": "2106.06213", "DOI": "10.18653/v1/2021.findings-acl.213", "CorpusId": 235417027}, "corpusId": 235417027, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5bc32a8f9a6245ae7f0761d539074dc7623ff61e", "title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection", "abstract": "Traditional toxicity detection models have focused on the single utterance level without deeper understanding of context. We introduce CONDA, a new dataset for in-game toxic language detection enabling joint intent classification and slot filling analysis, which is the core task of Natural Language Understanding (NLU). The dataset consists of 45K utterances from 12K conversations from the chat logs of 1.9K completed Dota 2 matches. We propose a robust dual semantic-level toxicity framework, which handles utterance and token-level patterns, and rich contextual chatting history. Accompanying the dataset is a thorough in-game toxicity analysis, which provides comprehensive understanding of context at utterance, token, and dual levels. Inspired by NLU, we also apply its metrics to the toxicity detection tasks for assessing toxicity and game-specific aspects. We evaluate strong NLU models on CONDA, providing fine-grained results for different intent classes and slot classes. Furthermore, we examine the coverage of toxicity nature in our dataset by comparing it with other toxicity datasets.", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.213.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-11", "journal": {"name": "ArXiv", "volume": "abs/2106.06213"}, "authors": [{"authorId": "69047048", "name": "H. Weld"}, {"authorId": "123914136", "name": "Guanghao Huang"}, {"authorId": "2119693533", "name": "Jean Lee"}, {"authorId": "38144094", "name": "T. Zhang"}, {"authorId": "1990752926", "name": "Kunze Wang"}, {"authorId": "2157818919", "name": "Xinghong Guo"}, {"authorId": "32545338", "name": "Siqu Long"}, {"authorId": "144179461", "name": "Josiah Poon"}, {"authorId": "2046142", "name": "S. Han"}]}, {"paperId": "fc51941a6221441692ce93d5c709fae367a64fd9", "externalIds": {"ACL": "2021.findings-acl.214", "DBLP": "journals/corr/abs-2105-09509", "ArXiv": "2105.09509", "DOI": "10.18653/v1/2021.findings-acl.214", "CorpusId": 234790267}, "corpusId": 234790267, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/fc51941a6221441692ce93d5c709fae367a64fd9", "title": "Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection", "abstract": "Event detection (ED) aims at detecting event trigger words in sentences and classifying them into specific event types. In real-world applications, ED typically does not have sufficient labelled data, thus can be formulated as a few-shot learning problem. To tackle the issue of low sample diversity in few-shot ED, we propose a novel knowledge-based few-shot event detection method which uses a definition-based encoder to introduce external event knowledge as the knowledge prior of event types. Furthermore, as external knowledge typically provides limited and imperfect coverage of event types, we introduce an adaptive knowledge-enhanced Bayesian meta-learning method to dynamically adjust the knowledge prior of event types. Experiments show our method consistently and substantially outperforms a number of baselines by at least 15 absolute F1 points under the same few-shot settings.", "venue": "Findings", "year": 2021, "referenceCount": 75, "citationCount": 30, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.214.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-20", "journal": {"pages": "2417-2429"}, "authors": [{"authorId": "1488732029", "name": "Shirong Shen"}, {"authorId": "1514917592", "name": "Tongtong Wu"}, {"authorId": "1730054", "name": "G. Qi"}, {"authorId": "4495301", "name": "Yuan-Fang Li"}, {"authorId": "2561045", "name": "Gholamreza Haffari"}, {"authorId": "2065970582", "name": "Sheng Bi"}]}, {"paperId": "1513639b0079694855e87006545d55edd57e5e81", "externalIds": {"DBLP": "conf/acl/KongHTGH21", "ACL": "2021.findings-acl.215", "ArXiv": "2105.08625", "DOI": "10.18653/v1/2021.findings-acl.215", "CorpusId": 234763221}, "corpusId": 234763221, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1513639b0079694855e87006545d55edd57e5e81", "title": "Stylized Story Generation with Style-Guided Planning", "abstract": "Current storytelling systems focus more ongenerating stories with coherent plots regard-less of the narration style, which is impor-tant for controllable text generation. There-fore, we propose a new task, stylized story gen-eration, namely generating stories with speci-fied style given a leading context. To tacklethe problem, we propose a novel generationmodel that first plans the stylized keywordsand then generates the whole story with theguidance of the keywords. Besides, we pro-pose two automatic metrics to evaluate theconsistency between the generated story andthe specified style. Experiments demonstratesthat our model can controllably generateemo-tion-driven orevent-driven stories based onthe ROCStories dataset (Mostafazadeh et al.,2016). Our study presents insights for stylizedstory generation in further research.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.215.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-18", "journal": {"pages": "2430-2436"}, "authors": [{"authorId": "2093919461", "name": "Xiangzhe Kong"}, {"authorId": "2128013982", "name": "Jialiang Huang"}, {"authorId": "2093919341", "name": "Ziquan Tung"}, {"authorId": "145902734", "name": "Jian Guan"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "aec8c31d3b8cf37b106fb52d07b74fe8bb23c863", "externalIds": {"DBLP": "conf/acl/WangCWWHL21", "ACL": "2021.findings-acl.216", "DOI": "10.18653/v1/2021.findings-acl.216", "CorpusId": 236477602}, "corpusId": 236477602, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/aec8c31d3b8cf37b106fb52d07b74fe8bb23c863", "title": "Dynamic Connected Networks for Chinese Spelling Check", "abstract": "Chinese spelling check (CSC) is a task to detect and correct spelling errors in Chinese text. Most state-of-the-art works on the CSC task adopt a BERT-based non-autoregressive language model, which relies on the output independence assumption. The inappropriate independence assumption prevents BERT-based models from learning the dependencies among target tokens, resulting in an incoherent problem. To address the above issue, we propose a novel architecture named Dynamic Connected Networks (DCN), which generates the candidate Chinese characters via a Pinyin Enhanced Candidate Generator and then utilizes an attention-based network to model the dependencies between two adjacent Chinese characters. The experimental results show that our proposed method achieves a new state-of-the-art performance on three human-annotated datasets.", "venue": "Findings", "year": 2021, "referenceCount": 25, "citationCount": 33, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.216.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2437-2446"}, "authors": [{"authorId": "2118640235", "name": "Baoxin Wang"}, {"authorId": "2256319", "name": "Wanxiang Che"}, {"authorId": "2118208508", "name": "Dayong Wu"}, {"authorId": "2108620507", "name": "Shijin Wang"}, {"authorId": "40936264", "name": "Guoping Hu"}, {"authorId": "40282288", "name": "Ting Liu"}]}, {"paperId": "806dc5c64d3a65f89e0f26ff9f51bb029c6908b2", "externalIds": {"DBLP": "journals/corr/abs-2106-00950", "ACL": "2021.findings-acl.217", "ArXiv": "2106.00950", "DOI": "10.18653/v1/2021.findings-acl.217", "CorpusId": 235293847}, "corpusId": 235293847, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/806dc5c64d3a65f89e0f26ff9f51bb029c6908b2", "title": "A Multi-Level Attention Model for Evidence-Based Fact Checking", "abstract": "Evidence-based fact checking aims to verify the truthfulness of a claim against evidence extracted from textual sources. Learning a representation that effectively captures relations between a claim and evidence can be challenging. Recent state-of-the-art approaches have developed increasingly sophisticated models based on graph structures. We present a simple model that can be trained on sequence structures. Our model enables inter-sentence attentions at different levels and can benefit from joint training. Results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that our model outperforms the graph-based approaches and yields 1.09% and 1.42% improvements in label accuracy and FEVER score, respectively, over the best published model.", "venue": "Findings", "year": 2021, "referenceCount": 47, "citationCount": 14, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.217.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "2447-2460"}, "authors": [{"authorId": "2782010", "name": "Canasai Kruengkrai"}, {"authorId": "1716857", "name": "J. Yamagishi"}, {"authorId": "38435748", "name": "Xin Wang"}]}, {"paperId": "69cb92f055d32de55327da24ac38e12db810f280", "externalIds": {"DBLP": "journals/corr/abs-2106-04833", "ArXiv": "2106.04833", "ACL": "2021.findings-acl.218", "DOI": "10.18653/v1/2021.findings-acl.218", "CorpusId": 235377430}, "corpusId": 235377430, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/69cb92f055d32de55327da24ac38e12db810f280", "title": "RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer", "abstract": "End-to-end simultaneous speech translation (SST), which directly translates speech in one language into text in another language in real-time, is useful in many scenarios but has not been fully investigated. In this work, we propose RealTranS, an end-to-end model for SST. To bridge the modality gap between speech and text, RealTranS gradually downsamples the input speech with interleaved convolution and unidirectional Transformer layers for acoustic modeling, and then maps speech features into text space with a weighted-shrinking operation and a semantic encoder. Besides, to improve the model performance in simultaneous scenarios, we propose a blank penalty to enhance the shrinking quality and a Wait-K-Stride-N strategy to allow local reranking during decoding. Experiments on public and widely-used datasets show that RealTranS with the Wait-K-Stride-N strategy outperforms prior end-to-end models as well as cascaded models in diverse latency settings.", "venue": "Findings", "year": 2021, "referenceCount": 51, "citationCount": 22, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.218.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-09", "journal": {"name": "ArXiv", "volume": "abs/2106.04833"}, "authors": [{"authorId": "46180553", "name": "Xingshan Zeng"}, {"authorId": "1989344", "name": "Liangyou Li"}, {"authorId": "1688015", "name": "Qun Liu"}]}, {"paperId": "27794bca3b7327aff29e2593e8b989b6a5af678b", "externalIds": {"DBLP": "conf/acl/ShenLLYH21", "ArXiv": "2106.00139", "ACL": "2021.findings-acl.219", "DOI": "10.18653/v1/2021.findings-acl.219", "CorpusId": 235265630}, "corpusId": 235265630, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/27794bca3b7327aff29e2593e8b989b6a5af678b", "title": "Training ELECTRA Augmented with Multi-word Selection", "abstract": "Pre-trained text encoders such as BERT and its variants have recently achieved state-of-the-art performances on many NLP tasks. While being effective, these pre-training methods typically demand massive computation resources. To accelerate pre-training, ELECTRA trains a discriminator that predicts whether each input token is replaced by a generator. However, this new task, as a binary classification, is less semantically informative. In this study, we present a new text encoder pre-training method that improves ELECTRA based on multi-task learning. Specifically, we train the discriminator to simultaneously detect replaced tokens and select original tokens from candidate sets. We further develop two techniques to effectively combine all pre-training tasks: (1) using attention-based networks for task-specific heads, and (2) sharing bottom layers of the generator and the discriminator. Extensive experiments on GLUE and SQuAD datasets demonstrate both the effectiveness and the efficiency of our proposed method.", "venue": "Findings", "year": 2021, "referenceCount": 39, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.219.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-31", "journal": {"pages": "2475-2486"}, "authors": [{"authorId": "3363642", "name": "Jiaming Shen"}, {"authorId": "2746747", "name": "Jialu Liu"}, {"authorId": "2115346248", "name": "Tianqi Liu"}, {"authorId": "82737548", "name": "Cong Yu"}, {"authorId": "153034701", "name": "Jiawei Han"}]}, {"paperId": "3f863cf9565de2c263d4256e6733a01d04f5f73f", "externalIds": {"DOI": "10.18653/v1/2021.findings-acl.220", "CorpusId": 242833882}, "corpusId": 242833882, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/3f863cf9565de2c263d4256e6733a01d04f5f73f", "title": "REAM\u266f: An Enhancement Approach to Reference-based Evaluation Metrics for Open-domain Dialog Generation: An Enhancement Approach to Reference-based Evaluation Metrics for Open-domain Dialog Generation", "abstract": null, "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021", "year": 2021, "referenceCount": 0, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "journal": {"name": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"}, "authors": [{"authorId": "2111015656", "name": "Jun Gao"}, {"authorId": "1844673750", "name": "Wei Bi"}, {"authorId": "1753529", "name": "Ruifeng Xu"}, {"authorId": "2072684668", "name": "Shuming Shi"}]}, {"paperId": "cec94332ff4618b28d0adb6ecddd2ef7a87d3188", "externalIds": {"ACL": "2021.findings-acl.221", "DBLP": "conf/acl/ChenTSW21", "DOI": "10.18653/v1/2021.findings-acl.221", "CorpusId": 236478166}, "corpusId": 236478166, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/cec94332ff4618b28d0adb6ecddd2ef7a87d3188", "title": "Relation Extraction with Type-aware Map Memories of Word Dependencies", "abstract": "Relation extraction is an important task in information extraction and retrieval that aims to extract relations among the given entities from running texts. To achieve a good performance for this task, previous studies have shown that a good modeling of the contextual information is required, where the dependency tree of the input sentence can be a bene\ufb01cial source among different types of contextual information. However, most of these studies focus on the dependency connections between words with limited attention paid to exploiting dependency types. In addition, they often treat different dependency connections equally in modeling so that suffer from the noise (inaccurate dependency parses) in the auto-generated dependency tree. In this paper, we propose a neural approach for relation extraction, with type-aware map memories (TaMM) for encoding dependency types obtained from an off-the-shelf dependency parser for the input sentence. Speci\ufb01cally, for each word in an entity, TaMM maps all associated words along with the dependencies among them to memory slots and then assigns a weight to each slot according to its contribution to relation extraction. Our approach not only leverages dependency connections and types between words, but also distinguishes reliable dependency information from noisy ones and appropriately model them. The effectiveness of our approach is demonstrated by the experiments on two English benchmark datasets, where our approach achieves state-of-the-art performance on both datasets. 1", "venue": "Findings", "year": 2021, "referenceCount": 48, "citationCount": 24, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.221.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2501-2512"}, "authors": [{"authorId": "2116379168", "name": "Guimin Chen"}, {"authorId": "2152947211", "name": "Yuanhe Tian"}, {"authorId": "1922182598", "name": "Yan Song"}, {"authorId": "2101317304", "name": "Xiang Wan"}]}, {"paperId": "70af4173983eccc0beac29ed4602bf9db5568b92", "externalIds": {"ACL": "2021.findings-acl.222", "DBLP": "journals/corr/abs-2006-16779", "ArXiv": "2006.16779", "MAG": "3040352674", "DOI": "10.18653/v1/2021.findings-acl.222", "CorpusId": 220265679}, "corpusId": 220265679, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/70af4173983eccc0beac29ed4602bf9db5568b92", "title": "PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning", "abstract": "To build a high-quality open-domain chatbot, we introduce the effective training process of PLATO-2 via curriculum learning. There are two stages involved in the learning process. In the first stage, a coarse-grained generation model is trained to learn response generation under the simplified framework of one-to-one mapping. In the second stage, a fine-grained generation model and an evaluation model are further trained to learn diverse response generation and response coherence estimation, respectively. PLATO-2 was trained on both Chinese and English data, whose effectiveness and superiority are verified through comprehensive evaluations, achieving new state-of-the-art results.", "venue": "Findings", "year": 2020, "referenceCount": 59, "citationCount": 110, "influentialCitationCount": 17, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.222.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-06-30", "journal": {"pages": "2513-2525"}, "authors": [{"authorId": "2026806395", "name": "Siqi Bao"}, {"authorId": "46350360", "name": "H. He"}, {"authorId": "2145903238", "name": "Fan Wang"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "144270731", "name": "Haifeng Wang"}, {"authorId": "2139636204", "name": "Wenquan Wu"}, {"authorId": "2149504787", "name": "Zhen Guo"}, {"authorId": "2109253056", "name": "Zhibin Liu"}, {"authorId": "2152775373", "name": "Xinchao Xu"}]}, {"paperId": "b99c61f6957c1b04ec1376b74f82dd1e83559695", "externalIds": {"DBLP": "conf/acl/KeJRCWSZH21", "ACL": "2021.findings-acl.223", "ArXiv": "2106.10502", "DOI": "10.18653/v1/2021.findings-acl.223", "CorpusId": 235490087}, "corpusId": 235490087, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b99c61f6957c1b04ec1376b74f82dd1e83559695", "title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs", "abstract": "Existing pre-trained models for knowledge-graph-to-text (KG-to-text) generation simply fine-tune text-to-text pre-trained models such as BART or T5 on KG-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called JointGT. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the embedding space via Optimal Transport. Experiments show that JointGT obtains new state-of-the-art performance on various KG-to-text datasets.", "venue": "Findings", "year": 2021, "referenceCount": 53, "citationCount": 54, "influentialCitationCount": 12, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.223.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-19", "journal": {"pages": "2526-2538"}, "authors": [{"authorId": "1886879", "name": "Pei Ke"}, {"authorId": "51111817", "name": "Haozhe Ji"}, {"authorId": "2068333149", "name": "Yuanyuan Ran"}, {"authorId": "2114347428", "name": "Xin Cui"}, {"authorId": "2142450232", "name": "Liwei Wang"}, {"authorId": "50258954", "name": "Linfeng Song"}, {"authorId": "145213540", "name": "Xiaoyan Zhu"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "2e9990d63103fce47e2017c74201daf3d7b59073", "externalIds": {"DBLP": "conf/acl/HuangWX21", "ACL": "2021.findings-acl.224", "DOI": "10.18653/v1/2021.findings-acl.224", "CorpusId": 236478345}, "corpusId": 236478345, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/2e9990d63103fce47e2017c74201daf3d7b59073", "title": "AdaST: Dynamically Adapting Encoder States in the Decoder for End-to-End Speech-to-Text Translation", "abstract": "In end-to-end speech translation, acoustic representations learned by the encoder are usually fixed and static, from the perspective of the decoder, which is not desirable for dealing with the cross-modal and cross-lingual challenge in speech translation. In this paper, we show the benefits of varying acoustic states according to decoder hidden states and propose an adaptive speech-to-text translation model that is able to dynamically adapt acoustic states in the decoder. We concatenate the acoustic state and target word embedding sequence and feed the concatenated sequence into subsequent blocks in the decoder. In order to model the deep interaction between acoustic states and target hidden states, a speech-text mixed attention sublayer is introduced to replace the conventional cross-attention network. Experiment results on two widely-used datasets show that the proposed method significantly outperforms stateof-the-art neural speech translation models.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2539-2545"}, "authors": [{"authorId": "1588102980", "name": "Wuwei Huang"}, {"authorId": "2118963912", "name": "Dexin Wang"}, {"authorId": "2694222", "name": "Deyi Xiong"}]}, {"paperId": "865bfd8034785a084f0c7d651119b8504e939535", "externalIds": {"ACL": "2021.findings-acl.225", "ArXiv": "2106.12806", "DBLP": "conf/acl/ChandrahasT21", "DOI": "10.18653/v1/2021.findings-acl.225", "CorpusId": 235624320}, "corpusId": 235624320, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/865bfd8034785a084f0c7d651119b8504e939535", "title": "OKGIT: Open Knowledge Graph Link Prediction with Implicit Types", "abstract": "Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation phrase, tail noun phrase) triples such as (tesla, return to, new york) extracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap for a domain, they are very sparse and far from being directly usable in an end task. Therefore, the task of predicting new facts, i.e., link prediction, becomes an important step while using these graphs in downstream tasks such as text comprehension, question answering, and web search query recommendation. Learning embeddings for OpenKGs is one approach for link prediction that has received some attention lately. However, on careful examination, we found that current OpenKG link prediction algorithms often predict noun phrases (NPs) with incompatible types for given noun and relation phrases. We address this problem in this work and propose OKGIT that improves OpenKG link prediction using novel type compatibility score and type regularization. With extensive experiments on multiple datasets, we show that the proposed method achieves state-of-the-art performance while producing type compatible NPs in the link prediction task.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 11, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.225.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-24", "journal": {"name": "ArXiv", "volume": "abs/2106.12806"}, "authors": [{"authorId": "14449047", "name": "Chandrahas"}, {"authorId": "2406435", "name": "Partha P. Talukdar"}]}, {"paperId": "a852e1d3adae5179ca75abe1b93b4dcb410cc082", "externalIds": {"DBLP": "conf/acl/WuZZWX21", "ACL": "2021.findings-acl.226", "DOI": "10.18653/v1/2021.findings-acl.226", "CorpusId": 236478188}, "corpusId": 236478188, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/a852e1d3adae5179ca75abe1b93b4dcb410cc082", "title": "Multimodal Fusion with Co-Attention Networks for Fake News Detection", "abstract": "Fake news with textual and visual contents has a better story-telling ability than text-only contents, and can be spread quickly with social media. People can be easily deceived by such fake news, and traditional expert identi\ufb01cation is labor-intensive. Therefore, automatic detection of multimodal fake news has become a new hot-spot issue. A shortcoming of existing approaches is their inability to fuse multi-modality features effectively. They simply concatenate unimodal features without considering inter-modality relations. Inspired by the way people read news with image and text, we propose a novel Multimodal Co-Attention Networks (MCAN) to better fuse textual and visual features for fake news detection. Ex-tensive experiments conducted on two real-world datasets demonstrate that MCAN can learn inter-dependencies among multimodal features and outperforms state-of-the-art methods.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 63, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.226.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2560-2569"}, "authors": [{"authorId": "2149121852", "name": "Yang Wu"}, {"authorId": "2105515515", "name": "Pengwei Zhan"}, {"authorId": "2129519432", "name": "Yunjian Zhang"}, {"authorId": "2109120341", "name": "Liming Wang"}, {"authorId": "40285789", "name": "Zhen Xu"}]}, {"paperId": "4db26cbd36396eee1ac8725bc2d3bfcd84382f7e", "externalIds": {"DBLP": "conf/acl/ChenZC21", "ACL": "2021.findings-acl.227", "DOI": "10.18653/v1/2021.findings-acl.227", "CorpusId": 236478268}, "corpusId": 236478268, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4db26cbd36396eee1ac8725bc2d3bfcd84382f7e", "title": "Joint Multi-Decoder Framework with Hierarchical Pointer Network for Frame Semantic Parsing", "abstract": "Current researches on frame semantic parsing include three subtasks, namely frame identi\ufb01cation, argument identi\ufb01cation and role classi\ufb01cation. Most of previous systems process these subtasks independently and ignore their interactions. We introduce a novel architec-ture based on multi-decoder strategy to handle these subtasks together. The multi-decoder strategy strengthens the interactions. Moreover, we design a hierarchical pointer network for argument identi\ufb01cation which reduces the computational complexity. To our best knowledge, it\u2019s the \ufb01rst practice to introduce the pointer network into frame semantic parsing. The experiments show improvement over state of the art models on FrameNet dataset.", "venue": "Findings", "year": 2021, "referenceCount": 20, "citationCount": 7, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2570-2578"}, "authors": [{"authorId": "2121348550", "name": "Xudong Chen"}, {"authorId": "2113919886", "name": "Ce Zheng"}, {"authorId": "39488576", "name": "Baobao Chang"}]}, {"paperId": "ed6b28d509757e2c63b77830b1b8c315490868a2", "externalIds": {"ArXiv": "2012.03536", "DBLP": "journals/corr/abs-2012-03536", "ACL": "2021.findings-acl.228", "MAG": "3111530193", "DOI": "10.18653/v1/2021.findings-acl.228", "CorpusId": 227342488}, "corpusId": 227342488, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ed6b28d509757e2c63b77830b1b8c315490868a2", "title": "H-FND: Hierarchical False-Negative Denoising for Distant Supervision Relation Extraction", "abstract": "Although distant supervision automatically generates training data for relation extraction, it also introduces false-positive (FP) and false-negative (FN) training instances to the generated datasets. Whereas both types of errors degrade the final model performance, previous work on distant supervision denoising focuses more on suppressing FP noise and less on resolving the FN problem. We here propose H-FND, a hierarchical false-negative denoising framework for robust distant supervision relation extraction, as an FN denoising solution. H-FND uses a hierarchical policy which first determines whether non-relation (NA) instances should be kept, discarded, or revised during the training process. For those learning instances which are to be revised, the policy further reassigns them appropriate relations, making them better training inputs. Experiments on SemEval-2010 and TACRED were conducted with controlled FN ratios that randomly turn the relations of training and validation instances into negatives to generate FN instances. In this setting, H-FND can revise FN instances correctly and maintains high F1 scores even when 50% of the instances have been turned into negatives. Experiment on NYT10 is further conducted to shows that H-FND is applicable in a realistic setting.", "venue": "Findings", "year": 2020, "referenceCount": 34, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.228.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-07", "journal": {"pages": "2579-2593"}, "authors": [{"authorId": "2108332060", "name": "Jhih-Wei Chen"}, {"authorId": "41020222", "name": "Tsu-Jui Fu"}, {"authorId": "2032215919", "name": "Chen-Kang Lee"}, {"authorId": "1749660", "name": "Wei-Yun Ma"}]}, {"paperId": "93316866fedaae835d5fe7f633f887e3571fb8b9", "externalIds": {"ACL": "2021.findings-acl.229", "DBLP": "journals/corr/abs-2106-09889", "ArXiv": "2106.09889", "DOI": "10.18653/v1/2021.findings-acl.229", "CorpusId": 235485398}, "corpusId": 235485398, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/93316866fedaae835d5fe7f633f887e3571fb8b9", "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks", "abstract": "In this paper, we present GEM as a General Evaluation benchmark for Multimodal tasks. Different from existing datasets such as GLUE, SuperGLUE, XGLUE and XTREME that mainly focus on natural language tasks, GEM is a large-scale vision-language benchmark, which consists of GEM-I for image-language tasks and GEM-V for video-language tasks. Comparing with existing multimodal datasets such as MSCOCO and Flicker30K for image-language tasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the largest vision-language dataset covering image-language tasks and video-language tasks at the same time, but also labeled in multiple languages. We also provide two baseline models for this benchmark. We will release the dataset, code and baseline models, aiming to advance the development of multilingual multimodal research.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.229.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-18", "journal": {"pages": "2594-2603"}, "authors": [{"authorId": "2069855086", "name": "Lin Su"}, {"authorId": "46429989", "name": "Nan Duan"}, {"authorId": "144530394", "name": "Edward Cui"}, {"authorId": "144906579", "name": "Lei Ji"}, {"authorId": "2151101534", "name": "Chenfei Wu"}, {"authorId": "35347136", "name": "Huaishao Luo"}, {"authorId": "2108078299", "name": "Yongfei Liu"}, {"authorId": "2119105813", "name": "Ming Zhong"}, {"authorId": "1490606819", "name": "Taroon Bharti"}, {"authorId": "35378689", "name": "Arun Sacheti"}]}, {"paperId": "dc6b7903cfbfd7e53c1bc2a6df9a9b42d7cbc558", "externalIds": {"ACL": "2021.findings-acl.230", "DBLP": "conf/acl/XieHDP21", "DOI": "10.18653/v1/2021.findings-acl.230", "CorpusId": 236477981}, "corpusId": 236477981, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/dc6b7903cfbfd7e53c1bc2a6df9a9b42d7cbc558", "title": "Graph Relational Topic Model with Higher-order Graph Attention Auto-encoders", "abstract": "Learning low-dimensional representations of networked documents is a crucial task for documents linked in network structures. Relational Topic Models (RTMs) have shown their strengths in modeling both document contents and relations to discover the latent topic semantic representations. However, higher-order correlation structure information among documents is largely ignored in these methods. Therefore, we propose a novel graph relational topic model (GRTM) for document network, to fully explore and mix neighborhood information of documents on each order, based on the Higher-order Graph Attention Network (HGAT) with the log-normal prior in the graph attention. The proposed method can address the aforementioned issue via the information propagation among document-document based on the HGAT probabilistic encoder, to learn efficient networked document representations in the latent topic space, which can fully reflect document contents, along with document connections. Experiments on several real-world document network datasets show that, through fully exploring information in documents and document networks, our model achieves better performance on unsupervised representation learning and outperforms existing competitive methods in various downstream tasks.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.230.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2604-2613"}, "authors": [{"authorId": "145229872", "name": "Qianqian Xie"}, {"authorId": "2077047231", "name": "Jimin Huang"}, {"authorId": "2407633", "name": "Pan Du"}, {"authorId": "47278503", "name": "Min Peng"}]}, {"paperId": "d8ba4028a807db2b5c5e11f91ab9084bb5feb9ba", "externalIds": {"DBLP": "conf/acl/YellinA21", "ACL": "2021.findings-acl.231", "DOI": "10.18653/v1/2021.findings-acl.231", "CorpusId": 236478110}, "corpusId": 236478110, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d8ba4028a807db2b5c5e11f91ab9084bb5feb9ba", "title": "Paths to Relation Extraction through Semantic Structure", "abstract": "Syntactic and semantic structure directly reflect relations expressed by the text at hand and are thus very useful for the relation extraction (RE) task. Their symbolic nature allows increased interpretability for end-users and developers, which is particularly appealing in RE. Although they have been somewhat overshadowed recently by the use of end-to-end neural network models and contextualized word embeddings, we show that they may be leveraged as input for neural networks to positive effect. We present two methods for integrating broad-coverage semantic structure (specifically, UCCA) into supervised neural RE models, demonstrating benefits over the use of exclusively syntactic integrations. The first method involves reduction of UCCA into a bilexical structure, while the second leverages a novel technique for encoding semantic DAG structures. Our approach is general and can be used for integrating a wide range of graphbased semantic structures.1", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2614-2626"}, "authors": [{"authorId": "119216375", "name": "Jonathan Yellin"}, {"authorId": "2769805", "name": "Omri Abend"}]}, {"paperId": "5041a39488c7f69305171737b347460691713044", "externalIds": {"ACL": "2021.findings-acl.232", "DBLP": "conf/acl/PangXYHF21", "DOI": "10.18653/v1/2021.findings-acl.232", "CorpusId": 236478060}, "corpusId": 236478060, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5041a39488c7f69305171737b347460691713044", "title": "Dynamic and Multi-Channel Graph Convolutional Networks for Aspect-Based Sentiment Analysis", "abstract": null, "venue": "Findings", "year": 2021, "referenceCount": 0, "citationCount": 23, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.232.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2627-2636"}, "authors": [{"authorId": "2121371126", "name": "Shiguan Pang"}, {"authorId": "2114922740", "name": "Yun Xue"}, {"authorId": "79403961", "name": "Zehao Yan"}, {"authorId": "2128984464", "name": "Weihao Huang"}, {"authorId": "21148418", "name": "Jinhui Feng"}]}, {"paperId": "dfa3638197c63066006412c1e03753241e0b68f0", "externalIds": {"ACL": "2021.findings-acl.233", "DBLP": "conf/acl/Stajner21", "MAG": "3173957073", "DOI": "10.18653/v1/2021.findings-acl.233", "CorpusId": 236478182}, "corpusId": 236478182, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/dfa3638197c63066006412c1e03753241e0b68f0", "title": "Automatic Text Simplification for Social Good: Progress and Challenges", "abstract": "Since the late 1990s, automatic text simplification (ATS) was promoted as a natural language processing (NLP) task with great potential to make texts more accessible to people with various reading or cognitive disabilities, and enable their better social inclusion. Large multidisciplinary projects showed promising steps in that direction. Since 2010, the field started attracting more attention but at the cost of major shifts in system architecture, target audience, and evaluation strategies. Somewhere along the way, the focus has shifted from ATS for social good towards building complex endto-end neural architectures that are not aimed at any particular target population. This study presents the trajectory of ATS for social good, the main issues in current ATS trends, and the ways forward that could bring the field back to its initial goals.", "venue": "Findings", "year": 2021, "referenceCount": 116, "citationCount": 31, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.233.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-01", "journal": {"pages": "2637-2652"}, "authors": [{"authorId": "1952894", "name": "Sanja \u0160tajner"}]}, {"paperId": "b1f3d7e03d681470d55a3e9f367f9cea46fe9528", "externalIds": {"ArXiv": "2106.09900", "DBLP": "journals/corr/abs-2106-09900", "ACL": "2021.findings-acl.234", "DOI": "10.18653/v1/2021.findings-acl.234", "CorpusId": 235485452}, "corpusId": 235485452, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b1f3d7e03d681470d55a3e9f367f9cea46fe9528", "title": "A Neural Edge-Editing Approach for Document-Level Relation Graph Extraction", "abstract": "In this paper, we propose a novel edge-editing approach to extract relation information from a document. We treat the relations in a document as a relation graph among entities in this approach. The relation graph is iteratively constructed by editing edges of an initial graph, which might be a graph extracted by another system or an empty graph. The way to edit edges is to classify them in a close-first manner using the document and temporally-constructed graph information; each edge is represented with a document context information by a pretrained transformer model and a graph context information by a graph convolutional neural network model. We evaluate our approach on the task to extract material synthesis procedures from materials science texts. The experimental results show the effectiveness of our approach in editing the graphs initialized by our in-house rule-based system and empty graphs.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.234.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-18", "journal": {"name": "ArXiv", "volume": "abs/2106.09900"}, "authors": [{"authorId": "2066427783", "name": "Kohei Makino"}, {"authorId": "2060015680", "name": "Makoto Miwa"}, {"authorId": "2115075487", "name": "Yutaka Sasaki"}]}, {"paperId": "9cc73eb0431b13af7d031c9eea14d8c626b59e59", "externalIds": {"ACL": "2021.findings-acl.235", "ArXiv": "2106.00420", "DBLP": "journals/corr/abs-2106-00420", "DOI": "10.18653/v1/2021.findings-acl.235", "CorpusId": 235266216}, "corpusId": 235266216, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9cc73eb0431b13af7d031c9eea14d8c626b59e59", "title": "Dialogue-oriented Pre-training", "abstract": "Pre-trained language models (PrLM) has been shown powerful in enhancing a broad range of downstream tasks including various dialogue related ones. However, PrLMs are usually trained on general plain text with common language model (LM) training objectives, which cannot sufficiently capture dialogue exclusive features due to the limitation of such training setting, so that there is an immediate need to fill the gap between a specific dialogue task and the LM task. As it is unlikely to collect huge dialogue data for dialogue-oriented pre-training, in this paper, we propose three strategies to simulate the conversation features on general plain text. Our proposed method differs from existing post-training methods that it may yield a general-purpose PrLM and does not individualize to any detailed task while keeping the capability of learning dialogue related features including speaker awareness, continuity and consistency. The resulted Dialog-PrLM is fine-tuned on three public multi-turn dialogue datasets and helps achieve significant and consistent improvement over the plain PrLMs.", "venue": "Findings", "year": 2021, "referenceCount": 47, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.235.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-01", "journal": {"name": "ArXiv", "volume": "abs/2106.00420"}, "authors": [{"authorId": "2110290078", "name": "Yi Xu"}, {"authorId": "153716230", "name": "Hai Zhao"}]}, {"paperId": "24f2e02437013fde3477621acfec48c53391853e", "externalIds": {"DBLP": "conf/acl/BianHHZZ21", "ACL": "2021.findings-acl.236", "DOI": "10.18653/v1/2021.findings-acl.236", "CorpusId": 236477887}, "corpusId": 236477887, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/24f2e02437013fde3477621acfec48c53391853e", "title": "GrantRel: Grant Information Extraction via Joint Entity and Relation Extraction", "abstract": "August 1\u20136, 2021. \u00a92021 Association for Computational Linguistics 2674 GrantRel: Grant Information Extraction via Joint Entity and Relation Extraction Junyi Bian 1 8 , Li Huang 1 8 , Xiaodi Huang 2 , Hong Zhou 3, Shanfeng Zhu 4 5 6 7 8 1 School of Computer Science, Fudan University, Shanghai 200433, China 2 School of Computing and Mathematics, Charles Sturt University Albury, NSW 2640, Australia 3 Atypon Systems, LLC, UK 4 Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, China 5 Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence (Fudan University), Ministry of Education, Shanghai 200433, China 6 MOE Frontiers Center for Brain Science, Fudan University, Shanghai 200433, China 7 Zhangjiang Fudan International Innovation Center, Shanghai 200433, China 8 Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai 200433, China {zhusf, 20110240003}@fudan.edu.cn, hzhou@atypon.com", "venue": "Findings", "year": 2021, "referenceCount": 25, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Education", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2674-2685"}, "authors": [{"authorId": "14889056", "name": "Junyi Bian"}, {"authorId": "2143700676", "name": "Li Huang"}, {"authorId": "2145264230", "name": "Xiaodi Huang"}, {"authorId": "2157473395", "name": "Hong Zhou"}, {"authorId": "7472263", "name": "Shanfeng Zhu"}]}, {"paperId": "5eec9616f58ac64f8510bff2e08ef4299b6c8df9", "externalIds": {"ACL": "2021.findings-acl.237", "DBLP": "conf/acl/ParkZ21", "DOI": "10.18653/v1/2021.findings-acl.237", "CorpusId": 236477342}, "corpusId": 236477342, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5eec9616f58ac64f8510bff2e08ef4299b6c8df9", "title": "Enhancing Language Generation with Effective Checkpoints of Pre-trained Language Model", "abstract": "This work empirically explores effective ex-ploiting of intermediate output from pre-trained language models (PrLMs) for language generation tasks. For this purpose, we propose an improved method to integrate public checkpoints of PrLMs for the most convenience and perform extensive experiments on 6 different kinds of PrLMs, including BERT, ELECTRA, GPT2, Multi-lingual BERT, and XLM RoBERTa. Evaluation with automatic met-rics shows that our approach signi\ufb01cantly im-proves the generation quality on the generation tasks, up to 1.8 BLEU points for neural machine translation (Korean-to-English, Korean-to-Chinese) and 1.8 ROUGE points improvements for text summarization.", "venue": "Findings", "year": 2021, "referenceCount": 25, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2686-2694"}, "authors": [{"authorId": "69959948", "name": "Jeonghyeok Park"}, {"authorId": "153716230", "name": "Hai Zhao"}]}, {"paperId": "fc1392ab293d3371e0166a4c42157a5e1431f1ae", "externalIds": {"DBLP": "conf/acl/YuALYSLH21", "ACL": "2021.findings-acl.238", "DOI": "10.18653/v1/2021.findings-acl.238", "CorpusId": 236477419}, "corpusId": 236477419, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/fc1392ab293d3371e0166a4c42157a5e1431f1ae", "title": "Making Flexible Use of Subtasks: A Multiplex Interaction Network for Unified Aspect-based Sentiment Analysis", "abstract": "Aspect Term Extraction (ATE), Opinion Term Extraction (OTE) and Aspect Sentiment Classification (ASC) are the essential building blocks of Aspect-based Sentiment Analysis (ABSA). They are typically treated as separate tasks and are individually studied by previous work. Recent studies intend to incorporate multiple sub-tasks into a unified framework, but suffer from the following major disadvantages: (1) ABSA models are extremely fragile when some sub-tasks are absent; (2) the interactive relations among subtasks are not adequate. To this end, we propose a multi-task learning approach named MIN (Multiplex Interaction Network) to make flexible use of sub-tasks for a unified ABSA. We divide the sub-tasks of ABSA into extractive sub-tasks and classification sub-tasks, and optimize these sub-tasks in a unified manner with multiplex interaction mechanisms. Specifically, we devise a pairwise attention to exploit bidirectional interactions between any arbitrary pair of extractive sub-tasks and a consistency-weighting to perform unidirectional interaction from an extractive sub-task to a classification sub-task. Since the proposed interaction mechanisms are task-agnostic, our model can also work well when some specific sub-tasks are absent. Extensive experiments on two widely used benchmarks with different numbers of sub-tasks demonstrate the superiority of the proposed model.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 7, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.238.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2695-2705"}, "authors": [{"authorId": "2036123548", "name": "Guoxin Yu"}, {"authorId": "48480626", "name": "Xiang Ao"}, {"authorId": "2114164798", "name": "Ling Luo"}, {"authorId": "2110950699", "name": "Min Yang"}, {"authorId": "2109329406", "name": "Xiaofei Sun"}, {"authorId": "49298465", "name": "Jiwei Li"}, {"authorId": "2158548", "name": "Qing He"}]}, {"paperId": "0c6b06a0498ad8156af499a3b9b2b612951b3504", "externalIds": {"DBLP": "journals/corr/abs-2105-03953", "ACL": "2021.findings-acl.239", "ArXiv": "2105.03953", "DOI": "10.18653/v1/2021.findings-acl.239", "CorpusId": 234336760}, "corpusId": 234336760, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0c6b06a0498ad8156af499a3b9b2b612951b3504", "title": "Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation", "abstract": "The data scarcity in low-resource languages has become a bottleneck to building robust neural machine translation systems. Fine-tuning a multilingual pre-trained model (e.g., mBART (Liu et al., 2020)) on the translation task is a good approach for low-resource languages; however, its performance will be greatly limited when there are unseen languages in the translation pairs. In this paper, we present a continual pre-training (CPT) framework on mBART to effectively adapt it to unseen languages. We first construct noisy mixed-language text from the monolingual corpus of the target language in the translation pair to cover both the source and target languages, and then, we continue pre-training mBART to reconstruct the original monolingual text. Results show that our method can consistently improve the fine-tuning performance upon the mBART baseline, as well as other strong baselines, across all tested low-resource translation pairs containing unseen languages. Furthermore, our approach also boosts the performance on translation pairs where both languages are seen in the original mBART's pre-training. The code is available at https://github.com/zliucr/cpt-nmt.", "venue": "Findings", "year": 2021, "referenceCount": 62, "citationCount": 22, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.239.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-09", "journal": {"pages": "2706-2718"}, "authors": [{"authorId": "2117941142", "name": "Zihan Liu"}, {"authorId": "9162688", "name": "Genta Indra Winata"}, {"authorId": "40539650", "name": "Pascale Fung"}]}, {"paperId": "19bd1dd8e810051a51e24d5befe501719bb00fca", "externalIds": {"ACL": "2021.findings-acl.240", "DBLP": "conf/acl/ShinN21", "DOI": "10.18653/v1/2021.findings-acl.240", "CorpusId": 236477993}, "corpusId": 236477993, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/19bd1dd8e810051a51e24d5befe501719bb00fca", "title": "Transformer-Exclusive Cross-Modal Representation for Vision and Language", "abstract": "Ever since the advent of deep learning, crossmodal representation learning has been dominated by the approaches involving convolutional neural networks for visual representation and recurrent neural networks for language representation. Transformer architecture, however, has rapidly taken over the recurrent neural networks in natural language processing tasks, and it has also been shown that vision tasks can be handled with transformer architecture, with compatible performance to convolutional neural networks. Such results naturally lead to speculation upon the possibility of tackling cross-modal representation for vision and language exclusively with transformer. This paper examines transformerexclusive cross-modal representation to explore such possibility, demonstrating its potentials as well as discussing its current limitations and its prospects.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2719-2725"}, "authors": [{"authorId": "145568592", "name": "Andrew Shin"}, {"authorId": "3288474", "name": "T. Narihira"}]}, {"paperId": "3584fcc11db19432812ce6c6704cf5b089c5343f", "externalIds": {"DBLP": "conf/acl/ZhangLL21a", "ACL": "2021.findings-acl.241", "DOI": "10.18653/v1/2021.findings-acl.241", "CorpusId": 236477646}, "corpusId": 236477646, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/3584fcc11db19432812ce6c6704cf5b089c5343f", "title": "Two Parents, One Child: Dual Transfer for Low-Resource Neural Machine Translation", "abstract": "Neural machine translation suffers when parallel data for training is scarce. Previous works have explored transfer learning to assist training in low-resource scenarios. However, they transfer either from high-resource parallel data, or from monolingual data. In this work, we propose a framework to transfer multiple sources of auxiliary data, including both high-resource parallel data and monolingual data of involved languages. Knowledge in those sources is respectively encoded in a high-resource translation model and pretrained language models, and dually transferred to the low-resource translation model by our approach. Extensive experiments show that our approach yields consistent improvements over strong competitors for multiple translation directions. Furthermore, our approach still exhibits benefit on top of back-translation, making it a useful addition to practitioners\u2019 toolbox.", "venue": "Findings", "year": 2021, "referenceCount": 39, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2726-2738"}, "authors": [{"authorId": "48985275", "name": "Meng Zhang"}, {"authorId": "1989344", "name": "Liangyou Li"}, {"authorId": "2115900360", "name": "Qun Liu"}]}, {"paperId": "30a0717872ed6951db28c11ae270099accfc29d0", "externalIds": {"DBLP": "conf/acl/WangCZQL21", "ACL": "2021.findings-acl.242", "DOI": "10.18653/v1/2021.findings-acl.242", "CorpusId": 236478257}, "corpusId": 236478257, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/30a0717872ed6951db28c11ae270099accfc29d0", "title": "Contrastive Aligned Joint Learning for Multilingual Summarization", "abstract": "Multilingual text summarization requires the ability to understand documents in multiple languages and generate summaries in the corresponding language, which poses more challenges on current summarization systems. However, this problem has been rarely studied due to the lack of large-scale supervised summarization data in multiple languages. In this paper, we \ufb01rst provide a large-scale multilingual summarization corpus MLGSum consisting of 1.1 million articles and summaries in 12 different languages. Based on it, we develop a uni\ufb01ed summarization model to understand the document and generate summaries in different languages. We use the contrastive learning strategy to train our multilingual summarization system (CALMS), which consists of two training objectives, contrastive sentence ranking (CSR) and sentence aligned substitution (SAS). The two training objectives are designed to share salient information extractive ability and align sentence-level representation across different languages. Experimental results indicate that CALMS achieves signi\ufb01cant improvement over mono-lingual models in all languages. We further transfer CALMS to other languages and \ufb01nd that it will also bene\ufb01t similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 23, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.242.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2739-2750"}, "authors": [{"authorId": "49371126", "name": "Danqing Wang"}, {"authorId": "2108182762", "name": "Jiaze Chen"}, {"authorId": null, "name": "Hao Zhou"}, {"authorId": "1767521", "name": "Xipeng Qiu"}, {"authorId": "143900005", "name": "Lei Li"}]}, {"paperId": "5909f29aa91ec1d73e8daadada179fe4377cea0d", "externalIds": {"DBLP": "conf/acl/BeelenNAHTM21", "ACL": "2021.findings-acl.243", "DOI": "10.18653/v1/2021.findings-acl.243", "CorpusId": 236477825}, "corpusId": 236477825, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5909f29aa91ec1d73e8daadada179fe4377cea0d", "title": "When Time Makes Sense: A Historically-Aware Approach to Targeted Sense Disambiguation", "abstract": "As languages evolve historically, making computational approaches sensitive to time can improve performance on specific tasks. In this work, we assess whether applying historical language models and time-aware methods help with determining the correct sense of polysemous words. We outline the task of time-sensitive Targeted Sense Disambiguation (TSD), which aims to detect instances of a sense or set of related senses in historical and time-stamped texts, and address two main goals: 1) we scrutinize the effect of applying historical language models on the performance of several TSD methods and 2) we assess different disambiguation methods that take into account the year in which a text was produced. We train historical BERT models on a corpus of nineteenth-century English books and draw on the Oxford English Dictionary (and its Historical Thesaurus) to create historically evolving sense representations. Our results show that using historical language models consistently improves performance whereas timesensitive disambiguation helps especially with older documents. \u2217 Contributions of each author (in alphabetical order): Conceptualization: KB, BMcG, FN; Data curation: KB, GT; Formal Analysis: MCA; Funding acquisition: BMcG; Methodology: KB, MCA, KH, BMcG, FN; Project management: KB, BMcG, FN; Software: KB, MCA, KH, FN; Supervision: BMcG; Reproducibility: KB, MCA, FN; Writing: KB, MCA, BMcG, FN.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2751-2761"}, "authors": [{"authorId": "34651948", "name": "K. Beelen"}, {"authorId": "3393380", "name": "F. Nanni"}, {"authorId": "2170183", "name": "Mariona Coll Ardanuy"}, {"authorId": "144485694", "name": "Kasra Hosseini"}, {"authorId": "146851742", "name": "Giorgia Tolfo"}, {"authorId": "144463772", "name": "Barbara McGillivray"}]}, {"paperId": "350c7a1877b90d825b84131985cdf44b7bbc59d7", "externalIds": {"DBLP": "conf/acl/AkhmouchDM21", "MAG": "3174148081", "ACL": "2021.findings-acl.244", "DOI": "10.18653/v1/2021.findings-acl.244", "CorpusId": 236478007}, "corpusId": 236478007, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/350c7a1877b90d825b84131985cdf44b7bbc59d7", "title": "Understanding Feature Focus in Multitask Settings for Lexico-semantic Relation Identification", "abstract": "Discovering whether words are semantically related and identifying the specific semantic relation that holds between them is of crucial importance for automatic reasoning on text data. For that purpose, different methodologies have been proposed that either (1) tackle feature engineering, (2) fine-tune latent semantic spaces, or (3) take advantage of cognitive links between semantic relations in multitask settings. In this paper, we investigate how feature engineering and multitask architectures can be improved and consequently combined to identify lexico-semantic relations. Evaluation results over a set of gold-standard datasets show that (1) combinations of similar features are beneficial (feature sets), (2) asymmetric distributional features are a strong cue to discriminate asymmetric relations as well as they play an important role in multitask architectures, (3) shared-private models improve over binary and fully-shared classifiers as well as they correctly balance the focus on features between private and shared layers1.", "venue": "Findings", "year": 2021, "referenceCount": 44, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-01", "journal": {"pages": "2762-2772"}, "authors": [{"authorId": "81689602", "name": "Houssam Akhmouch"}, {"authorId": "153563898", "name": "Gael Dias"}, {"authorId": "143773738", "name": "Jose G. Moreno"}]}, {"paperId": "5be31bd3a8c4905b7ee2ffc7c8f477ba6365b5de", "externalIds": {"DBLP": "conf/acl/LuoLLZ21", "ACL": "2021.findings-acl.245", "DOI": "10.18653/v1/2021.findings-acl.245", "CorpusId": 236477592}, "corpusId": 236477592, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5be31bd3a8c4905b7ee2ffc7c8f477ba6365b5de", "title": "Don\u2019t Miss the Labels: Label-semantic Augmented Meta-Learner for Few-Shot Text Classification", "abstract": "Increasing studies leverage pre-trained language models and meta-learning frameworks to solve few-shot text classi\ufb01cation problems. Most of the current studies focus on building a meta-learner from the information of input texts but ignore abundant semantic information beneath class labels. In this work, we show that class-label information can be utilized for extracting more discriminative feature representation of the input text from a pre-trained language model like BERT, and can achieve a performance boost when the samples are scarce. Building on top of this discovery, we propose a framework called Label-semantic augmented meta-learner (LaSAML) to make full use of label semantics. We systematically investigate various factors in this framework and show that it can be plugged into the existing few-shot text classi\ufb01cation system. Through extensive experiments, we demonstrate that the few-shot text classi\ufb01cation system upgraded by LaSAML can lead to signi\ufb01cant performance improvement over its original counterparts.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 33, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.245.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2773-2782"}, "authors": [{"authorId": "2125648143", "name": "Qiaoyang Luo"}, {"authorId": "120095791", "name": "Ling-ling Liu"}, {"authorId": "2166060948", "name": "Yuhao Lin"}, {"authorId": "31765881", "name": "Wei Zhang"}]}, {"paperId": "24992c4b3cfd6f3dc0d4428846b36082ee781d96", "externalIds": {"DBLP": "journals/corr/abs-2110-00413", "ACL": "2021.findings-acl.246", "ArXiv": "2110.00413", "DOI": "10.18653/v1/2021.findings-acl.246", "CorpusId": 236477682}, "corpusId": 236477682, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/24992c4b3cfd6f3dc0d4428846b36082ee781d96", "title": "Detecting Harmful Memes and Their Targets", "abstract": "Among the various modes of communication in social media, the use of Internet memes has emerged as a powerful means to convey political, psychological, and socio-cultural opinions. Although memes are typically humorous in nature, recent days have witnessed a proliferation of harmful memes targeted to abuse various social entities. As most harmful memes are highly satirical and abstruse without appropriate contexts, off-the-shelf multimodal models may not be adequate to understand their underlying semantics. In this work, we propose two novel problem formulations: detecting harmful memes and the social entities that these harmful memes target. To this end, we present HarMeme, the first benchmark dataset, containing 3,544 memes related to COVID-19. Each meme went through a rigorous two-stage annotation process. In the first stage, we labeled a meme as very harmful, partially harmful, or harmless;in the second stage, we further annotated the type of target(s) that each harmful meme points to: individual, organization, community, or society/general public/other. The evaluation results using ten unimodal and multimodal models highlight the importance of using multimodal signals for both tasks. We further discuss the limitations of these models and we argue that more research is needed to address these problems.", "venue": "Findings", "year": 2021, "referenceCount": 63, "citationCount": 48, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.246.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-24", "journal": {"name": "ArXiv", "volume": "abs/2110.00413"}, "authors": [{"authorId": "1564558163", "name": "Shraman Pramanick"}, {"authorId": "2105813793", "name": "Dimitar I. Dimitrov"}, {"authorId": "31137986", "name": "Rituparna Mukherjee"}, {"authorId": "1491627343", "name": "Shivam Sharma"}, {"authorId": "46815454", "name": "Md. Shad Akhtar"}, {"authorId": "2026545715", "name": "Preslav Nakov"}, {"authorId": "144054829", "name": "Tanmoy Chakraborty"}]}, {"paperId": "9bc1b2f8ab82921e8f226c732f51d0ce1bd48822", "externalIds": {"ArXiv": "2106.05546", "DBLP": "conf/acl/DingWLWTT21", "ACL": "2021.findings-acl.247", "DOI": "10.18653/v1/2021.findings-acl.247", "CorpusId": 235390574}, "corpusId": 235390574, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9bc1b2f8ab82921e8f226c732f51d0ce1bd48822", "title": "Progressive Multi-Granularity Training for Non-Autoregressive Translation", "abstract": "Non-autoregressive translation (NAT) significantly accelerates the inference process via predicting the entire target sequence. However, recent studies show that NAT is weak at learning high-mode of knowledge such as one-to-many translations. We argue that modes can be divided into various granularities which can be learned from easy to hard. In this study, we empirically show that NAT models are prone to learn fine-grained lower-mode knowledge, such as words and phrases, compared with sentences. Based on this observation, we propose progressive multi-granularity training for NAT. More specifically, to make the most of the training data, we break down the sentence-level examples into three types, i.e. words, phrases, sentences, and with the training goes, we progressively increase the granularities. Experiments on Romanian-English, English-German, Chinese-English, and Japanese-English demonstrate that our approach improves the phrase translation accuracy and model reordering ability, therefore resulting in better translation quality against strong NAT baselines. Also, we show that more deterministic fine-grained knowledge can further enhance performance.", "venue": "Findings", "year": 2021, "referenceCount": 43, "citationCount": 27, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.247.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-10", "journal": {"pages": "2797-2803"}, "authors": [{"authorId": "46573238", "name": "Liang Ding"}, {"authorId": "2111542852", "name": "Longyue Wang"}, {"authorId": "2151060023", "name": "Xuebo Liu"}, {"authorId": "1758353", "name": "Derek F. Wong"}, {"authorId": "143719920", "name": "D. Tao"}, {"authorId": "2909321", "name": "Zhaopeng Tu"}]}, {"paperId": "38d3657ee15f2612330eb5e036bbc38d9137f75a", "externalIds": {"ACL": "2021.findings-acl.248", "DBLP": "conf/acl/MauryaDKD21", "ArXiv": "2106.01597", "DOI": "10.18653/v1/2021.findings-acl.248", "CorpusId": 235313711}, "corpusId": 235313711, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/38d3657ee15f2612330eb5e036bbc38d9137f75a", "title": "ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation", "abstract": "Despite the recent advancement in NLP research, cross-lingual transfer for natural language generation is relatively understudied. In this work, we transfer supervision from high resource language (HRL) to multiple low-resource languages (LRLs) for natural language generation (NLG). We consider four NLG tasks (text summarization, question generation, news headline generation, and distractor generation) and three syntactically diverse languages, i.e., English, Hindi, and Japanese. We propose an unsupervised cross-lingual language generation framework (called ZmBART) that does not use any parallel or pseudo-parallel/back-translated data. In this framework, we further pre-train mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task using monolingual data of three languages. The objective function of the auxiliary task is close to the target tasks which enriches the multi-lingual latent representation of mBART and provides good initialization for target tasks. Then, this model is fine-tuned with task-specific supervised English data and directly evaluated with low-resource languages in the Zero-shot setting. To overcome catastrophic forgetting and spurious correlation issues, we applied freezing model component and data argumentation approaches respectively. This simple modeling approach gave us promising results.We experimented with few-shot training (with 1000 supervised data points) which boosted the model performance further. We performed several ablations and cross-lingual transferability analyses to demonstrate the robustness of ZmBART.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 20, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.248.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01597"}, "authors": [{"authorId": "1576803006", "name": "Kaushal Kumar Maurya"}, {"authorId": "2481485", "name": "M. Desarkar"}, {"authorId": "34803801", "name": "Yoshinobu Kano"}, {"authorId": "2034014986", "name": "K. Deepshikha"}]}, {"paperId": "31fb3586cab89c1314bb9863d00222aeaa77d7d3", "externalIds": {"ACL": "2021.findings-acl.249", "DBLP": "conf/acl/ChengLQZLWHYX21", "DOI": "10.18653/v1/2021.findings-acl.249", "CorpusId": 236478098}, "corpusId": 236478098, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/31fb3586cab89c1314bb9863d00222aeaa77d7d3", "title": "HacRED: A Large-Scale Relation Extraction Dataset Toward Hard Cases in Practical Applications", "abstract": "Relation extraction (RE) is an essential topic in natural language processing and has at-tracted extensive attention. Current RE ap-proaches achieve fantastic results on common datasets, while they still struggle on practical applications. In this paper, we analyze the above performance gap, the underlying reason of which is that practical applications in-trinsically have more hard cases. To make RE models more robust on such practical hard cases, we propose a case-oriented construction framework to build a H ard C ase R elation E xtraction D ataset ( HacRED ). The proposed HacRED consists of 65,225 relational facts annotated from 9,231 documents with sufficient and diverse hard cases. Notably, HacRED is one of the largest Chinese document-level RE datasets and achieves a high 96% F1 score on data quality. Furthermore, we apply the state-of-the-art RE models on this dataset and conduct a thorough evaluation. The results show that the performance of these models is far lower than humans, and RE applying on practical hard cases still requires further efforts. HacRED is publicly available", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 17, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.249.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2819-2831"}, "authors": [{"authorId": "2055427654", "name": "Qiao Cheng"}, {"authorId": "1719992", "name": "Juntao Liu"}, {"authorId": "51912474", "name": "Xiaoye Qu"}, {"authorId": "2109972180", "name": "Jin Zhao"}, {"authorId": "3366523", "name": "Jiaqing Liang"}, {"authorId": "2108271253", "name": "Zhefeng Wang"}, {"authorId": "2422046", "name": "Baoxing Huai"}, {"authorId": "1677643972", "name": "N. Yuan"}, {"authorId": "3011950", "name": "Yanghua Xiao"}]}, {"paperId": "48558617dc71742c847d20be2ff0546df50e15ce", "externalIds": {"DBLP": "journals/corr/abs-2105-14940", "ArXiv": "2105.14940", "ACL": "2021.findings-acl.250", "DOI": "10.18653/v1/2021.findings-acl.250", "CorpusId": 235254522}, "corpusId": 235254522, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/48558617dc71742c847d20be2ff0546df50e15ce", "title": "Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?", "abstract": "Recent studies on the analysis of the multilingual representations focus on identifying whether there is an emergence of language-independent representations, or whether a multilingual model partitions its weights among different languages. While most of such work has been conducted in a\"black-box\"manner, this paper aims to analyze individual components of a multilingual neural translation (NMT) model. In particular, we look at the encoder self-attention and encoder-decoder attention heads (in a many-to-one NMT model) that are more specific to the translation of a certain language pair than others by (1) employing metrics that quantify some aspects of the attention weights such as\"variance\"or\"confidence\", and (2) systematically ranking the importance of attention heads with respect to translation quality. Experimental results show that surprisingly, the set of most important attention heads are very similar across the language pairs and that it is possible to remove nearly one-third of the less important heads without hurting the translation quality greatly.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.250.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-31", "journal": {"pages": "2832-2841"}, "authors": [{"authorId": "2894340", "name": "Zae Myung Kim"}, {"authorId": "143823463", "name": "L. Besacier"}, {"authorId": "2841761", "name": "Vassilina Nikoulina"}, {"authorId": "2504683", "name": "D. Schwab"}]}, {"paperId": "c58ad64a5774023884f5aef01106a419a8e40463", "externalIds": {"ACL": "2021.findings-acl.251", "DBLP": "conf/acl/ChoiBNL21", "DOI": "10.18653/v1/2021.findings-acl.251", "CorpusId": 236477892}, "corpusId": 236477892, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c58ad64a5774023884f5aef01106a419a8e40463", "title": "Learning Sequential and Structural Information for Source Code Summarization", "abstract": "We propose a model that learns both the sequential and the structural features of code for source code summarization. We adopt the abstract syntax tree (AST) and graph convolution to model the structural information and the Transformer to model the sequential information. We convert code snip-pets into ASTs and apply graph convolution to obtain structurally-encoded node representations. Then, the sequences of the graph-convolutioned AST nodes are processed by the Transformer layers. Since structurally-neighboring nodes will have similar representations in graph-convolutioned trees, the Trans-former layers can effectively capture not only the sequential information but also the structural information such as sentences or blocks of source code. We show that our model out-performs the state-of-the-art for source code summarization by experiments and human evaluations.", "venue": "Findings", "year": 2021, "referenceCount": 22, "citationCount": 24, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.251.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2842-2851"}, "authors": [{"authorId": "46252888", "name": "YunSeok Choi"}, {"authorId": "72761736", "name": "Jinyeong Bak"}, {"authorId": "2080438236", "name": "CheolWon Na"}, {"authorId": "30603322", "name": "Jee-Hyong Lee"}]}, {"paperId": "b089b54e77739993c804b46fd0310e353ebb4f1d", "externalIds": {"ArXiv": "2107.12542", "ACL": "2021.findings-acl.252", "DBLP": "conf/acl/OuyangYCDHC21", "DOI": "10.18653/v1/2021.findings-acl.252", "CorpusId": 236447583}, "corpusId": 236447583, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b089b54e77739993c804b46fd0310e353ebb4f1d", "title": "Energy-based Unknown Intent Detection with Data Manipulation", "abstract": "Unknown intent detection aims to identify the out-of-distribution (OOD) utterance whose intent has never appeared in the training set. In this paper, we propose using energy scores for this task as the energy score is theoretically aligned with the density of the input and can be derived from any classifier. However, high-quality OOD utterances are required during the training stage in order to shape the energy gap between OOD and in-distribution (IND), and these utterances are difficult to collect in practice. To tackle this problem, we propose a data manipulation framework to Generate high-quality OOD utterances with importance weighTs (GOT). Experimental results show that the energy-based detector fine-tuned by GOT can achieve state-of-the-art results on two benchmark datasets.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.252.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-27", "journal": {"name": "ArXiv", "volume": "abs/2107.12542"}, "authors": [{"authorId": "2070558572", "name": "Yawen Ouyang"}, {"authorId": "2153258452", "name": "Jiasheng Ye"}, {"authorId": "2144839663", "name": "Yu Chen"}, {"authorId": "3035069", "name": "Xinyu Dai"}, {"authorId": "2124946880", "name": "Shujian Huang"}, {"authorId": "1838162", "name": "Jiajun Chen"}]}, {"paperId": "9b840156b434bdb41c18f036644cdd86882f2254", "externalIds": {"ACL": "2021.findings-acl.253", "DBLP": "conf/acl/CohenKHK21", "DOI": "10.18653/v1/2021.findings-acl.253", "CorpusId": 236477458}, "corpusId": 236477458, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9b840156b434bdb41c18f036644cdd86882f2254", "title": "Automatic Rephrasing of Transcripts-based Action Items", "abstract": "The automated transcription of spoken language, and meetings, in particular, is becoming more widespread as automatic speech recognition systems are becoming more accurate. This trend has significantly accelerated since the outbreak of the COVID-19 pandemic, which led to a major increase in the number of online meetings. However, the transcription of spoken language has not received much attention from the NLP community compared to documents and other forms of written language. In this paper, we study a variation of the summarization problem over the transcription of spoken language: given a transcribed meeting, and an action item (i.e., a commitment or request to perform a task), our goal is to generate a coherent and self-contained rephrasing of the action item. To this end, we compiled a novel dataset of annotated meeting transcripts, including human rephrasing of action items. We use state-of-the-art supervised text generation techniques and establish a strong baseline based on BART and UniLM (two pretrained transformer models). Due to the nature of natural speech, language is often broken and incomplete and the task is shown to be harder than an analogous task over email data. Particularly, we show that the baseline models can be greatly improved once models are provided with additional information. We compare two approaches: one incorporating features extracted by coreference-resolution. Additional annotations are used to train an auxiliary model to detect the relevant context in the text. Based on the systematic human evaluation, our best models exhibit near-human-level rephrasing capability on a constrained subset of the problem. \u00a9 2021 Association for Computational Linguistics", "venue": "Findings", "year": 2021, "referenceCount": 42, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2862-2873"}, "authors": [{"authorId": "2112930011", "name": "Amir Cohen"}, {"authorId": "1703796", "name": "Amir Kantor"}, {"authorId": "2121339727", "name": "Sagi Hilleli"}, {"authorId": "1707732", "name": "Eyal Kolman"}]}, {"paperId": "7a6e5c8a566a9f6805dcb765594815cdc0de194b", "externalIds": {"DBLP": "conf/acl/KhanujaJT21", "DOI": "10.18653/v1/2021.findings-acl.254", "CorpusId": 260434958}, "corpusId": 260434958, "publicationVenue": null, "url": "https://www.semanticscholar.org/paper/7a6e5c8a566a9f6805dcb765594815cdc0de194b", "title": "MergeDistill: Merging Language Models using Pre-trained Distillation", "abstract": null, "venue": "ACL/IJCNLP", "year": 2021, "referenceCount": 0, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.254.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2874-2887"}, "authors": [{"authorId": "1452678825", "name": "Simran Khanuja"}, {"authorId": "2109675545", "name": "Melvin Johnson"}, {"authorId": "2408872", "name": "P. Talukdar"}]}, {"paperId": "c2f36f14419565a0fed3032b7f1d1811daf6702e", "externalIds": {"DBLP": "conf/acl/ZhangTS21", "ACL": "2021.findings-acl.255", "ArXiv": "2004.11854", "MAG": "3017643757", "DOI": "10.18653/v1/2021.findings-acl.255", "CorpusId": 216144667}, "corpusId": 216144667, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c2f36f14419565a0fed3032b7f1d1811daf6702e", "title": "On Sparsifying Encoder Outputs in Sequence-to-Sequence Models", "abstract": "Sequence-to-sequence models usually transfer all encoder outputs to the decoder for generation. In this work, by contrast, we hypothesize that these encoder outputs can be compressed to shorten the sequence delivered for decoding. We take Transformer as the testbed and introduce a layer of stochastic gates in-between the encoder and the decoder. The gates are regularized using the expected value of the sparsity-inducing L0penalty, resulting in completely masking-out a subset of encoder outputs. In other words, via joint training, the L0DROP layer forces Transformer to route information through a subset of its encoder states. We investigate the effects of this sparsification on two machine translation and two summarization tasks. Experiments show that, depending on the task, around 40-70% of source encodings can be pruned without significantly compromising quality. The decrease of the output length endows L0DROP with the potential of improving decoding efficiency, where it yields a speedup of up to 1.65x on document summarization tasks against the standard Transformer. We analyze the L0DROP behaviour and observe that it exhibits systematic preferences for pruning certain word types, e.g., function words and punctuation get pruned most. Inspired by these observations, we explore the feasibility of specifying rule-based patterns that mask out encoder outputs based on information such as part-of-speech tags, word frequency and word position.", "venue": "Findings", "year": 2020, "referenceCount": 44, "citationCount": 12, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.255.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-04-24", "journal": {"name": "ArXiv", "volume": "abs/2004.11854"}, "authors": [{"authorId": "48335426", "name": "Biao Zhang"}, {"authorId": "144889265", "name": "Ivan Titov"}, {"authorId": "2082372", "name": "Rico Sennrich"}]}, {"paperId": "41c4495e3bb9ceaba4aa2e6e809d24d7ef6263d6", "externalIds": {"MAG": "3176870077", "DBLP": "conf/acl/PonkiyaKBP21", "ACL": "2021.findings-acl.256", "DOI": "10.18653/v1/2021.findings-acl.256", "CorpusId": 235430924}, "corpusId": 235430924, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/41c4495e3bb9ceaba4aa2e6e809d24d7ef6263d6", "title": "FrameNet-assisted Noun Compound Interpretation", "abstract": "Given a noun compound (NC), we address the problem of predicting the appropriate semantic label linking the constituents of the NC. This problem is called Noun Compound Interpretation (NCI). We use FrameNet as a semantic label repository. For example, given the noun compound (board approval), we predict the frame (DENY OR GRANT PERMISSION, as per FrameNet) as appropriate and the semantic role of the modifier word (AUTHORITY) as the semantic label linking board and approval; the resulting label is DENY OR GRANT PERMISSION:AUTHORITY. Our semantic label repository is very large (\u2248 11k labels) compared to the NC data available for training (approx 1900). Thus, learning in this case, especially for unseen semantic labels, is hard. We propose to solve this problem by predicting semantic labels in a continuous label embedding space, which is novel. This embedding space is created by learning label embeddings using the FrameNet data. The embeddings are then used to train two separate models \u2013 one for predicting Frames and the other for FEs. As the label embedding space captures the semantics of the labels, using these embeddings enables generalizing well on unseen labels, thus achieving zero-shot learning. Our preliminary investigations show that the proposed approach performs well for unseen labels, achieving 5% and 2% points improvements over baselines for the frame and FE prediction, respectively. The study shows the promise of the use of continuous space embeddings for noun compound interpretation and points to the need for further investigation.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-01", "journal": {"pages": "2901-2911"}, "authors": [{"authorId": "32003673", "name": "Girishkumar Ponkiya"}, {"authorId": "2920443", "name": "Diptesh Kanojia"}, {"authorId": "145532184", "name": "P. Bhattacharyya"}, {"authorId": "1747097", "name": "Girish Keshav Palshikar"}]}, {"paperId": "c7919e1a2d5f92f5ce686288e42326645d1f6bde", "externalIds": {"ACL": "2021.findings-acl.257", "DBLP": "conf/acl/BaiZKCM21", "DOI": "10.18653/v1/2021.findings-acl.257", "CorpusId": 236478146}, "corpusId": 236478146, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c7919e1a2d5f92f5ce686288e42326645d1f6bde", "title": "Hypernym Discovery via a Recurrent Mapping Model", "abstract": "Hypernym discovery aims to identify all possible hypernyms of a given term. The most recent hypernym discovery models exploit multiple mapping functions to project a term to different semantic spaces and then aggregate these embeddings to a general representation for further classification. We refer to this model as a parallel style model. In this work, we observe that there are hierarchical relations between a target terms\u2019 hypernyms. However, these hierarchical relations were not sufficiently considered in the previous parallel style model. To leverage the hierarchical relations, we propose a sequential style model that recurrently maps the query words to their hypernyms, starting from the most specific ones to the less specific ones. Empirical studies on SemEval-2018 Task 9 confirm the effectiveness of the presented model.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.257.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2912-2921"}, "authors": [{"authorId": "108125344", "name": "Yuhang Bai"}, {"authorId": "2109975984", "name": "Richong Zhang"}, {"authorId": "41035566", "name": "Fanshuang Kong"}, {"authorId": "2008604", "name": "J. Chen"}, {"authorId": "2047889", "name": "Yongyi Mao"}]}, {"paperId": "d073db916b5c04c59f4e58e4951159693c54dd80", "externalIds": {"DBLP": "conf/acl/ChoCHH21", "ACL": "2021.findings-acl.258", "DOI": "10.18653/v1/2021.findings-acl.258", "CorpusId": 236477606}, "corpusId": 236477606, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d073db916b5c04c59f4e58e4951159693c54dd80", "title": "Modeling the Influence of Verb Aspect on the Activation of Typical Event Locations with BERT", "abstract": "Prior studies on event knowledge in sentence comprehension have shown that the aspect of the main verb plays an important role in the processing of non-core semantic roles, such as locations: when the aspect of the main verb is imperfective, locations become more salient in the mental representation of the event and are easier for human comprehenders to process. In our study, we tested the popular language model BERT on two datasets derived from experimental studies to determine whether BERT\u2019s predictions of prototypical event locations were also influenced by aspect. We found that, although BERT efficiently modelled the typicality of locations, it did so independently of the verb aspect. Even when the transformer was forced to focus on the verb phrase by masking the context words in the sentence, the typicality predictions were still accurate; in addition, we found aspect to have a stronger influence on the scores, with locations in the imperfective setting being associated with lower surprisal values.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2922-2929"}, "authors": [{"authorId": "2121567090", "name": "Won-Ik Cho"}, {"authorId": "3433809", "name": "Emmanuele Chersoni"}, {"authorId": "1919193", "name": "Yu-Yin Hsu"}, {"authorId": "1405994600", "name": "Chu-Ren Huang"}]}, {"paperId": "c1826a38625760b32885cc62878a0396a1e6dbe0", "externalIds": {"DBLP": "journals/corr/abs-2106-15355", "ACL": "2021.findings-acl.259", "ArXiv": "2106.15355", "DOI": "10.18653/v1/2021.findings-acl.259", "CorpusId": 235669650}, "corpusId": 235669650, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c1826a38625760b32885cc62878a0396a1e6dbe0", "title": "On the Interaction of Belief Bias and Explanations", "abstract": "A myriad of explainability methods have been proposed in recent years, but there is little consensus on how to evaluate them. While automatic metrics allow for quick benchmarking, it isn't clear how such metrics reflect human interaction with explanations. Human evaluation is of paramount importance, but previous protocols fail to account for belief biases affecting human performance, which may lead to misleading conclusions. We provide an overview of belief bias, its role in human evaluation, and ideas for NLP practitioners on how to account for it. For two experimental paradigms, we present a case study of gradient-based explainability introducing simple ways to account for humans' prior beliefs: models of varying quality and adversarial examples. We show that conclusions about the highest performing methods change when introducing such controls, pointing to the importance of accounting for belief bias in evaluation.", "venue": "Findings", "year": 2021, "referenceCount": 66, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.259.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-06-29", "journal": {"pages": "2930-2942"}, "authors": [{"authorId": "1453616756", "name": "Ana Valeria Gonzalez"}, {"authorId": "145046059", "name": "Anna Rogers"}, {"authorId": "1700187", "name": "Anders S\u00f8gaard"}]}, {"paperId": "9d24d4304078f0be973220c63abcceb3ea1848c2", "externalIds": {"DBLP": "journals/corr/abs-2106-03084", "ACL": "2021.findings-acl.260", "ArXiv": "2106.03084", "DOI": "10.18653/v1/2021.findings-acl.260", "CorpusId": 235358993}, "corpusId": 235358993, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9d24d4304078f0be973220c63abcceb3ea1848c2", "title": "Combining Static Word Embeddings and Contextual Representations for Bilingual Lexicon Induction", "abstract": "Bilingual Lexicon Induction (BLI) aims to map words in one language to their translations in another, and is typically through learning linear projections to align monolingual word representation spaces. Two classes of word representations have been explored for BLI: static word embeddings and contextual representations, but there is no studies to combine both. In this paper, we propose a simple yet effective mechanism to combine the static word embeddings and the contextual representations to utilize the advantages of both paradigms. We test the combination mechanism on various language pairs under the supervised and unsupervised BLI benchmark settings. Experiments show that our mechanism consistently improves performances over robust BLI baselines on all language pairs by averagely improving 3.2 points in the supervised setting, and 3.1 points in the unsupervised setting.", "venue": "Findings", "year": 2021, "referenceCount": 43, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.260.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-06", "journal": {"name": "ArXiv", "volume": "abs/2106.03084"}, "authors": [{"authorId": "2107956192", "name": "Jinpeng Zhang"}, {"authorId": "1441095620", "name": "Baijun Ji"}, {"authorId": "2107063281", "name": "Nini Xiao"}, {"authorId": "2109002", "name": "Xiangyu Duan"}, {"authorId": "1390813134", "name": "Min Zhang"}, {"authorId": "48081361", "name": "Yangbin Shi"}, {"authorId": "1935569", "name": "Weihua Luo"}]}, {"paperId": "12d588cf21970e86f0be537c1d0123ea42093119", "externalIds": {"ArXiv": "2106.05634", "DBLP": "conf/acl/BaziotisTBH21", "ACL": "2021.findings-acl.261", "DOI": "10.18653/v1/2021.findings-acl.261", "CorpusId": 235390930}, "corpusId": 235390930, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/12d588cf21970e86f0be537c1d0123ea42093119", "title": "Exploring Unsupervised Pretraining Objectives for Machine Translation", "abstract": "Unsupervised cross-lingual pretraining has achieved strong results in neural machine translation (NMT), by drastically reducing the need for large parallel data. Most approaches adapt masked-language modeling (MLM) to sequence-to-sequence architectures, by masking parts of the input and reconstructing them in the decoder. In this work, we systematically compare masking with alternative objectives that produce inputs resembling real (full) sentences, by reordering and replacing words based on their context. We pretrain models with different methods on English$\\leftrightarrow$German, English$\\leftrightarrow$Nepali and English$\\leftrightarrow$Sinhala monolingual data, and evaluate them on NMT. In (semi-) supervised NMT, varying the pretraining objective leads to surprisingly small differences in the finetuned performance, whereas unsupervised NMT is much more sensitive to it. To understand these results, we thoroughly study the pretrained models using a series of probes and verify that they encode and use information in different ways. We conclude that finetuning on parallel data is mostly sensitive to few properties that are shared by most models, such as a strong decoder, in contrast to unsupervised NMT that also requires models with strong cross-lingual abilities.", "venue": "Findings", "year": 2021, "referenceCount": 47, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.261.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-10", "journal": {"pages": "2956-2971"}, "authors": [{"authorId": "40928701", "name": "Christos Baziotis"}, {"authorId": "144889265", "name": "Ivan Titov"}, {"authorId": "2539211", "name": "Alexandra Birch"}, {"authorId": "2259100", "name": "B. Haddow"}]}, {"paperId": "495235a416d35bbaf441e8c38b7cb20eb9a03848", "externalIds": {"DBLP": "conf/acl/ZhengMZ21", "ACL": "2021.findings-acl.262", "DOI": "10.18653/v1/2021.findings-acl.262", "CorpusId": 236477538}, "corpusId": 236477538, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/495235a416d35bbaf441e8c38b7cb20eb9a03848", "title": "Knowledge-Grounded Dialogue Generation with Term-level De-noising", "abstract": "Dialogue generation has been improved through injecting knowledge into generative models. However, addition of knowledge through simple selection of sentences or paragraphs is likely to introduce noise and dimin-ish the effectiveness of the generative models. In this paper, we present a novel K nowledge T erm W eighting M odel (KTWM) that incorporates term-level de-noising of the selected knowledge. KTWM includes a module for generating Simulated Response Vectors (SRVs) and uses SRVs attention distributions with the knowledge embeddings to determine knowledge term weights. Our experiments demonstrate that KTWM, com-bined with various knowledge selection algo-rithms, consistently achieves statistically signi\ufb01cant improvements over methods without term weighting when applied to two publicly available datasets Wizard of Wikipedia (Wiz) and Holl-E. The results are particularly improved for the Wiz test data with unseen topics, demonstrating the robustness of the KTWM noise-reduction approach.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.262.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "2972-2983"}, "authors": [{"authorId": "2152934259", "name": "Wen Zheng"}, {"authorId": "1398136050", "name": "Natasa Milic-Frayling"}, {"authorId": "2075359401", "name": "Ke Zhou"}]}, {"paperId": "12b336abe8b9889bfd2b82ff790e53603a899cbe", "externalIds": {"ACL": "2021.findings-acl.263", "DBLP": "journals/corr/abs-2105-13471", "ArXiv": "2105.13471", "DOI": "10.18653/v1/2021.findings-acl.263", "CorpusId": 235248056}, "corpusId": 235248056, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/12b336abe8b9889bfd2b82ff790e53603a899cbe", "title": "Inspecting the concept knowledge graph encoded by modern language models", "abstract": "The field of natural language understanding has experienced exponential progress in the last few years, with impressive results in several tasks. This success has motivated researchers to study the underlying knowledge encoded by these models. Despite this, attempts to understand their semantic capabilities have not been successful, often leading to non-conclusive, or contradictory conclusions among different works. Via a probing classifier, we extract the underlying knowledge graph of nine of the most influential language models of the last years, including word embeddings, text generators, and context encoders. This probe is based on concept relatedness, grounded on WordNet. Our results reveal that all the models encode this knowledge, but suffer from several inaccuracies. Furthermore, we show that the different architectures and training strategies lead to different model biases. We conduct a systematic evaluation to discover specific factors that explain why some concepts are challenging. We hope our insights will motivate the development of models that capture concepts more precisely.", "venue": "Findings", "year": 2021, "referenceCount": 74, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.263.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-27", "journal": {"pages": "2984-3000"}, "authors": [{"authorId": "13213628", "name": "C. Aspillaga"}, {"authorId": "145893609", "name": "Marcelo Mendoza"}, {"authorId": "2052429926", "name": "\u00c1lvaro Soto"}]}, {"paperId": "843db426afd8a43af077868f73dfa301c7f8be48", "externalIds": {"ArXiv": "2106.07930", "ACL": "2021.findings-acl.264", "DBLP": "journals/corr/abs-2106-07930", "DOI": "10.18653/v1/2021.findings-acl.264", "CorpusId": 235435767}, "corpusId": 235435767, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/843db426afd8a43af077868f73dfa301c7f8be48", "title": "Language Tags Matter for Zero-Shot Neural Machine Translation", "abstract": "Multilingual Neural Machine Translation (MNMT) has aroused widespread interest due to its efficiency. An exciting advantage of MNMT models is that they could also translate between unsupervised (zero-shot) language directions. Language tag (LT) strategies are often adopted to indicate the translation directions in MNMT. In this paper, we demonstrate that the LTs are not only indicators for translation directions but also crucial to zero-shot translation qualities. Unfortunately, previous work tends to ignore the importance of LT strategies. We demonstrate that a proper LT strategy could enhance the consistency of semantic representations and alleviate the off-target issue in zero-shot directions. Experimental results show that by ignoring the source language tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 19, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.264.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-15", "journal": {"pages": "3001-3007"}, "authors": [{"authorId": "2125109508", "name": "Liwei Wu"}, {"authorId": "3456696", "name": "Shanbo Cheng"}, {"authorId": "50468534", "name": "Mingxuan Wang"}, {"authorId": "143900005", "name": "Lei Li"}]}, {"paperId": "fac6e5ab3eb90b3651757b9d7a7927ed9367ae5f", "externalIds": {"ACL": "2021.findings-acl.265", "DBLP": "conf/acl/HuangQSZ21", "DOI": "10.18653/v1/2021.findings-acl.265", "CorpusId": 236477336}, "corpusId": 236477336, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/fac6e5ab3eb90b3651757b9d7a7927ed9367ae5f", "title": "Latent Reasoning for Low-Resource Question Generation", "abstract": "Multi-hop question generation requires com-plex reasoning and coherent language realiza-tion. Learning a generation model for the problem requires extensive multi-hop question answering (QA) data, which are limited due to the manual collection effort. A two-phase strategy addresses the insuf\ufb01ciency of multi-hop QA data by \ufb01rst generating and then composing single-hop sub-questions. Learning this generating and then composing two-phase model, however, requires manually la-beled question decomposition data, which is labor intensive. To overcome this limitation, we propose a novel generative approach that optimizes the two-phase model without question decomposition data. We treat the unobserved sub-questions as latent variables and propose an objective that estimates the true sub-questions via variational inference. We further generalize the generative modeling to single-hop QA data. We hypothesize that each single-hop question is a sub-question of an unobserved multi-hop question, and propose an objective that generates single-hop questions by decomposing latent multi-hop questions. We show that the two objectives can be uni\ufb01ed and both optimize the two-phase generation model. Experiments show that the proposed approach outperforms competitive baselines on H OTPOT QA, a benchmark multi-hop question answering dataset.", "venue": "Findings", "year": 2021, "referenceCount": 65, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3008-3022"}, "authors": [{"authorId": "14799547", "name": "Xinting Huang"}, {"authorId": "39899794", "name": "Jianzhong Qi"}, {"authorId": "2117103785", "name": "Yu Sun"}, {"authorId": "2118403745", "name": "Rui Zhang"}]}, {"paperId": "896c76bf99de9c1ba6fc3dfb695dfa66e73e1eb3", "externalIds": {"DBLP": "journals/corr/abs-2106-07285", "ACL": "2021.findings-acl.266", "ArXiv": "2106.07285", "DOI": "10.18653/v1/2021.findings-acl.266", "CorpusId": 235415305}, "corpusId": 235415305, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/896c76bf99de9c1ba6fc3dfb695dfa66e73e1eb3", "title": "Probing Pre-Trained Language Models for Disease Knowledge", "abstract": "Pre-trained language models such as ClinicalBERT have achieved impressive results on tasks such as medical Natural Language Inference. At first glance, this may suggest that these models are able to perform medical reasoning tasks, such as mapping symptoms to diseases. However, we find that standard benchmarks such as MedNLI contain relatively few examples that require such forms of reasoning. To better understand the medical reasoning capabilities of existing language models, in this paper we introduce DisKnE, a new benchmark for Disease Knowledge Evaluation. To construct this benchmark, we annotated each positive MedNLI example with the types of medical reasoning that are needed. We then created negative examples by corrupting these positive examples in an adversarial way. Furthermore, we define training-test splits per disease, ensuring that no knowledge about test diseases can be learned from the training data, and we canonicalize the formulation of the hypotheses to avoid the presence of artefacts. This leads to a number of binary classification problems, one for each type of reasoning and each disease. When analysing pre-trained models for the clinical/biomedical domain on the proposed benchmark, we find that their performance drops considerably.", "venue": "Findings", "year": 2021, "referenceCount": 44, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.266.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-14", "journal": {"name": "ArXiv", "volume": "abs/2106.07285"}, "authors": [{"authorId": "2008208677", "name": "Israa Alghanmi"}, {"authorId": "2254466", "name": "Luis Espinosa Anke"}, {"authorId": "2265382", "name": "S. Schockaert"}]}, {"paperId": "ea0ab5238d7bfb62370789cccdb1cf856cb38aaa", "externalIds": {"DBLP": "conf/acl/MohiuddinBJ21", "ArXiv": "2106.05141", "ACL": "2021.findings-acl.267", "DOI": "10.18653/v1/2021.findings-acl.267", "CorpusId": 235377244}, "corpusId": 235377244, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ea0ab5238d7bfb62370789cccdb1cf856cb38aaa", "title": "AugVic: Exploiting BiText Vicinity for Low-Resource NMT", "abstract": "The success of Neural Machine Translation (NMT) largely depends on the availability of large bitext training corpora. Due to the lack of such large corpora in low-resource language pairs, NMT systems often exhibit poor performance. Extra relevant monolingual data often helps, but acquiring it could be quite expensive, especially for low-resource languages. Moreover, domain mismatch between bitext (train/test) and monolingual data might degrade the performance. To alleviate such issues, we propose AUGVIC, a novel data augmentation framework for low-resource NMT which exploits the vicinal samples of the given bitext without using any extra monolingual data explicitly. It can diversify the in-domain bitext data with finer level control. Through extensive experiments on four low-resource language pairs comprising data from different domains, we have shown that our method is comparable to the traditional back-translation that uses extra in-domain monolingual data. When we combine the synthetic parallel data generated from AUGVIC with the ones from the extra monolingual data, we achieve further improvements. We show that AUGVIC helps to attenuate the discrepancies between relevant and distant-domain monolingual data in traditional back-translation. To understand the contributions of different components of AUGVIC, we perform an in-depth framework analysis.", "venue": "Findings", "year": 2021, "referenceCount": 56, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.267.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-09", "journal": {"name": "ArXiv", "volume": "abs/2106.05141"}, "authors": [{"authorId": "6838342", "name": "Tasnim Mohiuddin"}, {"authorId": "31773000", "name": "M Saiful Bari"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}]}, {"paperId": "8068a409f014b2f5c51f8579ce2f75d00b329515", "externalIds": {"ACL": "2021.findings-acl.268", "DBLP": "conf/acl/ZhangYYH21", "ArXiv": "2106.02011", "DOI": "10.18653/v1/2021.findings-acl.268", "CorpusId": 235313916}, "corpusId": 235313916, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8068a409f014b2f5c51f8579ce2f75d00b329515", "title": "Provably Secure Generative Linguistic Steganography", "abstract": "Generative linguistic steganography mainly utilized language models and applied steganographic sampling (stegosampling) to generate high-security steganographic text (stegotext). However, previous methods generally lead to statistical differences between the conditional probability distributions of stegotext and natural text, which brings about security risks. In this paper, to further ensure security, we present a novel provably secure generative linguistic steganographic method ADG, which recursively embeds secret information by Adaptive Dynamic Grouping of tokens according to their probability given by an off-the-shelf language model. We not only prove the security of ADG mathematically, but also conduct extensive experiments on three public corpora to further verify its imperceptibility. The experimental results reveal that the proposed method is able to generate stegotext with nearly perfect security.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 26, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.268.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.02011"}, "authors": [{"authorId": "2108337221", "name": "Siyu Zhang"}, {"authorId": "2111737096", "name": "Zhongliang Yang"}, {"authorId": "2109612892", "name": "Jinshuai Yang"}, {"authorId": "1731776", "name": "Yongfeng Huang"}]}, {"paperId": "5c95de51ac81569fd515df1a91fe0a6617536fd9", "externalIds": {"DBLP": "journals/corr/abs-2105-11174", "ACL": "2021.findings-acl.269", "ArXiv": "2105.11174", "DOI": "10.18653/v1/2021.findings-acl.269", "CorpusId": 235166616}, "corpusId": 235166616, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5c95de51ac81569fd515df1a91fe0a6617536fd9", "title": "Retrieval Enhanced Model for Commonsense Generation", "abstract": "Commonsense generation is a challenging task of generating a plausible sentence describing an everyday scenario using provided concepts. Its requirement of reasoning over commonsense knowledge and compositional generalization ability even puzzles strong pre-trained language generation models. We propose a novel framework using retrieval methods to enhance both the pre-training and fine-tuning for commonsense generation. We retrieve prototype sentence candidates by concept matching and use them as auxiliary input. For fine-tuning, we further boost its performance with a trainable sentence retriever. We demonstrate experimentally on the large-scale CommonGen benchmark that our approach achieves new state-of-the-art results.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 25, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.269.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-24", "journal": {"name": "ArXiv", "volume": "abs/2105.11174"}, "authors": [{"authorId": "144407394", "name": "Han Wang"}, {"authorId": "39798499", "name": "Yang Liu"}, {"authorId": "1456009348", "name": "Chenguang Zhu"}, {"authorId": "24962156", "name": "Linjun Shou"}, {"authorId": "50175330", "name": "Ming Gong"}, {"authorId": "2110197273", "name": "Yichong Xu"}, {"authorId": "48262024", "name": "Michael Zeng"}]}, {"paperId": "633489316d298b0316e2e33338cd120dfe379dd0", "externalIds": {"DBLP": "conf/acl/ChenCLCMWY21", "ACL": "2021.findings-acl.270", "ArXiv": "2106.02282", "DOI": "10.18653/v1/2021.findings-acl.270", "CorpusId": 235352985}, "corpusId": 235352985, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/633489316d298b0316e2e33338cd120dfe379dd0", "title": "Decoupled Dialogue Modeling and Semantic Parsing for Multi-Turn Text-to-SQL", "abstract": "Recently, Text-to-SQL for multi-turn dialogue has attracted great interest. Here, the user input of the current turn is parsed into the corresponding SQL query of the appropriate database, given all previous dialogue history. Current approaches mostly employ end-to-end models and consequently face two challenges. First, dialogue history modeling and Text-to-SQL parsing are implicitly combined, hence it is hard to carry out interpretable analysis and obtain targeted improvement. Second, SQL annotation of multi-turn dialogue is very expensive, leading to training data sparsity. In this paper, we propose a novel decoupled multi-turn Text-to-SQL framework, where an utterance rewrite model first explicitly solves completion of dialogue context, and then a single-turn Text-to-SQL parser follows. A dual learning approach is also proposed for the utterance rewrite model to address the data sparsity problem. Compared with end-to-end approaches, the proposed decoupled method can achieve excellent performance without any annotated in-domain data. With just a few annotated rewrite cases, the decoupled method outperforms the released state-of-the-art end-to-end models on both SParC and CoSQL datasets.", "venue": "Findings", "year": 2021, "referenceCount": 48, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.270.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02282"}, "authors": [{"authorId": "2117098991", "name": "Zhi Chen"}, {"authorId": "1390833791", "name": "Lu Chen"}, {"authorId": "2125544025", "name": "Hanqi Li"}, {"authorId": "150273321", "name": "Ruisheng Cao"}, {"authorId": "2087451371", "name": "Da Ma"}, {"authorId": "3000684", "name": "Mengyue Wu"}, {"authorId": "1736727", "name": "Kai Yu"}]}, {"paperId": "1ba1977a1ac7775fbeb987ac9200156f480fbf06", "externalIds": {"DBLP": "journals/corr/abs-2106-01559", "ACL": "2021.findings-acl.271", "ArXiv": "2106.01559", "DOI": "10.18653/v1/2021.findings-acl.271", "CorpusId": 235313592}, "corpusId": 235313592, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1ba1977a1ac7775fbeb987ac9200156f480fbf06", "title": "Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning", "abstract": "Relational fact extraction aims to extract semantic triplets from unstructured text. In this work, we show that all of the relational fact extraction models can be organized according to a graph-oriented analytical perspective. An efficient model, aDjacency lIst oRiented rElational faCT (DIRECT), is proposed based on this analytical framework. To alleviate challenges of error propagation and sub-task loss equilibrium, DIRECT employs a novel adaptive multi-task learning strategy with dynamic sub-task loss balancing. Extensive experiments are conducted on two benchmark datasets, and results prove that the proposed model outperforms a series of state-of-the-art (SoTA) models for relational triplet extraction.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.271.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"pages": "3075-3087"}, "authors": [{"authorId": "1387821612", "name": "Fubang Zhao"}, {"authorId": "1695957219", "name": "Zhuoren Jiang"}, {"authorId": "153818687", "name": "Yangyang Kang"}, {"authorId": "2060934", "name": "Changlong Sun"}, {"authorId": "1713802", "name": "Xiaozhong Liu"}]}, {"paperId": "5397ef2da78aac248826b66156bed824d8aa03fb", "externalIds": {"DBLP": "journals/corr/abs-2106-01186", "ACL": "2021.findings-acl.272", "ArXiv": "2106.01186", "DOI": "10.18653/v1/2021.findings-acl.272", "CorpusId": 235293834}, "corpusId": 235293834, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5397ef2da78aac248826b66156bed824d8aa03fb", "title": "Self-Supervised Document Similarity Ranking via Contextualized Language Models and Hierarchical Inference", "abstract": "We present a novel model for the problem of ranking a collection of documents according to their semantic similarity to a source (query) document. While the problem of document-to-document similarity ranking has been studied, most modern methods are limited to relatively short documents or rely on the existence of\"ground-truth\"similarity labels. Yet, in most common real-world cases, similarity ranking is an unsupervised problem as similarity labels are unavailable. Moreover, an ideal model should not be restricted by documents' length. Hence, we introduce SDR, a self-supervised method for document similarity that can be applied to documents of arbitrary length. Importantly, SDR can be effectively applied to extremely long documents, exceeding the 4,096 maximal token limits of Longformer. Extensive evaluations on large document datasets show that SDR significantly outperforms its alternatives across all metrics. To accelerate future research on unlabeled long document similarity ranking, and as an additional contribution to the community, we herein publish two human-annotated test sets of long documents similarity evaluation. The SDR code and datasets are publicly available.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.272.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "3088-3098"}, "authors": [{"authorId": "1441128149", "name": "Dvir Ginzburg"}, {"authorId": "46252132", "name": "Itzik Malkiel"}, {"authorId": "48797862", "name": "Oren Barkan"}, {"authorId": "27743758", "name": "Avi Caciularu"}, {"authorId": "1683070", "name": "Noam Koenigstein"}]}, {"paperId": "21e3fec6532fb8355de79453d135d84ee882d251", "externalIds": {"DBLP": "journals/corr/abs-2106-02359", "ArXiv": "2106.02359", "ACL": "2021.findings-acl.273", "DOI": "10.18653/v1/2021.findings-acl.273", "CorpusId": 235352924}, "corpusId": 235352924, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/21e3fec6532fb8355de79453d135d84ee882d251", "title": "How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact", "abstract": "Recent years have seen many breakthroughs in natural language processing (NLP), transitioning it from a mostly theoretical field to one with many real-world applications. Noting the rising number of applications of other machine learning and AI techniques with pervasive societal impact, we anticipate the rising importance of developing NLP technologies for social good. Inspired by theories in moral philosophy and global priorities research, we aim to promote a guideline for social good in the context of NLP. We lay the foundations via the moral philosophy definition of social good, propose a framework to evaluate the direct and indirect real-world impact of NLP tasks, and adopt the methodology of global priorities research to identify priority causes for NLP research. Finally, we use our theoretical framework to provide some practical guidelines for future NLP research for social good. Our data and code are available at http://github.com/zhijing-jin/nlp4sg_acl2021. In addition, we curate a list of papers and resources on NLP for social good at https://github.com/zhijing-jin/NLP4SocialGood_Papers.", "venue": "Findings", "year": 2021, "referenceCount": 96, "citationCount": 17, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.273.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Philosophy", "source": "s2-fos-model"}, {"category": "Sociology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02359"}, "authors": [{"authorId": "2111472502", "name": "Zhijing Jin"}, {"authorId": "46245587", "name": "Geeticka Chauhan"}, {"authorId": "66139470", "name": "Brian Tse"}, {"authorId": "2790926", "name": "Mrinmaya Sachan"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}]}, {"paperId": "bd87fa465ec2bf05685af10b216eb3f3a2ee42b9", "externalIds": {"DBLP": "conf/acl/HuangHMWCLL21", "ACL": "2021.findings-acl.274", "DOI": "10.18653/v1/2021.findings-acl.274", "CorpusId": 236477366}, "corpusId": 236477366, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/bd87fa465ec2bf05685af10b216eb3f3a2ee42b9", "title": "IgSEG: Image-guided Story Ending Generation", "abstract": "In this work, we propose a new task called Image-guided Story Ending Generation (IgSEG). Given a multi-sentence story plot and an ending-related image, IgSEG aims to generate a story ending that conforms to the contextual logic and the relevant visual concepts. In contrast to the story ending generation task, which generates open-ended endings, the ma-jor challenges of IgSEG are to comprehend the given context and image suf\ufb01ciently, and mine the appropriate semantics from the image to make the generated story ending informative, reasonable, and coherent. To address the challenges, we propose a Multi-layer Graph convolution and Cascade-LSTM (MGCL) based model which mainly comprises of two collaborative modules: i) a multi-layer graph convolutional network to learn the dependency relations of sentences and the logical clue of the context; ii) a multiple context-image attention module to generate the story endings by gradu-ally incorporating textual and visual semantic concepts. Our MGCL is thus capable of building logically consistent and semantically rich story endings. To evaluate the proposed model, we modify the existing VIST dataset to obtain the VIST-Ending dataset. Empirically, our MGCL outperforms all the strong baselines on both automatic and human evaluation.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3114-3123"}, "authors": [{"authorId": "2111526601", "name": "Qingbao Huang"}, {"authorId": "2111201342", "name": "Chuan Huang"}, {"authorId": "2090535738", "name": "Linzhang Mo"}, {"authorId": "1754240987", "name": "Jielong Wei"}, {"authorId": "1752876325", "name": "Yi Cai"}, {"authorId": "1701688", "name": "Ho-fung Leung"}, {"authorId": "2117895101", "name": "Qing Li"}]}, {"paperId": "62e08a6e155fe0fe223d802cdf5bf49fefb6df2e", "externalIds": {"DBLP": "journals/corr/abs-2105-12969", "ArXiv": "2105.12969", "ACL": "2021.findings-acl.275", "DOI": "10.18653/v1/2021.findings-acl.275", "CorpusId": 235212203}, "corpusId": 235212203, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/62e08a6e155fe0fe223d802cdf5bf49fefb6df2e", "title": "Improve Query Focused Abstractive Summarization by Incorporating Answer Relevance", "abstract": "Query focused summarization (QFS) models aim to generate summaries from source documents that can answer the given query. Most previous work on QFS only considers the query relevance criterion when producing the summary. However, studying the effect of answer relevance in the summary generating process is also important. In this paper, we propose QFS-BART, a model that incorporates the explicit answer relevance of the source documents given the query via a question answering model, to generate coherent and answer-related summaries. Furthermore, our model can take advantage of large pre-trained models which improve the summarization performance significantly. Empirical results on the Debatepedia dataset show that the proposed model achieves the new state-of-the-art performance.", "venue": "Findings", "year": 2021, "referenceCount": 48, "citationCount": 16, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.275.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-27", "journal": {"pages": "3124-3131"}, "authors": [{"authorId": "144610224", "name": "Dan Su"}, {"authorId": "1660855299", "name": "Tiezheng Yu"}, {"authorId": "2057151752", "name": "Pascale Fung"}]}, {"paperId": "fb0b53383dd2e25eeb66535b5ebaed3157acfee1", "externalIds": {"DBLP": "journals/corr/abs-2107-00124", "ArXiv": "2107.00124", "ACL": "2021.findings-acl.276", "DOI": "10.18653/v1/2021.findings-acl.276", "CorpusId": 235679451}, "corpusId": 235679451, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/fb0b53383dd2e25eeb66535b5ebaed3157acfee1", "title": "Learning a Reversible Embedding Mapping using Bi-Directional Manifold Alignment", "abstract": "We propose a Bi-Directional Manifold Alignment (BDMA) that learns a non-linear mapping between two manifolds by explicitly training it to be bijective. We demonstrate BDMA by training a model for a pair of languages rather than individual, directed source and target combinations, reducing the number of models by 50%. We show that models trained with BDMA in the\"forward\"(source to target) direction can successfully map words in the\"reverse\"(target to source) direction, yielding equivalent (or better) performance to standard unidirectional translation models where the source and target language is flipped. We also show how BDMA reduces the overall size of the model.", "venue": "Findings", "year": 2021, "referenceCount": 20, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.276.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-30", "journal": {"pages": "3132-3139"}, "authors": [{"authorId": "2116290", "name": "Ashwinkumar Ganesan"}, {"authorId": "2034063", "name": "Francis Ferraro"}, {"authorId": "143979239", "name": "T. Oates"}]}, {"paperId": "cebaf1435bcee484b41cf0b97ba6fd86a36b38d0", "externalIds": {"ACL": "2021.findings-acl.277", "ArXiv": "2107.02418", "DBLP": "conf/acl/SunZCGWCZL21", "DOI": "10.18653/v1/2021.findings-acl.277", "CorpusId": 235742855}, "corpusId": 235742855, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/cebaf1435bcee484b41cf0b97ba6fd86a36b38d0", "title": "Probabilistic Graph Reasoning for Natural Proof Generation", "abstract": "In this paper, we investigate the problem of reasoning over natural language statements. Prior neural based approaches do not explicitly consider the inter-dependency among answers and their proofs. In this paper, we propose PRobr, a novel approach for joint answer prediction and proof generation. PRobr defines a joint probabilistic distribution over all possible proof graphs and answers via an induced graphical model. We then optimize the model using variational approximation on top of neural textual representation. Experiments on multiple datasets under diverse settings (fully supervised, few-shot and zero-shot evaluation) verify the effectiveness of PRobr, e.g., achieving 10%-30% improvement on QA accuracy in few/zero-shot evaluation. Our codes and models can be found at https://github.com/changzhisun/PRobr/.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 5, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.277.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-06", "journal": {"pages": "3140-3151"}, "authors": [{"authorId": "2118133838", "name": "Changzhi Sun"}, {"authorId": "2108167542", "name": "Xinbo Zhang"}, {"authorId": "5040052", "name": "Jiangjie Chen"}, {"authorId": "2056157609", "name": "Chun Gan"}, {"authorId": "3174675", "name": "Yuanbin Wu"}, {"authorId": "2108182762", "name": "Jiaze Chen"}, {"authorId": "2111824520", "name": "Hao Zhou"}, {"authorId": "143900005", "name": "Lei Li"}]}, {"paperId": "a6c780c18ea2535427e478907873efa3042f32e2", "externalIds": {"DBLP": "conf/acl/LiuLTW21", "ACL": "2021.findings-acl.278", "DOI": "10.18653/v1/2021.findings-acl.278", "CorpusId": 236478383}, "corpusId": 236478383, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/a6c780c18ea2535427e478907873efa3042f32e2", "title": "Enhancing Zero-shot and Few-shot Stance Detection with Commonsense Knowledge Graph", "abstract": "In this paper, we consider a realistic scenario on stance detection with more application potential, i.e., zero-shot and few-shot stance detection, which identi\ufb01es stances for a wide range of topics with no or very few training examples. Conventional data-driven approaches are not applicable to the above zero-shot and few-shot scenarios. For human beings, commonsense knowledge is a crucial element of understanding and reasoning. In the absence of annotated data and cryptic expression of users\u2019 stance, we believe that introducing commonsense relational knowledge as support for reasoning can further improve the generalization and reasoning ability of the model in the zero-shot and few-shot scenarios. Speci\ufb01cally, we introduce a commonsense knowledge enhanced model to exploit both the structural-level and semantic-level information of the relational knowledge. Extensive experiments demonstrate that our model outperforms the state-of-the-art methods on zero-shot and few-shot stance detection task.", "venue": "Findings", "year": 2021, "referenceCount": 20, "citationCount": 44, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.278.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3152-3157"}, "authors": [{"authorId": "144207288", "name": "R. Liu"}, {"authorId": "144981131", "name": "Zheng Lin"}, {"authorId": "2010896289", "name": "Yutong Tan"}, {"authorId": "2154491752", "name": "Weiping Wang"}]}, {"paperId": "114be5db62209a0d0682279f5a054a316f56697e", "externalIds": {"ACL": "2021.findings-acl.279", "DBLP": "journals/corr/abs-2012-14827", "ArXiv": "2012.14827", "DOI": "10.18653/v1/2021.findings-acl.279", "CorpusId": 229923411}, "corpusId": 229923411, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/114be5db62209a0d0682279f5a054a316f56697e", "title": "Dialogue Graph Modeling for Conversational Machine Reading", "abstract": "Conversational Machine Reading (CMR) aims at answering questions in a complicated manner. Machine needs to answer questions through interactions with users based on given rule document, user scenario and dialogue history, and ask questions to clarify if necessary. In this paper, we propose a dialogue graph modeling framework to improve the understanding and reasoning ability of machine on CMR task. There are three types of graph in total. Specifically, Discourse Graph is designed to learn explicitly and extract the discourse relation among rule texts as well as the extra knowledge of scenario; Decoupling Graph is used for understanding local and contextualized connection within rule texts. And finally a global graph for fusing the information together and reply to the user with our final decision being either\"Yes/No/Irrelevant\"or to ask a follow-up question to clarify.", "venue": "Findings", "year": 2020, "referenceCount": 44, "citationCount": 33, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.279.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-30", "journal": {"name": "ArXiv", "volume": "abs/2012.14827"}, "authors": [{"authorId": "2042897657", "name": "Siru Ouyang"}, {"authorId": "3322871", "name": "Zhuosheng Zhang"}, {"authorId": "47941144", "name": "Hai Zhao"}]}, {"paperId": "ceed352d2d5c0d29264f5c81328bfc92dee6ce5e", "externalIds": {"DBLP": "conf/acl/WibowoNAFAPW21", "ACL": "2021.findings-acl.280", "DOI": "10.18653/v1/2021.findings-acl.280", "CorpusId": 236477980}, "corpusId": 236477980, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ceed352d2d5c0d29264f5c81328bfc92dee6ce5e", "title": "IndoCollex: A Testbed for Morphological Transformation of Indonesian Word Colloquialism", "abstract": "Indonesian language is heavily riddled with colloquialism whether in written or spoken forms. In this paper, we identify a class of Indonesian colloquial words that have undergone morphological transformations from their standard forms, categorize their word formations, and propose a benchmark dataset of Indonesian Colloquial Lexicons (IndoCollex) consisting of informal words on Twitter ex-pertly annotated with their standard forms and their word formation types/tags. We evaluate several models for character-level transduction to perform morphological word normalization on this testbed to understand their failure cases and provide baselines for future work. As IndoCollex catalogues word formation phenomena that are also present in the non-standard text of other languages, it can also provide an attractive testbed for methods tailored for cross-lingual word normalization and non-standard word formation.", "venue": "Findings", "year": 2021, "referenceCount": 68, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3170-3183"}, "authors": [{"authorId": "49918371", "name": "Haryo Akbarianto Wibowo"}, {"authorId": "66436856", "name": "Made Nindyatama Nityasya"}, {"authorId": "1754469865", "name": "Afra Feyza Aky\u00fcrek"}, {"authorId": "2121370457", "name": "Suci Fitriany"}, {"authorId": "8129718", "name": "Alham Fikri Aji"}, {"authorId": "2368148", "name": "Radityo Eko Prasojo"}, {"authorId": "2129412", "name": "D. Wijaya"}]}, {"paperId": "e327186b75e8359ea10c7258c2fdcf93c437e2a5", "externalIds": {"DBLP": "conf/acl/ChenFZCH21", "ACL": "2021.findings-acl.281", "DOI": "10.18653/v1/2021.findings-acl.281", "CorpusId": 236477804}, "corpusId": 236477804, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e327186b75e8359ea10c7258c2fdcf93c437e2a5", "title": "Manifold Adversarial Augmentation for Neural Machine Translation", "abstract": "Improving the robustness of neural machine translation models on variations of input sentences is an active area of research. In this paper, we propose a simple data augmentation approach by sampling virtual sentences from the vicinity distributions in higher-level representations, constructed either from individual training samples via adversarial learning or pairs of training samples through mixup. By simplifying and extending previous work that operates at the token level, our method can construct virtual training samples in a broader space and achieve improved translation accuracy compared to the previous stateof-the-art. In addition, we present a simple variation of the mixup strategy to better utilize the pseudo training samples created from backtranslation, obtaining further improvement in performance.", "venue": "Findings", "year": 2021, "referenceCount": 18, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.281.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3184-3189"}, "authors": [{"authorId": "23121526", "name": "Guandan Chen"}, {"authorId": "2053962725", "name": "Kai Fan"}, {"authorId": "2153280666", "name": "Kaibo Zhang"}, {"authorId": "2152687324", "name": "Boxing Chen"}, {"authorId": "2109670639", "name": "Zhongqiang Huang"}]}, {"paperId": "5ae2ef93ed93e4b11ed096c570448623f9a31f80", "externalIds": {"ArXiv": "2106.07343", "DBLP": "conf/acl/HouLCCL21", "ACL": "2021.findings-acl.282", "DOI": "10.18653/v1/2021.findings-acl.282", "CorpusId": 235422049}, "corpusId": 235422049, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5ae2ef93ed93e4b11ed096c570448623f9a31f80", "title": "Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling", "abstract": "In this paper, we investigate few-shot joint learning for dialogue language understanding. Most existing few-shot models learn a single task each time with only a few examples. However, dialogue language understanding contains two closely related tasks, i.e., intent detection and slot filling, and often benefits from jointly learning the two tasks. This calls for new few-shot learning techniques that are able to capture task relations from only a few examples and jointly learn multiple tasks. To achieve this, we propose a similarity-based few-shot learning scheme, named Contrastive Prototype Merging network (ConProm), that learns to bridge metric spaces of intent and slot on data-rich domains, and then adapt the bridged metric space to the specific few-shot domain. Experiments on two public datasets, Snips and FewJoint, show that our model significantly outperforms the strong baselines in one and five shots settings.", "venue": "Findings", "year": 2021, "referenceCount": 49, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.282.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-25", "journal": {"name": "ArXiv", "volume": "abs/2106.07343"}, "authors": [{"authorId": "8471176", "name": "Yutai Hou"}, {"authorId": "30538619", "name": "Y. Lai"}, {"authorId": "40262099", "name": "Cheng Chen"}, {"authorId": "2256319", "name": "Wanxiang Che"}, {"authorId": "40282288", "name": "Ting Liu"}]}, {"paperId": "b8cba3c5880a8ad29259408ef67fbdc6b41d2027", "externalIds": {"ACL": "2021.findings-acl.283", "DBLP": "conf/acl/LukovnikovF21", "DOI": "10.18653/v1/2021.findings-acl.283", "CorpusId": 236477942}, "corpusId": 236477942, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b8cba3c5880a8ad29259408ef67fbdc6b41d2027", "title": "Insertion-based Tree Decoding", "abstract": "Sequences are typically decoded in a left-to-right fashion, requiring as many decoding steps as there are tokens in the sequence. Recently, several works have proposed non-autoregressive decoders that are sub-linear, al-lowing to decode a sequence using fewer decoding steps than the length of the sequence, and thus substantially speed up inference. In contrast, non-autoregressive decoding of trees is less well-analysed, even though trees are used in important applications like semantic parsing and code generation. In this work, we present a novel general-purpose partially autoregressive tree decoder that uses tree-based insertion operations to generate trees in sub-linear time. We evaluate our approach on semantic parsing and compare it against strong baselines, including an insertion-based sequence decoder. The results demonstrate that the partially autoregressive tree decoder reaches competitive accuracies while clearly reducing the number of decoding steps.", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3201-3213"}, "authors": [{"authorId": "1981679", "name": "Denis Lukovnikov"}, {"authorId": "35988982", "name": "Asja Fischer"}]}, {"paperId": "5b6ef4a959f40894101fc98305079bc3603ea265", "externalIds": {"ACL": "2021.findings-acl.284", "DBLP": "conf/acl/HansenS21", "DOI": "10.18653/v1/2021.findings-acl.284", "CorpusId": 236478036}, "corpusId": 236478036, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5b6ef4a959f40894101fc98305079bc3603ea265", "title": "Is the Lottery Fair? Evaluating Winning Tickets Across Demographics", "abstract": "Recent studies have suggested that weight pruning, e.g. using lottery ticket extraction techniques (Frankle and Carbin, 2018), comes at the risk of compromising the group fairness of machine learning models (Paganini, 2020; Hooker et al., 2020), but to the best of our knowledge, no one has empirically evaluated this hypothesis at scale in the context of natural language processing. We present experiments with two text classi\ufb01cation datasets annotated with demographic information: the Trustpilot Corpus (sentiment) and CivilComments (toxicity). We evaluate the fairness of lottery ticket extraction through layer-wise and global weight pruning across three lan-guages and two tasks. Our results suggest that there is a small increase in group disparity, which is most pronounced at high pruning rates and correlates with instability. The fairness of models trained with distributionally robust optimization objectives is sometimes less sensitive to pruning, but results are not consistent. The code for our experiments is available at https://github. com/vpetren/fairness_lottery .", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3214-3224"}, "authors": [{"authorId": "148293273", "name": "Victor Petr\u00e9n Bach Hansen"}, {"authorId": "1700187", "name": "Anders S\u00f8gaard"}]}, {"paperId": "47ad38b92b843ef2521db9a650038d7d00e4e067", "externalIds": {"DBLP": "journals/corr/abs-2106-08062", "ACL": "2021.findings-acl.285", "ArXiv": "2106.08062", "DOI": "10.18653/v1/2021.findings-acl.285", "CorpusId": 235436032}, "corpusId": 235436032, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/47ad38b92b843ef2521db9a650038d7d00e4e067", "title": "SSMix: Saliency-Based Span Mixup for Text Classification", "abstract": "Data augmentation with mixup has shown to be effective on various computer vision tasks. Despite its great success, there has been a hurdle to apply mixup to NLP tasks since text consists of discrete tokens with variable length. In this work, we propose SSMix, a novel mixup method where the operation is performed on input text rather than on hidden vectors like previous approaches. SSMix synthesizes a sentence while preserving the locality of two original texts by span-based mixing and keeping more tokens related to the prediction relying on saliency information. With extensive experiments, we empirically validate that our method outperforms hidden-level mixup methods on a wide range of text classification benchmarks, including textual entailment, sentiment classification, and question-type classification. Our code is available at https://github.com/clovaai/ssmix.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 39, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.285.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-15", "journal": {"name": "ArXiv", "volume": "abs/2106.08062"}, "authors": [{"authorId": "47948265", "name": "Soyoung Yoon"}, {"authorId": "11798271", "name": "Gyuwan Kim"}, {"authorId": "2152042873", "name": "Kyumin Park"}]}, {"paperId": "344e5e3492cb4ab3622be60bc3284368b3dbbb62", "externalIds": {"DBLP": "conf/acl/BhattR21", "ACL": "2021.findings-acl.286", "ArXiv": "2106.01170", "DOI": "10.18653/v1/2021.findings-acl.286", "CorpusId": 235294145}, "corpusId": 235294145, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/344e5e3492cb4ab3622be60bc3284368b3dbbb62", "title": "Detecting Bot-Generated Text by Characterizing Linguistic Accommodation in Human-Bot Interactions", "abstract": "Language generation models' democratization benefits many domains, from answering health-related questions to enhancing education by providing AI-driven tutoring services. However, language generation models' democratization also makes it easier to generate human-like text at-scale for nefarious activities, from spreading misinformation to targeting specific groups with hate speech. Thus, it is essential to understand how people interact with bots and develop methods to detect bot-generated text. This paper shows that bot-generated text detection methods are more robust across datasets and models if we use information about how people respond to it rather than using the bot's text directly. We also analyze linguistic alignment, providing insight into differences between human-human and human-bot conversations.", "venue": "Findings", "year": 2021, "referenceCount": 55, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.286.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "3235-3247"}, "authors": [{"authorId": "152575655", "name": "Paras Bhatt"}, {"authorId": "26355137", "name": "Anthony Rios"}]}, {"paperId": "1c2e60e0e31d06c909ddffbb2387987b449aceb0", "externalIds": {"DBLP": "conf/acl/BaoWZ21", "ACL": "2021.findings-acl.287", "ArXiv": "2105.14553", "DOI": "10.18653/v1/2021.findings-acl.287", "CorpusId": 235254289}, "corpusId": 235254289, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1c2e60e0e31d06c909ddffbb2387987b449aceb0", "title": "Defending Pre-trained Language Models from Adversarial Word Substitution Without Performance Sacrifice", "abstract": "Pre-trained contextualized language models (PrLMs) have led to strong performance gains in downstream natural language understanding tasks. However, PrLMs can still be easily fooled by adversarial word substitution, which is one of the most challenging textual adversarial attack methods. Existing defence approaches suffer from notable performance loss and complexities. Thus, this paper presents a compact and performance-preserved framework, Anomaly Detection with Frequency-Aware Randomization (ADFAR). In detail, we design an auxiliary anomaly detection classifier and adopt a multi-task learning procedure, by which PrLMs are able to distinguish adversarial input samples. Then, in order to defend adversarial word substitution, a frequency-aware randomization process is applied to those recognized adversarial input samples. Empirical results show that ADFAR significantly outperforms those newly proposed defense methods over various tasks with much higher inference speed. Remarkably, ADFAR does not impair the overall performance of PrLMs. The code is available at https://github.com/LilyNLP/ADFAR", "venue": "Findings", "year": 2021, "referenceCount": 47, "citationCount": 23, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.287.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-30", "journal": {"pages": "3248-3258"}, "authors": [{"authorId": "2043235048", "name": "Rongzhou Bao"}, {"authorId": "1519289435", "name": "Jiayi Wang"}, {"authorId": "153716230", "name": "Hai Zhao"}]}, {"paperId": "d8bf331ad447b93160067613f4283d59837d1a33", "externalIds": {"ACL": "2021.findings-acl.288", "DBLP": "conf/acl/Coavoux21", "DOI": "10.18653/v1/2021.findings-acl.288", "CorpusId": 236477357}, "corpusId": 236477357, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d8bf331ad447b93160067613f4283d59837d1a33", "title": "BERT-Proof Syntactic Structures: Investigating Errors in Discontinuous Constituency Parsing", "abstract": "The combined use of neural scoring systems and BERT \ufb01ne-tuning has led to very high results in many natural language processing (NLP) tasks. These high results raise two important questions about the contribution and the limitations of pretrained-language models: (i) what are the remaining errors in the best-performing systems? (ii) what are the types of test examples where pretrained language models help the most? In this paper, we investigate both questions for the task of English discontinuous constituency parsing on the Penn Tree-bank, for which recent models obtain close to 95 F 1 score. To do so, we propose two meth-ods for automatically analysing the errors of discontinuous parser. First, we annotate and release a test-suite focused on the syntactic phenomena responsible for discontinuities in the Penn Treebank, enabling us to obtain a per-phenomenon evaluation of a parser\u2019s output. Second, we extend the Berkeley Parser Analyser \u2014 a tool that classi\ufb01es parsing errors according to prede\ufb01ned structural patterns \u2014, to discontinuous trees. We apply both methods to characterize errors of a state-of-the-art transition-based discontinuous parser, and to provide an overview of the contribution of BERT to this task.", "venue": "Findings", "year": 2021, "referenceCount": 56, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.288.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "journal": {"pages": "3259-3272"}, "authors": [{"authorId": "3443469", "name": "Maximin Coavoux"}]}, {"paperId": "42ce2dd46dac1be5607d080ceb2a07128c7c95bb", "externalIds": {"ArXiv": "2106.00479", "DBLP": "journals/corr/abs-2106-00479", "ACL": "2021.findings-acl.289", "DOI": "10.18653/v1/2021.findings-acl.289", "CorpusId": 235265761}, "corpusId": 235265761, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/42ce2dd46dac1be5607d080ceb2a07128c7c95bb", "title": "DoT: An efficient Double Transformer for NLP tasks with tables", "abstract": "Transformer-based approaches have been successfully used to obtain state-of-the-art accuracy on natural language processing (NLP) tasks with semi-structured tables. These model architectures are typically deep, resulting in slow training and inference, especially for long inputs. To improve efficiency while maintaining a high accuracy, we propose a new architecture, DoT, a double transformer model, that decomposes the problem into two sub-tasks: A shallow pruning transformer that selects the top-K tokens, followed by a deep task-specific transformer that takes as input those K tokens. Additionally, we modify the task-specific attention to incorporate the pruning scores. The two transformers are jointly trained by optimizing the task-specific loss. We run experiments on three benchmarks, including entailment and question-answering. We show that for a small drop of accuracy, DoT improves training and inference time by at least 50%. We also show that the pruning transformer effectively selects relevant tokens enabling the end-to-end model to maintain similar accuracy as slower baseline models. Finally, we analyse the pruning and give some insight into its impact on the task model.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 8, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.289.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-01", "journal": {"pages": "3273-3283"}, "authors": [{"authorId": "48876302", "name": "Syrine Krichene"}, {"authorId": "40150608", "name": "Thomas M\u00fcller"}, {"authorId": "117595858", "name": "Julian Martin Eisenschlos"}]}, {"paperId": "24467da89797924cc0fb3931184c17c25b472b37", "externalIds": {"DBLP": "conf/acl/ParnowLZ21", "ACL": "2021.findings-acl.290", "ArXiv": "2105.14209", "DOI": "10.18653/v1/2021.findings-acl.290", "CorpusId": 235254145}, "corpusId": 235254145, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/24467da89797924cc0fb3931184c17c25b472b37", "title": "Grammatical Error Correction as GAN-like Sequence Labeling", "abstract": "In Grammatical Error Correction (GEC), sequence labeling models enjoy fast inference compared to sequence-to-sequence models; however, inference in sequence labeling GEC models is an iterative process, as sentences are passed to the model for multiple rounds of correction, which exposes the model to sentences with progressively fewer errors at each round. Traditional GEC models learn from sentences with fixed error rates. Coupling this with the iterative correction process causes a mismatch between training and inference that affects final performance. In order to address this mismatch, we propose a GAN-like sequence labeling model, which consists of a grammatical error detector as a discriminator and a grammatical error labeler with Gumbel-Softmax sampling as a generator. By sampling from real error distributions, our errors are more genuine compared to traditional synthesized GEC errors, thus alleviating the aforementioned mismatch and allowing for better training. Our results on several evaluation benchmarks demonstrate that our proposed approach is effective and improves the previous state-of-the-art baseline.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 9, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.290.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-29", "journal": {"pages": "3284-3290"}, "authors": [{"authorId": "1400348703", "name": "Kevin Parnow"}, {"authorId": "2109645239", "name": "Zuchao Li"}, {"authorId": "153716230", "name": "Hai Zhao"}]}, {"paperId": "79b7ab5c51f2d188eb5912137e6ffe1732639535", "externalIds": {"DBLP": "journals/corr/abs-2105-13225", "ACL": "2021.findings-acl.291", "ArXiv": "2105.13225", "DOI": "10.18653/v1/2021.findings-acl.291", "CorpusId": 235212555}, "corpusId": 235212555, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/79b7ab5c51f2d188eb5912137e6ffe1732639535", "title": "Neural Entity Recognition with Gazetteer based Fusion", "abstract": "Incorporating external knowledge into Named Entity Recognition (NER) systems has been widely studied in the generic domain. In this paper, we focus on clinical domain where only limited data is accessible and interpretability is important. Recent advancement in technology and the acceleration of clinical trials has resulted in the discovery of new drugs, procedures as well as medical conditions. These factors motivate towards building robust zero-shot NER systems which can quickly adapt to new medical terminology. We propose an auxiliary gazetteer model and fuse it with an NER system, which results in better robustness and interpretability across different clinical datasets. Our gazetteer based fusion model is data efficient, achieving +1.7 micro-F1 gains on the i2b2 dataset using 20% training data, and brings + 4.7 micro-F1 gains on novel entity mentions never presented during training. Moreover, our fusion model is able to quickly adapt to new mentions in gazetteers without re-training and the gains from the proposed fusion model are transferable to related datasets.", "venue": "Findings", "year": 2021, "referenceCount": 17, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.291.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-27", "journal": {"pages": "3291-3295"}, "authors": [{"authorId": "144207643", "name": "Q. Sun"}, {"authorId": "50339091", "name": "Parminder Bhatia"}]}, {"paperId": "ca763f4dedeaa932ea2732ed090da3df4a148449", "externalIds": {"ACL": "2021.findings-acl.292", "DBLP": "conf/acl/MontellaRH21", "ArXiv": "2106.04311", "DOI": "10.18653/v1/2021.findings-acl.292", "CorpusId": 235368186}, "corpusId": 235368186, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ca763f4dedeaa932ea2732ed090da3df4a148449", "title": "Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time Curvatures", "abstract": "Knowledge Graph (KG) completion has been excessively studied with a massive number of models proposed for the Link Prediction (LP) task. The main limitation of such models is their insensitivity to time. Indeed, the temporal aspect of stored facts is often ignored. To this end, more and more works consider time as a parameter to complete KGs. In this paper, we first demonstrate that, by simply increasing the number of negative samples, the recent AttH model can achieve competitive or even better performance than the state-of-the-art on Temporal KGs (TKGs), albeit its nontemporality. We further propose Hercules, a time-aware extension of AttH model, which defines the curvature of a Riemannian manifold as the product of both relation and time. Our experiments show that both Hercules and AttH achieve competitive or new state-of-the-art performances on ICEWS04 and ICEWS05-15 datasets. Therefore, one should raise awareness when learning TKGs representations to identify whether time truly boosts performances.", "venue": "Findings", "year": 2021, "referenceCount": 43, "citationCount": 12, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.292.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-08", "journal": {"pages": "3296-3308"}, "authors": [{"authorId": "46185285", "name": "S\u00e9bastien Montella"}, {"authorId": "1388702112", "name": "L. Rojas-Barahona"}, {"authorId": "1686239", "name": "Johannes Heinecke"}]}, {"paperId": "1662b96f9ad4a51a843dd6540ddc62e6d59fb03c", "externalIds": {"ACL": "2021.findings-acl.293", "DBLP": "conf/acl/GuptaXUYF21", "ArXiv": "2106.04016", "DOI": "10.18653/v1/2021.findings-acl.293", "CorpusId": 235368330}, "corpusId": 235368330, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1662b96f9ad4a51a843dd6540ddc62e6d59fb03c", "title": "Disfl-QA: A Benchmark Dataset for Understanding Disfluencies in Question Answering", "abstract": "Disfluencies is an under-studied topic in NLP, even though it is ubiquitous in human conversation. This is largely due to the lack of datasets containing disfluencies. In this paper, we present a new challenge question answering dataset, Disfl-QA, a derivative of SQuAD, where humans introduce contextual disfluencies in previously fluent questions. Disfl-QA contains a variety of challenging disfluencies that require a more comprehensive understanding of the text than what was necessary in prior datasets. Experiments show that the performance of existing state-of-the-art question answering models degrades significantly when tested on Disfl-QA in a zero-shot setting.We show data augmentation methods partially recover the loss in performance and also demonstrate the efficacy of using gold data for fine-tuning. We argue that we need large-scale disfluency datasets in order for NLP models to be robust to them. The dataset is publicly available at: https://github.com/google-research-datasets/disfl-qa.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 23, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.293.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-08", "journal": {"name": "ArXiv", "volume": "abs/2106.04016"}, "authors": [{"authorId": "2121241705", "name": "Aditya Gupta"}, {"authorId": "2121525724", "name": "Jiacheng Xu"}, {"authorId": "33145619", "name": "Shyam Upadhyay"}, {"authorId": "2022168", "name": "Diyi Yang"}, {"authorId": "1779225", "name": "Manaal Faruqui"}]}, {"paperId": "c5221a553d47de08ef507eaac019d933afd07eda", "externalIds": {"ArXiv": "2106.10826", "DBLP": "journals/corr/abs-2106-10826", "ACL": "2021.findings-acl.294", "DOI": "10.18653/v1/2021.findings-acl.294", "CorpusId": 235489989}, "corpusId": 235489989, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c5221a553d47de08ef507eaac019d933afd07eda", "title": "Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification", "abstract": "Existing bias mitigation methods to reduce disparities in model outcomes across cohorts have focused on data augmentation, debiasing model embeddings, or adding fairness-based optimization objectives during training. Separately, certified word substitution robustness methods have been developed to decrease the impact of spurious features and synonym substitutions on model predictions. While their end goals are different, they both aim to encourage models to make the same prediction for certain changes in the input. In this paper, we investigate the utility of certified word substitution robustness methods to improve equality of odds and equality of opportunity on multiple text classification tasks. We observe that certified robustness methods improve fairness, and using both robustness and bias mitigation methods in training results in an improvement in both fronts", "venue": "Findings", "year": 2021, "referenceCount": 53, "citationCount": 23, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.294.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-21", "journal": {"name": "ArXiv", "volume": "abs/2106.10826"}, "authors": [{"authorId": "100984698", "name": "Yada Pruksachatkun"}, {"authorId": "1387484410", "name": "Satyapriya Krishna"}, {"authorId": "3475586", "name": "J. Dhamala"}, {"authorId": "2139538015", "name": "Rahul Gupta"}, {"authorId": "2110821190", "name": "Kai Wei Chang"}]}, {"paperId": "d7a3fee762555b7adfaec2d0ec157cc1fbded891", "externalIds": {"ACL": "2021.findings-acl.295", "DBLP": "conf/acl/DaiH21", "DOI": "10.18653/v1/2021.findings-acl.295", "CorpusId": 236477470}, "corpusId": 236477470, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d7a3fee762555b7adfaec2d0ec157cc1fbded891", "title": "A Joint Model for Structure-based News Genre Classification with Application to Text Summarization", "abstract": "Journalists usually organize and present the contents of a news article following a welldefined structure. In this paper, we propose a novel joint model for structure-based news genre classification that simultaneously identifies one of four commonly used news structures (including Inverted Pyramid and three other structures) for a news article as well as recognizes a sequence of news elements within the article that define the corresponding news structure. Experiments show that the joint model consistently outperforms its variants that perform two tasks independently, which supports our motivation that preserving the two-way dependencies and constraints between a type of news structure and its sequence of news elements enables the model to better predict both of them. Although being not perfect, the system predicted news structure type and news elements have improved the performance of text summarization when incorporated into a recent neural network system.", "venue": "Findings", "year": 2021, "referenceCount": 39, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3332-3342"}, "authors": [{"authorId": "22194540", "name": "Zeyu Dai"}, {"authorId": "1455107421", "name": "Ruihong Huang"}]}, {"paperId": "e5d31a0b7650bf7e9e85f6c7c7bcf5d898df8d59", "externalIds": {"ArXiv": "2106.01904", "DBLP": "conf/acl/BertoliniWWP21", "ACL": "2021.findings-acl.296", "DOI": "10.18653/v1/2021.findings-acl.296", "CorpusId": 235313434}, "corpusId": 235313434, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e5d31a0b7650bf7e9e85f6c7c7bcf5d898df8d59", "title": "Representing Syntax and Composition with Geometric Transformations", "abstract": "The exploitation of syntactic graphs (SyGs) as a word's context has been shown to be beneficial for distributional semantic models (DSMs), both at the level of individual word representations and in deriving phrasal representations via composition. However, notwithstanding the potential performance benefit, the syntactically-aware DSMs proposed to date have huge numbers of parameters (compared to conventional DSMs) and suffer from data sparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic relations) has been largely limited to linear maps. The knowledge graphs' literature, on the other hand, has proposed light-weight models employing different geometric transformations (GTs) to encode edges in a knowledge graph (KG). Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation.", "venue": "Findings", "year": 2021, "referenceCount": 42, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.296.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"pages": "3343-3353"}, "authors": [{"authorId": "48582037", "name": "Lorenzo Bertolini"}, {"authorId": "2500077", "name": "Julie Weeds"}, {"authorId": "35258592", "name": "David J. Weir"}, {"authorId": "1884835955", "name": "Qiwei Peng"}]}, {"paperId": "bf10dec914620deba8c25c1aa37ad97a0fd437e0", "externalIds": {"DBLP": "conf/acl/ChakrabartyGPM21", "ACL": "2021.findings-acl.297", "ArXiv": "2106.01195", "DOI": "10.18653/v1/2021.findings-acl.297", "CorpusId": 235293926}, "corpusId": 235293926, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/bf10dec914620deba8c25c1aa37ad97a0fd437e0", "title": "Figurative Language in Recognizing Textual Entailment", "abstract": "We introduce a collection of recognizing textual entailment (RTE) datasets focused on figurative language. We leverage five existing datasets annotated for a variety of figurative language -- simile, metaphor, and irony -- and frame them into over 12,500 RTE examples.We evaluate how well state-of-the-art models trained on popular RTE datasets capture different aspects of figurative language. Our results and analyses indicate that these models might not sufficiently capture figurative language, struggling to perform pragmatic inference and reasoning about world knowledge. Ultimately, our datasets provide a challenging testbed for evaluating RTE models.", "venue": "Findings", "year": 2021, "referenceCount": 51, "citationCount": 22, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.297.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01195"}, "authors": [{"authorId": "51448832", "name": "Tuhin Chakrabarty"}, {"authorId": "39013305", "name": "Debanjan Ghosh"}, {"authorId": "48926630", "name": "Adam Poliak"}, {"authorId": "2295928", "name": "S. Muresan"}]}, {"paperId": "c513260095193c918419deb89c491aa19ab8491d", "externalIds": {"DBLP": "journals/corr/abs-2106-01581", "ArXiv": "2106.01581", "ACL": "2021.findings-acl.298", "DOI": "10.18653/v1/2021.findings-acl.298", "CorpusId": 235313949}, "corpusId": 235313949, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c513260095193c918419deb89c491aa19ab8491d", "title": "To Point or Not to Point: Understanding How Abstractive Summarizers Paraphrase Text", "abstract": "Abstractive neural summarization models have seen great improvements in recent years, as shown by ROUGE scores of the generated summaries. But despite these improved metrics, there is limited understanding of the strategies different models employ, and how those strategies relate their understanding of language. To understand this better, we run several experiments to characterize how one popular abstractive model, the pointer-generator model of See et al. (2017), uses its explicit copy/generation switch to control its level of abstraction (generation) vs extraction (copying). On an extractive-biased dataset, the model utilizes syntactic boundaries to truncate sentences that are otherwise often copied verbatim. When we modify the copy/generation switch and force the model to generate, only simple paraphrasing abilities are revealed alongside factual inaccuracies and hallucinations. On an abstractive-biased dataset, the model copies infrequently but shows similarly limited abstractive abilities. In line with previous research, these results suggest that abstractive summarization models lack the semantic understanding necessary to generate paraphrases that are both abstractive and faithful to the source document.", "venue": "Findings", "year": 2021, "referenceCount": 21, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.298.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01581"}, "authors": [{"authorId": "153448954", "name": "Matthew Wilber"}, {"authorId": "2106715510", "name": "William Timkey"}, {"authorId": "3220165", "name": "Marten van Schijndel"}]}, {"paperId": "f87a921fe6984953634356b5cd895cc43dbdd2c9", "externalIds": {"ACL": "2021.findings-acl.299", "DBLP": "conf/acl/PangLTY21", "ArXiv": "2106.02278", "DOI": "10.18653/v1/2021.findings-acl.299", "CorpusId": 235352668}, "corpusId": 235352668, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f87a921fe6984953634356b5cd895cc43dbdd2c9", "title": "AgreeSum: Agreement-Oriented Multi-Document Summarization", "abstract": "We aim to renew interest in a particular multi-document summarization (MDS) task which we call AgreeSum: agreement-oriented multi-document summarization. Given a cluster of articles, the goal is to provide abstractive summaries that represent information common and faithful to all input articles. Given the lack of existing datasets, we create a dataset for AgreeSum, and provide annotations on article-summary entailment relations for a subset of the clusters in the dataset. We aim to create strong baselines for the task by applying the top-performing pretrained single-document summarization model PEGASUS onto AgreeSum, leveraging both annotated clusters by supervised losses, and unannotated clusters by T5-based entailment-related and language-related losses. Compared to other baselines, both automatic evaluation and human evaluation show better article-summary and cluster-summary entailment in generated summaries. On a separate note, we hope that our article-summary entailment annotations contribute to the community's effort in improving abstractive summarization faithfulness.", "venue": "Findings", "year": 2021, "referenceCount": 55, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.299.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"pages": "3377-3391"}, "authors": [{"authorId": "46230016", "name": "Richard Yuanzhe Pang"}, {"authorId": "143828990", "name": "\u00c1. Lelkes"}, {"authorId": "2057663102", "name": "Vinh Q. Tran"}, {"authorId": "82737548", "name": "Cong Yu"}]}, {"paperId": "5a09edeb26f9f116f2c0503cd020f38fb943f79b", "externalIds": {"ArXiv": "2105.06990", "ACL": "2021.findings-acl.300", "DBLP": "conf/acl/KovalevaKRR21", "DOI": "10.18653/v1/2021.findings-acl.300", "CorpusId": 235313996}, "corpusId": 235313996, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5a09edeb26f9f116f2c0503cd020f38fb943f79b", "title": "BERT Busters: Outlier Dimensions that Disrupt Transformers", "abstract": "Multiple studies have shown that Transformers are remarkably robust to pruning. Contrary to this received wisdom, we demonstrate that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs (<0.0001% of model weights). In case of BERT and other pre-trained encoder Transformers, the affected component is the scaling factors and biases in the LayerNorm. The outliers are high-magnitude normalization parameters that emerge early in pre-training and show up consistently in the same dimensional position throughout the model. We show that disabling them significantly degrades both the MLM loss and the downstream task performance. This effect is observed across several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet and ELECTRA; we also show a similar effect in GPT-2.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 37, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.300.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Physics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-14", "journal": {"pages": "3392-3405"}, "authors": [{"authorId": "152176221", "name": "Olga Kovaleva"}, {"authorId": "2066256691", "name": "Saurabh Kulshreshtha"}, {"authorId": "145046059", "name": "Anna Rogers"}, {"authorId": "1681193", "name": "Anna Rumshisky"}]}, {"paperId": "7eaad1f5b126839acb60b4bd2b91ca4b53d4aac0", "externalIds": {"ACL": "2021.findings-acl.301", "DBLP": "conf/acl/FornaciariHNRTA21", "DOI": "10.18653/v1/2021.findings-acl.301", "CorpusId": 236477967}, "corpusId": 236477967, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7eaad1f5b126839acb60b4bd2b91ca4b53d4aac0", "title": "\u201cWe will Reduce Taxes\u201d - Identifying Election Pledges with Language Models", "abstract": "In an election campaign, political parties pledge to implement various projects\u2013should they be elected. But do they follow through? To track election pledges from parties\u2019 election manifestos, we need to distinguish between pledges and general statements. In this paper, we use election manifestos of Swedish and Indian political parties to learn neural models that distinguish actual pledges from generic political positions. Since pledges might vary by election year and party, we implement a Multi-Task Learning (MTL) setup, predicting election year and manifesto\u2019s party as auxiliary tasks. Pledges can also span several sentences, so we use hierarchical models that incorporate contextual information. Lastly, we evaluate the models in a Zero-Shot Learning (ZSL) framework across countries and languages. Our results indicate that year and party have predictive power even in ZSL, while context introduces some noise. We finally discuss the linguistic features of pledges.", "venue": "Findings", "year": 2021, "referenceCount": 24, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3406-3419"}, "authors": [{"authorId": "2223077", "name": "Tommaso Fornaciari"}, {"authorId": "2022288", "name": "Dirk Hovy"}, {"authorId": "116684424", "name": "Elin Naurin"}, {"authorId": "2121365872", "name": "Julia Runeson"}, {"authorId": "145862945", "name": "R. Thomson"}, {"authorId": "2121366731", "name": "Pankaj Adhikari"}]}, {"paperId": "1a575075ba357723009a9a8905d5dccf9115ae6c", "externalIds": {"DBLP": "conf/acl/BanerjeeGYB21", "ACL": "2021.findings-acl.302", "ArXiv": "2012.02356", "DOI": "10.18653/v1/2021.findings-acl.302", "CorpusId": 235262536}, "corpusId": 235262536, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1a575075ba357723009a9a8905d5dccf9115ae6c", "title": "WeaQA: Weak Supervision via Captions for Visual Question Answering", "abstract": "Methodologies for training visual question answering (VQA) models assume the availability of datasets with human-annotated \\textit{Image-Question-Answer} (I-Q-A) triplets. This has led to heavy reliance on datasets and a lack of generalization to new types of questions and scenes. Linguistic priors along with biases and errors due to annotator subjectivity have been shown to percolate into VQA models trained on such samples. We study whether models can be trained without any human-annotated Q-A pairs, but only with images and their associated textual descriptions or captions. We present a method to train models with synthetic Q-A pairs generated procedurally from captions. Additionally, we demonstrate the efficacy of spatial-pyramid image patches as a simple but effective alternative to dense and costly object bounding box annotations used in existing VQA models. Our experiments on three VQA benchmarks demonstrate the efficacy of this weakly-supervised approach, especially on the VQA-CP challenge, which tests performance under changing linguistic priors.", "venue": "Findings", "year": 2020, "referenceCount": 96, "citationCount": 22, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.302.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-04", "journal": {"pages": "3420-3435"}, "authors": [{"authorId": "120722271", "name": "Pratyay Banerjee"}, {"authorId": "120838645", "name": "Tejas Gokhale"}, {"authorId": "1784500", "name": "Yezhou Yang"}, {"authorId": "2064619864", "name": "Chitta Baral"}]}, {"paperId": "15bb07d0996ece844de8cae24d3dc15972e6841a", "externalIds": {"DBLP": "journals/corr/abs-2106-11388", "ACL": "2021.findings-acl.303", "ArXiv": "2106.11388", "DOI": "10.18653/v1/2021.findings-acl.303", "CorpusId": 235592929}, "corpusId": 235592929, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/15bb07d0996ece844de8cae24d3dc15972e6841a", "title": "How well do you know your summarization datasets?", "abstract": "State-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. Despite their prevalence, we know very little about the underlying characteristics (data noise, summarization complexity, etc.) of these datasets, and how these affect system performance and the reliability of automatic metrics like ROUGE. In this study, we manually analyze 600 samples from three popular summarization datasets. Our study is driven by a six-class typology which captures different noise types (missing facts, entities) and degrees of summarization difficulty (extractive, abstractive). We follow with a thorough analysis of 27 state-of-the-art summarization models and 5 popular metrics, and report our key insights: (1) Datasets have distinct data quality and complexity distributions, which can be traced back to their collection process. (2) The performance of models and reliability of metrics is dependent on sample complexity. (3) Faithful summaries often receive low scores because of the poor diversity of references. We release the code, annotated data and model outputs.", "venue": "Findings", "year": 2021, "referenceCount": 53, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.303.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-21", "journal": {"name": "ArXiv", "volume": "abs/2106.11388"}, "authors": [{"authorId": "3376877", "name": "Priyam Tejaswin"}, {"authorId": "2114226429", "name": "Dhruv Naik"}, {"authorId": "145779142", "name": "Peng Liu"}]}, {"paperId": "b88703f68f4abb9e69b86fb42dff85aa4a76fca4", "externalIds": {"DBLP": "conf/acl/TangTLCGCGF21", "ACL": "2021.findings-acl.304", "DOI": "10.18653/v1/2021.findings-acl.304", "CorpusId": 236477365}, "corpusId": 236477365, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b88703f68f4abb9e69b86fb42dff85aa4a76fca4", "title": "Multilingual Translation from Denoising Pre-Training", "abstract": "Recent work demonstrates the potential of training one model for multilingual machine translation. In parallel, denoising pretraining using unlabeled monolingual data as a starting point for \ufb01netuning bitext machine translation systems has demonstrated strong performance gains. However, little has been explored on the potential to combine denoising pretraining with multilingual machine translation in a single model. In this work, we \ufb01ll this gap by studying how multilingual translation models can be created through multilingual \ufb01netuning . Fintuning multilingual model from a denoising pretrained model incorporates the bene\ufb01ts of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is rare. Further, we create the ML50 benchmark to facilitate re-producible research by standardizing training and evaluation data. On ML50, we show that multilingual \ufb01netuning signi\ufb01cantly improves over multilingual models trained from scratch and bilingual \ufb01netuning for translation into English. We also \ufb01nd that multilingual \ufb01ne-tuning can signi\ufb01cantly improve over multilingual models trained from scratch for zero-shot translation on non-English directions. Finally, we discuss that the pretraining and \ufb01netuning paradigm alone is not enough to address the challenges of multilingual models for to-Many directions performance.", "venue": "Findings", "year": 2021, "referenceCount": 57, "citationCount": 97, "influentialCitationCount": 22, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.304.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3450-3466"}, "authors": [{"authorId": "3036900", "name": "Y. Tang"}, {"authorId": "33806547", "name": "C. Tran"}, {"authorId": "2116235416", "name": "Xian Li"}, {"authorId": "2158170998", "name": "Peng-Jen Chen"}, {"authorId": "39589154", "name": "Naman Goyal"}, {"authorId": "113810201", "name": "Vishrav Chaudhary"}, {"authorId": "3016273", "name": "Jiatao Gu"}, {"authorId": "144270981", "name": "Angela Fan"}]}, {"paperId": "e7b1d326c5a0b0b7f2f886f3f0a58c2cce2e02e9", "externalIds": {"DBLP": "conf/acl/SayyedD21", "ACL": "2021.findings-acl.305", "DOI": "10.18653/v1/2021.findings-acl.305", "CorpusId": 236478365}, "corpusId": 236478365, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e7b1d326c5a0b0b7f2f886f3f0a58c2cce2e02e9", "title": "Annotations Matter: Leveraging Multi-task Learning to Parse UD and SUD", "abstract": "Using multiple treebanks to improve parsing performance has shown positive results. However, to what extent similar, yet competing annotation decisions play in parser behavior is unclear. We investigate this within a multi-task learning (MTL) dependency parser setup on two parallel treebanks, UD and SUD, which, while possessing similar annotation schemes, differ in specific linguistic annotation preferences. We perform a set of experiments with different MTL architectural choices, comparing performance across various input embeddings. We find languages tend to pattern in loose typological associations, but generally the performance within an MTL setting is lower than single model baseline parsers for each annotation scheme. The main contributing factor seems to be the competing syntactic annotation information shared between treebanks in an MTL setting, which is shown in experiments against differently annotated treebanks. This suggests that the impact of how the signal is encoded for annotations and its influence on possible negative transfer is more important than that of the input embeddings in an MTL setting.", "venue": "Findings", "year": 2021, "referenceCount": 60, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3467-3481"}, "authors": [{"authorId": "13737375", "name": "Z. Sayyed"}, {"authorId": "3467352", "name": "D. Dakota"}]}, {"paperId": "4b905b2577962362764d263460034513f417c49e", "externalIds": {"DBLP": "journals/corr/abs-2106-01064", "ArXiv": "2106.01064", "ACL": "2021.findings-acl.306", "DOI": "10.18653/v1/2021.findings-acl.306", "CorpusId": 235294159}, "corpusId": 235294159, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4b905b2577962362764d263460034513f417c49e", "title": "Generating Informative Conclusions for Argumentative Texts", "abstract": "The purpose of an argumentative text is to support a certain conclusion. Yet, they are often omitted, expecting readers to infer them rather. While appropriate when reading an individual text, this rhetorical device limits accessibility when browsing many texts (e.g., on a search engine or on social media). In these scenarios, an explicit conclusion makes for a good candidate summary of an argumentative text. This is especially true if the conclusion is informative, emphasizing specific concepts from the text. With this paper we introduce the task of generating informative conclusions: First, Webis-ConcluGen-21 is compiled, a large-scale corpus of 136,996 samples of argumentative texts and their conclusions. Second, two paradigms for conclusion generation are investigated; one extractive, the other abstractive in nature. The latter exploits argumentative knowledge that augment the data via control codes and finetuning the BART model on several subsets of the corpus. Third, insights are provided into the suitability of our corpus for the task, the differences between the two generation paradigms, the trade-off between informativeness and conciseness, and the impact of encoding argumentative knowledge. The corpus, code, and the trained models are publicly available.", "venue": "Findings", "year": 2021, "referenceCount": 58, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.306.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "3482-3493"}, "authors": [{"authorId": "18417916", "name": "S. Syed"}, {"authorId": "2065425673", "name": "Khalid Al-Khatib"}, {"authorId": "2300829", "name": "Milad Alshomary"}, {"authorId": "2626599", "name": "Henning Wachsmuth"}, {"authorId": "3046200", "name": "Martin Potthast"}]}, {"paperId": "d6357b1c61611f744acbae69484acd7f21c89dff", "externalIds": {"ArXiv": "2101.00411", "DBLP": "journals/corr/abs-2101-00411", "ACL": "2021.findings-acl.307", "DOI": "10.18653/v1/2021.findings-acl.307", "CorpusId": 230435739}, "corpusId": 230435739, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d6357b1c61611f744acbae69484acd7f21c89dff", "title": "Substructure Substitution: Structured Data Augmentation for NLP", "abstract": "We study a family of data augmentation methods, substructure substitution (SUB2), for natural language processing (NLP) tasks. SUB2 generates new examples by substituting substructures (e.g., subtrees or subsequences) with ones with the same label, which can be applied to many structured NLP tasks such as part-of-speech tagging and parsing. For more general tasks (e.g., text classification) which do not have explicitly annotated substructures, we present variations of SUB2 based on constituency parse trees, introducing structure-aware data augmentation methods to general NLP tasks. For most cases, training with the augmented dataset by SUB2 achieves better performance than training with the original training set. Further experiments show that SUB2 has more consistent performance than other investigated augmentation methods, across different tasks and sizes of the seed dataset.", "venue": "Findings", "year": 2021, "referenceCount": 80, "citationCount": 29, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.307.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-01-02", "journal": {"name": "ArXiv", "volume": "abs/2101.00411"}, "authors": [{"authorId": "8815141", "name": "Freda Shi"}, {"authorId": "2924113", "name": "Karen Livescu"}, {"authorId": "1700980", "name": "Kevin Gimpel"}]}, {"paperId": "d7adb567dc176fd7db8e820caf6635482128b9d0", "externalIds": {"DBLP": "conf/acl/LopezYPVCBSHWS21", "ACL": "2021.findings-acl.308", "DOI": "10.18653/v1/2021.findings-acl.308", "CorpusId": 236477598}, "corpusId": 236477598, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d7adb567dc176fd7db8e820caf6635482128b9d0", "title": "Towards Protecting Vital Healthcare Programs by Extracting Actionable Knowledge from Policy", "abstract": "In challenging economic times, obtaining value for money by ensuring financial integrity and fairer distribution of services are among the top priorities for social and health-care systems globally. However, healthcare billing policies are complex and identifying non-compliance is often narrow-scope, manual and expensive. Maintaining \u2018integrity\u2019 is a challenge ensuring that scarce resources get to those in need and are not lost to fraud and waste. Our approach fuses recent advances in dependency parsing with a policy ontology to convert the content of regulatory healthcare policy into human-friendly policy rules, that are amenable to machineexecution, with human oversight. We describe the ontology-guided transformation of textual patterns into a semantically-meaningful knowledge graph of rules, outline our experiments and evaluate results against policy rules obtained from professional investigators. The aim is to make a policy-compliance \u2018landscape\u2019 visible to healthcare programs helping them identify Fraud, Waste or Abuse.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Political Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3509-3521"}, "authors": [{"authorId": "123036674", "name": "Vanessa L\u00f3pez"}, {"authorId": "2028658520", "name": "Nagesh Yadav"}, {"authorId": "46310814", "name": "Gabriele Picco"}, {"authorId": "37372513", "name": "Inge Vejsbjerg"}, {"authorId": "2121346859", "name": "Eoin Carrol"}, {"authorId": "2061980793", "name": "Seamus Brady"}, {"authorId": "2642735", "name": "M. Sbodio"}, {"authorId": "2158062", "name": "Hoang Thanh Lam"}, {"authorId": "2146973896", "name": "Miao Wei"}, {"authorId": "1411223435", "name": "J. Segrave-Daly"}]}, {"paperId": "be4bc115836e0a1cdef82e26a586c9fe26a0d8a5", "externalIds": {"ACL": "2021.findings-acl.309", "DBLP": "conf/acl/KamallooRPG21", "ArXiv": "2105.13608", "DOI": "10.18653/v1/2021.findings-acl.309", "CorpusId": 235248098}, "corpusId": 235248098, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/be4bc115836e0a1cdef82e26a586c9fe26a0d8a5", "title": "Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data Augmentation via MiniMax", "abstract": "In Natural Language Processing (NLP), finding data augmentation techniques that can produce high-quality human-interpretable examples has always been challenging. Recently, leveraging kNN such that augmented examples are retrieved from large repositories of unlabelled sentences has made a step toward interpretable augmentation. Inspired by this paradigm, we introduce Minimax-kNN, a sample efficient data augmentation strategy tailored for Knowledge Distillation (KD). We exploit a semi-supervised approach based on KD to train a model on augmented data. In contrast to existing kNN augmentation techniques that blindly incorporate all samples, our method dynamically selects a subset of augmented samples that maximizes KL-divergence between the teacher and student models. This step aims to extract the most efficient samples to ensure our augmented data covers regions in the input space with maximum loss value. We evaluated our technique on several text classification tasks and demonstrated that Minimax-kNN consistently outperforms strong baselines. Our results show that Minimax-kNN requires fewer augmented examples and less computation to achieve superior performance over the state-of-the-art kNN-based augmentation techniques.", "venue": "Findings", "year": 2021, "referenceCount": 55, "citationCount": 16, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.309.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-28", "journal": {"name": "ArXiv", "volume": "abs/2105.13608"}, "authors": [{"authorId": "2023642", "name": "Ehsan Kamalloo"}, {"authorId": "1924511", "name": "Mehdi Rezagholizadeh"}, {"authorId": "5062230", "name": "P. Passban"}, {"authorId": "10245861", "name": "A. Ghodsi"}]}, {"paperId": "64902a5077ee68011cd467398dbb66511e8e891a", "externalIds": {"ArXiv": "2106.12066", "DBLP": "journals/corr/abs-2106-12066", "ACL": "2021.findings-acl.310", "DOI": "10.18653/v1/2021.findings-acl.310", "CorpusId": 235606305}, "corpusId": 235606305, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/64902a5077ee68011cd467398dbb66511e8e891a", "title": "It\u2019s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning", "abstract": "Commonsense reasoning is one of the key problems in natural language processing, but the relative scarcity of labeled data holds back the progress for languages other than English. Pretrained cross-lingual models are a source of powerful language-agnostic representations, yet their inherent reasoning capabilities are still actively studied. In this work, we design a simple approach to commonsense reasoning which trains a linear classifier with weights of multi-head attention as features. To evaluate this approach, we create a multilingual Winograd Schema corpus by processing several datasets from prior work within a standardized pipeline and measure cross-lingual generalization ability in terms of out-of-sample performance. The method performs competitively with recent supervised and unsupervised approaches for commonsense reasoning, even when applied to other languages in a zero-shot manner. Also, we demonstrate that most of the performance is given by the same small subset of attention heads for all studied languages, which provides evidence of universal reasoning capabilities in multilingual encoders.", "venue": "Findings", "year": 2021, "referenceCount": 49, "citationCount": 18, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.310.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-22", "journal": {"name": "ArXiv", "volume": "abs/2106.12066"}, "authors": [{"authorId": "34501167", "name": "Alexey Tikhonov"}, {"authorId": "1491753352", "name": "Max Ryabinin"}]}, {"paperId": "bfa915760eec185cf7cac0eff56d3b434da91a79", "externalIds": {"ACL": "2021.findings-acl.311", "DBLP": "conf/acl/Garcia-OlanoOBG21", "ArXiv": "2106.09502", "DOI": "10.18653/v1/2021.findings-acl.311", "CorpusId": 235458104}, "corpusId": 235458104, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/bfa915760eec185cf7cac0eff56d3b434da91a79", "title": "Biomedical Interpretable Entity Representations", "abstract": "Pre-trained language models induce dense entity representations that offer strong performance on entity-centric NLP tasks, but such representations are not immediately interpretable. This can be a barrier to model uptake in important domains such as biomedicine. There has been recent work on general interpretable representation learning (Onoe and Durrett, 2020), but these domain-agnostic representations do not readily transfer to the important domain of biomedicine. In this paper, we create a new entity type system and training set from a large corpus of biomedical texts by mapping entities to concepts in a medical ontology, and from these to Wikipedia pages whose categories are our types. From this mapping we derive Biomedical Interpretable Entity Representations(BIERs), in which dimensions correspond to fine-grained entity types, and values are predicted probabilities that a given entity is of the corresponding type. We propose a novel method that exploits BIER's final sparse and intermediate dense representations to facilitate model and entity type debugging. We show that BIERs achieve strong performance in biomedical tasks including named entity disambiguation and entity label classification, and we provide error analysis to highlight the utility of their interpretability, particularly in low-supervision settings. Finally, we provide our induced 68K biomedical type system, the corresponding 37 million triples of derived data used to train BIER models and our best performing model.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.311.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-17", "journal": {"name": "ArXiv", "volume": "abs/2106.09502"}, "authors": [{"authorId": "1405390891", "name": "Diego Garcia-Olano"}, {"authorId": "115412405", "name": "Yasumasa Onoe"}, {"authorId": "36942521", "name": "Ioana Baldini"}, {"authorId": "1379535322", "name": "J. Ghosh"}, {"authorId": "1912476", "name": "Byron C. Wallace"}, {"authorId": "1712865", "name": "Kush R. Varshney"}]}, {"paperId": "c83110f84af2cecbd83ef4c49e729c706a6436b2", "externalIds": {"ArXiv": "2105.04458", "DBLP": "conf/acl/KumarPZ21", "ACL": "2021.findings-acl.312", "DOI": "10.18653/v1/2021.findings-acl.312", "CorpusId": 234340177}, "corpusId": 234340177, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c83110f84af2cecbd83ef4c49e729c706a6436b2", "title": "Learning Robust Latent Representations for Controllable Speech Synthesis", "abstract": "State-of-the-art Variational Auto-Encoders (VAEs) for learning disentangled latent representations give impressive results in discovering features like pitch, pause duration, and accent in speech data, leading to highly controllable text-to-speech (TTS) synthesis. However, these LSTM-based VAEs fail to learn latent clusters of speaker attributes when trained on either limited or noisy datasets. Further, different latent variables start encoding the same features, limiting the control and expressiveness during speech synthesis. To resolve these issues, we propose RTI-VAE (Reordered Transformer with Information reduction VAE) where we minimize the mutual information between different latent variables and devise a modified Transformer architecture with layer reordering to learn controllable latent representations in speech data. We show that RTI-VAE reduces the cluster overlap of speaker attributes by at least 30\\% over LSTM-VAE and by at least 7\\% over vanilla Transformer-VAE.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.312.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-10", "journal": {"name": "ArXiv", "volume": "abs/2105.04458"}, "authors": [{"authorId": "2109681523", "name": "Shakti Kumar"}, {"authorId": "51131881", "name": "Jithin Pradeep"}, {"authorId": "1510932771", "name": "Hussain Zaidi"}]}, {"paperId": "996f0d401acd11e95ce5586010e7e4e18f5c3bb9", "externalIds": {"DBLP": "journals/corr/abs-2105-13782", "ACL": "2021.findings-acl.313", "ArXiv": "2105.13782", "DOI": "10.18653/v1/2021.findings-acl.313", "CorpusId": 235248117}, "corpusId": 235248117, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/996f0d401acd11e95ce5586010e7e4e18f5c3bb9", "title": "How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation", "abstract": "Having recognized gender bias as a major issue affecting current translation technologies, researchers have primarily attempted to mitigate it by working on the data front. However, whether algorithmic aspects concur to exacerbate unwanted outputs remains so far under-investigated. In this work, we bring the analysis on gender bias in automatic translation onto a seemingly neutral yet critical component: word segmentation. Can segmenting methods influence the ability to translate gender? Do certain segmentation approaches penalize the representation of feminine linguistic markings? We address these questions by comparing 5 existing segmentation strategies on the target side of speech translation systems. Our results on two language pairs (English-Italian/French) show that state-of-the-art sub-word splitting (BPE) comes at the cost of higher gender bias. In light of this finding, we propose a combined approach that preserves BPE overall translation quality, while leveraging the higher ability of character-based segmentation to properly translate gender.", "venue": "Findings", "year": 2021, "referenceCount": 100, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.313.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-28", "journal": {"pages": "3576-3589"}, "authors": [{"authorId": "1736801422", "name": "Marco Gaido"}, {"authorId": "1741330216", "name": "Beatrice Savoldi"}, {"authorId": "2486762", "name": "L. Bentivogli"}, {"authorId": "2138026", "name": "Matteo Negri"}, {"authorId": "145862931", "name": "M. Turchi"}]}, {"paperId": "1c2c7ca6436ebb3097c17cd14bd374a319ae4f8c", "externalIds": {"DBLP": "conf/acl/TsarapatsanisA21", "ACL": "2021.findings-acl.314", "MAG": "3176964086", "ArXiv": "2105.02751", "DOI": "10.18653/v1/2021.findings-acl.314", "CorpusId": 233864979}, "corpusId": 233864979, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1c2c7ca6436ebb3097c17cd14bd374a319ae4f8c", "title": "On the Ethical Limits of Natural Language Processing on Legal Text", "abstract": "Natural language processing (NLP) methods for analyzing legal text offer legal scholars and practitioners a range of tools allowing to empirically analyze law on a large scale. However, researchers seem to struggle when it comes to identifying ethical limits to using NLP systems for acquiring genuine insights both about the law and the systems' predictive capacity. In this paper we set out a number of ways in which to think systematically about such issues. We place emphasis on three crucial normative parameters which have, to the best of our knowledge, been underestimated by current debates: (a) the importance of academic freedom, (b) the existence of a wide diversity of legal and ethical norms domestically but even more so internationally and (c) the threat of moralism in research related to computational law. For each of these three parameters we provide specific recommendations for the legal NLP community. Our discussion is structured around the study of a real-life scenario that has prompted recent debate in the legal NLP research community.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 26, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.314.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Law", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-06", "journal": {"name": "ArXiv", "volume": "abs/2105.02751"}, "authors": [{"authorId": "8245526", "name": "D. Tsarapatsanis"}, {"authorId": "3238627", "name": "Nikolaos Aletras"}]}, {"paperId": "c301f1a644fc40552ff3cfae63b6afed3ba0779a", "externalIds": {"DBLP": "journals/corr/abs-2105-14888", "ArXiv": "2105.14888", "ACL": "2021.findings-acl.315", "DOI": "10.18653/v1/2021.findings-acl.315", "CorpusId": 235254726}, "corpusId": 235254726, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c301f1a644fc40552ff3cfae63b6afed3ba0779a", "title": "An Exploratory Analysis of the Relation between Offensive Language and Mental Health", "abstract": "In this paper, we analyze the interplay between the use of offensive language and mental health. We acquired publicly available datasets created for offensive language identification and depression detection and we train computational models to compare the use of offensive language in social media posts written by groups of individuals with and without self-reported depression diagnosis. We also look at samples written by groups of individuals whose posts show signs of depression according to recent related studies. Our analysis indicates that offensive language is more frequently used in the samples written by individuals with self-reported depression as well as individuals showing signs of depression. The results discussed here open new avenues in research in politeness/offensiveness and mental health.", "venue": "Findings", "year": 2021, "referenceCount": 51, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.315.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-31", "journal": {"name": "ArXiv", "volume": "abs/2105.14888"}, "authors": [{"authorId": "2007651543", "name": "Ana-Maria Bucur"}, {"authorId": "145130358", "name": "Marcos Zampieri"}, {"authorId": "2467676", "name": "Liviu P. Dinu"}]}, {"paperId": "066f6096e566f0317737a69d3e7d60cb259930d3", "externalIds": {"ACL": "2021.findings-acl.316", "DBLP": "conf/acl/LangWHG21", "DOI": "10.18653/v1/2021.findings-acl.316", "CorpusId": 236478314}, "corpusId": 236478314, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/066f6096e566f0317737a69d3e7d60cb259930d3", "title": "Transforming Term Extraction: Transformer-Based Approaches to Multilingual Term Extraction Across Domains", "abstract": "Automated Term Extraction (ATE), even though well-investigated, continues to be a challenging task. Approaches conventionally extract terms on corpus or document level and the benefits of neural models still remain underexplored with very few exceptions. We introduce three transformer-based term extraction models operating on sentence level: a language model for token classification, one for sequence classification, and an innovative use of Neural Machine Translation (NMT), which learns to reduce sentences to terms. All three models are trained and tested on the dataset of the ATE challenge TermEval 2020 in English, French, and Dutch across four specialized domains. The two best performing approaches are also evaluated on the ACL RD-TEC 2.0 dataset. Our models outperform previous baselines, one of which is BERT-based, by a substantial margin, with the token-classifier language model performing best.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.316.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3607-3620"}, "authors": [{"authorId": "2055383275", "name": "Christian Lang"}, {"authorId": "2029658284", "name": "Lennart Wachowiak"}, {"authorId": "152530262", "name": "B. Heinisch"}, {"authorId": "2640975", "name": "Dagmar Gromann"}]}, {"paperId": "87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1", "externalIds": {"ACL": "2021.findings-acl.317", "DBLP": "conf/acl/TafjordDC21", "ArXiv": "2012.13048", "DOI": "10.18653/v1/2021.findings-acl.317", "CorpusId": 229371222}, "corpusId": 229371222, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1", "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language", "abstract": "Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proof(s) that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.", "venue": "Findings", "year": 2020, "referenceCount": 28, "citationCount": 128, "influentialCitationCount": 33, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.317.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-24", "journal": {"pages": "3621-3634"}, "authors": [{"authorId": "3385516", "name": "Oyvind Tafjord"}, {"authorId": "40135250", "name": "Bhavana Dalvi"}, {"authorId": "48323507", "name": "Peter Clark"}]}, {"paperId": "5c09c7b9d749e7a1f90573b0cfd53606f1038d73", "externalIds": {"DBLP": "journals/corr/abs-2106-09141", "ACL": "2021.findings-acl.318", "ArXiv": "2106.09141", "DOI": "10.18653/v1/2021.findings-acl.318", "CorpusId": 235458570}, "corpusId": 235458570, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5c09c7b9d749e7a1f90573b0cfd53606f1038d73", "title": "Probing Image-Language Transformers for Verb Understanding", "abstract": "Multimodal image-language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval). We are interested in shedding light on the quality of their pretrained representations -- in particular, if these models can distinguish different types of verbs or if they rely solely on nouns in a given sentence. To do so, we collect a dataset of image-sentence pairs (in English) consisting of 421 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset). We use this dataset to evaluate pretrained image-language transformers and find that they fail more in situations that require verb understanding compared to other parts of speech. We also investigate what category of verbs are particularly challenging.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 56, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.318.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-16", "journal": {"pages": "3635-3644"}, "authors": [{"authorId": "2234342", "name": "Lisa Anne Hendricks"}, {"authorId": "3208081", "name": "Aida Nematzadeh"}]}, {"paperId": "90453b0e6f035d741449bc82abbc7ce5cbf915d1", "externalIds": {"DBLP": "conf/acl/RingenbergSR21", "ACL": "2021.findings-acl.319", "DOI": "10.18653/v1/2021.findings-acl.319", "CorpusId": 236477447}, "corpusId": 236477447, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/90453b0e6f035d741449bc82abbc7ce5cbf915d1", "title": "Implications of Using Internet Sting Corpora to Approximate Underage Victims", "abstract": "Law enforcement officers (LEOs) and the justice system employ NLP models for classifying and triaging child exploitation cases due to the textual communications between predators and victims. The usefulness of these systems depend on the quality of data that can be used for training. Data in the domain are scarce, sensitive, and emotionally taxing for annotators. NLP researchers approximate victimization conversations using transcripts from internet stings performed by either vigilantes or LEOs, with an implicit assumption that vigilante or LEO conversations represent the victimization process. Psychology research, however, states that underage victim chats differ from internet stings in goal and modus operandi. We present a methodology and observations from annotating a corpus of victim, vigilante, and LEO conversations with convicted predators with the goal of comparing these chats. The corpus is annotated for stages and tactics of the victimization process described within psychology research. As predicted by psychological research, we found significant differences in the three classes of chats that are usually not taken into account in chat classification.", "venue": "Findings", "year": 2021, "referenceCount": 55, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3645-3656"}, "authors": [{"authorId": "2630734", "name": "Tatiana R. Ringenberg"}, {"authorId": "1389271530", "name": "Kathryn C. Seigfried-Spellar"}, {"authorId": "8777666", "name": "Julia Taylor Rayz"}]}, {"paperId": "02d22338bd95b829dcaafdf6766034a1f029b447", "externalIds": {"ArXiv": "2004.14357", "DBLP": "conf/acl/WangLML21", "ACL": "2021.findings-acl.320", "MAG": "3023410782", "DOI": "10.18653/v1/2021.findings-acl.320", "CorpusId": 216642004}, "corpusId": 216642004, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/02d22338bd95b829dcaafdf6766034a1f029b447", "title": "Detecting Domain Polarity-Changes of Words in a Sentiment Lexicon", "abstract": "Sentiment lexicons are instrumental for sentiment analysis. One can use a set of sentiment words provided in a sentiment lexicon and a lexicon-based classifier to perform sentiment classification. One major issue with this approach is that many sentiment words are domain dependent. That is, they may be positive in some domains but negative in some others. We refer to this problem as domain polarity-changes of words. Detecting such words and correcting their sentiment for an application domain is very important. In this paper, we propose a graph-based technique to tackle this problem. Experimental results show its effectiveness on multiple real-world datasets.", "venue": "Findings", "year": 2020, "referenceCount": 58, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.320.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-04-29", "journal": {"name": "ArXiv", "volume": "abs/2004.14357"}, "authors": [{"authorId": "1717480", "name": "Shuai Wang"}, {"authorId": "2767360", "name": "Guangyi Lv"}, {"authorId": "35346748", "name": "S. Mazumder"}, {"authorId": "47655430", "name": "Bing Liu"}]}, {"paperId": "33316e3cba3cf40859648b542187337e4f6043af", "externalIds": {"ACL": "2021.findings-acl.321", "DBLP": "conf/acl/VillegasMA21", "ArXiv": "2105.04047", "DOI": "10.18653/v1/2021.findings-acl.321", "CorpusId": 234339299}, "corpusId": 234339299, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/33316e3cba3cf40859648b542187337e4f6043af", "title": "Analyzing Online Political Advertisements", "abstract": "Online political advertising is a central aspect of modern election campaigning for influencing public opinion. Computational analysis of political ads is of utmost importance in political science to understand the characteristics of digital campaigning. It is also important in computational linguistics to study features of political discourse and communication on a large scale. In this work, we present the first computational study on online political ads with the aim to (1) infer the political ideology of an ad sponsor; and (2) identify whether the sponsor is an official political party or a third-party organization. We develop two new large datasets for the two tasks consisting of ads from the U.S.. Evaluation results show that our approach that combines textual and visual information from pre-trained neural models outperforms a state-of-the-art method for generic commercial ad classification. Finally, we provide an in-depth analysis of the limitations of our best-performing models and linguistic analysis to study the characteristics of political ads discourse.", "venue": "Findings", "year": 2021, "referenceCount": 73, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.321.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Sociology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-09", "journal": {"pages": "3669-3680"}, "authors": [{"authorId": "1661213189", "name": "Danae S\u00e1nchez Villegas"}, {"authorId": "3344806", "name": "S. Mokaram"}, {"authorId": "3238627", "name": "Nikolaos Aletras"}]}, {"paperId": "572b9183d3eaf45a31c9308f20e420c5f922588e", "externalIds": {"DBLP": "conf/acl/WangICR21", "ACL": "2021.findings-acl.322", "ArXiv": "2106.11533", "DOI": "10.18653/v1/2021.findings-acl.322", "CorpusId": 235592830}, "corpusId": 235592830, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/572b9183d3eaf45a31c9308f20e420c5f922588e", "title": "Do Language Models Perform Generalizable Commonsense Inference?", "abstract": "Inspired by evidence that pretrained language models (LMs) encode commonsense knowledge, recent work has applied LMs to automatically populate commonsense knowledge graphs (CKGs). However, there is a lack of understanding on their generalization to multiple CKGs, unseen relations, and novel entities. This paper analyzes the ability of LMs to perform generalizable commonsense inference, in terms of knowledge capacity, transferability, and induction. Our experiments with these three aspects show that: (1) LMs can adapt to different schemas defined by multiple CKGs but fail to reuse the knowledge to generalize to new relations. (2) Adapted LMs generalize well to unseen subjects, but less so on novel objects. Future work should investigate how to improve the transferability and induction of commonsense mining from LMs.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 16, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.322.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-22", "journal": {"name": "ArXiv", "volume": "abs/2106.11533"}, "authors": [{"authorId": "2118952190", "name": "Peifeng Wang"}, {"authorId": "2512264", "name": "Filip Ilievski"}, {"authorId": "1998918", "name": "Muhao Chen"}, {"authorId": "1384550891", "name": "Xiang Ren"}]}, {"paperId": "c0731ec306b3182834a55f85b712c9d6e1538529", "externalIds": {"DBLP": "conf/acl/KongF21", "ACL": "2021.findings-acl.323", "DOI": "10.18653/v1/2021.findings-acl.323", "CorpusId": 236478090}, "corpusId": 236478090, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c0731ec306b3182834a55f85b712c9d6e1538529", "title": "Probing Multi-modal Machine Translation with Pre-trained Language Model", "abstract": "Multi-modal machine translation (MMT) aimed at using images to help disambiguate the target during translation and improving robustness, but some recent works showed that the contribution of visual features is either negligible or incremental. In this paper, we show that incorporating pre-trained (vision) language model (VLP) on the source side can improve the multi-modal translation quality significantly. Motivated by BERT, VLP aims to learn better cross-modal representations that improve target sequence generation. We simply adapt BERT to a cross-modal domain for the vision language pre-training, and the downstream multi-modal machine translation can substantially benefit from the pre-training. We also introduce an attention based modality loss to promote the image-text alignment in the latent semantic space. Ablation study verifies that it is effective in further improving the translation quality. Our experiments on the widely used Multi-30K dataset show increased BLEU score up to 6.2 points compared with the text-only model, achieving the state-of-the-art results with a large margin in the semi-unconstrained scenario and indicating a possible direction to rejuvenate the multi-modal machine translation.", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3689-3699"}, "authors": [{"authorId": "2111951353", "name": "Yawei Kong"}, {"authorId": "2053962725", "name": "Kai Fan"}]}, {"paperId": "188f8a19dbfd2884ea818db822a17866e978ce75", "externalIds": {"DBLP": "conf/acl/TouilebB21", "ArXiv": "2105.07400", "ACL": "2021.findings-acl.324", "DOI": "10.18653/v1/2021.findings-acl.324", "CorpusId": 235211772}, "corpusId": 235211772, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/188f8a19dbfd2884ea818db822a17866e978ce75", "title": "The interplay between language similarity and script on a novel multi-layer Algerian dialect corpus", "abstract": "Recent years have seen a rise in interest for cross-lingual transfer between languages with similar typology, and between languages of various scripts. However, the interplay between language similarity and difference in script on cross-lingual transfer is a less studied problem. We explore this interplay on cross-lingual transfer for two supervised tasks, namely part-of-speech tagging and sentiment analysis. We introduce a newly annotated corpus of Algerian user-generated comments comprising parallel annotations of Algerian written in Latin, Arabic, and code-switched scripts, as well as annotations for sentiment and topic categories. We perform baseline experiments by fine-tuning multi-lingual language models. We further explore the effect of script vs. language similarity in cross-lingual transfer by fine-tuning multi-lingual models on languages which are a) typologically distinct, but use the same script, b) typologically similar, but use a distinct script, or c) are typologically similar and use the same script. We find there is a delicate relationship between script and typology for part-of-speech, while sentiment analysis is less sensitive.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.324.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-16", "journal": {"name": "ArXiv", "volume": "abs/2105.07400"}, "authors": [{"authorId": "3083347", "name": "Samia Touileb"}, {"authorId": "144435436", "name": "Jeremy Barnes"}]}, {"paperId": "31129ea14f1143c7d37767927db702b85299745b", "externalIds": {"DBLP": "journals/corr/abs-2105-11260", "ACL": "2021.findings-acl.325", "ArXiv": "2105.11260", "DOI": "10.18653/v1/2021.findings-acl.325", "CorpusId": 235166112}, "corpusId": 235166112, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/31129ea14f1143c7d37767927db702b85299745b", "title": "Few-Shot Upsampling for Protest Size Detection", "abstract": "We propose a new task and dataset for a common problem in social science research:\"upsampling\"coarse document labels to fine-grained labels or spans. We pose the problem in a question answering format, with the answers providing the fine-grained labels. We provide a benchmark dataset and baselines on a socially impactful task: identifying the exact crowd size at protests and demonstrations in the United States given only order-of-magnitude information about protest attendance, a very small sample of fine-grained examples, and English-language news text. We evaluate several baseline models, including zero-shot results from rule-based and question-answering models, few-shot models fine-tuned on a small set of documents, and weakly supervised models using a larger set of coarsely-labeled documents. We find that our rule-based model initially outperforms a zero-shot pre-trained transformer language model but that further fine-tuning on a very small subset of 25 examples substantially improves out-of-sample performance. We also demonstrate a method for fine-tuning the transformer span on only the coarse labels that performs similarly to our rule-based approach. This work will contribute to social scientists' ability to generate data to understand the causes and successes of collective action.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.325.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-24", "journal": {"name": "ArXiv", "volume": "abs/2105.11260"}, "authors": [{"authorId": "9817048", "name": "Andrew Halterman"}, {"authorId": "40897451", "name": "Benjamin J. Radford"}]}, {"paperId": "c4ff1df07755699b76c9f6a7891b6c67a15a320f", "externalIds": {"DBLP": "conf/acl/NikkarinenPBC21", "ACL": "2021.findings-acl.326", "ArXiv": "2106.02289", "DOI": "10.18653/v1/2021.findings-acl.326", "CorpusId": 235353020}, "corpusId": 235353020, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c4ff1df07755699b76c9f6a7891b6c67a15a320f", "title": "Modeling the Unigram Distribution", "abstract": "The unigram distribution is the non-contextual probability of finding a specific word form in a corpus. While of central importance to the study of language, it is commonly approximated by each word's sample frequency in the corpus. This approach, being highly dependent on sample size, assigns zero probability to any out-of-vocabulary (oov) word form. As a result, it produces negatively biased probabilities for any oov word form, while positively biased probabilities to in-corpus words. In this work, we argue in favor of properly modeling the unigram distribution -- claiming it should be a central task in natural language processing. With this in mind, we present a novel model for estimating it in a language (a neuralization of Goldwater et al.'s (2011) model) and show it produces much better estimates across a diverse set of 7 languages than the na\\\"ive use of neural character-level language models.", "venue": "Findings", "year": 2021, "referenceCount": 39, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.326.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"pages": "3721-3729"}, "authors": [{"authorId": "146990003", "name": "Irene Nikkarinen"}, {"authorId": "1388571351", "name": "Tiago Pimentel"}, {"authorId": "6894443", "name": "Dami\u00e1n E. Blasi"}, {"authorId": "2070989574", "name": "Ryan Cotterell"}]}, {"paperId": "03360b29206604818c28afca3b70debb24fa372b", "externalIds": {"ACL": "2021.findings-acl.327", "DBLP": "conf/acl/ZafarDSADK21", "ArXiv": "2106.04631", "DOI": "10.18653/v1/2021.findings-acl.327", "CorpusId": 235377172}, "corpusId": 235377172, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/03360b29206604818c28afca3b70debb24fa372b", "title": "On the Lack of Robust Interpretability of Neural Text Classifiers", "abstract": "With the ever-increasing complexity of neural language models, practitioners have turned to methods for understanding the predictions of these models. One of the most well-adopted approaches for model interpretability is feature-based interpretability, i.e., ranking the features in terms of their impact on model predictions. Several prior studies have focused on assessing the fidelity of feature-based interpretability methods, i.e., measuring the impact of dropping the top-ranked features on the model output. However, relatively little work has been conducted on quantifying the robustness of interpretations. In this work, we assess the robustness of interpretations of neural text classifiers, specifically, those based on pretrained Transformer encoders, using two randomization tests. The first compares the interpretations of two models that are identical except for their initializations. The second measures whether the interpretations differ between a model with trained parameters and a model with random parameters. Both tests show surprising deviations from expected behavior, raising questions about the extent of insights that practitioners may draw from interpretations.", "venue": "Findings", "year": 2021, "referenceCount": 62, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.327.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-08", "journal": {"pages": "3730-3740"}, "authors": [{"authorId": "2625318", "name": "M. B. Zafar"}, {"authorId": "2192704", "name": "Michele Donini"}, {"authorId": "153794305", "name": "Dylan Slack"}, {"authorId": "2824663", "name": "C. Archambeau"}, {"authorId": "152220539", "name": "Sanjiv Ranjan Das"}, {"authorId": "1769861", "name": "K. Kenthapadi"}]}, {"paperId": "71671dde0ea5e62a491062986b64db1f96b71295", "externalIds": {"ACL": "2021.findings-acl.328", "DBLP": "journals/corr/abs-2107-00596", "ArXiv": "2107.00596", "DOI": "10.18653/v1/2021.findings-acl.328", "CorpusId": 235694370}, "corpusId": 235694370, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/71671dde0ea5e62a491062986b64db1f96b71295", "title": "Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction", "abstract": "The recent advancement of pre-trained Transformer models has propelled the development of effective text mining models across various biomedical tasks. However, these models are primarily learned on the textual data and often lack the domain knowledge of the entities to capture the context beyond the sentence. In this study, we introduced a novel framework that enables the model to learn multi-omnics biological information about entities (proteins) with the help of additional multi-modal cues like molecular structure. Towards this, rather developing modality-specific architectures, we devise a generalized and optimized graph based multi-modal learning mechanism that utilizes the GraphBERT model to encode the textual and molecular structure information and exploit the underlying features of various modalities to enable end-to-end learning. We evaluated our proposed method on ProteinProtein Interaction task from the biomedical corpus, where our proposed generalized approach is observed to be benefited by the additional domain-specific modality.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.328.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-01", "journal": {"name": "ArXiv", "volume": "abs/2107.00596"}, "authors": [{"authorId": "2117118081", "name": "Sriram Pingali"}, {"authorId": "4137125", "name": "S. Yadav"}, {"authorId": "2941099", "name": "Pratik Dutta"}, {"authorId": "145470045", "name": "S. Saha"}]}, {"paperId": "225cbcb6ea433208ec1cf9eb2d55a14fa917dceb", "externalIds": {"DBLP": "journals/corr/abs-2106-03337", "ArXiv": "2106.03337", "ACL": "2021.findings-acl.329", "DOI": "10.18653/v1/2021.findings-acl.329", "CorpusId": 235359080}, "corpusId": 235359080, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/225cbcb6ea433208ec1cf9eb2d55a14fa917dceb", "title": "Summary Grounded Conversation Generation", "abstract": "Many conversation datasets have been constructed in the recent years using crowdsourcing. However, the data collection process can be time consuming and presents many challenges to ensure data quality. Since language generation has improved immensely in recent years with the advancement of pre-trained language models, we investigate how such models can be utilized to generate entire conversations, given only a summary of a conversation as the input. We explore three approaches to generate summary grounded conversations, and evaluate the generated conversations using automatic measures and human judgements. We also show that the accuracy of conversation summarization can be improved by augmenting a conversation summarization dataset with generated conversations.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.329.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-07", "journal": {"name": "ArXiv", "volume": "abs/2106.03337"}, "authors": [{"authorId": "144543562", "name": "R. Chulaka Gunasekara"}, {"authorId": "2169082", "name": "Guy Feigenblat"}, {"authorId": "2464133", "name": "B. Sznajder"}, {"authorId": "1703799", "name": "Sachindra Joshi"}, {"authorId": "1775524", "name": "D. Konopnicki"}]}, {"paperId": "749dc7adadc015e78e4c92a373e859fb4922e092", "externalIds": {"MAG": "3177494585", "ACL": "2021.findings-acl.330", "DBLP": "conf/acl/AgrawalXC21", "DOI": "10.18653/v1/2021.findings-acl.330", "CorpusId": 236477934}, "corpusId": 236477934, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/749dc7adadc015e78e4c92a373e859fb4922e092", "title": "A Non-Autoregressive Edit-Based Approach to Controllable Text Simplification", "abstract": "We introduce a new approach for the task of Controllable Text Simpli\ufb01cation, where systems rewrite a complex English sentence so that it can be understood by readers at different grade levels in the US K-12 system. It uses a non-autoregressive model to iteratively edit an input sequence and incorporates lexical complexity information seamlessly into the re\ufb01nement process to generate simpli\ufb01cations that better match the desired output complexity than strong autoregressive baselines. Analysis shows that our model\u2019s local edit operations are combined to achieve more complex sim-pli\ufb01cation operations such as content deletion and paraphrasing, as well as sentence splitting.", "venue": "Findings", "year": 2021, "referenceCount": 44, "citationCount": 20, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.330.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-01", "journal": {"pages": "3757-3769"}, "authors": [{"authorId": "5112699", "name": "Sweta Agrawal"}, {"authorId": "47210642", "name": "Weijia Xu"}, {"authorId": "2954727", "name": "Marine Carpuat"}]}, {"paperId": "1cde1aa4f7bcebc47b35518cec452893ea6b824c", "externalIds": {"DBLP": "conf/acl/HuZTZPLNR21", "ACL": "2021.findings-acl.331", "ArXiv": "2106.03983", "DOI": "10.18653/v1/2021.findings-acl.331", "CorpusId": 235367897}, "corpusId": 235367897, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1cde1aa4f7bcebc47b35518cec452893ea6b824c", "title": "Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference", "abstract": "Multilingual transformers (XLM, mT5) have been shown to have remarkable transfer skills in zero-shot settings. Most transfer studies, however, rely on automatically translated resources (XNLI, XQuAD), making it hard to discern the particular linguistic knowledge that is being transferred, and the role of expert annotated monolingual datasets when developing task-specific models. We investigate the cross-lingual transfer abilities of XLM-R for Chinese and English natural language inference (NLI), with a focus on the recent large-scale Chinese dataset OCNLI. To better understand linguistic transfer, we created 4 categories of challenge and adversarial tasks (totaling 17 new datasets) for Chinese that build on several well-known resources for English (e.g., HANS, NLI stress-tests). We find that cross-lingual models trained on English NLI do transfer well across our Chinese tasks (e.g., in 3/4 of our challenge categories, they perform as well/better than the best monolingual models, even on 3/5 uniquely Chinese linguistic phenomena such as idioms, pro drop). These results, however, come with important caveats: cross-lingual models often perform best when trained on a mixture of English and high-quality monolingual NLI data (OCNLI), and are often hindered by automatically translated resources (XNLI-zh). For many phenomena, all models continue to struggle, highlighting the need for our new diagnostics to help benchmark Chinese and cross-lingual models. All new datasets/code are released at https://github.com/huhailinguist/ChineseNLIProbing.", "venue": "Findings", "year": 2021, "referenceCount": 58, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.331.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-07", "journal": {"pages": "3770-3785"}, "authors": [{"authorId": "145309512", "name": "Hai Hu"}, {"authorId": "1596810069", "name": "He Zhou"}, {"authorId": "105042132", "name": "Zuoyu Tian"}, {"authorId": "2108139880", "name": "Yiwen Zhang"}, {"authorId": "2146277459", "name": "Yina Ma"}, {"authorId": "2155221186", "name": "Yanting Li"}, {"authorId": "40383658", "name": "Yixin Nie"}, {"authorId": "46666605", "name": "Kyle Richardson"}]}, {"paperId": "9f8f19ebfe3b760967445ed23ad144f90faf4316", "externalIds": {"ACL": "2021.findings-acl.332", "DBLP": "conf/acl/BhattasaliR21", "DOI": "10.18653/v1/2021.findings-acl.332", "CorpusId": 236477640}, "corpusId": 236477640, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9f8f19ebfe3b760967445ed23ad144f90faf4316", "title": "Using surprisal and fMRI to map the neural bases of broad and local contextual prediction during natural language comprehension", "abstract": "Context guides comprehenders\u2019 expectations during language processing, and informationtheoretic surprisal is commonly used as an index of cognitive processing effort. However, prior work using surprisal has considered only within-sentence context, using n-grams, neural language models, or syntactic structure as conditioning context. In this paper, we extend the surprisal approach to use broader topical context, investigating the influence of local and topical context on processing via an analysis of fMRI time courses collected during naturalistic listening. Lexical surprisal calculated from ngram and LSTM language models is used to capture effects of local context; to capture the effects of broader context a new metric based on topic models, topical surprisal, is introduced. We identify distinct patterns of neural activation for lexical surprisal and topical surprisal. These differing neuro-anatomical correlates suggest that local and broad contextual cues during sentence processing recruit different brain regions and that those regions of the language network functionally contribute to processing different dimensions of contextual information during comprehension. More generally, our approach adds to a growing literature using methods from computational linguistics to operationalize and test hypotheses about neuro-cognitive mechanisms in sentence processing.", "venue": "Findings", "year": 2021, "referenceCount": 65, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3786-3798"}, "authors": [{"authorId": "2121368512", "name": "Shohini Bhattasali"}, {"authorId": "1680292", "name": "P. Resnik"}]}, {"paperId": "ab8ea4bf7b58edea54edbd179c48bbc24b8aeadf", "externalIds": {"DBLP": "conf/acl/Perez-MayosGMW21", "ACL": "2021.findings-acl.333", "ArXiv": "2105.04688", "DOI": "10.18653/v1/2021.findings-acl.333", "CorpusId": 234357842}, "corpusId": 234357842, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ab8ea4bf7b58edea54edbd179c48bbc24b8aeadf", "title": "Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models", "abstract": "Multilingual Transformer-based language models, usually pretrained on more than 100 languages, have been shown to achieve outstanding results in a wide range of cross-lingual transfer tasks. However, it remains unknown whether the optimization for different languages conditions the capacity of the models to generalize over syntactic structures, and how languages with syntactic phenomena of different complexity are affected. In this work, we explore the syntactic generalization capabilities of the monolingual and multilingual versions of BERT and RoBERTa. More specifically, we evaluate the syntactic generalization potential of the models on English and Spanish tests, comparing the syntactic abilities of monolingual and multilingual models on the same language (English), and of multilingual models on two different languages (English and Spanish). For English, we use the available SyntaxGym test suite; for Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic tests in Spanish, designed to evaluate the syntactic generalization capabilities of language models through the SyntaxGym online platform.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.333.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-10", "journal": {"pages": "3799-3812"}, "authors": [{"authorId": "1410920781", "name": "Laura P\u00e9rez-Mayos"}, {"authorId": "2115655708", "name": "Alba T'aboas Garc'ia"}, {"authorId": "2738095", "name": "Simon Mille"}, {"authorId": "9092408", "name": "L. Wanner"}]}, {"paperId": "6a8cb4fb5a20c7e5733a9bd50cd5feaad6c11360", "externalIds": {"DBLP": "conf/acl/ZhongGKS21", "ACL": "2021.findings-acl.334", "ArXiv": "2105.06020", "DOI": "10.18653/v1/2021.findings-acl.334", "CorpusId": 234482939}, "corpusId": 234482939, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6a8cb4fb5a20c7e5733a9bd50cd5feaad6c11360", "title": "Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level", "abstract": "Larger language models have higher accuracy on average, but are they better on every single instance (datapoint)? Some work suggests larger models have higher out-of-distribution robustness, while other work suggests they have lower accuracy on rare subgroups. To understand these differences, we investigate these models at the level of individual instances. However, one major challenge is that individual predictions are highly sensitive to noise in the randomness in training. We develop statistically rigorous methods to address this, and after accounting for pretraining and finetuning noise, we find that our BERT-Large is worse than BERT-Mini on at least 1-4% of instances across MNLI, SST-2, and QQP, compared to the overall accuracy improvement of 2-10%. We also find that finetuning noise increases with model size and that instance-level accuracy has momentum: improvement from BERT-Mini to BERT-Medium correlates with improvement from BERT-Medium to BERT-Large. Our findings suggest that instance-level predictions provide a rich source of information; we therefore, recommend that researchers supplement model weights with model predictions.", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 27, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.334.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-13", "journal": {"pages": "3813-3827"}, "authors": [{"authorId": "51011000", "name": "Ruiqi Zhong"}, {"authorId": "2143028576", "name": "Dhruba Ghosh"}, {"authorId": "38666915", "name": "D. Klein"}, {"authorId": "5164568", "name": "J. Steinhardt"}]}, {"paperId": "8797767dc66f7d0acb50b01276a3e3b378c79b0c", "externalIds": {"ACL": "2021.findings-acl.335", "DBLP": "conf/acl/ParkerY21", "DOI": "10.18653/v1/2021.findings-acl.335", "CorpusId": 236477519}, "corpusId": 236477519, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8797767dc66f7d0acb50b01276a3e3b378c79b0c", "title": "Named Entity Recognition through Deep Representation Learning and Weak Supervision", "abstract": "Weakly supervised methods estimate the labels for a dataset using the predictions of several noisy supervision sources. Many machine learning practitioners have begun using weak supervision to more quickly and cheaply an-notate data compared to traditional manual labeling. In this paper, we focus on the speci\ufb01c problem of weakly supervised named entity recognition (NER) and propose an end-to-end model to learn optimal assignments of latent NER tags using observed tokens and weak labels provided by labeling functions. To capture the sequential dependencies between the latent and observed variables, we propose a sequential graphical model where the com-ponents are approximated using neural networks. State-of-the-art contextual embeddings are used to further discriminate the quality of noisy weak labels in various contexts. Results of experiments on four public weakly supervised named entity recognition datasets show a signi\ufb01cant improvement in F1 score over recent approaches.", "venue": "Findings", "year": 2021, "referenceCount": 25, "citationCount": 3, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.335.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3828-3839"}, "authors": [{"authorId": "2114806194", "name": "Jerrod Parker"}, {"authorId": "150311558", "name": "S. Yu"}]}, {"paperId": "84f52e131e31a47bd0d9f40a2f9fbc770024b9c9", "externalIds": {"DBLP": "conf/acl/RossMP21", "ACL": "2021.findings-acl.336", "ArXiv": "2012.13985", "DOI": "10.18653/v1/2021.findings-acl.336", "CorpusId": 229679941}, "corpusId": 229679941, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/84f52e131e31a47bd0d9f40a2f9fbc770024b9c9", "title": "Explaining NLP Models via Minimal Contrastive Editing (MiCE)", "abstract": "Humans have been shown to give contrastive explanations, which explain why an observed event happened rather than some other counterfactual event (the contrast case). Despite the influential role that contrastivity plays in how humans explain, this property is largely missing from current methods for explaining NLP models. We present Minimal Contrastive Editing (MiCE), a method for producing contrastive explanations of model predictions in the form of edits to inputs that change model outputs to the contrast case. Our experiments across three tasks--binary sentiment classification, topic classification, and multiple-choice question answering--show that MiCE is able to produce edits that are not only contrastive, but also minimal and fluent, consistent with human contrastive edits. We demonstrate how MiCE edits can be used for two use cases in NLP system development--debugging incorrect model outputs and uncovering dataset artifacts--and thereby illustrate that producing contrastive explanations is a promising research direction for model interpretability.", "venue": "Findings", "year": 2020, "referenceCount": 78, "citationCount": 74, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.336.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-27", "journal": {"name": "ArXiv", "volume": "abs/2012.13985"}, "authors": [{"authorId": "32739287", "name": "Alexis Ross"}, {"authorId": "3451494", "name": "Ana Marasovi\u0107"}, {"authorId": "39139825", "name": "Matthew E. Peters"}]}, {"paperId": "ff9d04fc15a2c52d982b5b7daa787a373ed7f899", "externalIds": {"ArXiv": "2106.01221", "DBLP": "conf/acl/YueDWLSC21", "ACL": "2021.findings-acl.337", "DOI": "10.18653/v1/2021.findings-acl.337", "CorpusId": 235293967}, "corpusId": 235293967, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ff9d04fc15a2c52d982b5b7daa787a373ed7f899", "title": "Differential Privacy for Text Analytics via Natural Text Sanitization", "abstract": "Texts convey sophisticated knowledge. However, texts also convey sensitive information. Despite the success of general-purpose language models and domain-specific mechanisms with differential privacy (DP), existing text sanitization mechanisms still provide low utility, as cursed by the high-dimensional text representation. The companion issue of utilizing sanitized texts for downstream analytics is also under-explored. This paper takes a direct approach to text sanitization. Our insight is to consider both sensitivity and similarity via our new local DP notion. The sanitized texts also contribute to our sanitization-aware pretraining and fine-tuning, enabling privacy-preserving natural language processing over the BERT language model with promising utility. Surprisingly, the high utility does not boost up the success rate of inference attacks.", "venue": "Findings", "year": 2021, "referenceCount": 48, "citationCount": 23, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.337.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "3853-3866"}, "authors": [{"authorId": "145548079", "name": "Xiang Yue"}, {"authorId": "2938213", "name": "Minxin Du"}, {"authorId": "49980880", "name": "Tianhao Wang"}, {"authorId": "2110479359", "name": "Yaliang Li"}, {"authorId": "1515546612", "name": "Huan Sun"}, {"authorId": "145876490", "name": "Sherman S. M. Chow"}]}, {"paperId": "d08a6a41e2b16928a1dc93b259bffbe37dae021d", "externalIds": {"DBLP": "conf/acl/GuptaTB21", "ACL": "2021.findings-acl.338", "ArXiv": "2106.05894", "DOI": "10.18653/v1/2021.findings-acl.338", "CorpusId": 235391065}, "corpusId": 235391065, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d08a6a41e2b16928a1dc93b259bffbe37dae021d", "title": "Synthesizing Adversarial Negative Responses for Robust Response Ranking and Evaluation", "abstract": "Open-domain neural dialogue models have achieved high performance in response ranking and evaluation tasks. These tasks are formulated as a binary classification of responses given in a dialogue context, and models generally learn to make predictions based on context-response content similarity. However, over-reliance on content similarity makes the models less sensitive to the presence of inconsistencies, incorrect time expressions and other factors important for response appropriateness and coherence. We propose approaches for automatically creating adversarial negative training data to help ranking and evaluation models learn features beyond content similarity. We propose mask-and-fill and keyword-guided approaches that generate negative examples for training more robust dialogue systems. These generated adversarial responses have high content similarity with the contexts but are either incoherent, inappropriate or not fluent. Our approaches are fully data-driven and can be easily incorporated in existing models and datasets. Experiments on classification, ranking and evaluation tasks across multiple datasets demonstrate that our approaches outperform strong baselines in providing informative negative examples for training dialogue systems.", "venue": "Findings", "year": 2021, "referenceCount": 96, "citationCount": 16, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.338.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-10", "journal": {"pages": "3867-3883"}, "authors": [{"authorId": "1491232062", "name": "Prakhar Gupta"}, {"authorId": "2073587169", "name": "Yulia Tsvetkov"}, {"authorId": "1744846", "name": "Jeffrey P. Bigham"}]}, {"paperId": "6b1c351c7969e70daf13b4af83ac256c45041074", "externalIds": {"DBLP": "conf/acl/KapanipathiARRG21", "ACL": "2021.findings-acl.339", "ArXiv": "2012.01707", "DOI": "10.18653/v1/2021.findings-acl.339", "CorpusId": 235303644}, "corpusId": 235303644, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6b1c351c7969e70daf13b4af83ac256c45041074", "title": "Leveraging Abstract Meaning Representation for Knowledge Base Question Answering", "abstract": "Knowledge base question answering (KBQA)is an important task in Natural Language Processing. Existing approaches face significant challenges including complex question understanding, necessity for reasoning, and lack of large end-to-end training datasets. In this work, we propose Neuro-Symbolic Question Answering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning Representation (AMR) parses for task-independent question understanding; (2) a simple yet effective graph transformation approach to convert AMR parses into candidate logical queries that are aligned to the KB; (3) a pipeline-based approach which integrates multiple, reusable modules that are trained specifically for their individual tasks (semantic parser, entity andrelationship linkers, and neuro-symbolic reasoner) and do not require end-to-end training data. NSQA achieves state-of-the-art performance on two prominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD1.0). Furthermore, our analysis emphasizes that AMR is a powerful tool for KBQA systems.", "venue": "Findings", "year": 2020, "referenceCount": 41, "citationCount": 73, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.339.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-03", "journal": {"pages": "3884-3894"}, "authors": [{"authorId": "2223082", "name": "Pavan Kapanipathi"}, {"authorId": "145749443", "name": "I. Abdelaziz"}, {"authorId": "30090681", "name": "Srinivas Ravishankar"}, {"authorId": "1781292", "name": "S. Roukos"}, {"authorId": "1703070", "name": "Alexander G. Gray"}, {"authorId": "3394760", "name": "Ram\u00f3n Fern\u00e1ndez Astudillo"}, {"authorId": "34245177", "name": "Maria Chang"}, {"authorId": "2470518", "name": "Cristina Cornelio"}, {"authorId": "2066641453", "name": "Saswati Dana"}, {"authorId": "2297836", "name": "Achille Fokoue"}, {"authorId": "50252087", "name": "Dinesh Garg"}, {"authorId": "1711133", "name": "A. Gliozzo"}, {"authorId": "1963709", "name": "Sairam Gurajada"}, {"authorId": "9621738", "name": "Hima P. Karanam"}, {"authorId": "66983659", "name": "Naweed Khan"}, {"authorId": "50564082", "name": "Dinesh Khandelwal"}, {"authorId": "2110153639", "name": "Young-suk Lee"}, {"authorId": "1573872877", "name": "Yunyao Li"}, {"authorId": "1767482", "name": "F. Luus"}, {"authorId": "3067966", "name": "Ndivhuwo Makondo"}, {"authorId": "2689774", "name": "Nandana Mihindukulasooriya"}, {"authorId": "2524647", "name": "Tahira Naseem"}, {"authorId": "2965855", "name": "S. Neelam"}, {"authorId": "145378077", "name": "Lucian Popa"}, {"authorId": "47016316", "name": "Revanth Reddy Gangi Reddy"}, {"authorId": "2146658", "name": "Ryan Riegel"}, {"authorId": "3415700", "name": "Gaetano Rossiello"}, {"authorId": "2027112872", "name": "Udit Sharma"}, {"authorId": "2127473246", "name": "G. P. S. Bhargav"}, {"authorId": "2115621395", "name": "Mo Yu"}]}, {"paperId": "67a2a1c14d795f00ef97026e9bf8320c53537ba0", "externalIds": {"DBLP": "conf/acl/BianchiH21", "ACL": "2021.findings-acl.340", "DOI": "10.18653/v1/2021.findings-acl.340", "CorpusId": 236477333}, "corpusId": 236477333, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/67a2a1c14d795f00ef97026e9bf8320c53537ba0", "title": "On the Gap between Adoption and Understanding in NLP", "abstract": "There are some issues with current research trends in NLP that can hamper the free development of scientific research. We identify five of particular concern: 1) the early adoption of methods without sufficient understanding or analysis; 2) the preference for computational methods regardless of risks associated with their limitations; 3) the resulting bias in the papers we publish; 4) the impossibility of re-running some experiments due to their cost; 5) the dangers of unexplainable methods. If these issues are not addressed, we risk a loss of reproducibility, reputability, and subsequently public trust in our field. In this position paper, we outline each of these points and suggest ways forward.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 22, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.340.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3895-3901"}, "authors": [{"authorId": "123794140", "name": "F. Bianchi"}, {"authorId": "2022288", "name": "Dirk Hovy"}]}, {"paperId": "6260975a9a50ab68f136ab79f4a912e253aa2680", "externalIds": {"DBLP": "conf/acl/Dougrez-LewisLK21", "ACL": "2021.findings-acl.341", "DOI": "10.18653/v1/2021.findings-acl.341", "CorpusId": 236477429}, "corpusId": 236477429, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6260975a9a50ab68f136ab79f4a912e253aa2680", "title": "Learning Disentangled Latent Topics for Twitter Rumour Veracity Classification", "abstract": "With the rapid growth of social media in the past decade, the news are no longer controlled by just a few mainstream sources. Users themselves create large numbers of potentially fictitious rumours, necessitating automated veracity classification systems. Here we present a novel approach towards automatically classifying rumours circulating on Twitter with respect to their veracity. We use a model built on Variational Autoencoder which disentangles the informational content of a tweet from the manner in which the information is written. This is achieved by obtaining latent topic vectors in an adversarial learning setting using the auxiliary task of stance classification. The latent vectors learnt in this way are used to predict rumour veracity, obtaining state-of-the-art accuracy scores on the PHEME dataset.1", "venue": "Findings", "year": 2021, "referenceCount": 13, "citationCount": 10, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.341.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3902-3908"}, "authors": [{"authorId": "2121340562", "name": "John Dougrez-Lewis"}, {"authorId": "48717312", "name": "M. Liakata"}, {"authorId": "144318711", "name": "E. Kochkina"}, {"authorId": "1390509967", "name": "Yulan He"}]}, {"paperId": "a4e50201369c70f25e82d005ca4fe754086dc9a9", "externalIds": {"ACL": "2021.findings-acl.342", "DBLP": "conf/acl/MerkhoferMMH21", "DOI": "10.18653/v1/2021.findings-acl.342", "CorpusId": 236477801}, "corpusId": 236477801, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/a4e50201369c70f25e82d005ca4fe754086dc9a9", "title": "Perceptual Models of Machine-Edited Text", "abstract": "We introduce a novel dataset of human judgments of machine-edited text and initial models of those perceptions. Six machine-editing methods ranging from character swapping to variational autoencoders are applied to collections of English-language social media text and scientific abstracts. The edits are judged in context for detectability and the extent to which they preserve the meaning of the original. Automated measures of semantic similarity and fluency are evaluated individually and combined to produce composite models of human perception. Both meaning preservation and detectability are predicted within 6% of the upper bound of human consensus labeling.", "venue": "Findings", "year": 2021, "referenceCount": 62, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3909-3920"}, "authors": [{"authorId": "3466990", "name": "Elizabeth M. Merkhofer"}, {"authorId": "2060263405", "name": "Monica Mendoza"}, {"authorId": "152775164", "name": "Rebecca Marvin"}, {"authorId": "145035103", "name": "John C. Henderson"}]}, {"paperId": "903703495a11d17811581fb41a291a6c3539adc8", "externalIds": {"ACL": "2021.findings-acl.343", "DBLP": "conf/acl/ThirukovalluruM21", "DOI": "10.18653/v1/2021.findings-acl.343", "CorpusId": 236477874}, "corpusId": 236477874, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/903703495a11d17811581fb41a291a6c3539adc8", "title": "Scaling Within Document Coreference to Long Texts", "abstract": "State of the art end-to-end coreference resolution models use expensive span representations and antecedent prediction mechanisms. These approaches are expensive both in terms of their memory requirements as well as compute time, and are particularly ill-suited for long documents. In this paper, we propose an approximation to end-to-end models which scales gracefully to documents of any length. Replacing span representations with token representations, we reduce the time/memory complexity via token windows and nearest neighbor sparsi\ufb01cation methods for more ef\ufb01cient antecedent prediction. We show our ap-proach\u2019s resulting reduction of training and inference time compared to state-of-the-art methods with only a minimal loss in accuracy.", "venue": "Findings", "year": 2021, "referenceCount": 18, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.343.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3921-3931"}, "authors": [{"authorId": "6476826", "name": "Raghuveer Thirukovalluru"}, {"authorId": "2121348263", "name": "Nicholas Monath"}, {"authorId": "50812160", "name": "K. Shridhar"}, {"authorId": "1771307", "name": "M. Zaheer"}, {"authorId": "2790926", "name": "Mrinmaya Sachan"}, {"authorId": "2064329879", "name": "Andrew McCallum"}]}, {"paperId": "fd90d2d2853c5b550eab7db203db9f4e7e5a2aaa", "externalIds": {"ArXiv": "2105.08206", "DBLP": "conf/acl/ReidZ21", "ACL": "2021.findings-acl.344", "DOI": "10.18653/v1/2021.findings-acl.344", "CorpusId": 234762899}, "corpusId": 234762899, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/fd90d2d2853c5b550eab7db203db9f4e7e5a2aaa", "title": "LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer", "abstract": "Many types of text style transfer can be achieved with only small, precise edits (e.g. sentiment transfer from I had a terrible time... to I had a great time...). We propose a coarse-to-fine editor for style transfer that transforms text using Levenshtein edit operations (e.g. insert, replace, delete). Unlike prior single-span edit methods, our method concurrently edits multiple spans in the source text. To train without parallel style text pairs (e.g. pairs of +/- sentiment statements), we propose an unsupervised data synthesis procedure. We first convert text to style-agnostic templates using style classifier attention (e.g. I had a SLOT time...), then fill in slots in these templates using fine-tuned pretrained language models. Our method outperforms existing generation and editing style transfer methods on sentiment (Yelp, Amazon) and politeness (Polite) transfer. In particular, multi-span editing achieves higher performance and more diverse output than single-span editing. Moreover, compared to previous methods on unsupervised data synthesis, our method results in higher quality parallel style pairs and improves model performance.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 49, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.344.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-18", "journal": {"pages": "3932-3944"}, "authors": [{"authorId": "1557386977", "name": "Machel Reid"}, {"authorId": "3428769", "name": "Victor Zhong"}]}, {"paperId": "4d243265ce959caf07e5f882fc703c34b75b86f2", "externalIds": {"ArXiv": "2105.14357", "ACL": "2021.findings-acl.345", "DBLP": "conf/acl/PalKBMWB21", "DOI": "10.18653/v1/2021.findings-acl.345", "CorpusId": 235254266}, "corpusId": 235254266, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4d243265ce959caf07e5f882fc703c34b75b86f2", "title": "Constructing Flow Graphs from Procedural Cybersecurity Texts", "abstract": "Following procedural texts written in natural languages is challenging. We must read the whole text to identify the relevant information or identify the instruction flows to complete a task, which is prone to failures. If such texts are structured, we can readily visualize instruction-flows, reason or infer a particular step, or even build automated systems to help novice agents achieve a goal. However, this structure recovery task is a challenge because of such texts' diverse nature. This paper proposes to identify relevant information from such texts and generate information flows between sentences. We built a large annotated procedural text dataset (CTFW) in the cybersecurity domain (3154 documents). This dataset contains valuable instructions regarding software vulnerability analysis experiences. We performed extensive experiments on CTFW with our LM-GNN model variants in multiple settings. To show the generalizability of both this task and our method, we also experimented with procedural texts from two other domains (Maintenance Manual and Cooking), which are substantially different from cybersecurity. Our experiments show that Graph Convolution Network with BERT sentence embeddings outperforms BERT in all three domains", "venue": "Findings", "year": 2021, "referenceCount": 55, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.345.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-29", "journal": {"pages": "3945-3957"}, "authors": [{"authorId": "50494955", "name": "Kuntal Kumar Pal"}, {"authorId": "1975172", "name": "Kazuaki Kashihara"}, {"authorId": "120722271", "name": "Pratyay Banerjee"}, {"authorId": "1817207", "name": "Swaroop Mishra"}, {"authorId": "2007575162", "name": "Ruoyu Wang"}, {"authorId": "2064619864", "name": "Chitta Baral"}]}, {"paperId": "89e2154d608cc8eced17bd7b276e278c89f8f0c1", "externalIds": {"ArXiv": "2009.06097", "ACL": "2021.findings-acl.346", "DBLP": "conf/acl/WangZGCFSCL21", "DOI": "10.18653/v1/2021.findings-acl.346", "CorpusId": 221655697}, "corpusId": 221655697, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/89e2154d608cc8eced17bd7b276e278c89f8f0c1", "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering", "abstract": "Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically with respect to the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. The proposed framework is pivoted on two unique types of Transformer layer: Sliding-Window Layer and Cluster-Former Layer, which encode local sequence information and global context jointly and iteratively. This new design allows information integration beyond local windows, which is especially beneficial for question answering (QA) tasks that rely on long-range dependencies. Experiments show that Cluster-Former achieves state-of-the-art performance on several major QA benchmarks.", "venue": "Findings", "year": 2020, "referenceCount": 42, "citationCount": 33, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.346.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-09-13", "journal": {"pages": "3958-3968"}, "authors": [{"authorId": "2992833", "name": "Shuohang Wang"}, {"authorId": "2116644664", "name": "Luowei Zhou"}, {"authorId": "144702900", "name": "Zhe Gan"}, {"authorId": "2118386757", "name": "Yen-Chun Chen"}, {"authorId": "51444591", "name": "Yuwei Fang"}, {"authorId": "2419809", "name": "S. Sun"}, {"authorId": "145215470", "name": "Yu Cheng"}, {"authorId": "46700348", "name": "Jingjing Liu"}]}, {"paperId": "9e542485a262ca6380d0df43a0a80aea6d1fd675", "externalIds": {"DBLP": "conf/acl/EskanderLKCKPM21", "ACL": "2021.findings-acl.347", "DOI": "10.18653/v1/2021.findings-acl.347", "CorpusId": 236477459}, "corpusId": 236477459, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9e542485a262ca6380d0df43a0a80aea6d1fd675", "title": "Minimally-Supervised Morphological Segmentation using Adaptor Grammars with Linguistic Priors", "abstract": "With the increasing interest in low-resource languages, unsupervised morphological segmentation has become an active area of research, where approaches based on Adaptor Grammars achieve state-of-the-art results. We demonstrate the power of harnessing linguistic knowledge as priors within Adaptor Grammars in a minimally-supervised learning fashion. We introduce two types of priors: 1) grammar definition, where we design language-specific grammars; and 2) linguistprovided affixes, collected by an expert in the language and seeded into the grammars. We use Japanese and Georgian as respective case studies for the two types of priors and introduce new datasets for these languages, with gold morphological segmentation for evaluation. We show that the use of priors results in error reductions of 8.9 % and 34.2 %, respectively, over the equivalent state-of-the-art unsupervised system.", "venue": "Findings", "year": 2021, "referenceCount": 18, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "3969-3974"}, "authors": [{"authorId": "37558091", "name": "R. Eskander"}, {"authorId": "145471184", "name": "Cass Lowry"}, {"authorId": "102998695", "name": "Sujay Khandagale"}, {"authorId": "1725413425", "name": "Francesca Callejas"}, {"authorId": "1761739", "name": "Judith L. Klavans"}, {"authorId": "3283425", "name": "M. Polinsky"}, {"authorId": "2295928", "name": "S. Muresan"}]}, {"paperId": "241abdd1a8f2e98b0b0dbb04df54a4795d799ffe", "externalIds": {"ArXiv": "2106.09790", "DBLP": "journals/corr/abs-2106-09790", "ACL": "2021.findings-acl.348", "MAG": "3173765582", "DOI": "10.18653/v1/2021.findings-acl.348", "CorpusId": 235485037}, "corpusId": 235485037, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/241abdd1a8f2e98b0b0dbb04df54a4795d799ffe", "title": "Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause Extraction", "abstract": "Detecting what emotions are expressed in text is a well-studied problem in natural language processing. However, research on finer grained emotion analysis such as what causes an emotion is still in its infancy. We present solutions that tackle both emotion recognition and emotion cause detection in a joint fashion. Considering that common-sense knowledge plays an important role in understanding implicitly expressed emotions and the reasons for those emotions, we propose novel methods that combine common-sense knowledge via adapted knowledge models with multi-task learning to perform joint emotion classification and emotion cause tagging. We show performance improvement on both tasks when including common-sense reasoning and a multitask framework. We provide a thorough analysis to gain insights into model performance.", "venue": "Findings", "year": 2021, "referenceCount": 38, "citationCount": 24, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.348.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-17", "journal": {"pages": "3975-3989"}, "authors": [{"authorId": "1402934614", "name": "Elsbeth Turcan"}, {"authorId": "1717480", "name": "Shuai Wang"}, {"authorId": "2432216", "name": "Rishita Anubhai"}, {"authorId": "47812409", "name": "Kasturi Bhattacharjee"}, {"authorId": "1403907739", "name": "Yaser Al-Onaizan"}, {"authorId": "2295928", "name": "S. Muresan"}]}, {"paperId": "7fdb0c6d5cbb4bbee78a306554e5937b67a3e491", "externalIds": {"MAG": "3175564582", "DBLP": "conf/acl/AgarwalN21", "ACL": "2021.findings-acl.349", "DOI": "10.18653/v1/2021.findings-acl.349", "CorpusId": 236478172}, "corpusId": 236478172, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7fdb0c6d5cbb4bbee78a306554e5937b67a3e491", "title": "The Utility and Interplay of Gazetteers and Entity Segmentation for Named Entity Recognition in English", "abstract": "Recent papers have introduced methods to incorporate gazetteer features and entity segmentation techniques in neural named entity recognition models. These papers rely on different resources and include features not related to the use of gazetteers, rendering impossible the comparison of the relative effectiveness of the approaches. Here, we provide a comprehensive overview of methods for incorporating gazetteers and for entity segmentation. We evaluate representative methods from each in similar settings for a fair comparison and identify the ones that are consistently better across datasets and input representations. We further show that gazetteers improve entity segmentation and not just entity typing. Hence, we explore their utility in recognizing long entities, a problem for which entity segmentation techniques were developed. Our work explains the mechanisms via which gazetteers improve the performance of neural NER models.", "venue": "Findings", "year": 2021, "referenceCount": 60, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.349.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-08-01", "journal": {"pages": "3990-4002"}, "authors": [{"authorId": "47016717", "name": "Oshin Agarwal"}, {"authorId": "3115414", "name": "A. Nenkova"}]}, {"paperId": "7af9d490ebe4fef40ba125e67056beee825b3b1c", "externalIds": {"ACL": "2021.findings-acl.350", "DBLP": "conf/acl/GomesGRC21", "DOI": "10.18653/v1/2021.findings-acl.350", "CorpusId": 236478196}, "corpusId": 236478196, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7af9d490ebe4fef40ba125e67056beee825b3b1c", "title": "On the Cost-Effectiveness of Stacking of Neural and Non-Neural Methods for Text Classification: Scenarios and Performance Prediction", "abstract": "Nowadays, neural networks algorithms, such as those based on Attention and Transformers, have excelled on Automatic Text Classification (ATC). However, such enhanced performance comes at high computational costs. Stacking of simpler classifiers that exploit algorithmic and representational complementarity has also been shown to produce superior performance in ATC, enjoying high effectiveness and potentially lower computational costs than complex neural networks. In this master's thesis, we present the first and largest comparative study to exploit the cost-effectiveness of Stacking in ATC, consisting of Transformers and non-neural algorithms. In particular, we are interested in answering the following research question: Is it possible to obtain an effective ensemble with significantly less computational cost than the best learning model for a given dataset? Besides answering that question, another main contribution of this thesis is the proposal of a low-cost oracle-based method that can predict the best ensemble in each scenario using only a fraction of the training data.", "venue": "Findings", "year": 2022, "referenceCount": 47, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.350.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-07-31", "journal": {"pages": "4003-4014"}, "authors": [{"authorId": "12751540", "name": "Christian Gomes"}, {"authorId": "1686355", "name": "Marcos Andr\u00e9 Gon\u00e7alves"}, {"authorId": "145534473", "name": "L. Rocha"}, {"authorId": "144914138", "name": "S\u00e9rgio D. Canuto"}]}, {"paperId": "4bedcae34d4526ea9fb7220eb6a90218673e2288", "externalIds": {"ACL": "2021.findings-acl.351", "DBLP": "conf/acl/NgoPN21", "DOI": "10.18653/v1/2021.findings-acl.351", "CorpusId": 236477674}, "corpusId": 236477674, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4bedcae34d4526ea9fb7220eb6a90218673e2288", "title": "Unsupervised Domain Adaptation for Event Detection using Domain-specific Adapters", "abstract": "Due to the multi-dimensional variation of tex-tual data, detection of event triggers from new domains can become a lot more challenging. This prompts a need to research on domain adaptation methods for event detection task, especially for the most practical unsupervised setting. Recently, large transformer-based language models, e.g. BERT, have become essen-tial to achieve top performance for event detection. However, their unwieldy nature also pre-vents effective adaptation across domains. To this end, this work proposes a D omain-speci\ufb01c A dapter-based A daptation ( DAA ) framework to improve the adaptability of BERT-based models for event detection across domains. By explicitly representing data from different domains with separate adapter modules in each layer of BERT, DAA introduces a novel joint representation learning mechanism and a Wasserstein distance-based technique for data selection in adversarial learning to substantially boost the performance on target domains. Extensive experiments and analysis over different datasets (i.e., LitBank, TimeBank, and ACE-05) demonstrate the effectiveness of our approach.", "venue": "Findings", "year": 2021, "referenceCount": 44, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.351.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4015-4025"}, "authors": [{"authorId": "1692755523", "name": "Nghia Trung Ngo"}, {"authorId": "2064597503", "name": "Duy Phung"}, {"authorId": "1811211", "name": "Thien Huu Nguyen"}]}, {"paperId": "4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733", "externalIds": {"ACL": "2021.findings-acl.352", "DBLP": "conf/acl/DeznabiIF21", "DOI": "10.18653/v1/2021.findings-acl.352", "CorpusId": 236477932}, "corpusId": 236477932, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733", "title": "Predicting in-hospital mortality by combining clinical notes with time-series data", "abstract": "In intensive care units (ICUs), patient health is monitored through (1) continuous vital signals from various medical devices, and (2) clinical notes consisting of opinions and summaries from doctors which are recorded in electronic health records (EHR). It is difficult to jointly model these two sources of information because clinical notes, unlike vital signals, are collected at irregular intervals and their contents are relatively unstructured. In this paper, we present a model that combines both sources of information about ICU patients to make accurate in-hospital mortality predictions. We apply a fine-tuned BERT model to each of the patient\u2019s clinical notes. The resulting embeddings are then combined to obtain the overall embedding for the entire text part of the data. This is then combined with the output of an LSTM model that encodes patients\u2019 vital signals. Our model improves upon the state of the art for mortality prediction, attaining an AUC score of 0.9, compared to the previous 0.87, setting a new standard for mortality prediction on the MIMIC III benchmark.1", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.352.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4026-4031"}, "authors": [{"authorId": "51180682", "name": "I. Deznabi"}, {"authorId": "2136562", "name": "Mohit Iyyer"}, {"authorId": "2592453", "name": "M. Fiterau"}]}, {"paperId": "7569d4b5ab8f609dadb7a1f4c4839327b6b36de7", "externalIds": {"ACL": "2021.findings-acl.353", "DBLP": "conf/acl/WuDY21", "DOI": "10.18653/v1/2021.findings-acl.353", "CorpusId": 236477351}, "corpusId": 236477351, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7569d4b5ab8f609dadb7a1f4c4839327b6b36de7", "title": "Sequence Models for Computational Etymology of Borrowings", "abstract": "We computationally model the processes of word borrowing from a donor word to an incorporated word, and vice versa, by answering two questions: (1) what does a word look like incorporated into another language, and in the opposite direction (2) where did a word come from? We employ neural sequence models, focusing on six specific borrowing relations: calques, partial calques, semantic loans, phono-semantic matches, transliterations, and generic borrowings. We experiment with several model variants, including LSTM encoderdecoders, copy attention, and Transformers. In both directions, we find that an LSTM model can beat strong baselines, with the quantity of data strongly influencing model performance.", "venue": "Findings", "year": 2021, "referenceCount": 17, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4032-4037"}, "authors": [{"authorId": "2110027458", "name": "Winston Wu"}, {"authorId": "1800354", "name": "Kevin Duh"}, {"authorId": "1693517", "name": "David Yarowsky"}]}, {"paperId": "b393a7a481345e8a5567d8fe56209a1c1fe88d04", "externalIds": {"ACL": "2021.findings-acl.354", "ArXiv": "2010.12873", "DBLP": "conf/acl/YanRCZRZKLR21", "DOI": "10.18653/v1/2021.findings-acl.354", "CorpusId": 225070241}, "corpusId": 225070241, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b393a7a481345e8a5567d8fe56209a1c1fe88d04", "title": "Learning Contextualized Knowledge Structures for Commonsense Reasoning", "abstract": "Recently, knowledge graph (KG) augmented models have achieved noteworthy success on various commonsense reasoning tasks. However, KG edge (fact) sparsity and noisy edge extraction/generation often hinder models from obtaining useful knowledge to reason over. To address these issues, we propose a new KG-augmented model: Hybrid Graph Network (HGN). Unlike prior methods, HGN learns to jointly contextualize extracted and generated knowledge by reasoning over both within a unified graph structure. Given the task input context and an extracted KG subgraph, HGN is trained to generate embeddings for the subgraph's missing edges to form a\"hybrid\"graph, then reason over the hybrid graph while filtering out context-irrelevant edges. We demonstrate HGN's effectiveness through considerable performance gains across four commonsense reasoning benchmarks, plus a user study on edge validness and helpfulness.", "venue": "Findings", "year": 2020, "referenceCount": 49, "citationCount": 28, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.354.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-10-24", "journal": {"pages": "4038-4051"}, "authors": [{"authorId": "49781448", "name": "Jun Yan"}, {"authorId": "1838175151", "name": "Mrigank Raman"}, {"authorId": "2114015857", "name": "Aaron Chan"}, {"authorId": null, "name": "Tianyu Zhang"}, {"authorId": "1862090", "name": "Ryan A. Rossi"}, {"authorId": "7574699", "name": "Handong Zhao"}, {"authorId": "2109571021", "name": "Sungchul Kim"}, {"authorId": "1793409", "name": "Nedim Lipka"}, {"authorId": "1384550891", "name": "Xiang Ren"}]}, {"paperId": "54db327cd53fe043449c9f242d3fc34c593a70ef", "externalIds": {"DBLP": "conf/acl/SotnikovaCDR21", "ACL": "2021.findings-acl.355", "DOI": "10.18653/v1/2021.findings-acl.355", "CorpusId": 236477764}, "corpusId": 236477764, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/54db327cd53fe043449c9f242d3fc34c593a70ef", "title": "Analyzing Stereotypes in Generative Text Inference Tasks", "abstract": "Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or common-sensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality", "venue": "Findings", "year": 2021, "referenceCount": 49, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4052-4065"}, "authors": [{"authorId": "2172271065", "name": "Anna Sotnikova"}, {"authorId": "48696491", "name": "Yang Trista Cao"}, {"authorId": "1722360", "name": "Hal Daum\u00e9"}, {"authorId": "2034613", "name": "Rachel Rudinger"}]}, {"paperId": "77ee8a42b007ce1fa22718647298f496ef0ac499", "externalIds": {"DBLP": "journals/corr/abs-2106-15838", "ACL": "2021.findings-acl.356", "ArXiv": "2106.15838", "DOI": "10.18653/v1/2021.findings-acl.356", "CorpusId": 235683036}, "corpusId": 235683036, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/77ee8a42b007ce1fa22718647298f496ef0ac499", "title": "HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction", "abstract": "Text-to-Graph extraction aims to automatically extract information graphs consisting of mentions and types from natural language texts. Existing approaches, such as table filling and pairwise scoring, have shown impressive performance on various information extraction tasks, but they are difficult to scale to datasets with longer input texts because of their second-order space/time complexities with respect to the input length. In this work, we propose a Hybrid Span Generator (HySPA) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-of-the-art on the joint entity and relation extraction task.", "venue": "Findings", "year": 2021, "referenceCount": 44, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.356.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-30", "journal": {"pages": "4066-4078"}, "authors": [{"authorId": "46177458", "name": "Liliang Ren"}, {"authorId": "1726046634", "name": "Chenkai Sun"}, {"authorId": "144016781", "name": "Heng Ji"}, {"authorId": "3118681", "name": "J. Hockenmaier"}]}, {"paperId": "43946aa78c80b6d7e6ffff837bdf4cff85f6a935", "externalIds": {"ACL": "2021.findings-acl.357", "DBLP": "conf/acl/GangalJHB21", "ArXiv": "2106.02833", "DOI": "10.18653/v1/2021.findings-acl.357", "CorpusId": 235358430}, "corpusId": 235358430, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/43946aa78c80b6d7e6ffff837bdf4cff85f6a935", "title": "Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation", "abstract": "Multiple different responses are often plausible for a given open domain dialog context. Prior work has shown the importance of having multiple valid reference responses for meaningful and robust automated evaluations. In such cases, common practice has been to collect more human written references. However, such collection can be expensive, time consuming, and not easily scalable. Instead, we propose a novel technique for automatically expanding a human generated reference to a set of candidate references. We fetch plausible references from knowledge sources, and adapt them so that they are more fluent in context of the dialog instance in question. More specifically, we use (1) a commonsense knowledge base to elicit a large number of plausible reactions given the dialog history (2) relevant instances retrieved from dialog corpus, using similar past as well as future contexts. We demonstrate that our automatically expanded reference sets lead to large improvements in correlations of automated metrics with human ratings of system outputs for DailyDialog dataset.", "venue": "Findings", "year": 2021, "referenceCount": 52, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.357.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-05", "journal": {"name": "ArXiv", "volume": "abs/2106.02833"}, "authors": [{"authorId": "3375999", "name": "Varun Gangal"}, {"authorId": "2006291", "name": "Harsh Jhamtani"}, {"authorId": "144547315", "name": "E. Hovy"}, {"authorId": "1400419309", "name": "Taylor Berg-Kirkpatrick"}]}, {"paperId": "c2d33e97c811cfb0b4aa8956cdcaeab828c7f356", "externalIds": {"ArXiv": "2106.01033", "ACL": "2021.findings-acl.358", "DBLP": "journals/corr/abs-2106-01033", "DOI": "10.18653/v1/2021.findings-acl.358", "CorpusId": 235293924}, "corpusId": 235293924, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c2d33e97c811cfb0b4aa8956cdcaeab828c7f356", "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text", "abstract": "Understanding who blames or supports whom in news text is a critical research question in computational social science. Traditional methods and datasets for sentiment analysis are, however, not suitable for the domain of political text as they do not consider the direction of sentiments expressed between entities. In this paper, we propose a novel NLP task of identifying directed sentiment relationship between political entities from a given news document, which we call directed sentiment extraction. From a million-scale news corpus, we construct a dataset of news sentences where sentiment relations of political entities are manually annotated. We present a simple but effective approach for utilizing a pretrained transformer, which infers the target class by predicting multiple question-answering tasks and combining the outcomes. We demonstrate the utility of our proposed method for social science research questions by analyzing positive and negative opinions between political entities in two major events: 2016 U.S. presidential election and COVID-19. The newly proposed problem, data, and method will facilitate future studies on interdisciplinary NLP methods and applications. \u00a9 2021 Association for Computational Linguistics", "venue": "Findings", "year": 2021, "referenceCount": 46, "citationCount": 3, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.358.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "4091-4102"}, "authors": [{"authorId": "1993530420", "name": "Kunwoo Park"}, {"authorId": "1383271298", "name": "Zhufeng Pan"}, {"authorId": "1834047", "name": "Jungseock Joo"}]}, {"paperId": "663d5e73ee992e6e39029de386a11103935c4084", "externalIds": {"ACL": "2021.findings-acl.359", "DBLP": "conf/acl/TrinhR21", "DOI": "10.18653/v1/2021.findings-acl.359", "CorpusId": 236478373}, "corpusId": 236478373, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/663d5e73ee992e6e39029de386a11103935c4084", "title": "New Dataset and Strong Baselines for the Grammatical Error Correction of Russian", "abstract": "Motivated by recent advancements in grammatical error correction in English and existing issues in the field, we describe a new resource, an annotated learner corpus of Russian, extracted from the Lang-8 language learning website. This new dataset is benchmarked against two grammatical error correction models that use state-of-the-art neural architectures. Results are provided on the newlycreated corpus and are compared against performance on another, existing resource. We also evaluate the contribution of the Lang-8 training data to the grammatical error correction of Russian and perform type-based analysis of the models. The expert annotations are available for research purposes.", "venue": "Findings", "year": 2021, "referenceCount": 45, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.359.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4103-4111"}, "authors": [{"authorId": "51268287", "name": "V. Trinh"}, {"authorId": "2271568", "name": "Alla Rozovskaya"}]}, {"paperId": "1da5615b2cbc6a4afe8f4a3469c94ba38d69c4d9", "externalIds": {"DBLP": "conf/acl/SamirBS21", "ACL": "2021.findings-acl.360", "DOI": "10.18653/v1/2021.findings-acl.360", "CorpusId": 236477827}, "corpusId": 236477827, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1da5615b2cbc6a4afe8f4a3469c94ba38d69c4d9", "title": "A Formidable Ability: Detecting Adjectival Extremeness with DSMs", "abstract": "While distributional semantic models (DSMs) can successfully capture the similarity structure within a semantic domain, less is known about their ability to represent abstract semantic properties that hold across domains. Such properties can form the basis for abstract semantic classes that are a crucial aspect of human semantic knowledge. For example, the abstract class of extreme adjectives (such as brilliant and freezing) spans a wide range of domains (here, INTELLIGENCE and TEMPERATURE). Using a model that compares query items to an aggregate DSM representation of a set of extreme adjectives, we show that novel adjectives can be classified accurately, supporting the insight that a cross-domain property like extremeness can be captured in a word\u2019s DSM representation. We then use the extremeness classifier to model the emergence of intensifier meaning in adverbs, demonstrating, in a separate task, the effectiveness of detecting this abstract semantic property. 1 Distributional Models and Abstract Semantic Classes Distributional semantic models (DSMs) are widely used as representations of word-level semantics. However, open questions remain as to precisely which aspects of human semantic knowledge DSMs effectively capture (e.g., Baroni et al., 2014; Hollis and Westbury, 2016; Schnabel et al., 2015; Utsumi, 2020). For example, popular DSMs such as word2vec and GloVe have been shown to predict human ratings of semantic features of objects (Rubinstein et al., 2015; Grand et al., 2018). However, performance is variable across features and object categories (Grand et al., 2018), and in particular, is better for taxonomic properties (\u2018is an animal\u2019, \u2018is a weapon\u2019) than for general attributive properties (\u2018is yellow\u2019, \u2018is dangerous\u2019) (Rubinstein et al., 2015). While people may or may not have semantic categories such as \u201call yellow things\u201d, abstract semantic classes are an important part of human linguistic knowledge that should be captured in a computational system. Note that by abstract we mean the schematic properties of word meaning, rather than the content-related classes;1 such properties abstract over commonalities of meaning that may cross traditional semantic domains. Consider, e.g., a semantic verb class such as changeof-state (Levin, 1993; Kipper et al., 2008), with members such as melt (the TEMPERATURE domain) and quicken (SPEED), or relational adjectives (Boleda et al., 2012), including, e.g., Chinese (NATIONALITY) or pulmonary (BODY-PART). Much work shows the ability of DSMs to match human knowledge of semantic properties within a domain (e.g., Baroni et al., 2014; Pereira et al., 2016; An et al., 2018; Grand et al., 2018), but there is little work, to our knowledge, on whether the similarity structure of a DSM is sensitive to commonalities of abstract properties that hold across a variety of semantic domains.2 Research on vector-based representations of analogy suggests that DSMs may be limited in their ability to represent crossdomain word relations: Rogers et al. (2017) show that cross-domain analogical relations like hypernymy (e.g., turtle:reptile::salmon:fish) are significantly harder to solve than within-domain ones (e.g., Paris:France::Ottawa:Canada). Lu et al. (2019) make significant progress towards representing such relations, showing that a DSM can form the basis for detecting the cross-domain word The same distinction between schematic and content is applied in Paradis (2001); Cruse and Togia (1996). It is also worth noting explicitly that our use of the term \u2018abstract\u2019 in this sense is not to be interpreted as \u2018not concrete\u2019. DSMs may, e.g., encode concreteness and valence/arousal/dominance (e.g., Hollis and Westbury, 2016; Hollis et al., 2017), but the former can be viewed as a taxonomic property, and the latter as within the EMOTION domain.", "venue": "Findings", "year": 2021, "referenceCount": 48, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.360.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4112-4125"}, "authors": [{"authorId": "2047738160", "name": "Farhan Samir"}, {"authorId": "2076212", "name": "Barend Beekhuizen"}, {"authorId": "145584212", "name": "S. Stevenson"}]}, {"paperId": "3fa01ebe92d8bab53b2756beeecfe6faa9e573bb", "externalIds": {"DBLP": "journals/corr/abs-2105-08855", "ACL": "2021.findings-acl.361", "ArXiv": "2105.08855", "DOI": "10.18653/v1/2021.findings-acl.361", "CorpusId": 234777911}, "corpusId": 234777911, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/3fa01ebe92d8bab53b2756beeecfe6faa9e573bb", "title": "Effective Attention Sheds Light On Interpretability", "abstract": "An attention matrix of a transformer self-attention sublayer can provably be decomposed into two components and only one of them (effective attention) contributes to the model output. This leads us to ask whether visualizing effective attention gives different conclusions than interpretation of standard attention. Using a subset of the GLUE tasks and BERT, we carry out an analysis to compare the two attention matrices, and show that their interpretations differ. Effective attention is less associated with the features related to the language modeling pretraining such as the separator token, and it has more potential to illustrate linguistic features captured by the model for solving the end-task. Given the found differences, we recommend using effective attention for studying a transformer's behavior since it is more pertinent to the model output by design.", "venue": "Findings", "year": 2021, "referenceCount": 27, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.361.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-18", "journal": {"name": "ArXiv", "volume": "abs/2105.08855"}, "authors": [{"authorId": "2087314342", "name": "Kaiser Sun"}, {"authorId": "3451494", "name": "Ana Marasovi\u0107"}]}, {"paperId": "60b59b92739a24de4b4255628da63ac5534e0051", "externalIds": {"ACL": "2021.findings-acl.362", "DBLP": "conf/acl/BornKMS21", "DOI": "10.18653/v1/2021.findings-acl.362", "CorpusId": 236478359}, "corpusId": 236478359, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/60b59b92739a24de4b4255628da63ac5534e0051", "title": "Compositionality of Complex Graphemes in the Undeciphered Proto-Elamite Script using Image and Text Embedding Models", "abstract": "We introduce a language modeling architecture which operates over sequences of images, or over multimodal sequences of images with associated labels. We use this architecture alongside other embedding models to investigate a category of signs called complex graphemes (CGs) in the undeciphered proto-Elamite script. We argue that CGs have meanings which are at least partly compositional, and we discover novel rules governing the construction of CGs. We \ufb01nd that a language model over sign images produces more inter-pretable results than a model over text or over sign images and text, which suggests that the names given to signs may be obscuring signals in the corpus. Our results reveal previously unknown regularities in proto-Elamite sign use that can inform future decipherment efforts, and our image-aware language model provides a novel way to abstract away from biases introduced by human annotators.", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4136-4146"}, "authors": [{"authorId": "49673896", "name": "Logan Born"}, {"authorId": "51283117", "name": "Kathryn Kelley"}, {"authorId": "119431562", "name": "M. Monroe"}, {"authorId": "3028658", "name": "Anoop Sarkar"}]}, {"paperId": "daab62304d0b1beeddad06846eaadce9c7610d9d", "externalIds": {"DBLP": "conf/acl/JiJFMSB21", "ArXiv": "2106.01335", "ACL": "2021.findings-acl.363", "DOI": "10.18653/v1/2021.findings-acl.363", "CorpusId": 235293987}, "corpusId": 235293987, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/daab62304d0b1beeddad06846eaadce9c7610d9d", "title": "On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers", "abstract": "How much information do NLP tasks really need from a transformer's attention mechanism at application-time (inference)? From recent work, we know that there is sparsity in transformers and that the floating-points within its computation can be discretized to fewer values with minimal loss to task accuracies. However, this requires retraining or even creating entirely new models, both of which can be expensive and carbon-emitting. Focused on optimizations that do not require training, we systematically study the full range of typical attention values necessary. This informs the design of an inference-time quantization technique using both pruning and log-scaled mapping which produces only a few (e.g. $2^3$) unique values. Over the tasks of question answering and sentiment analysis, we find nearly 80% of attention values can be pruned to zeros with minimal ($<1.0\\%$) relative loss in accuracy. We use this pruning technique in conjunction with quantizing the attention values to only a 3-bit format, without retraining, resulting in only a 0.8% accuracy reduction on question answering with fine-tuned RoBERTa.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.363.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01335"}, "authors": [{"authorId": "51129266", "name": "Tianchu Ji"}, {"authorId": "2116971683", "name": "Shraddhan Jain"}, {"authorId": "1843152", "name": "M. Ferdman"}, {"authorId": "2719024", "name": "Peter Milder"}, {"authorId": "145035129", "name": "H. A. Schwartz"}, {"authorId": "35217367", "name": "Niranjan Balasubramanian"}]}, {"paperId": "6f9fc51102cf49bff4f4e2b336739a45f8389c80", "externalIds": {"ACL": "2021.findings-acl.364", "DBLP": "journals/corr/abs-2106-01465", "ArXiv": "2106.01465", "DOI": "10.18653/v1/2021.findings-acl.364", "CorpusId": 235313409}, "corpusId": 235313409, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6f9fc51102cf49bff4f4e2b336739a45f8389c80", "title": "Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?", "abstract": "Is it possible to use natural language to intervene in a model's behavior and alter its prediction in a desired way? We investigate the effectiveness of natural language interventions for reading-comprehension systems, studying this in the context of social stereotypes. Specifically, we propose a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a question-answering (QA) model's unethical behavior by communicating context-specific principles of ethics and equity to it. To this end, we build upon recent methods for quantifying a system's social stereotypes, augmenting them with different kinds of ethical interventions and the desired model behavior under such interventions. Our zero-shot evaluation finds that even today's powerful neural language models are extremely poor ethical-advice takers, that is, they respond surprisingly little to ethical interventions even though these interventions are stated as simple sentences. Few-shot learning improves model behavior but remains far from the desired outcome, especially when evaluated for various types of generalization. Our new task thus poses a novel language understanding challenge for the community.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 26, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.364.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "4158-4164"}, "authors": [{"authorId": "2110117732", "name": "Jieyu Zhao"}, {"authorId": "1783281", "name": "Daniel Khashabi"}, {"authorId": "2236429", "name": "Tushar Khot"}, {"authorId": "48229640", "name": "Ashish Sabharwal"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}]}, {"paperId": "8ed62fb352098f3f943e2c1b08bb9998589de03d", "externalIds": {"DBLP": "conf/acl/ChuSG21", "MAG": "3112136407", "ACL": "2021.findings-acl.365", "ArXiv": "2012.04194", "DOI": "10.18653/v1/2021.findings-acl.365", "CorpusId": 227745060}, "corpusId": 227745060, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8ed62fb352098f3f943e2c1b08bb9998589de03d", "title": "Unsupervised Label Refinement Improves Dataless Text Classification", "abstract": "Dataless text classification is capable of classifying documents into previously unseen labels by assigning a score to any document paired with a label description. While promising, it crucially relies on accurate descriptions of the label set for each downstream task. This reliance causes dataless classifiers to be highly sensitive to the choice of label descriptions and hinders the broader application of dataless classification in practice. In this paper, we ask the following question: how can we improve dataless text classification using the inputs of the downstream task dataset? Our primary solution is a clustering based approach. Given a dataless classifier, our approach refines its set of predictions using k-means clustering. We demonstrate the broad applicability of our approach by improving the performance of two widely used classifier architectures, one that encodes text-category pairs with two independent encoders and one with a single joint encoder. Experiments show that our approach consistently improves dataless classification across different datasets and makes the classifier more robust to the choice of label descriptions.", "venue": "Findings", "year": 2020, "referenceCount": 42, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.365.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-12-08", "journal": {"name": "ArXiv", "volume": "abs/2012.04194"}, "authors": [{"authorId": "2802149", "name": "Z. Chu"}, {"authorId": "1714215", "name": "K. Stratos"}, {"authorId": "1700980", "name": "Kevin Gimpel"}]}, {"paperId": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c", "externalIds": {"ACL": "2021.findings-acl.366", "ArXiv": "2106.06823", "DBLP": "conf/acl/ParanjapeMGHZ21", "DOI": "10.18653/v1/2021.findings-acl.366", "CorpusId": 234797436}, "corpusId": 234797436, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7747ecbc26b1688e6cad1a6ce83914efa2a3c04c", "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks", "abstract": "Many commonsense reasoning NLP tasks involve choosing between one or more possible answers to a question or prompt based on knowledge that is often implicit. Large pretrained language models (PLMs) can achieve near-human performance on such tasks, while providing little human-interpretable evidence of the underlying reasoning they use. In this work, we show how to use these same models to generate such evidence: inspired by the contrastive nature of human explanations, we use PLMs to complete explanation prompts which contrast alternatives according to the key attribute(s) required to justify the correct answer (for example, peanuts are usually salty while raisins are sweet). Conditioning model decisions on these explanations improves performance on two commonsense reasoning benchmarks, as compared to previous non-contrastive alternatives. These explanations are also judged by humans to be more relevant for solving the task, and facilitate a novel method to evaluate explanation faithfulfness.", "venue": "Findings", "year": 2021, "referenceCount": 49, "citationCount": 43, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.366.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-12", "journal": {"pages": "4179-4192"}, "authors": [{"authorId": "8005713", "name": "Bhargavi Paranjape"}, {"authorId": "38614754", "name": "Julian Michael"}, {"authorId": "2320509", "name": "Marjan Ghazvininejad"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}]}, {"paperId": "69910e6f11930353a2c0296e46631d41a2124b80", "externalIds": {"DBLP": "conf/acl/SousaPPG21", "ACL": "2021.findings-acl.367", "DOI": "10.18653/v1/2021.findings-acl.367", "CorpusId": 236477800}, "corpusId": 236477800, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/69910e6f11930353a2c0296e46631d41a2124b80", "title": "SMS Spam Detection Through Skip-gram Embeddings and Shallow Networks", "abstract": "The drastic decrease in mobile SMS costs turned phone users more prone to spam messages, usually with unwanted marketing or questionable content. As such, researchers have proposed different methods for detecting SMS spam messages. This paper presents a technique for embedding SMS messages into vector spaces that is suitable for spam detection. The proposed approach relies on mining patterns that are relevant for distinguishing spam from legitimate messages. A subset of those patterns is used to construct a function that maps text messages into a multidimensional vector space. The extracted patterns are represented as skip-grams of token attributes, where a skip-gram can be seen as a generalization of the n-gram model that allows a distance greater than one between matched tokens in the text. We evaluate the proposed approach using the generated vectors for spam classification on the UCI Spam Collection dataset. The experiments showed that our method combined with shallow networks reached accuracy that is competitive with state-of-the-art approaches.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.367.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4193-4201"}, "authors": [{"authorId": "145631350", "name": "Gustavo Jos\u00e9 de Sousa"}, {"authorId": "144869373", "name": "D. C. G. Pedronette"}, {"authorId": "1759037", "name": "J. Papa"}, {"authorId": "2395588", "name": "I. R. Guilherme"}]}, {"paperId": "f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d", "externalIds": {"ACL": "2021.findings-acl.368", "ArXiv": "2106.03427", "DBLP": "journals/corr/abs-2106-03427", "DOI": "10.18653/v1/2021.findings-acl.368", "CorpusId": 235358897}, "corpusId": 235358897, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d", "title": "Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring", "abstract": "Despite recent progress, learning new tasks through language instructions remains an extremely challenging problem. On the ALFRED benchmark for task learning, the published state-of-the-art system only achieves a task success rate of less than 10% in an unseen environment, compared to the human performance of over 90%. To address this issue, this paper takes a closer look at task learning. In a departure from a widely applied end-to-end architecture, we decomposed task learning into three sub-problems: sub-goal planning, scene navigation, and object manipulation; and developed a model HiTUT (stands for Hierarchical Tasks via Unified Transformers) that addresses each sub-problem in a unified manner to learn a hierarchical task structure. On the ALFRED benchmark, HiTUT has achieved the best performance with a remarkably higher generalization ability. In the unseen environment, HiTUT achieves over 160% performance gain in success rate compared to the previous state of the art. The explicit representation of task structures also enables an in-depth understanding of the nature of the problem and the ability of the agent, which provides insight for future benchmark development and evaluation.", "venue": "Findings", "year": 2021, "referenceCount": 47, "citationCount": 51, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.368.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-07", "journal": {"pages": "4202-4213"}, "authors": [{"authorId": "46868553", "name": "Yichi Zhang"}, {"authorId": "1707259", "name": "J. Chai"}]}, {"paperId": "6df96339baab731d79974eedcf85e4401ea9e5fe", "externalIds": {"DBLP": "journals/corr/abs-2109-14039", "ACL": "2021.findings-acl.369", "ArXiv": "2109.14039", "DOI": "10.18653/v1/2021.findings-acl.369", "CorpusId": 236477415}, "corpusId": 236477415, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6df96339baab731d79974eedcf85e4401ea9e5fe", "title": "Marked Attribute Bias in Natural Language Inference", "abstract": "Reporting and providing test sets for harmful bias in NLP applications is essential for building a robust understanding of the current problem. We present a new observation of gender bias in a downstream NLP application: marked attribute bias in natural language inference. Bias in downstream applications can stem from training data, word embeddings, or be amplified by the model in use. However, focusing on biased word embeddings is potentially the most impactful first step due to their universal nature. Here we seek to understand how the intrinsic properties of word embeddings contribute to this observed marked attribute effect, and whether current post-processing methods address the bias successfully. An investigation of the current debiasing landscape reveals two open problems: none of the current debiased embeddings mitigate the marked attribute error, and none of the intrinsic bias measures are predictive of the marked attribute effect. By noticing that a new type of intrinsic bias measure correlates meaningfully with the marked attribute effect, we propose a new postprocessing debiasing scheme for static word embeddings. The proposed method applied to existing embeddings achieves new best results on the marked attribute bias test set. See https://github.com/hillary-dawkins/MAB.", "venue": "Findings", "year": 2021, "referenceCount": 28, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.369.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-28", "journal": {"pages": "4214-4226"}, "authors": [{"authorId": "49044849", "name": "Hillary Dawkins"}]}, {"paperId": "18f37f62d2bf3c2e34e2bde78545b47e92d7b72d", "externalIds": {"ArXiv": "2105.09996", "DBLP": "journals/corr/abs-2105-09996", "ACL": "2021.findings-acl.370", "DOI": "10.18653/v1/2021.findings-acl.370", "CorpusId": 235125628}, "corpusId": 235125628, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/18f37f62d2bf3c2e34e2bde78545b47e92d7b72d", "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding", "abstract": "We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks. Existing pre-training are task-specific by adopting either a single cross-modal encoder that requires both modalities, limiting their use for retrieval-style end tasks or more complex multitask learning with two unimodal encoders, limiting early cross-modal fusion. We instead introduce new pretraining masking schemes that better mix across modalities (e.g. by forcing masks for text to predict the closest video embeddings) while also maintaining separability (e.g. unimodal predictions are sometimes required, without using all the input). Experimental results show strong performance across a wider range of tasks than any previous methods, often outperforming task-specific pre-training. Code is made available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 79, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.370.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-20", "journal": {"name": "ArXiv", "volume": "abs/2105.09996"}, "authors": [{"authorId": "145902876", "name": "Hu Xu"}, {"authorId": "134007132", "name": "Gargi Ghosh"}, {"authorId": "2319973", "name": "Po-Yao (Bernie) Huang"}, {"authorId": "152384865", "name": "Prahal Arora"}, {"authorId": "100969823", "name": "Masoumeh Aminzadeh"}, {"authorId": "2322150", "name": "Christoph Feichtenhofer"}, {"authorId": "1740721", "name": "Florian Metze"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}]}, {"paperId": "5e1669dd9b568f2a8a0804c2eea241006568b594", "externalIds": {"DBLP": "journals/corr/abs-2105-12936", "ACL": "2021.findings-acl.371", "ArXiv": "2105.12936", "DOI": "10.18653/v1/2021.findings-acl.371", "CorpusId": 235212583}, "corpusId": 235212583, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5e1669dd9b568f2a8a0804c2eea241006568b594", "title": "Corpus-Level Evaluation for Event QA: The IndiaPoliceEvents Corpus Covering the 2002 Gujarat Violence", "abstract": "Automated event extraction in social science applications often requires corpus-level evaluations: for example, aggregating text predictions across metadata and unbiased estimates of recall. We combine corpus-level evaluation requirements with a real-world, social science setting and introduce the IndiaPoliceEvents corpus--all 21,391 sentences from 1,257 English-language Times of India articles about events in the state of Gujarat during March 2002. Our trained annotators read and label every document for mentions of police activity events, allowing for unbiased recall evaluations. In contrast to other datasets with structured event representations, we gather annotations by posing natural questions, and evaluate off-the-shelf models for three different tasks: sentence classification, document ranking, and temporal aggregation of target events. We present baseline results from zero-shot BERT-based models fine-tuned on natural language inference and passage retrieval tasks. Our novel corpus-level evaluations and annotation approach can guide creation of similar social-science-oriented resources in the future.", "venue": "Findings", "year": 2021, "referenceCount": 96, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.371.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-27", "journal": {"pages": "4240-4253"}, "authors": [{"authorId": "2114703564", "name": "A. Halterman"}, {"authorId": "145137850", "name": "Katherine A. Keith"}, {"authorId": "2721029", "name": "Sheikh Muhammad Sarwar"}, {"authorId": "1401020033", "name": "Brendan T. O'Connor"}]}, {"paperId": "1a57318be32b740aef1d9b2070db6c0cc565ab0a", "externalIds": {"ACL": "2021.findings-acl.372", "DBLP": "conf/acl/ZhaoDSZWC21", "ArXiv": "2105.14669", "DOI": "10.18653/v1/2021.findings-acl.372", "CorpusId": 235254260}, "corpusId": 235254260, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1a57318be32b740aef1d9b2070db6c0cc565ab0a", "title": "Memory-Efficient Differentiable Transformer Architecture Search", "abstract": "Differentiable architecture search (DARTS) is successfully applied in many vision tasks. However, directly using DARTS for Transformers is memory-intensive, which renders the search process infeasible. To this end, we propose a multi-split reversible network and combine it with DARTS. Specifically, we devise a backpropagation-with-reconstruction algorithm so that we only need to store the last layer's outputs. By relieving the memory burden for DARTS, it allows us to search with larger hidden size and more candidate operations. We evaluate the searched architecture on three sequence-to-sequence datasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14 English-Czech. Experimental results show that our network consistently outperforms standard Transformers across the tasks. Moreover, our method compares favorably with big-size Evolved Transformers, reducing search computation by an order of magnitude.", "venue": "Findings", "year": 2021, "referenceCount": 51, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.372.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-31", "journal": {"pages": "4254-4264"}, "authors": [{"authorId": "2007790204", "name": "Yuekai Zhao"}, {"authorId": "145307652", "name": "Li Dong"}, {"authorId": "1752875", "name": "Yelong Shen"}, {"authorId": "47294286", "name": "Zhihua Zhang"}, {"authorId": "49807919", "name": "Furu Wei"}, {"authorId": "2109136147", "name": "Weizhu Chen"}]}, {"paperId": "816ea6211b032b83ab67b9e65d351864359bb506", "externalIds": {"ArXiv": "2107.08212", "ACL": "2021.findings-acl.373", "DBLP": "conf/acl/LiuWWDCST21", "DOI": "10.18653/v1/2021.findings-acl.373", "CorpusId": 236087983}, "corpusId": 236087983, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/816ea6211b032b83ab67b9e65d351864359bb506", "title": "On the Copying Behaviors of Pre-Training for Neural Machine Translation", "abstract": "Previous studies have shown that initializing neural machine translation (NMT) models with the pre-trained language models (LM) can speed up the model training and boost the model performance. In this work, we identify a critical side-effect of pre-training for NMT, which is due to the discrepancy between the training objectives of LM-based pre-training and NMT. Since the LM objective learns to reconstruct a few source tokens and copy most of them, the pre-training initialization would affect the copying behaviors of NMT models. We provide a quantitative analysis of copying behaviors by introducing a metric called copying ratio, which empirically shows that pre-training based NMT models have a larger copying ratio than the standard one. In response to this problem, we propose a simple and effective method named copying penalty to control the copying behaviors in decoding. Extensive experiments on both in-domain and out-of-domain benchmarks show that the copying penalty method consistently improves translation performance by controlling copying behaviors for pre-training based NMT models. Source code is freely available at https://github.com/SunbowLiu/CopyingPenalty.", "venue": "Findings", "year": 2021, "referenceCount": 43, "citationCount": 16, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.373.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-17", "journal": {"pages": "4265-4275"}, "authors": [{"authorId": "2151060023", "name": "Xuebo Liu"}, {"authorId": "2111542852", "name": "Longyue Wang"}, {"authorId": "1758353", "name": "Derek F. Wong"}, {"authorId": "46573238", "name": "Liang Ding"}, {"authorId": "1774304", "name": "Lidia S. Chao"}, {"authorId": "2072684668", "name": "Shuming Shi"}, {"authorId": "2909321", "name": "Zhaopeng Tu"}]}, {"paperId": "5b2a13b85ef9d3ad61639c3b400e447f3217ec98", "externalIds": {"ArXiv": "2106.00955", "DBLP": "conf/acl/HsuLSM21", "ACL": "2021.findings-acl.374", "DOI": "10.18653/v1/2021.findings-acl.374", "CorpusId": 235294302}, "corpusId": 235294302, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5b2a13b85ef9d3ad61639c3b400e447f3217ec98", "title": "Answer Generation for Retrieval-based Question Answering Systems", "abstract": "Recent advancements in transformer-based models have greatly improved the ability of Question Answering (QA) systems to provide correct answers; in particular, answer sentence selection (AS2) models, core components of retrieval-based systems, have achieved impressive results. While generally effective, these models fail to provide a satisfying answer when all retrieved candidates are of poor quality, even if they contain correct information. In AS2, models are trained to select the best answer sentence among a set of candidates retrieved for a given question. In this work, we propose to generate answers from a set of AS2 top candidates. Rather than selecting the best candidate, we train a sequence to sequence transformer model to generate an answer from a candidate set. Our tests on three English AS2 datasets show improvement up to 32 absolute points in accuracy over the state of the art.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.374.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "4276-4282"}, "authors": [{"authorId": "23608432", "name": "Chao-Chun Hsu"}, {"authorId": "2106628203", "name": "Eric Lind"}, {"authorId": "3328733", "name": "Luca Soldaini"}, {"authorId": "1719404", "name": "Alessandro Moschitti"}]}, {"paperId": "2550fafc0cbd8bbf7aadd864ac569596d33db038", "externalIds": {"ACL": "2021.findings-acl.375", "ArXiv": "2106.02192", "DBLP": "journals/corr/abs-2106-02192", "DOI": "10.18653/v1/2021.findings-acl.375", "CorpusId": 235352900}, "corpusId": 235352900, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/2550fafc0cbd8bbf7aadd864ac569596d33db038", "title": "Grounding \u2018Grounding\u2019 in NLP", "abstract": "The NLP community has seen substantial recent interest in grounding to facilitate interaction between language technologies and the world. However, as a community, we use the term broadly to reference any linking of text to data or non-textual modality. In contrast, Cognitive Science more formally defines\"grounding\"as the process of establishing what mutual information is required for successful communication between two interlocutors -- a definition which might implicitly capture the NLP usage but differs in intent and scope. We investigate the gap between these definitions and seek answers to the following questions: (1) What aspects of grounding are missing from NLP tasks? Here we present the dimensions of coordination, purviews and constraints. (2) How is the term\"grounding\"used in the current research? We study the trends in datasets, domains, and tasks introduced in recent NLP conferences. And finally, (3) How to advance our current definition to bridge the gap with Cognitive Science? We present ways to both create new tasks or repurpose existing ones to make advancements towards achieving a more complete sense of grounding.", "venue": "Findings", "year": 2021, "referenceCount": 169, "citationCount": 31, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.375.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-04", "journal": {"name": "ArXiv", "volume": "abs/2106.02192"}, "authors": [{"authorId": "37619618", "name": "Khyathi Raghavi Chandu"}, {"authorId": "3312309", "name": "Yonatan Bisk"}, {"authorId": "1690706", "name": "A. Black"}]}, {"paperId": "219d18c17c756fd33cc3a2b1fd661405777a3384", "externalIds": {"ACL": "2021.findings-acl.376", "DBLP": "conf/acl/TianCQS21", "DOI": "10.18653/v1/2021.findings-acl.376", "CorpusId": 236477518}, "corpusId": 236477518, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/219d18c17c756fd33cc3a2b1fd661405777a3384", "title": "Federated Chinese Word Segmentation with Global Character Associations", "abstract": "Chinese word segmentation (CWS) is a fundamental task for Chinese information processing, which always suffers from out-ofvocabulary word issues, especially when it is tested on data from different sources. Although one possible solution is to use more training data, in real applications, these data are stored at different locations and thus are invisible and isolated among each other owing to the privacy or legal issues (e.g., clinical reports from different hospitals). To address this issue and benefit from extra data, we propose a neural model for CWS with federated learning (FL) adopted to help CWS deal with data isolation, where a mechanism of global character associations is proposed to enhance FL to learn from different data sources. Experimental results on a simulated environment with five nodes confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1", "venue": "Findings", "year": 2021, "referenceCount": 46, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.376.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4306-4313"}, "authors": [{"authorId": "2152947211", "name": "Yuanhe Tian"}, {"authorId": "2116379168", "name": "Guimin Chen"}, {"authorId": "2112656385", "name": "Han Qin"}, {"authorId": "1922182598", "name": "Yan Song"}]}, {"paperId": "16e65d305b5561fd104dae8d2a9fdff8f1b5e7b5", "externalIds": {"DBLP": "conf/acl/ShiraniTTDLESA21", "ACL": "2021.findings-acl.377", "DOI": "10.18653/v1/2021.findings-acl.377", "CorpusId": 236477474}, "corpusId": 236477474, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/16e65d305b5561fd104dae8d2a9fdff8f1b5e7b5", "title": "PSED: A Dataset for Selecting Emphasis in Presentation Slides", "abstract": "Emphasizing words in presentation slides allows viewers to direct their gaze to focal points without reading the entire slide, retaining their attention on the speaker. Despite many studies on automatic slide generation, few have addressed helping authors choose which words to emphasize. Motivated by this, we study the problem of choosing candidates for emphasis by introducing a new dataset containing presentation slides with a wide variety of topics. We evaluated a range of state-of-the-art models on this novel dataset by organizing a shared task and inviting multiple researchers to model emphasis in slides.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4314-4320"}, "authors": [{"authorId": "51136162", "name": "Amirreza Shirani"}, {"authorId": "46206886", "name": "Gia-Lac Tran"}, {"authorId": "2064711468", "name": "Hieu Trinh"}, {"authorId": "2075390842", "name": "Franck Dernoncourt"}, {"authorId": "1793409", "name": "Nedim Lipka"}, {"authorId": "2586368", "name": "J. Echevarria"}, {"authorId": "1794626", "name": "T. Solorio"}, {"authorId": "2934421", "name": "P. Asente"}]}, {"paperId": "fe41501e55d3bb83f9cfc33aac725dbd942b6419", "externalIds": {"MAG": "3086996502", "ArXiv": "2009.07058", "DBLP": "conf/acl/ClouatreTZC21", "ACL": "2021.findings-acl.378", "DOI": "10.18653/v1/2021.findings-acl.378", "CorpusId": 221703752}, "corpusId": 221703752, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/fe41501e55d3bb83f9cfc33aac725dbd942b6419", "title": "MLMLM: Link Prediction with Mean Likelihood Masked Language Model", "abstract": "Knowledge Bases (KBs) are easy to query, verifiable, and interpretable. They however scale with man-hours and high-quality data. Masked Language Models (MLMs), such as BERT, scale with computing power as well as unstructured raw text data. The knowledge contained within those models is however not directly interpretable. We propose to perform link prediction with MLMs to address both the KBs scalability issues and the MLMs interpretability issues. To do that we introduce MLMLM, Mean Likelihood Masked Language Model, an approach comparing the mean likelihood of generating the different entities to perform link prediction in a tractable manner. We obtain State of the Art (SotA) results on the WN18RR dataset and the best non-entity-embedding based results on the FB15k-237 dataset. We also obtain convincing results on link prediction on previously unseen entities, making MLMLM a suitable approach to introducing new entities to a KB.", "venue": "Findings", "year": 2020, "referenceCount": 33, "citationCount": 24, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.378.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-09-15", "journal": {"pages": "4321-4331"}, "authors": [{"authorId": "66684367", "name": "Louis Clou\u00e2tre"}, {"authorId": "150266445", "name": "P. Trempe"}, {"authorId": "145698212", "name": "A. Zouaq"}, {"authorId": "123607932", "name": "Sarath Chandar"}]}, {"paperId": "ac97f277002a9b514e8d667df8367cb979f8b86f", "externalIds": {"DBLP": "conf/acl/LiuWJV21", "ArXiv": "2108.07886", "ACL": "2021.findings-acl.379", "DOI": "10.18653/v1/2021.findings-acl.379", "CorpusId": 236478025}, "corpusId": 236478025, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ac97f277002a9b514e8d667df8367cb979f8b86f", "title": "Modulating Language Models with Emotions", "abstract": "Generating context-aware language that embodies diverse emotions is an important step towards building empathetic NLP systems. In this paper, we propose a formulation of modulated layer normalization -- a technique inspired by computer vision -- that allows us to use large-scale language models for emotional response generation. In automatic and human evaluation on the MojiTalk dataset, our proposed modulated layer normalization method outperforms prior baseline methods while maintaining diversity, fluency, and coherence. Our method also obtains competitive performance even when using only 10% of the available training data.", "venue": "Findings", "year": 2021, "referenceCount": 61, "citationCount": 6, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.379.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-17", "journal": {"pages": "4332-4339"}, "authors": [{"authorId": "7247867", "name": "Ruibo Liu"}, {"authorId": "119640649", "name": "Jason Wei"}, {"authorId": "1727055797", "name": "Chenyan Jia"}, {"authorId": "1918441", "name": "Soroush Vosoughi"}]}, {"paperId": "b93600b4fd9b2676d27db6179457a319d659f30d", "externalIds": {"ACL": "2021.findings-acl.380", "DBLP": "conf/acl/NojiO21", "ArXiv": "2105.14822", "DOI": "10.18653/v1/2021.findings-acl.380", "CorpusId": 235253831}, "corpusId": 235253831, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b93600b4fd9b2676d27db6179457a319d659f30d", "title": "Effective Batching for Recurrent Neural Network Grammars", "abstract": "As a language model that integrates traditional symbolic operations and flexible neural representations, recurrent neural network grammars (RNNGs) have attracted great attention from both scientific and engineering perspectives. However, RNNGs are known to be harder to scale due to the difficulty of batched training. In this paper, we propose effective batching for RNNGs, where every operation is computed in parallel with tensors across multiple sentences. Our PyTorch implementation effectively employs a GPU and achieves x6 speedup compared to the existing C++ DyNet implementation with model-independent auto-batching. Moreover, our batched RNNG also accelerates inference and achieves x20-150 speedup for beam search depending on beam sizes. Finally, we evaluate syntactic generalization performance of the scaled RNNG against the LSTM baseline, based on the large training data of 100M tokens from English Wikipedia and the broad-coverage targeted syntactic evaluation benchmark. Our RNNG implementation is available at https://github.com/aistairc/rnng-pytorch/.", "venue": "Findings", "year": 2021, "referenceCount": 45, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.380.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-31", "journal": {"pages": "4340-4352"}, "authors": [{"authorId": "3253887", "name": "Hiroshi Noji"}, {"authorId": "50856622", "name": "Yohei Oseki"}]}, {"paperId": "c7051ff470194c96a66f45d0a2b3fdf9159fb0fe", "externalIds": {"DBLP": "journals/corr/abs-2105-13465", "ArXiv": "2105.13465", "ACL": "2021.findings-acl.381", "DOI": "10.18653/v1/2021.findings-acl.381", "CorpusId": 235247970}, "corpusId": 235247970, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c7051ff470194c96a66f45d0a2b3fdf9159fb0fe", "title": "Verb Sense Clustering using Contextualized Word Representations for Semantic Frame Induction", "abstract": "Contextualized word representations have proven useful for various natural language processing tasks. However, it remains unclear to what extent these representations can cover hand-coded semantic information such as semantic frames, which specify the semantic role of the arguments associated with a predicate. In this paper, we focus on verbs that evoke different frames depending on the context, and we investigate how well contextualized word representations can recognize the difference of frames that the same verb evokes. We also explore which types of representation are suitable for semantic frame induction. In our experiments, we compare seven different contextualized word representations for two English frame-semantic resources, FrameNet and PropBank. We demonstrate that several contextualized word representations, especially BERT and its variants, are considerably informative for semantic frame induction. Furthermore, we examine the extent to which the contextualized representation of a verb can estimate the number of frames that the verb can evoke.", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.381.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-27", "journal": {"name": "ArXiv", "volume": "abs/2105.13465"}, "authors": [{"authorId": "1490775466", "name": "Kosuke Yamada"}, {"authorId": "2293543", "name": "Ryohei Sasano"}, {"authorId": "2874038", "name": "Koichi Takeda"}]}, {"paperId": "b830fba9dd228f63aaecfb040461acb1df6bb64c", "externalIds": {"ACL": "2021.findings-acl.382", "DBLP": "conf/acl/DoanH21", "DOI": "10.18653/v1/2021.findings-acl.382", "CorpusId": 236478240}, "corpusId": 236478240, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b830fba9dd228f63aaecfb040461acb1df6bb64c", "title": "Benchmarking Neural Topic Models: An Empirical Study", "abstract": "Neural topic modeling approach has been attracting much attention recently as it is able to leverage the advantages of both neural networks and probabilistic topic models. Previous works have proposed several models that are based on this framework and obtained impressive experimental results compared to traditional probabilistic models. However, the reported result is not consistent across the works, making them hard for gaining a rigorous assessment of these approaches. This work aims to address this issue by offering an extensive empirical evaluation of typical neural topic models in different aspects using large, diverse datasets as well as a thorough set of metrics. Precisely, we examine the performance of these models in three tasks, namely uncovering cohesive topics, modeling the input documents, and representing them for downstream classification. Our results show that while the neural topic models are better in the first and the third tasks, the traditional probabilistic models are still a strong baseline and are better in the second task in many cases. These findings give us more insights for choosing offthe-shelf topic modeling toolboxes in different contexts, as well as for designing more comprehensive evaluation for neural topic models.", "venue": "Findings", "year": 2021, "referenceCount": 25, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.382.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4363-4368"}, "authors": [{"authorId": "32214450", "name": "Thanh-Nam Doan"}, {"authorId": "11298055", "name": "Tuan-Anh Hoang"}]}, {"paperId": "0d00c94ed73f0e8f9ecc683eb6f8e2af7d127ee2", "externalIds": {"DBLP": "conf/acl/HuangLHXLS21", "ACL": "2021.findings-acl.383", "DOI": "10.18653/v1/2021.findings-acl.383", "CorpusId": 236477523}, "corpusId": 236477523, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0d00c94ed73f0e8f9ecc683eb6f8e2af7d127ee2", "title": "Enhancing Chinese Word Segmentation via Pseudo Labels for Practicability", "abstract": "Pre-trained language models (e.g., BERT) significantly alleviate two traditional challenging problems for Chinese word segmentation (CWS): segmentation ambiguity and out-ofvocabulary (OOV) words. However, such improvements are usually achieved on traditional benchmark datasets and not close to an important goal of CWS: practicability (i.e., low complexity as a standalone task and high beneficiality to downstream tasks). To make a trade-off between traditional evaluation and practicability for CWS, we propose a semisupervised neural method via pseudo labels. The neural method consists of a teacher model and a student model, which distills knowledge from unlabeled data to the student model so as to improve both in-domain and out-ofdomain CWS. Experiments show that our proposed method can not only keep the practicability of the lightweight student model but also improve the performance of segmentation effectively. We also evaluate a range of heterogeneous neural architectures of CWS on downstream Chinese NLP tasks. Results of further experiments demonstrate that our proposed segmenter is reliable and practical as a pre-processing step of the downstream NLP tasks at the minimum cost.1", "venue": "Findings", "year": 2021, "referenceCount": 42, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.383.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4369-4381"}, "authors": [{"authorId": "2112768206", "name": "Kaiyu Huang"}, {"authorId": "2108326792", "name": "Junpeng Liu"}, {"authorId": "2610330", "name": "Degen Huang"}, {"authorId": "2694222", "name": "Deyi Xiong"}, {"authorId": "46271578", "name": "Zhuang Liu"}, {"authorId": "34739384", "name": "Jinsong Su"}]}, {"paperId": "e13d317fe0178a8b8b67f4af995e7fac12c35014", "externalIds": {"ACL": "2021.findings-acl.384", "DBLP": "conf/acl/DahalMB21", "DOI": "10.18653/v1/2021.findings-acl.384", "CorpusId": 236478044}, "corpusId": 236478044, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e13d317fe0178a8b8b67f4af995e7fac12c35014", "title": "Analysis of Tree-Structured Architectures for Code Generation", "abstract": "Code generation is the task of generating code snippets from input user specifications in natural language. Leveraging the linguisticallymotivated hierarchical structure of the input can benefit code generation, especially since the specifications are complex sentences containing multiple variables and operations over various data structures. Moreover, recent advances in Transformer architectures have led to improved performance with tree-to-tree style generation for other seq2seq tasks e.g., machine translation. Hence, we present an empirical analysis of the significance of input parse trees for code generation. We run textto-tree, linearized tree-to-tree, and structured tree-to-tree models, using constituency-based parse trees as input, where the target is Abstract Syntax Tree (AST) of the code. We evaluate our models on the Python-based code generation dataset CoNaLa and a semantic parsing dataset ATIS. We find that constituency trees encoded using a structure-aware model improve performance for both datasets. We also provide an analysis of those aspects of the input parse trees which are most impactful. For instance, we find that structure-aware encodings are better at modelling inputs with multiple variables and capturing long-range dependencies for code generation.1", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.384.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4382-4391"}, "authors": [{"authorId": "2121361734", "name": "Samip Dahal"}, {"authorId": "8785371", "name": "A. Maharana"}, {"authorId": "143977268", "name": "Mohit Bansal"}]}, {"paperId": "d076d0639df7c9729c2ab8fd3361efc1af4077ad", "externalIds": {"DBLP": "conf/acl/XuMZC21", "ACL": "2021.findings-acl.385", "ArXiv": "2105.12900", "DOI": "10.18653/v1/2021.findings-acl.385", "CorpusId": 235212149}, "corpusId": 235212149, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d076d0639df7c9729c2ab8fd3361efc1af4077ad", "title": "How Does Distilled Data Complexity Impact the Quality and Confidence of Non-Autoregressive Machine Translation?", "abstract": "While non-autoregressive (NAR) models are showing great promise for machine translation, their use is limited by their dependence on knowledge distillation from autoregressive models. To address this issue, we seek to understand why distillation is so effective. Prior work suggests that distilled training data is less complex than manual translations. Based on experiments with the Levenshtein Transformer and the Mask-Predict NAR models on the WMT14 German-English task, this paper shows that different types of complexity have different impacts: while reducing lexical diversity and decreasing reordering complexity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model confidence, which affects the calibration of different NAR models differently.", "venue": "Findings", "year": 2021, "referenceCount": 32, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.385.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-27", "journal": {"pages": "4392-4400"}, "authors": [{"authorId": "47210642", "name": "Weijia Xu"}, {"authorId": "2118866998", "name": "Shuming Ma"}, {"authorId": "40232931", "name": "Dongdong Zhang"}, {"authorId": "2954727", "name": "Marine Carpuat"}]}, {"paperId": "ff8214ffa8ee898fc9dc85933fb108e53e3e7a49", "externalIds": {"ACL": "2021.findings-acl.386", "DBLP": "conf/acl/ZhaoDZC21", "DOI": "10.18653/v1/2021.findings-acl.386", "CorpusId": 236478348}, "corpusId": 236478348, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ff8214ffa8ee898fc9dc85933fb108e53e3e7a49", "title": "Leveraging Topic Relatedness for Argument Persuasion", "abstract": "Argumentation exposes individuals to conflicting viewpoints and can help them make more informed decisions based on the pros and cons of a particular issue. While recent studies of argumentation in Natural Language Processing have mainly focused on understanding the effect of various factors of persuasion (i.e. the source, audience, and language style), the impact of exploiting the relationships among controversial topics when predicting argument persuasiveness remains under-explored. In this paper, we model the relatedness among controversial topics utilizing an embedding-based method based on individuals\u2019 stances on the topics. We then leverage these topic embedding features and incorporate topic semantics features extracted from the arguments along with the previously studied factors of persuasion. We show that incorporating both types of topic relatedness features explicitly leads to significant improvement in predicting persuasiveness and also helps enhance generalization to rare topics, in a few-shot setting.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4401-4407"}, "authors": [{"authorId": "1500662261", "name": "Xinran Zhao"}, {"authorId": "41152329", "name": "Esin Durmus"}, {"authorId": "2111112132", "name": "Hongming Zhang"}, {"authorId": "2064285348", "name": "Claire Cardie"}]}, {"paperId": "066529517e46417825624f1416e200d15a6e3b64", "externalIds": {"ArXiv": "2106.01023", "ACL": "2021.findings-acl.387", "DBLP": "journals/corr/abs-2106-01023", "DOI": "10.18653/v1/2021.findings-acl.387", "CorpusId": 235294276}, "corpusId": 235294276, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/066529517e46417825624f1416e200d15a6e3b64", "title": "One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers", "abstract": "Pre-trained language models (PLMs) achieve great success in NLP. However, their huge model sizes hinder their applications in many practical systems. Knowledge distillation is a popular technique to compress PLMs, which learns a small student model from a large teacher PLM. However, the knowledge learned from a single teacher may be limited and even biased, resulting in low-quality student model. In this paper, we propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs. In MT-BERT we design a multi-teacher co-finetuning method to jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching. In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MT-BERT in compressing PLMs.", "venue": "Findings", "year": 2021, "referenceCount": 27, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.387.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01023"}, "authors": [{"authorId": "2118839668", "name": "Chuhan Wu"}, {"authorId": "2397264", "name": "Fangzhao Wu"}, {"authorId": "1731776", "name": "Yongfeng Huang"}]}, {"paperId": "8ede9ccbaad8780304d47482b9e404d3de88e658", "externalIds": {"ArXiv": "2108.00577", "DBLP": "journals/corr/abs-2108-00577", "ACL": "2021.findings-acl.388", "DOI": "10.18653/v1/2021.findings-acl.388", "CorpusId": 236477617}, "corpusId": 236477617, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8ede9ccbaad8780304d47482b9e404d3de88e658", "title": "Logic-Consistency Text Generation from Semantic Parses", "abstract": "Text generation from semantic parses is to generate textual descriptions for formal representation inputs such as logic forms and SQL queries. This is challenging due to two reasons: (1) the complex and intensive inner logic with the data scarcity constraint, (2) the lack of automatic evaluation metrics for logic consistency. To address these two challenges, this paper first proposes SNOWBALL, a framework for logic consistent text generation from semantic parses that employs an iterative training procedure by recursively augmenting the training set with quality control. Second, we propose a novel automatic metric, BLEC, for evaluating the logical consistency between the semantic parses and generated texts. The experimental results on two benchmark datasets, Logic2Text and Spider, demonstrate the SNOWBALL framework enhances the logic consistency on both BLEC and human evaluation. Furthermore, our statistical analysis reveals that BLEC is more logically consistent with human evaluation than general-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data and code are available at https://github.com/Ciaranshu/relogic.", "venue": "Findings", "year": 2021, "referenceCount": 45, "citationCount": 22, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.388.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-02", "journal": {"name": "ArXiv", "volume": "abs/2108.00577"}, "authors": [{"authorId": "2068917529", "name": "Chang Shu"}, {"authorId": "2108051142", "name": "Yusen Zhang"}, {"authorId": "2118187202", "name": "Xiangyu Dong"}, {"authorId": "2055357805", "name": "Peng Shi"}, {"authorId": "48881008", "name": "Tao Yu"}, {"authorId": "144142360", "name": "Rui Zhang"}]}, {"paperId": "cdf4a757298add156fc181c4b1c649f8bb75f80c", "externalIds": {"DBLP": "conf/acl/MichaelZ21", "ACL": "2021.findings-acl.389", "DOI": "10.18653/v1/2021.findings-acl.389", "CorpusId": 236477832}, "corpusId": 236477832, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/cdf4a757298add156fc181c4b1c649f8bb75f80c", "title": "Inducing Semantic Roles Without Syntax", "abstract": "Semantic roles are a key component of linguistic predicate-argument structure, but develop-ing ontologies of these roles requires signi\ufb01cant expertise and manual effort. Methods exist for automatically inducing semantic roles using syntactic representations, but syntax can also be dif\ufb01cult to de\ufb01ne, annotate, and predict. We show it is possible to automatically induce semantic roles from QA-SRL, a scal-able and ontology-free semantic annotation scheme that uses question-answer pairs to rep-resent predicate-argument structure. By asso-ciating arguments with distributions over QA-SRL questions and clustering them in a mixture model, our method outperforms all previous models as well as a new state-of-the-art baseline over gold syntax. We show that our method works because QA-SRL acts as surrogate syntax , capturing non-overt arguments and syntactic alternations, which are central motivators for the use of semantic role labeling systems. 1", "venue": "Findings", "year": 2021, "referenceCount": 53, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4427-4442"}, "authors": [{"authorId": "38614754", "name": "Julian Michael"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}]}, {"paperId": "2800db93276eadcf43d1485957c25a594f1f9c7b", "externalIds": {"DBLP": "conf/acl/HsuCHK21", "ArXiv": "2105.06950", "ACL": "2021.findings-acl.390", "MAG": "3173770572", "DOI": "10.18653/v1/2021.findings-acl.390", "CorpusId": 234682123}, "corpusId": 234682123, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/2800db93276eadcf43d1485957c25a594f1f9c7b", "title": "Plot and Rework: Modeling Storylines for Visual Storytelling", "abstract": "Writing a coherent and engaging story is not easy. Creative writers use their knowledge and worldview to put disjointed elements together to form a coherent storyline, and work and rework iteratively toward perfection. Automated visual storytelling (VIST) models, however, make poor use of external knowledge and iterative generation when attempting to create stories. This paper introduces PR-VIST, a framework that represents the input image sequence as a story graph in which it finds the best path to form a storyline. PR-VIST then takes this path and learns to generate the final story via an iterative training process. This framework produces stories that are superior in terms of diversity, coherence, and humanness, per both automatic and human evaluations. An ablation study shows that both plotting and reworking contribute to the model's superiority.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.390.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-14", "journal": {"pages": "4443-4453"}, "authors": [{"authorId": "48162772", "name": "Chi-Yang Hsu"}, {"authorId": "9628638", "name": "Yun-Wei Chu"}, {"authorId": "144188081", "name": "Ting-Hao 'Kenneth' Huang"}, {"authorId": "1746959", "name": "Lun-Wei Ku"}]}, {"paperId": "87ce2a72c2fc0fc751cb23c4cf115a54b3b06896", "externalIds": {"ACL": "2021.findings-acl.391", "DBLP": "conf/acl/ZhangHZWLS21", "DOI": "10.18653/v1/2021.findings-acl.391", "CorpusId": 236478008}, "corpusId": 236478008, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/87ce2a72c2fc0fc751cb23c4cf115a54b3b06896", "title": "Disentangled Code Representation Learning for Multiple Programming Languages", "abstract": "Developing effective distributed representations of source code is fundamental yet challenging for many software engineering tasks such as code clone detection, code search, code translation and transformation. However, current code embedding approaches that represent the semantic and syntax of code in a mixed way are less interpretable and the re-sulting embedding can not be easily general-ized across programming languages. In this paper, we propose a disentangled code representation learning approach to separate the semantic from the syntax of source code under a multi-programming-language setting, obtaining better interpretability and generalizability. Specially, we design three losses dedicated to the characteristics of source code to enforce the disentanglement effectively. We conduct comprehensive experiments on a real-world dataset composed of programming exercises implemented by multiple solutions that are semantically identical but grammatically distin-guished. The experimental results validate the superiority of our proposed disentangled code representation, compared to several baselines, across three types of downstream tasks, i.e., code clone detection, code translation, and code-to-code search.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.391.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4454-4466"}, "authors": [{"authorId": "47539929", "name": "Jingfeng Zhang"}, {"authorId": "2147289313", "name": "Haiwen Hong"}, {"authorId": "46867455", "name": "Yin Zhang"}, {"authorId": "2147200426", "name": "Yao Wan"}, {"authorId": null, "name": "Ye Liu"}, {"authorId": "34296085", "name": "Yulei Sui"}]}, {"paperId": "c0e191909f4fef84aec41cfbe3ec826a9454ea58", "externalIds": {"ArXiv": "2106.12976", "ACL": "2021.findings-acl.392", "DBLP": "conf/acl/LahnalaZWKARMP21", "DOI": "10.18653/v1/2021.findings-acl.392", "CorpusId": 235624116}, "corpusId": 235624116, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c0e191909f4fef84aec41cfbe3ec826a9454ea58", "title": "Exploring Self-Identified Counseling Expertise in Online Support Forums", "abstract": "A growing number of people engage in online health forums, making it important to understand the quality of the advice they receive. In this paper, we explore the role of expertise in responses provided to help-seeking posts regarding mental health. We study the differences between (1) interactions with peers; and (2) interactions with self-identified mental health professionals. First, we show that a classifier can distinguish between these two groups, indicating that their language use does in fact differ. To understand this difference, we perform several analyses addressing engagement aspects, including whether their comments engage the support-seeker further as well as linguistic aspects, such as dominant language and linguistic style matching. Our work contributes toward the developing efforts of understanding how health experts engage with health information- and support-seekers in social networks. More broadly, it is a step toward a deeper understanding of the styles of interactions that cultivate supportive engagement in online communities.", "venue": "Findings", "year": 2021, "referenceCount": 53, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.392.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-24", "journal": {"pages": "4467-4480"}, "authors": [{"authorId": "1801600316", "name": "Allison Lahnala"}, {"authorId": "2118524123", "name": "Yuntian Zhao"}, {"authorId": "145645240", "name": "Charles F Welch"}, {"authorId": "1727211", "name": "Jonathan K. Kummerfeld"}, {"authorId": "2113905187", "name": "Lawrence An"}, {"authorId": "34613400", "name": "K. Resnicow"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}, {"authorId": "1396239754", "name": "Ver\u00f3nica P\u00e9rez-Rosas"}]}, {"paperId": "7dd25287048fbc8753f412f30d30509e87308071", "externalIds": {"ACL": "2021.findings-acl.393", "DBLP": "conf/acl/ZengN21", "DOI": "10.18653/v1/2021.findings-acl.393", "CorpusId": 236478341}, "corpusId": 236478341, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7dd25287048fbc8753f412f30d30509e87308071", "title": "An Investigation of Suitability of Pre-Trained Language Models for Dialogue Generation \u2013 Avoiding Discrepancies", "abstract": "Pre-trained language models have been widely used in response generation for open-domain dialogue. These approaches are built within 4 frameworks: Transformer-ED, Transformer-Dec, Transformer-MLM and Transformer-AR. In this study, we experimentally compare them using both large and small-scale data. This reveals that decoder-only architecture is better than stacked encoder-decoder, and both left-to-right and bi-directional attention have their own advantages. We further de\ufb01ne two concepts of model discrepancy, which provides a new explanation to the model performance. As discrepancies may hinder performance, we propose two solutions to reduce them, which successfully improve the model performance.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.393.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4481-4494"}, "authors": [{"authorId": "151504883", "name": "Yan Zeng"}, {"authorId": "143619007", "name": "Jian-Yun Nie"}]}, {"paperId": "077108a733f9b505437d404bf44d85a5858a434f", "externalIds": {"DBLP": "journals/corr/abs-2106-13715", "ACL": "2021.findings-acl.394", "ArXiv": "2106.13715", "DOI": "10.18653/v1/2021.findings-acl.394", "CorpusId": 235652393}, "corpusId": 235652393, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/077108a733f9b505437d404bf44d85a5858a434f", "title": "Learning to Sample Replacements for ELECTRA Pre-Training", "abstract": "ELECTRA pretrains a discriminator to detect replaced tokens, where the replacements are sampled from a generator trained with masked language modeling. Despite the compelling performance, ELECTRA suffers from the following two issues. First, there is no direct feedback loop from discriminator to generator, which renders replacement sampling inefficient. Second, the generator's prediction tends to be over-confident along with training, making replacements biased to correct tokens. In this paper, we propose two methods to improve replacement sampling for ELECTRA pre-training. Specifically, we augment sampling with a hardness prediction mechanism, so that the generator can encourage the discriminator to learn what it has not acquired. We also prove that efficient sampling reduces the training variance of the discriminator. Moreover, we propose to use a focal loss for the generator in order to relieve oversampling of correct tokens as replacements. Experimental results show that our method improves ELECTRA pre-training on various downstream tasks.", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.394.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-25", "journal": {"name": "ArXiv", "volume": "abs/2106.13715"}, "authors": [{"authorId": "34128716", "name": "Y. Hao"}, {"authorId": "145307652", "name": "Li Dong"}, {"authorId": "10699417", "name": "Hangbo Bao"}, {"authorId": "145389711", "name": "Ke Xu"}, {"authorId": "49807919", "name": "Furu Wei"}]}, {"paperId": "355b66a65aee97822eb7404183ee72b18cb648de", "externalIds": {"ACL": "2021.findings-acl.395", "ArXiv": "2106.01751", "DBLP": "conf/acl/KumarT21", "DOI": "10.18653/v1/2021.findings-acl.395", "CorpusId": 235313574}, "corpusId": 235313574, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/355b66a65aee97822eb7404183ee72b18cb648de", "title": "Reordering Examples Helps during Priming-based Few-Shot Learning", "abstract": "The ability to learn from limited data, or few-shot learning, is a desirable and often critical requirement for NLP systems. While many existing methods do poorly at learning from a handful of examples, large pretrained language models have recently been shown to be efficient few-shot learners. One approach to few-shot learning, which does not require finetuning of model parameters, is to augment the language model's input with priming text which is typically constructed using task specific descriptions and examples. In this work, we further explore priming-based few-shot learning, with focus on using examples as prompts. We show that presenting examples in the right order is key for generalization. We introduce PERO (Prompting with Examples in the Right Order), where we formulate few-shot learning as search over the set of permutations of the training examples. We show that PERO can learn to generalize efficiently using as few as 10 examples, in contrast to existing approaches. While the newline token is a natural choice for separating the examples in the prompt, we show that learning a new separator token can potentially provide further gains in performance. We demonstrate the effectiveness of the proposed method on the tasks of sentiment classification, natural language inference and fact retrieval. Finally, we analyze the learned prompts to reveal novel insights, including the idea that two training examples in the right order alone can provide competitive performance for sentiment classification and natural language inference.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 38, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.395.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"pages": "4507-4518"}, "authors": [{"authorId": "36212180", "name": "Sawan Kumar"}, {"authorId": "2406435", "name": "Partha P. Talukdar"}]}, {"paperId": "ceda73e32bdd8f4e62e503a748f1752e3fba094f", "externalIds": {"DBLP": "conf/acl/GuoR21", "ACL": "2021.findings-acl.396", "DOI": "10.18653/v1/2021.findings-acl.396", "CorpusId": 236477462}, "corpusId": 236477462, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ceda73e32bdd8f4e62e503a748f1752e3fba094f", "title": "Constrained Labeled Data Generation for Low-Resource Named Entity Recognition", "abstract": "Named Entity Recognition (NER) in low-resource languages has been a long-standing challenge in NLP. Recent work has shown great progress in two directions: developing cross-lingual features/models to transfer knowledge to low-resource languages, and translating source-language training data into low-resource target-language training data by projecting annotations with cheap resources. We focus on the second direction in this study. Existing methods suffer from the low quality of the resulting annotated data in the target language; for example, they cannot handle word order and lexical ambiguity well. To handle these limitations we propose a novel approach that uses the projected annotation to generate pseudo supervised data with a transformer language model and a constrained beam search. This allows us to generate more diverse, higher quality, as well as higher quantities of annotated data in the target language. Experiments demonstrate that, when combining our method with available cross-lingual features, it achieves state-of-the-art or competitive performance on NER in a low-resource setting, espe-cially for languages that are distant from our source language, English. 1", "venue": "Findings", "year": 2021, "referenceCount": 40, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.396.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4519-4533"}, "authors": [{"authorId": "1491243862", "name": "Ruohao Guo"}, {"authorId": "144590225", "name": "D. Roth"}]}, {"paperId": "ea667d3f5df2954c7365b8d1218889e2fc514829", "externalIds": {"ACL": "2021.findings-acl.397", "DBLP": "conf/acl/GarimellaAKYNCS21", "DOI": "10.18653/v1/2021.findings-acl.397", "CorpusId": 236477795}, "corpusId": 236477795, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ea667d3f5df2954c7365b8d1218889e2fc514829", "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation", "abstract": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in \ufb01ll-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the ef\ufb01cacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.", "venue": "Findings", "year": 2021, "referenceCount": 43, "citationCount": 18, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4534-4545"}, "authors": [{"authorId": "31099365", "name": "Aparna Garimella"}, {"authorId": "2121347719", "name": "Akhash Amarnath"}, {"authorId": "2110632520", "name": "K. Kumar"}, {"authorId": "2121368400", "name": "Akash Pramod Yalla"}, {"authorId": "3365985", "name": "Anandhavelu Natarajan"}, {"authorId": "2954043", "name": "Niyati Chhaya"}, {"authorId": "2881425", "name": "Balaji Vasan Srinivasan"}]}, {"paperId": "4c1a845d34ad84b50f5dea1c749d5fa7df697847", "externalIds": {"DBLP": "conf/acl/NishidaNY21", "ArXiv": "2109.08354", "ACL": "2021.findings-acl.398", "DOI": "10.18653/v1/2021.findings-acl.398", "CorpusId": 236478106}, "corpusId": 236478106, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4c1a845d34ad84b50f5dea1c749d5fa7df697847", "title": "Task-adaptive Pre-training of Language Models with Word Embedding Regularization", "abstract": "Pre-trained language models (PTLMs) acquire domain-independent linguistic knowledge through pre-training with massive textual resources. Additional pre-training is effective in adapting PTLMs to domains that are not well covered by the pre-training corpora. Here, we focus on the static word embeddings of PTLMs for domain adaptation to teach PTLMs domain-specific meanings of words. We propose a novel fine-tuning process: task-adaptive pre-training with word embedding regularization (TAPTER). TAPTER runs additional pre-training by making the static word embeddings of a PTLM close to the word embeddings obtained in the target domain with fastText. TAPTER requires no additional corpus except for the training data of the downstream task. We confirmed that TAPTER improves the performance of the standard fine-tuning and the task-adaptive pre-training on BioASQ (question answering in the biomedical domain) and on SQuAD (the Wikipedia domain) when their pre-training corpora were not dominated by in-domain data.", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.398.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-17", "journal": {"name": "ArXiv", "volume": "abs/2109.08354"}, "authors": [{"authorId": "2054143697", "name": "Kosuke Nishida"}, {"authorId": "2006479562", "name": "Kyosuke Nishida"}, {"authorId": "1743985", "name": "Sen Yoshida"}]}, {"paperId": "7ac2f6e5a25eaf6a1a59b0c96ff7cdbfbc17b535", "externalIds": {"DBLP": "conf/acl/MitaY21", "ACL": "2021.findings-acl.399", "ArXiv": "2106.03031", "DOI": "10.18653/v1/2021.findings-acl.399", "CorpusId": 235358263}, "corpusId": 235358263, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7ac2f6e5a25eaf6a1a59b0c96ff7cdbfbc17b535", "title": "Do Grammatical Error Correction Models Realize Grammatical Generalization?", "abstract": "There has been an increased interest in data generation approaches to grammatical error correction (GEC) using pseudo data. However, these approaches suffer from several issues that make them inconvenient for real-world deployment including a demand for large amounts of training data. On the other hand, some errors based on grammatical rules may not necessarily require a large amount of data if GEC models can realize grammatical generalization. This study explores to what extent GEC models generalize grammatical knowledge required for correcting errors. We introduce an analysis method using synthetic and real GEC datasets with controlled vocabularies to evaluate whether models can generalize to unseen errors. We found that a current standard Transformer-based GEC model fails to realize grammatical generalization even in simple settings with limited vocabulary and syntax, suggesting that it lacks the generalization ability required to correct errors from provided training examples.", "venue": "Findings", "year": 2021, "referenceCount": 24, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.399.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-06", "journal": {"name": "ArXiv", "volume": "abs/2106.03031"}, "authors": [{"authorId": "35643168", "name": "Masato Mita"}, {"authorId": "3486313", "name": "Hitomi Yanaka"}]}, {"paperId": "17972be63441aff7ff2dc6e941d6ca7c935bb339", "externalIds": {"ACL": "2021.findings-acl.400", "DBLP": "conf/acl/GarimellaCL21", "DOI": "10.18653/v1/2021.findings-acl.400", "CorpusId": 236477726}, "corpusId": 236477726, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/17972be63441aff7ff2dc6e941d6ca7c935bb339", "title": "Domain-Aware Dependency Parsing for Questions", "abstract": "Parsing natural language questions in specific domains is crucial to a wide range of applications from question-answering to dialog systems. Pre-trained parsers are usually trained on corpora dominated by non-questions, and thus perform poorly on domain-specific questions. Retraining parsers with domain-specific questions labeled with syntactic parse trees is expensive, as these annotations require linguistic expertise. In this paper, we propose an automatic labeled domain question generation framework by leveraging domain knowledge and seed domain questions. We evaluate our approach in two domains, and release the generated question datasets. Our experimental results demonstrate that auto-generated labeled questions indeed lead to significant (4.9% \u2212 9%) increase in the accuracy of state-of-the-art (SoTA) parsers on domain questions.", "venue": "Findings", "year": 2021, "referenceCount": 14, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4562-4568"}, "authors": [{"authorId": "31099365", "name": "Aparna Garimella"}, {"authorId": "1779119", "name": "Laura Chiticariu"}, {"authorId": "1573872877", "name": "Yunyao Li"}]}, {"paperId": "56441f4f3beb3276350d811c67307e28d5f61b43", "externalIds": {"ACL": "2021.findings-acl.401", "DBLP": "conf/acl/LiG21", "DOI": "10.18653/v1/2021.findings-acl.401", "CorpusId": 236478070}, "corpusId": 236478070, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/56441f4f3beb3276350d811c67307e28d5f61b43", "title": "Using Social and Linguistic Information to Adapt Pretrained Representations for Political Perspective Identification", "abstract": "Understanding the political perspective shaping the way events are discussed in the media is increasingly important due to the dramatic change in news distribution. With the advance in text classi\ufb01cation models, the performance of political perspective detection is also improving rapidly. However, current deep learning based text models often require a large amount of supervised data for training, which can be very expensive to obtain for this task. Meanwhile, models pre-trained on the general source and task (e.g. BERT) lack the ability to focus on bias-related text span. In this paper, we propose a novel framework that pre-trains the text model using signals from the rich social and linguistic context that is readily available, including entity mentions, news sharing, and frame indicators. The pre-trained models bene\ufb01t from tasks related to bias detection and therefore are easier to train with the bias labels. We demonstrate the effectiveness of our proposed framework by experiments on two news bias datasets. The models with pre-training achieve signi\ufb01cant improvement in performance and are capable of identifying the text span for bias better.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 19, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.401.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4569-4579"}, "authors": [{"authorId": "2145421216", "name": "Chang Li"}, {"authorId": "2877164", "name": "Dan Goldwasser"}]}, {"paperId": "ece13dbeb8094ba580701469952de38c841b9894", "externalIds": {"DBLP": "conf/acl/ZhaoYCL21", "ACL": "2021.findings-acl.402", "DOI": "10.18653/v1/2021.findings-acl.402", "CorpusId": 236477650}, "corpusId": 236477650, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ece13dbeb8094ba580701469952de38c841b9894", "title": "Enhancing Dialogue-based Relation Extraction by Speaker and Trigger Words Prediction", "abstract": "Identifying relations from dialogues is more challenging than traditional sentence-level relation extraction (RE), since the difficulties of speaker information representation and the long-range semantic reasoning. Despite the successful efforts, existing methods do not fully consider the particularity of dialogues, making them difficult to truly understand the semantics between conversational arguments. In this paper, we propose two beneficial tasks, speaker prediction and trigger words prediction, to enhance the extraction of dialoguebased relations. Specifically, speaker prediction captures the characteristics of speakerrelated entities, and the trigger words prediction provides supportive contexts for relations between arguments. Extensive experiments on the DialogRE dataset show noticeable improvements compared to the baseline models, which achieves a new state-of-the-art performance with a 65.5% of F1 score and a 60.5% of F1c score, respectively.", "venue": "Findings", "year": 2021, "referenceCount": 15, "citationCount": 5, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.402.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4580-4585"}, "authors": [{"authorId": "1414129299", "name": "Tianyang Zhao"}, {"authorId": "144667269", "name": "Zhao Yan"}, {"authorId": "2154235", "name": "Yunbo Cao"}, {"authorId": "1707275", "name": "Zhoujun Li"}]}, {"paperId": "c5a810bdec24cf170aa32bada901695e55be71f6", "externalIds": {"ACL": "2021.findings-acl.403", "DBLP": "conf/acl/ZhouGSPZJ21", "DOI": "10.18653/v1/2021.findings-acl.403", "CorpusId": 236477422}, "corpusId": 236477422, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c5a810bdec24cf170aa32bada901695e55be71f6", "title": "Modeling Event-Pair Relations in External Knowledge Graphs for Script Reasoning", "abstract": "Script reasoning infers subsequent events from a given event chain, which involves the ability to understand relations between events. A human-labeled script reasoning dataset is usually of small size with limited event relations, which highlights the necessity to leverage external eventuality knowledge graphs (KG) consisting of numerous triple facts to describe the inferential relation between events. Existing methods adopt a retrieval and integration paradigm to focus merely on the graph triples that have event overlap with a script, but ignore much more supportive triples in the KG with similar inferential patterns, leading to under-exploiting. To fully exploit the KG, we propose a knowledge model to learn the inferential relations between events from the whole eventuality KG and then support downstream models by directly capturing the relation between events in a script. We further present a neural script adapter to extend the knowledge model for inferring the associated relations between an event chain and a subsequent event candidate. We evaluate the proposed approach on a popular multi-choice narrative cloze task for script reasoning and achieve new state-of-the-art accuracy, compared with baselines ei-ther incorporating external KG or not.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.403.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4586-4596"}, "authors": [{"authorId": "2110348767", "name": "Yucheng Zhou"}, {"authorId": "2442662", "name": "Xiubo Geng"}, {"authorId": null, "name": "Tao Shen"}, {"authorId": "145525190", "name": "J. Pei"}, {"authorId": "2108126306", "name": "Wenqiang Zhang"}, {"authorId": "71790825", "name": "Daxin Jiang"}]}, {"paperId": "5aab57cc0530560d82c74c055f664280619d7e81", "externalIds": {"ArXiv": "2106.03634", "ACL": "2021.findings-acl.404", "DBLP": "conf/acl/Aroca-Ouellette21", "DOI": "10.18653/v1/2021.findings-acl.404", "CorpusId": 235358436}, "corpusId": 235358436, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5aab57cc0530560d82c74c055f664280619d7e81", "title": "PROST: Physical Reasoning about Objects through Space and Time", "abstract": "We present a new probing dataset named PROST: Physical Reasoning about Objects Through Space and Time. This dataset contains 18,736 multiple-choice questions made from 14 manually curated templates, covering 10 physical reasoning concepts. All questions are designed to probe both causal and masked language models in a zero-shot setting. We conduct an extensive analysis which demonstrates that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer options are presented to them, they struggle when the superlative in a question is inverted (e.g., most<->least), and increasing the amount of pretraining data and parameters only yields minimal improvements. These results provide support for the hypothesis that current pretrained models' ability to reason about physical interactions is inherently limited by a lack of real world experience. By highlighting these limitations, we hope to motivate the development of models with a human-like understanding of the physical world.", "venue": "Findings", "year": 2021, "referenceCount": 54, "citationCount": 25, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-07", "journal": {"pages": "4597-4608"}, "authors": [{"authorId": "1413111141", "name": "Stephane T Aroca-Ouellette"}, {"authorId": "2107059659", "name": "Cory Paik"}, {"authorId": "3077150", "name": "A. Roncone"}, {"authorId": "3422953", "name": "Katharina Kann"}]}, {"paperId": "7dcb0b16b059588376d57b4df7b31fb002784cc0", "externalIds": {"ACL": "2021.findings-acl.405", "DBLP": "conf/acl/ZhengCXB21", "DOI": "10.18653/v1/2021.findings-acl.405", "CorpusId": 236478152}, "corpusId": 236478152, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7dcb0b16b059588376d57b4df7b31fb002784cc0", "title": "Revisiting the Evaluation of End-to-end Event Extraction", "abstract": "Event extraction (EE) aims to harvest event instances from plain text, where each instance is composed of a group of event arguments with specific event roles. Existing end-to-end EE research usually adopts the role-averaged evaluation that produces evaluation measures by averaging evaluation statistics of each event role. However, although this averaged metric can indicate the model performance to some extent, we find that such metric can be pretty misleading to downstream applications that utilize an event instance as a whole, where one wrongly identified event argument can substantially alter the whole meaning of an event instance. To mitigate this gap and provide a more complete understanding of performance, we propose two new evaluation metrics that also consider an event instance as a whole and explicitly penalize wrongly identified event arguments. Moreover, to support diverse preferences of evaluation metrics motivated by different scenarios, we propose a new training paradigm based on reinforcement learning for a typical end-to-end EE model, i.e., Doc2EDAG. Our extensive experiments show that the new training improves the initial one by a large margin (about 10%) under new metrics. Nevertheless, the current performance is still far from satisfactory, and optimizing towards these new metrics calls for more future research.", "venue": "Findings", "year": 2021, "referenceCount": 25, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4609-4617"}, "authors": [{"authorId": "145119697", "name": "Shun Zheng"}, {"authorId": "2075436482", "name": "Wei Cao"}, {"authorId": "145738420", "name": "Wei Xu"}, {"authorId": "152441498", "name": "Jiang Bian"}]}, {"paperId": "2ebba8d3acd0fec329edfda24ef8dac96ee9c6a4", "externalIds": {"ACL": "2021.findings-acl.406", "ArXiv": "2106.06636", "DBLP": "conf/acl/ChenMZH21", "DOI": "10.18653/v1/2021.findings-acl.406", "CorpusId": 235422036}, "corpusId": 235422036, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/2ebba8d3acd0fec329edfda24ef8dac96ee9c6a4", "title": "Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized Streaming ASR", "abstract": "Simultaneous speech-to-text translation is widely useful in many scenarios. The conventional cascaded approach uses a pipeline of streaming ASR followed by simultaneous MT, but suffers from error propagation and extra latency. To alleviate these issues, recent efforts attempt to directly translate the source speech into target text simultaneously, but this is much harder due to the combination of two separate tasks. We instead propose a new paradigm with the advantages of both cascaded and end-to-end approaches. The key idea is to use two separate, but synchronized, decoders on streaming ASR and direct speech-to-text translation (ST), respectively, and the intermediate results of ASR guide the decoding policy of (but is not fed as input to) ST. During training time, we use multitask learning to jointly learn these two tasks with a shared encoder. En-to-De and En-to-Es experiments on the MuSTC dataset demonstrate that our proposed technique achieves substantially better translation quality at similar levels of latency.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 20, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.406.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-11", "journal": {"pages": "4618-4624"}, "authors": [{"authorId": "47739808", "name": "Junkun Chen"}, {"authorId": "1847848", "name": "Mingbo Ma"}, {"authorId": "40223399", "name": "Renjie Zheng"}, {"authorId": "48545084", "name": "Liang Huang"}]}, {"paperId": "940824fb28d4c92382e481e8250100925ef83c87", "externalIds": {"DBLP": "journals/corr/abs-2105-14600", "ArXiv": "2105.14600", "ACL": "2021.findings-acl.407", "DOI": "10.18653/v1/2021.findings-acl.407", "CorpusId": 235254235}, "corpusId": 235254235, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/940824fb28d4c92382e481e8250100925ef83c87", "title": "HIT - A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation", "abstract": "Understanding linguistics and morphology of resource-scarce code-mixed texts remains a key challenge in text processing. Although word embedding comes in handy to support downstream tasks for low-resource languages, there are plenty of scopes in improving the quality of language representation particularly for code-mixed languages. In this paper, we propose HIT, a robust representation learning method for code-mixed texts. HIT is a hierarchical transformer-based framework that captures the semantic relationship among words and hierarchically learns the sentence-level semantics using a fused attention mechanism. HIT incorporates two attention modules, a multi-headed self-attention and an outer product attention module, and computes their weighted sum to obtain the attention weights. Our evaluation of HIT on one European (Spanish) and five Indic (Hindi, Bengali, Tamil, Telugu, and Malayalam) languages across four NLP tasks on eleven datasets suggests significant performance improvement against various state-of-the-art systems. We further show the adaptability of learned representation across tasks in a transfer learning setup (with and without fine-tuning).", "venue": "Findings", "year": 2021, "referenceCount": 36, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.407.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-30", "journal": {"pages": "4625-4639"}, "authors": [{"authorId": "34920835", "name": "Ayan Sengupta"}, {"authorId": "2106414464", "name": "S. Bhattacharjee"}, {"authorId": "144054829", "name": "Tanmoy Chakraborty"}, {"authorId": "46815454", "name": "Md. Shad Akhtar"}]}, {"paperId": "21532eb5ff79a18d9075d729dc5b5df40c8729d8", "externalIds": {"DBLP": "conf/acl/MaheshwariCKRI21", "ACL": "2021.findings-acl.408", "ArXiv": "2008.09887", "DOI": "10.18653/v1/2021.findings-acl.408", "CorpusId": 235421599}, "corpusId": 235421599, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/21532eb5ff79a18d9075d729dc5b5df40c8729d8", "title": "Semi-Supervised Data Programming with Subset Selection", "abstract": "The paradigm of data programming, which uses weak supervision in the form of rules/labelling functions, and semi-supervised learning, which augments small amounts of labelled data with a large unlabelled dataset, have shown great promise in several text classification scenarios. In this work, we argue that by not using any labelled data, data programming based approaches can yield sub-optimal performances, particularly when the labelling functions are noisy. The first contribution of this work is an introduction of a framework, \\model which is a semi-supervised data programming paradigm that learns a \\emph{joint model} that effectively uses the rules/labelling functions along with semi-supervised loss functions on the feature space. Next, we also study \\modelss which additionally does subset selection on top of the joint semi-supervised data programming objective and \\emph{selects} a set of examples that can be used as the labelled set by \\model. The goal of \\modelss is to ensure that the labelled data can \\emph{complement} the labelling functions, thereby benefiting from both data-programming as well as appropriately selected data for human labelling. We demonstrate that by effectively combining semi-supervision, data-programming, and subset selection paradigms, we significantly outperform the current state-of-the-art on seven publicly available datasets. \\footnote{The source code is available at \\url{https://github.com/ayushbits/Semi-Supervised-LFs-Subset-Selection}}", "venue": "Findings", "year": 2020, "referenceCount": 31, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.408.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-08-22", "journal": {"pages": "4640-4651"}, "authors": [{"authorId": "46202956", "name": "Ayush Maheshwari"}, {"authorId": "32309729", "name": "Oishik Chatterjee"}, {"authorId": "2112203718", "name": "Krishnateja Killamsetty"}, {"authorId": "150114500", "name": "Ganesh Ramakrishnan"}, {"authorId": "145074006", "name": "Rishabh K. Iyer"}]}, {"paperId": "a66ab2feae147f4ec59203019d51525ab5f0a92b", "externalIds": {"ACL": "2021.findings-acl.409", "DBLP": "conf/acl/DiwanCS21", "ArXiv": "2106.01703", "DOI": "10.18653/v1/2021.findings-acl.409", "CorpusId": 235313356}, "corpusId": 235313356, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/a66ab2feae147f4ec59203019d51525ab5f0a92b", "title": "Fingerprinting Fine-tuned Language Models in the Wild", "abstract": "There are concerns that the ability of language models (LMs) to generate high quality synthetic text can be misused to launch spam, disinformation, or propaganda. Therefore, the research community is actively working on developing approaches to detect whether a given text is organic or synthetic. While this is a useful first step, it is important to be able to further fingerprint the author LM to attribute its origin. Prior work on fingerprinting LMs is limited to attributing synthetic text generated by a handful (usually<10) of pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad of ways (e.g., on a domain-specific text corpus) before being used to generate synthetic text. It is challenging to fingerprinting fine-tuned LMs because the universe of fine-tuned LMs is much larger in realistic scenarios. To address this challenge, we study the problem of large-scale fingerprinting of fine-tuned LMs in the wild. Using a real-world dataset of synthetic text generated by 108 different fine-tuned LMs, we conduct comprehensive experiments to demonstrate the limitations of existing fingerprinting approaches. Our results show that fine-tuning itself is the most effective in attributing the synthetic text generated by fine-tuned LMs.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.409.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-03", "journal": {"name": "ArXiv", "volume": "abs/2106.01703"}, "authors": [{"authorId": "1659748509", "name": "Nirav Diwan"}, {"authorId": "144054829", "name": "Tanmoy Chakraborty"}, {"authorId": "34616778", "name": "Zubair Shafiq"}]}, {"paperId": "c7c09cd0013b7f9fff085d22b910dfea56f30ee4", "externalIds": {"MAG": "3177127579", "DBLP": "conf/acl/ShiWYCMKK21", "ACL": "2021.findings-acl.410", "DOI": "10.18653/v1/2021.findings-acl.410", "CorpusId": 236477869}, "corpusId": 236477869, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c7c09cd0013b7f9fff085d22b910dfea56f30ee4", "title": "Analyzing Code Embeddings for Coding Clinical Narratives", "abstract": "Medical professionals review clinical narratives to assign medical codes as per the International Classi\ufb01cation of Diseases (ICD) for billing and care management. This manual process is inef\ufb01cient and error-prone as it in-volves a nuanced one-to-many mapping. Recent works on automated ICD coding learn mappings between low-dimensional representations of the reports and the codes. While they propose novel neural networks for encoding varied types of information about the codes, it is unclear as to what information in the medical codes is helpful for performance improvement and why. Here, we compare different ways to represent, or embed, the codes based on their textual, structural and statistical characteristics, using a single deep learning baseline model in quantitative evaluations on discharge reports from the MIMIC-III Intensive Care Unit database. We also qualitatively anal-yse the nature of the cases that bene\ufb01t most from the code embeddings and demonstrate that code embeddings are important for pre-dicting ambiguous and oblique codes.", "venue": "Findings", "year": 2021, "referenceCount": 21, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-08-01", "journal": {"pages": "4665-4672"}, "authors": [{"authorId": "2153422488", "name": "Wei Shi"}, {"authorId": "39365627", "name": "Jiewen Wu"}, {"authorId": "2045602755", "name": "Xiwen Yang"}, {"authorId": "2185019", "name": "Nancy F. Chen"}, {"authorId": "27114860", "name": "Ivan Ho Mien"}, {"authorId": "2109200350", "name": "Jung-jae Kim"}, {"authorId": "33428484", "name": "Pavitra Krishnaswamy"}]}, {"paperId": "eea82bf4389ac91e2ee66fcf550a611f621c3eb9", "externalIds": {"DBLP": "conf/acl/QiCWLCS21", "ArXiv": "2105.12585", "ACL": "2021.findings-acl.411", "DOI": "10.18653/v1/2021.findings-acl.411", "CorpusId": 235195798}, "corpusId": 235195798, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/eea82bf4389ac91e2ee66fcf550a611f621c3eb9", "title": "Automatic Construction of Sememe Knowledge Bases via Dictionaries", "abstract": "A sememe is defined as the minimum semantic unit in linguistics. Sememe knowledge bases (SKBs), which comprise words annotated with sememes, enable sememes to be applied to natural language processing. So far a large body of research has showcased the unique advantages and effectiveness of SKBs in various tasks. However, most languages have no SKBs, and manual construction of SKBs is time-consuming and labor-intensive. To tackle this challenge, we propose a simple and fully automatic method of building an SKB via an existing dictionary. We use this method to build an English SKB and a French SKB, and conduct comprehensive evaluations from both intrinsic and extrinsic perspectives. Experimental results demonstrate that the automatically built English SKB is even superior to HowNet, the most widely used SKB that takes decades to build manually. And both the English and French SKBs can bring obvious performance enhancement in multiple downstream tasks. All the code and data of this paper (except the copyrighted dictionaries) can be obtained at https://github.com/thunlp/DictSKB.", "venue": "Findings", "year": 2021, "referenceCount": 84, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.411.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-26", "journal": {"name": "ArXiv", "volume": "abs/2105.12585"}, "authors": [{"authorId": "51466208", "name": "Fanchao Qi"}, {"authorId": "123331686", "name": "Yangyi Chen"}, {"authorId": "2109126455", "name": "Fengyu Wang"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "2117025507", "name": "Xiao Chen"}, {"authorId": "1753344", "name": "Maosong Sun"}]}, {"paperId": "fe7382db243694c67c667cf2ec80072577d2372b", "externalIds": {"ACL": "2021.findings-acl.412", "DBLP": "conf/acl/HouJLB21", "MAG": "3174267585", "DOI": "10.18653/v1/2021.findings-acl.412", "CorpusId": 236477903}, "corpusId": 236477903, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/fe7382db243694c67c667cf2ec80072577d2372b", "title": "Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning", "abstract": "Multi-hop reasoning is an effective and ex-plainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to \ufb01nd an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coincidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (Rule-Aware RL). It injects high quality symbolic rules into the model\u2019s reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 16, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.412.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-01", "journal": {"pages": "4687-4692"}, "authors": [{"authorId": "1560391730", "name": "Zhongni Hou"}, {"authorId": "2149111400", "name": "Xiaolong Jin"}, {"authorId": "46947005", "name": "Zixuan Li"}, {"authorId": "2075398318", "name": "Long Bai"}]}, {"paperId": "ecf5618b513aa5c4d5bf62ca251923a188251117", "externalIds": {"DBLP": "conf/acl/HasanBIMLKRS21", "ArXiv": "2106.13822", "ACL": "2021.findings-acl.413", "DOI": "10.18653/v1/2021.findings-acl.413", "CorpusId": 235658519}, "corpusId": 235658519, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/ecf5618b513aa5c4d5bf62ca251923a188251117", "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages", "abstract": "Contemporary works on abstractive text summarization have focused primarily on high-resource languages like English, mostly due to the limited availability of datasets for low/mid-resource ones. In this work, we present XL-Sum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model, with XL-Sum and experiment on multilingual and low-resource summarization tasks. XL-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, XL-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at \\url{https://github.com/csebuetnlp/xl-sum}.", "venue": "Findings", "year": 2021, "referenceCount": 51, "citationCount": 137, "influentialCitationCount": 33, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.413.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-25", "journal": {"pages": "4693-4703"}, "authors": [{"authorId": "1400373232", "name": "Tahmid Hasan"}, {"authorId": "49785688", "name": "Abhik Bhattacharjee"}, {"authorId": null, "name": "Md Saiful Islam"}, {"authorId": "1955443619", "name": "Kazi Samin"}, {"authorId": "4495301", "name": "Yuan-Fang Li"}, {"authorId": "145059666", "name": "Yong-Bin Kang"}, {"authorId": "2218125433", "name": "M. Rahman"}, {"authorId": "2046603", "name": "Rifat Shahriyar"}]}, {"paperId": "f29d5cb8f405903fc8af7a5d7ab4bf7d65796e95", "externalIds": {"DBLP": "conf/acl/SantyRC21", "ACL": "2021.findings-acl.414", "ArXiv": "2106.01105", "DOI": "10.18653/v1/2021.findings-acl.414", "CorpusId": 235293811}, "corpusId": 235293811, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/f29d5cb8f405903fc8af7a5d7ab4bf7d65796e95", "title": "Use of Formal Ethical Reviews in NLP Literature: Historical Trends and Current Practices", "abstract": "Ethical aspects of research in language technologies have received much attention recently. It is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution. How commonly do we see mention of ethical approvals in NLP research? What types of research or aspects of studies are usually subject to such reviews? With the rising concerns and discourse around the ethics of NLP, do we also observe a rise in formal ethical reviews of NLP studies? And, if so, would this imply that there is a heightened awareness of ethical issues that was previously lacking? We aim to address these questions by conducting a detailed quantitative and qualitative analysis of the ACL Anthology, as well as comparing the trends in our field to those of other related disciplines, such as cognitive science, machine learning, data mining, and systems.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 3, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.414.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-06-02", "journal": {"pages": "4704-4710"}, "authors": [{"authorId": "50074956", "name": "Sebastin Santy"}, {"authorId": "2106627712", "name": "Anku Rani"}, {"authorId": "143990839", "name": "M. Choudhury"}]}, {"paperId": "9afec11bc43451b920b390359a68a559dd380da7", "externalIds": {"ACL": "2021.findings-acl.415", "ArXiv": "2107.08357", "DBLP": "conf/acl/WangXGERC21", "DOI": "10.18653/v1/2021.findings-acl.415", "CorpusId": 236087582}, "corpusId": 236087582, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9afec11bc43451b920b390359a68a559dd380da7", "title": "As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation", "abstract": "Mistranslated numbers have the potential to cause serious effects, such as financial loss or medical misinformation. In this work we develop comprehensive assessments of the robustness of neural machine translation systems to numerical text via behavioural testing. We explore a variety of numerical translation capabilities a system is expected to exhibit and design effective test examples to expose system underperformance. We find that numerical mistranslation is a general issue: major commercial systems and state-of-the-art research models fail on many of our test examples, for high- and low-resource languages. Our tests reveal novel errors that have not previously been reported in NMT systems, to the best of our knowledge. Lastly, we discuss strategies to mitigate numerical mistranslation.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 9, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.415.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-18", "journal": {"name": "ArXiv", "volume": "abs/2107.08357"}, "authors": [{"authorId": "2152811136", "name": "Jun Wang"}, {"authorId": "2115472755", "name": "Chang Xu"}, {"authorId": "144204682", "name": "Francisco Guzm\u00e1n"}, {"authorId": "1398503968", "name": "Ahmed El-Kishky"}, {"authorId": "1868067", "name": "Benjamin I. P. Rubinstein"}, {"authorId": "143620680", "name": "Trevor Cohn"}]}, {"paperId": "5ef0554efe188ef7ab7f89a67bc9c1e3b31eeffb", "externalIds": {"ACL": "2021.findings-acl.416", "DBLP": "conf/acl/LevySW21", "ArXiv": "2101.00379", "DOI": "10.18653/v1/2021.findings-acl.416", "CorpusId": 234482105}, "corpusId": 234482105, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5ef0554efe188ef7ab7f89a67bc9c1e3b31eeffb", "title": "Investigating Memorization of Conspiracy Theories in Text Generation", "abstract": "The adoption of natural language generation (NLG) models can leave individuals vulnerable to the generation of harmful information memorized by the models, such as conspiracy theories. While previous studies examine conspiracy theories in the context of social media, they have not evaluated their presence in the new space of generative language models. In this work, we investigate the capability of language models to generate conspiracy theory text. Specifically, we aim to answer: can we test pretrained generative language models for the memorization and elicitation of conspiracy theories without access to the model's training data? We highlight the difficulties of this task and discuss it in the context of memorization, generalization, and hallucination. Utilizing a new dataset consisting of conspiracy theory topics and machine-generated conspiracy theories helps us discover that many conspiracy theories are deeply rooted in the pretrained language models. Our experiments demonstrate a relationship between model parameters such as size and temperature and their propensity to generate conspiracy theory text. These results indicate the need for a more thorough review of NLG applications before release and an in-depth discussion of the drawbacks of memorization in generative language models.", "venue": "Findings", "year": 2021, "referenceCount": 52, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.416.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-01-02", "journal": {"pages": "4718-4729"}, "authors": [{"authorId": "49285370", "name": "Sharon Levy"}, {"authorId": "48227633", "name": "Michael Stephen Saxon"}, {"authorId": "152876475", "name": "W. Wang"}]}, {"paperId": "1698871ecc3e9cfac5994e7a80693c0b6cceefd0", "externalIds": {"ACL": "2021.findings-acl.417", "DBLP": "conf/acl/WuLZQZ21", "DOI": "10.18653/v1/2021.findings-acl.417", "CorpusId": 236478321}, "corpusId": 236478321, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/1698871ecc3e9cfac5994e7a80693c0b6cceefd0", "title": "A Text-Centered Shared-Private Framework via Cross-Modal Prediction for Multimodal Sentiment Analysis", "abstract": "Multimodal fusion is a core problem for multimodal sentiment analysis. Previous works usually treat all three modal features equally and implicitly explore the interactions between different modalities. In this paper, we break this kind of methods in two ways. Firstly, we observe that textual modality plays the most important role in multimodal sentiment analysis, and this can be seen from the previous works. Secondly, we observe that comparing to the textual modality, the other two kinds of nontextual modalities (visual and acoustic) can provide two kinds of semantics, shared and private semantics. The shared semantics from the other two modalities can obviously enhance the textual semantics and make the sentiment analysis model more robust, and the private semantics can be complementary to the textual semantics and meanwhile provide different views to improve the performance of sentiment analysis together with the shared semantics. Motivated by these two observations, we propose a text-centered shared-private framework (TCSP) for multimodal fusion, which consists of the cross-modal prediction and sentiment regression parts. Experiments on the MOSEI and MOSI datasets demonstrate the effectiveness of our shared-private framework, which outperforms all baselines. Furthermore, our approach provides a new way to utilize the unlabeled data for multimodal sentiment analysis.", "venue": "Findings", "year": 2021, "referenceCount": 20, "citationCount": 35, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.417.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4730-4738"}, "authors": [{"authorId": "143792101", "name": "Yang Wu"}, {"authorId": "2112306085", "name": "Zijie Lin"}, {"authorId": "49339265", "name": "Yanyan Zhao"}, {"authorId": "152277111", "name": "Bing Qin"}, {"authorId": "2153461773", "name": "Li-Nan Zhu"}]}, {"paperId": "7e63226e228ffb8cbb6e3c55ae5b83efb845c4b8", "externalIds": {"DBLP": "journals/corr/abs-2106-05249", "ACL": "2021.findings-acl.418", "ArXiv": "2106.05249", "DOI": "10.18653/v1/2021.findings-acl.418", "CorpusId": 235376953}, "corpusId": 235376953, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/7e63226e228ffb8cbb6e3c55ae5b83efb845c4b8", "title": "What Would a Teacher Do? Predicting Future Talk Moves", "abstract": "Recent advances in natural language processing (NLP) have the ability to transform how classroom learning takes place. Combined with the increasing integration of technology in today's classrooms, NLP systems leveraging question answering and dialog processing techniques can serve as private tutors or participants in classroom discussions to increase student engagement and learning. To progress towards this goal, we use the classroom discourse framework of academically productive talk (APT) to learn strategies that make for the best learning experience. In this paper, we introduce a new task, called future talk move prediction (FTMP): it consists of predicting the next talk move -- an utterance strategy from APT -- given a conversation history with its corresponding talk moves. We further introduce a neural network model for this task, which outperforms multiple baselines by a large margin. Finally, we compare our model's performance on FTMP to human performance and show several similarities between the two.", "venue": "Findings", "year": 2021, "referenceCount": 50, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.418.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-09", "journal": {"pages": "4739-4751"}, "authors": [{"authorId": "47079359", "name": "Ananya Ganesh"}, {"authorId": "145755155", "name": "Martha Palmer"}, {"authorId": "3422953", "name": "Katharina Kann"}]}, {"paperId": "0bab7c91be83f836ee3f76154277246676babe50", "externalIds": {"ACL": "2021.findings-acl.419", "DBLP": "conf/acl/GaoCLZY21", "DOI": "10.18653/v1/2021.findings-acl.419", "CorpusId": 236477889}, "corpusId": 236477889, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0bab7c91be83f836ee3f76154277246676babe50", "title": "BioGen: Generating Biography Summary under Table Guidance on Wikipedia", "abstract": "Capturing the salient information from an input article has been a long-standing challenge for summarization. On Wikipedia, most of the wiki pages about people contain a factual table that lists the basic properties of the people. Illuminatingly, a factual table can be regarded as a natural summary of the key information in the corresponding article. Thus, in this paper we propose the task of tableguided abstractive biography summarization, which utilizes factual tables to capture important information and then generate a summary of a biography. We first introduce the TaGS (Table-Guided Summarization) dataset1, the first large-scale biography summarization dataset with tables. Next, we report some statistics about this dataset to validate the quality of the dataset. We also benchmark several commonly used summarization methods on TaGS and hope this will inspire more exciting methods.", "venue": "Findings", "year": 2021, "referenceCount": 31, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4752-4757"}, "authors": [{"authorId": "2112311595", "name": "Shen Gao"}, {"authorId": "2116950235", "name": "Xiuying Chen"}, {"authorId": "2144545923", "name": "Chang Liu"}, {"authorId": "144060462", "name": "Dongyan Zhao"}, {"authorId": "144539156", "name": "Rui Yan"}]}, {"paperId": "3eb8cb91a91819ec4d68d1fa61ef12cd43b4492f", "externalIds": {"DBLP": "conf/acl/ArthurRH21", "ACL": "2021.findings-acl.420", "DOI": "10.18653/v1/2021.findings-acl.420", "CorpusId": 236478193}, "corpusId": 236478193, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/3eb8cb91a91819ec4d68d1fa61ef12cd43b4492f", "title": "Multilingual Simultaneous Neural Machine Translation", "abstract": "Simultaneous machine translation (S I MT) involves translating source utterances to the target language in real-time before the speaker utterance completes. This paper proposes the multilingual approach to S I MT, where a single model simultaneously translates between multiple language-pairs. This not only results in more ef\ufb01ciency in terms of the number of models and parameters (hence simpler deployment), but may also lead to higher performing models by capturing commonalities among the languages. We further explore simple and effective multilingual architectures based on two strong recently proposed S I MT models. Our results on translating from two Germanic languages (German, Dutch) and three Romance languages (French, Italian, Romanian) into English show (i) the single multilingual model is on-par or better than individual models, and (ii) multilingual S I MT models trained based on language families are on-par or better than the universal model trained for all languages. 1", "venue": "Findings", "year": 2021, "referenceCount": 29, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.420.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4758-4766"}, "authors": [{"authorId": "144332819", "name": "Philip Arthur"}, {"authorId": "2121451202", "name": "Dongwon Ryu"}, {"authorId": "2561045", "name": "Gholamreza Haffari"}]}, {"paperId": "cede7dce7b826d6442350a16e3bbcec9da923bfa", "externalIds": {"DBLP": "conf/acl/YuGX21", "ACL": "2021.findings-acl.421", "DOI": "10.18653/v1/2021.findings-acl.421", "CorpusId": 236478306}, "corpusId": 236478306, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/cede7dce7b826d6442350a16e3bbcec9da923bfa", "title": "Cross-Domain Review Generation for Aspect-Based Sentiment Analysis", "abstract": "Supervised learning methods have proven to be effective for Aspect-Based Sentiment Analysis (ABSA). However, the lack of \ufb01ne-grained labeled data hinders their effectiveness in many domains. To address this is-sue, unsupervised domain adaptation methods are desired to transfer knowledge from a labeled source domain to any unlabeled target domain. In this paper, we propose a new domain adaptation paradigm called cross-domain review generation (CDRG), which aims to generate target-domain reviews with \ufb01ne-grained annotation based on the source-domain labeled reviews. To achieve this goal, we propose a two-step approach as a concrete realization of CDRG. It \ufb01rst converts a source-domain review to a domain-independent review by masking its source-speci\ufb01c attributes, and then converts the domain-independent review to a target-domain review with a masked language model pre-trained in the target domain. We further propose two ways to leverage the generated target-domain reviews for two cross-domain ABSA tasks. Extensive experiments demonstrate the superiority of our CDRG-based approaches over the state-of-the-art domain adaptation methods.", "venue": "Findings", "year": 2021, "referenceCount": 49, "citationCount": 18, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.421.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "journal": {"pages": "4767-4777"}, "authors": [{"authorId": "143621743", "name": "Jianfei Yu"}, {"authorId": "2143630268", "name": "Chenggong Gong"}, {"authorId": "1491639587", "name": "Rui Xia"}]}, {"paperId": "0f192e9c7a1e3fdc6e051fc502f74b04c53bb3a3", "externalIds": {"DBLP": "journals/corr/abs-2106-03297", "ACL": "2021.findings-acl.422", "ArXiv": "2106.03297", "DOI": "10.18653/v1/2021.findings-acl.422", "CorpusId": 235358300}, "corpusId": 235358300, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0f192e9c7a1e3fdc6e051fc502f74b04c53bb3a3", "title": "On the Language Coverage Bias for Neural Machine Translation", "abstract": "Language coverage bias, which indicates the content-dependent differences between sentence pairs originating from the source and target languages, is important for neural machine translation (NMT) because the target-original training data is not well exploited in current practice. By carefully designing experiments, we provide comprehensive analyses of the language coverage bias in the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation. We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants.", "venue": "Findings", "year": 2021, "referenceCount": 39, "citationCount": 16, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.422.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-07", "journal": {"pages": "4778-4790"}, "authors": [{"authorId": "12782489", "name": "Shuo Wang"}, {"authorId": "2909321", "name": "Zhaopeng Tu"}, {"authorId": "3468510", "name": "Zhixing Tan"}, {"authorId": "2072684668", "name": "Shuming Shi"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "2152797839", "name": "Yang Liu"}]}, {"paperId": "6110581bd5650ebdf777f008b10e1db6a6e03bc0", "externalIds": {"DBLP": "conf/acl/HuangCWZXS21", "ACL": "2021.findings-acl.423", "DOI": "10.18653/v1/2021.findings-acl.423", "CorpusId": 236477612}, "corpusId": 236477612, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/6110581bd5650ebdf777f008b10e1db6a6e03bc0", "title": "Named Entity Recognition via Noise Aware Training Mechanism with Data Filter", "abstract": "Named entity recognition (NER) is a fundamental task in natural language processing, these is a long held belief that datasets benefit the model. However, not all the data help with generalization, and some samples may contain ambiguous entities or noisy labels. The existing methods can not distinguish hard samples from noisy samples well, and becomes particularly challenging in the case of overfitting. This paper proposes a new method called Noise-Aware-with-Filter (NAF) to solve the issues from two sides. From the perspective of the data, we design a Logit-MaximumDifference (LMD) mechanism, which maximizes the diversity between different samples to help the model identify noisy samples. From the perspective of the model, we design an Incomplete-Trust (In-trust) loss function, which boosts LCRF with a robust DistrustCross-Entropy(DCE) term. Our proposed Intrust can effectively alleviate the overfitting caused by previous loss function. Experiments on six real-world Chinese and English NER datasets show that NAF outperforms the previous methods, and which obtained the state-ofthe-art(SOTA) results on the CoNLL2003 and CoNLL++ datasets.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4791-4803"}, "authors": [{"authorId": "2121360698", "name": "Xiusheng Huang"}, {"authorId": "152829071", "name": "Yubo Chen"}, {"authorId": "2142612131", "name": "Shun Wu"}, {"authorId": "11447228", "name": "Jun Zhao"}, {"authorId": "2154871127", "name": "Yuantao Xie"}, {"authorId": "2152822047", "name": "Weijian Sun"}]}, {"paperId": "d888ac9e9ac54dacb5fa8b6ce29ee6ecf64d9710", "externalIds": {"DBLP": "conf/acl/TongCS21", "ACL": "2021.findings-acl.424", "DOI": "10.18653/v1/2021.findings-acl.424", "CorpusId": 236477466}, "corpusId": 236477466, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d888ac9e9ac54dacb5fa8b6ce29ee6ecf64d9710", "title": "A Multi-Task Approach for Improving Biomedical Named Entity Recognition by Incorporating Multi-Granularity information", "abstract": "Neural biomedical named entity recognition (BioNER) methods usually require a large amount of annotated data, while the annotated BioNER datasets are often difficult to obtain and small in scale due to the limitations of privacy, ethics and high degree of specialization. To alleviate the lack of training samples, unlike conventional methods that only use token-level information, this paper proposes a method that simultaneously utilize the latent multi-granularity information in the dataset. Concretely, the proposed model is based on a multi-task approach, which leverages different training objectives by introducing auxiliary tasks, i.e. binary classification, multi-class and multi-token classification. Experimental results over three BioNER datasets show that the proposed model produces better performance over the BioBERT baseline and can get more than 3% improvements of F1score in low-resource scenarios. Finally, we released our code at https://github.com/ zgzjdx/MT-BioNER.", "venue": "Findings", "year": 2021, "referenceCount": 48, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.424.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4804-4813"}, "authors": [{"authorId": "2007715582", "name": "Yiqi Tong"}, {"authorId": "47558200", "name": "Yidong Chen"}, {"authorId": "114438707", "name": "Xiaodon Shi"}]}, {"paperId": "cbde5598c1a78285adfcfd77fb3636f5498987a0", "externalIds": {"ACL": "2021.findings-acl.425", "DBLP": "conf/acl/LiuLLC21", "DOI": "10.18653/v1/2021.findings-acl.425", "CorpusId": 236477807}, "corpusId": 236477807, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/cbde5598c1a78285adfcfd77fb3636f5498987a0", "title": "EBERT: Efficient BERT Inference with Dynamic Structured Pruning", "abstract": "Pruning has been demonstrated as an effective way of reducing computational complexity for deep networks, especially CNNs for computer vision tasks. In this paper, we investigate the opportunity to accelerate the inference of large-scale pre-trained language model via pruning. We propose EBERT, a dynamic structured pruning algorithm for efficient BERT inference. Unlike previous methods that randomly prune the model weights for static inference, EBERT dynamically determines and prunes the unimportant heads in multi-head self-attention layers and the unimportant structured computations in feed-forward network for each input sample at run-time. Experimental results show that our proposed EBERT outperforms other state-of-the-art methods on different tasks.", "venue": "Findings", "year": 2021, "referenceCount": 30, "citationCount": 22, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.425.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4814-4823"}, "authors": [{"authorId": "2109371824", "name": "Zejian Liu"}, {"authorId": "2146327608", "name": "Fanrong Li"}, {"authorId": "2155119406", "name": "Gang Li"}, {"authorId": "2112798578", "name": "Jian Cheng"}]}, {"paperId": "4460d4f468e266ef69b8572c5f7704b2e55fe73b", "externalIds": {"ACL": "2021.findings-acl.426", "DBLP": "conf/acl/TymoshenkoM21", "DOI": "10.18653/v1/2021.findings-acl.426", "CorpusId": 236477851}, "corpusId": 236477851, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/4460d4f468e266ef69b8572c5f7704b2e55fe73b", "title": "Strong and Light Baseline Models for Fact-Checking Joint Inference", "abstract": "How to combine several pieces of evidence to verify a claim is an interesting semantic task. Very complex methods have been proposed, combining different evidence vectors using an evidence interaction graph. In this paper, we show that in case of inference based on transformer models, two effective approaches use either (i) a simple application of max pooling over the Transformer evidence vectors; or (ii) computing a weighted sum of the evidence vectors. Our experiments on the FEVER claim verification task show that the methods above achieve the state of the art, constituting strong baseline for much more computationally complex methods.", "venue": "Findings", "year": 2021, "referenceCount": 16, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4824-4830"}, "authors": [{"authorId": "2065830069", "name": "K. Tymoshenko"}, {"authorId": "1719404", "name": "Alessandro Moschitti"}]}, {"paperId": "c4d0df5f7a700f8c999bb2fa5a7a9df0b2d2dd97", "externalIds": {"ArXiv": "2105.14778", "DBLP": "conf/acl/WangLYZZZY21", "ACL": "2021.findings-acl.427", "DOI": "10.18653/v1/2021.findings-acl.427", "CorpusId": 235253772}, "corpusId": 235253772, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c4d0df5f7a700f8c999bb2fa5a7a9df0b2d2dd97", "title": "Sketch and Refine: Towards Faithful and Informative Table-to-Text Generation", "abstract": "Table-to-text generation refers to generating a descriptive text from a key-value table. Traditional autoregressive methods, though can generate text with high fluency, suffer from low coverage and poor faithfulness problems. To mitigate these problems, we propose a novel Skeleton-based two-stage method that combines both Autoregressive and Non-Autoregressive generations (SANA). Our approach includes: (1) skeleton generation with an autoregressive pointer network to select key tokens from the source table; (2) edit-based non-autoregressive generation model to produce texts via iterative insertion and deletion operations. By integrating hard constraints from the skeleton, the non-autoregressive model improves the generation's coverage over the source table and thus enhances its faithfulness. We conduct automatic and human evaluations on both WikiPerson and WikiBio datasets. Experimental results demonstrate that our method outperforms the previous state-of-the-art methods in both automatic and human evaluation, especially on coverage and faithfulness. In particular, we achieve PARENT-T recall of 99.47 in WikiPerson, improving over the existing best results by more than 10 points.", "venue": "Findings", "year": 2021, "referenceCount": 49, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.427.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-31", "journal": {"pages": "4831-4843"}, "authors": [{"authorId": "2155302144", "name": "Peng Wang"}, {"authorId": "35996608", "name": "Junyang Lin"}, {"authorId": null, "name": "An Yang"}, {"authorId": "144161025", "name": "Chang Zhou"}, {"authorId": "29343468", "name": "Yichang Zhang"}, {"authorId": "1709595", "name": "Jingren Zhou"}, {"authorId": "38385080", "name": "Hongxia Yang"}]}, {"paperId": "c61459f2ddf053a2768105c00a020ec4a81b1091", "externalIds": {"DBLP": "conf/acl/DiaoSSSZ21", "ACL": "2021.findings-acl.428", "DOI": "10.18653/v1/2021.findings-acl.428", "CorpusId": 236477812}, "corpusId": 236477812, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c61459f2ddf053a2768105c00a020ec4a81b1091", "title": "TILGAN: Transformer-based Implicit Latent GAN for Diverse and Coherent Text Generation", "abstract": "Conventional autoregressive models have achieved great success in text generation but suffer from the exposure bias problem in that token sequences in the training and in the generation stages are mismatched. While generative adversarial networks (GANs) can rem-edy this problem, existing implementations of GANs directly on discrete outputs tend to be unstable and lack diversity. In this work, we propose TILGAN , a T ransformer-based I mplicit L atent GAN , which combines a Transformer autoencoder and GAN in the latent space with a novel design and distribution matching based on the Kullback-Leibler (KL) divergence. Speci\ufb01cally, to improve local and global coherence, we explicitly introduce a multi-scale discriminator to capture the semantic information at varying scales among the sequence of hidden representations encoded by Transformer. Moreover, the decoder is enhanced by an additional KL loss to be consistent with the latent-generator. Experimental results on three benchmark datasets demonstrate the validity and effectiveness of our model, by obtaining signi\ufb01cant improvements and a better quality-diversity trade-off in automatic and human evaluation for both unconditional and conditional generation tasks. 1", "venue": "Findings", "year": 2021, "referenceCount": 45, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4844-4858"}, "authors": [{"authorId": "50826757", "name": "Shizhe Diao"}, {"authorId": "2111109710", "name": "Xinwei Shen"}, {"authorId": "2121340452", "name": "Kashun Shum"}, {"authorId": "1922182598", "name": "Yan Song"}, {"authorId": "2146324423", "name": "Tong Zhang"}]}, {"paperId": "38cfcd4681cf85ab507ec0586c753182a4c8eecb", "externalIds": {"ACL": "2021.findings-acl.429", "DBLP": "conf/acl/Kementchedjhieva21", "ArXiv": "2106.01060", "DOI": "10.18653/v1/2021.findings-acl.429", "CorpusId": 235294065}, "corpusId": 235294065, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/38cfcd4681cf85ab507ec0586c753182a4c8eecb", "title": "John praised Mary because _he_? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs", "abstract": "Some interpersonal verbs can implicitly attribute causality to either their subject or their object and are therefore said to carry an implicit causality (IC) bias. Through this bias, causal links can be inferred from a narrative, aiding language comprehension. We investigate whether pre-trained language models (PLMs) encode IC bias and use it at inference time. We find that to be the case, albeit to different degrees, for three distinct PLM architectures. However, causes do not always need to be implicit -- when a cause is explicitly stated in a subordinate clause, an incongruent IC bias associated with the verb in the main clause leads to a delay in human processing. We hypothesize that the temporary challenge humans face in integrating the two contradicting signals, one from the lexical semantics of the verb, one from the sentence-level semantics, would be reflected in higher error rates for models on tasks dependent on causal links. The results of our study lend support to this hypothesis, suggesting that PLMs tend to prioritize lexical patterns over higher-order signals.", "venue": "Findings", "year": 2021, "referenceCount": 53, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.429.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01060"}, "authors": [{"authorId": "51208524", "name": "Yova Kementchedjhieva"}, {"authorId": "2109790043", "name": "M. Anderson"}, {"authorId": "1700187", "name": "Anders S\u00f8gaard"}]}, {"paperId": "c3d4f9f721a2b1164f043d7ca2db10daaeb19e68", "externalIds": {"ACL": "2021.findings-acl.430", "DBLP": "conf/acl/ZhangMLW21", "DOI": "10.18653/v1/2021.findings-acl.430", "CorpusId": 236477788}, "corpusId": 236477788, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c3d4f9f721a2b1164f043d7ca2db10daaeb19e68", "title": "Do It Once: An Embarrassingly Simple Joint Matching Approach to Response Selection", "abstract": "Existing matching models for response selection adopt the independent matching (IM) approach. To complete a prediction, they have to perform N independent matches, where N is the number of response options. In this paper, we explore a joint matching (JM) approach which performs matching only once regardless of the number of options. The JM approach does not change the structure of matching component but only modifies its input and output format. It also enables a cheap but effective data augmentation method. Extensive experiments on the MuTual dataset demonstrate that, even with the simplest formulation, JM outperforms IM approach by a large margin and reduces training time by over half.", "venue": "Findings", "year": 2021, "referenceCount": 15, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4872-4877"}, "authors": [{"authorId": "153823979", "name": "Linhao Zhang"}, {"authorId": "2113509189", "name": "Dehong Ma"}, {"authorId": "48831399", "name": "Sujian Li"}, {"authorId": "1781885", "name": "Houfeng Wang"}]}, {"paperId": "d7fe1ddd5c2cf02d640d16a70a418562eac5963a", "externalIds": {"DBLP": "conf/acl/GlavasV21", "ACL": "2021.findings-acl.431", "DOI": "10.18653/v1/2021.findings-acl.431", "CorpusId": 236478093}, "corpusId": 236478093, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d7fe1ddd5c2cf02d640d16a70a418562eac5963a", "title": "Climbing the Tower of Treebanks: Improving Low-Resource Dependency Parsing via Hierarchical Source Selection", "abstract": "Recent work on multilingual dependency parsing focused on developing highly multilingual parsers that can be applied to a wide range of low-resource languages. In this work, we substantially outperform such \u201cone model to rule them all\u201d approach with a heuristic selection of languages and treebanks on which to train the parser for a specific target language. Our approach, dubbed TOWER, first hierarchically clusters all Universal Dependencies languages based on their mutual syntactic similarity computed from human-coded URIEL vectors. For each low-resource target language, we then climb this language hierarchy starting from the leaf node of that language and heuristically choose the hierarchy level at which to collect training treebanks. This treebank selection heuristic is based on: (i) the aggregate size of all treebanks subsumed by the hierarchy level and (ii) the similarity of the languages in the training sample with the target language. For languages without development treebanks, we additionally use (ii) for model selection (i.e., early stopping) in order to prevent overfitting to development treebanks of closest languages. Our TOWER approach shows substantial gains for low-resource languages over two state-ofthe-art multilingual parsers, with more than 20 LAS point gains for some of those languages. Parsing models and code available at: https: //github.com/codogogo/towerparse.", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4878-4888"}, "authors": [{"authorId": "2472657", "name": "Goran Glavas"}, {"authorId": "1747849", "name": "Ivan Vulic"}]}, {"paperId": "9588603547d14c81beca87f6de399334b8d3645d", "externalIds": {"ACL": "2021.findings-acl.432", "DBLP": "conf/acl/ChanLLZZSY21", "DOI": "10.18653/v1/2021.findings-acl.432", "CorpusId": 236477596}, "corpusId": 236477596, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9588603547d14c81beca87f6de399334b8d3645d", "title": "Enhancing the Open-Domain Dialogue Evaluation in Latent Space", "abstract": "The notorious one-to-many nature of open-domain dialogues poses huge challenges for automatic evaluation methods. Recent studies attempt to mitigate this issue by considering the similarity of the generated response with the conversational context and design discriminative models to learn from multiple positive responses. Despite the promising results, they can not be applied to general scenarios where training data with multiple responses is unavailable. To this end, in this paper, we propose a self-supervised setting to obtain a smooth latent space that can both capture discourse-level context information and implicitly model more references in latent space. Speci\ufb01cally, we present EMS, an E nhanced dialogue evaluation M etric in latent S pace. Experimental results on two real-world dialogue datasets con\ufb01rm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines.", "venue": "Findings", "year": 2021, "referenceCount": 52, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.432.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4889-4900"}, "authors": [{"authorId": "51177175", "name": "Zhangming Chan"}, {"authorId": "2978364", "name": "Lemao Liu"}, {"authorId": "2109013629", "name": "Juntao Li"}, {"authorId": "2108558798", "name": "Haisong Zhang"}, {"authorId": "144060462", "name": "Dongyan Zhao"}, {"authorId": "2072684668", "name": "Shuming Shi"}, {"authorId": "144539156", "name": "Rui Yan"}]}, {"paperId": "15e653d727fe90e627a328e0ebe9d47cb93cc5f1", "externalIds": {"DBLP": "journals/corr/abs-2105-02855", "ArXiv": "2105.02855", "ACL": "2021.findings-acl.433", "DOI": "10.18653/v1/2021.findings-acl.433", "CorpusId": 233864536}, "corpusId": 233864536, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/15e653d727fe90e627a328e0ebe9d47cb93cc5f1", "title": "Adapting Monolingual Models: Data can be Scarce when Language Similarity is High", "abstract": "For many (minority) languages, the resources needed to train large models are not available. We investigate the performance of zero-shot transfer learning with as little data as possible, and the influence of language similarity in this process. We retrain the lexical layers of four BERT-based models using data from two low-resource target language varieties, while the Transformer layers are independently fine-tuned on a POS-tagging task in the model's source language. By combining the new lexical layers and fine-tuned Transformer layers, we achieve high task performance for both target languages. With high language similarity, 10MB of data appears sufficient to achieve substantial monolingual transfer performance. Monolingual BERT-based models generally achieve higher downstream task performance after retraining the lexical layer than multilingual BERT, even when the target language is included in the multilingual model.", "venue": "Findings", "year": 2021, "referenceCount": 23, "citationCount": 17, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.433.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-06", "journal": {"name": "ArXiv", "volume": "abs/2105.02855"}, "authors": [{"authorId": "144611157", "name": "Wietse de Vries"}, {"authorId": "151105659", "name": "Martijn Bartelds"}, {"authorId": "2742475", "name": "M. Nissim"}, {"authorId": "49028541", "name": "M. Wieling"}]}, {"paperId": "bcf9b9238aef3c69f10d5a05136feb71f9a3de79", "externalIds": {"ACL": "2021.findings-acl.434", "DBLP": "conf/acl/YinWQX21", "DOI": "10.18653/v1/2021.findings-acl.434", "CorpusId": 236477688}, "corpusId": 236477688, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/bcf9b9238aef3c69f10d5a05136feb71f9a3de79", "title": "BatchMixup: Improving Training by Interpolating Hidden States of the Entire Mini-batch", "abstract": "Usually, we train a neural system on a sequence of mini-batches of labeled instances. Each mini-batch is composed of k samples, and each sample will learn a representation vector. M IXUP implicitly generates synthetic samples through linearly interpolating inputs and their corresponding labels of random sample pairs in the same mini-batch. This means that M IXUP only generates new points on the edges connecting every two original points in the representation space. We observed that the new points by the standard M IXUP cover pretty limited regions in the entire space of the mini-batch. In this work, we propose B ATCH M IXUP \u2014improving the model learning by interpolating hidden states of the entire mini-batch. B ATCH M IXUP can generate new points scattered throughout the space corresponding to the mini-batch. In experiments, B ATCH M IXUP shows superior performance than competitive baselines in improving the performance of NLP tasks while using different ratios of training data.", "venue": "Findings", "year": 2021, "referenceCount": 20, "citationCount": 16, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4908-4912"}, "authors": [{"authorId": "40483594", "name": "Wenpeng Yin"}, {"authorId": "2197900626", "name": "Huan Wang"}, {"authorId": "2125132004", "name": "Jin Qu"}, {"authorId": "2054594326", "name": "Caiming Xiong"}]}, {"paperId": "42f8a3da7021bc725fa14fdb63fa9c7c9fc934f6", "externalIds": {"DBLP": "journals/corr/abs-2106-09449", "ArXiv": "2106.09449", "ACL": "2021.findings-acl.435", "DOI": "10.18653/v1/2021.findings-acl.435", "CorpusId": 235458620}, "corpusId": 235458620, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/42f8a3da7021bc725fa14fdb63fa9c7c9fc934f6", "title": "DocNLI: A Large-scale Dataset for Document-level Natural Language Inference", "abstract": "Natural language inference (NLI) is formulated as a unified framework for solving various NLP problems such as relation extraction, question answering, summarization, etc. It has been studied intensively in the past few years thanks to the availability of large-scale labeled datasets. However, most existing studies focus on merely sentence-level inference, which limits the scope of NLI's application in downstream NLP problems. This work presents DocNLI -- a newly-constructed large-scale dataset for document-level NLI. DocNLI is transformed from a broad range of NLP problems and covers multiple genres of text. The premises always stay in the document granularity, whereas the hypotheses vary in length from single sentences to passages with hundreds of words. Additionally, DocNLI has pretty limited artifacts which unfortunately widely exist in some popular sentence-level NLI datasets. Our experiments demonstrate that, even without fine-tuning, a model pretrained on DocNLI shows promising performance on popular sentence-level benchmarks, and generalizes well to out-of-domain NLP tasks that rely on inference at document granularity. Task-specific fine-tuning can bring further improvements. Data, code, and pretrained models can be found at https://github.com/salesforce/DocNLI.", "venue": "Findings", "year": 2021, "referenceCount": 28, "citationCount": 56, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.435.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-17", "journal": {"name": "ArXiv", "volume": "abs/2106.09449"}, "authors": [{"authorId": "40483594", "name": "Wenpeng Yin"}, {"authorId": "9215251", "name": "Dragomir R. Radev"}, {"authorId": "2054594326", "name": "Caiming Xiong"}]}, {"paperId": "9dbc8ba1bca2336ab4d07530882a7ca8b78cb925", "externalIds": {"ACL": "2021.findings-acl.436", "ArXiv": "2105.10193", "DBLP": "conf/acl/SahayNMRI21", "DOI": "10.18653/v1/2021.findings-acl.436", "CorpusId": 235125575}, "corpusId": 235125575, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/9dbc8ba1bca2336ab4d07530882a7ca8b78cb925", "title": "Rule Augmented Unsupervised Constituency Parsing", "abstract": "Recently, unsupervised parsing of syntactic trees has gained considerable attention. A prototypical approach to such unsupervised parsing employs reinforcement learning and auto-encoders. However, no mechanism ensures that the learnt model leverages the well-understood language grammar. We propose an approach that utilizes very generic linguistic knowledge of the language present in the form of syntactic rules, thus inducing better syntactic structures. We introduce a novel formulation that takes advantage of the syntactic grammar rules and is independent of the base system. We achieve new state-of-the-art results on two benchmarks datasets, MNLI and WSJ. The source code of the paper is available at https://github.com/anshuln/Diora_with_rules.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.436.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-21", "journal": {"pages": "4923-4932"}, "authors": [{"authorId": "46256453", "name": "Atul Sahay"}, {"authorId": "100506648", "name": "Anshul Nasery"}, {"authorId": "46202956", "name": "Ayush Maheshwari"}, {"authorId": "150114500", "name": "Ganesh Ramakrishnan"}, {"authorId": "145074006", "name": "Rishabh K. Iyer"}]}, {"paperId": "da7f2f8e7c4e68e995c8eeda324b8cfaeeb27f10", "externalIds": {"DBLP": "conf/acl/Armengol-Estape21", "ArXiv": "2107.07903", "ACL": "2021.findings-acl.437", "DOI": "10.18653/v1/2021.findings-acl.437", "CorpusId": 236034103}, "corpusId": 236034103, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/da7f2f8e7c4e68e995c8eeda324b8cfaeeb27f10", "title": "Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? A Comprehensive Assessment for Catalan", "abstract": "Multilingual language models have been a crucial breakthrough as they considerably reduce the need of data for under-resourced languages. Nevertheless, the superiority of language-specific models has already been proven for languages having access to large amounts of data. In this work, we focus on Catalan with the aim to explore to what extent a medium-sized monolingual language model is competitive with state-of-the-art large multilingual models. For this, we: (1) build a clean, high-quality textual Catalan corpus (CaText), the largest to date (but only a fraction of the usual size of the previous work in monolingual language models), (2) train a Transformer-based language model for Catalan (BERTa), and (3) devise a thorough evaluation in a diversity of settings, comprising a complete array of downstream tasks, namely, Part of Speech Tagging, Named Entity Recognition and Classification, Text Classification, Question Answering, and Semantic Textual Similarity, with most of the corresponding datasets being created ex novo. The result is a new benchmark, the Catalan Language Understanding Benchmark (CLUB), which we publish as an open resource, together with the clean textual corpus, the language model, and the cleaning pipeline. Using state-of-the-art multilingual models and a monolingual model trained only on Wikipedia as baselines, we consistently observe the superiority of our model across tasks and settings.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 28, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.437.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-16", "journal": {"pages": "4933-4946"}, "authors": [{"authorId": "1405319696", "name": "Jordi Armengol-Estap'e"}, {"authorId": "1416319999", "name": "C. Carrino"}, {"authorId": "1405597689", "name": "Carlos Rodr\u00edguez-Penagos"}, {"authorId": "73879139", "name": "Ona de Gibert Bonet"}, {"authorId": "1405518065", "name": "Carme Armentano-Oller"}, {"authorId": "1403836100", "name": "Aitor Gonzalez-Agirre"}, {"authorId": "144431961", "name": "Maite Melero"}, {"authorId": "2066499928", "name": "Marta Villegas"}]}, {"paperId": "40b3bebc595ca091b4ee654e12272ad9201c04dc", "externalIds": {"DBLP": "conf/acl/DurraniSD21", "ArXiv": "2105.15179", "ACL": "2021.findings-acl.438", "DOI": "10.18653/v1/2021.findings-acl.438", "CorpusId": 235254612}, "corpusId": 235254612, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/40b3bebc595ca091b4ee654e12272ad9201c04dc", "title": "How transfer learning impacts linguistic knowledge in deep NLP models?", "abstract": "Transfer learning from pre-trained neural language models towards downstream tasks has been a predominant theme in NLP recently. Several researchers have shown that deep NLP models learn non-trivial amount of linguistic knowledge, captured at different layers of the model. We investigate how fine-tuning towards downstream NLP tasks impacts the learned linguistic knowledge. We carry out a study across popular pre-trained models BERT, RoBERTa and XLNet using layer and neuron-level diagnostic classifiers. We found that for some GLUE tasks, the network relies on the core linguistic information and preserve it deeper in the network, while for others it forgets. Linguistic information is distributed in the pre-trained language models but becomes localized to the lower layers post fine-tuning, reserving higher layers for the task specific knowledge. The pattern varies across architectures, with BERT retaining linguistic information relatively deeper in the network compared to RoBERTa and XLNet, where it is predominantly delegated to the lower layers.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 39, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.438.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-31", "journal": {"pages": "4947-4957"}, "authors": [{"authorId": "145938140", "name": "Nadir Durrani"}, {"authorId": "145775792", "name": "Hassan Sajjad"}, {"authorId": "6415321", "name": "Fahim Dalvi"}]}, {"paperId": "2d3e4e5690b992d3099caa1606d4310a0d632868", "externalIds": {"DBLP": "conf/acl/JumeletDSHS21", "ACL": "2021.findings-acl.439", "ArXiv": "2105.13818", "DOI": "10.18653/v1/2021.findings-acl.439", "CorpusId": 235248063}, "corpusId": 235248063, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/2d3e4e5690b992d3099caa1606d4310a0d632868", "title": "Language Models Use Monotonicity to Assess NPI Licensing", "abstract": "We investigate the semantic knowledge of language models (LMs), focusing on (1) whether these LMs create categories of linguistic environments based on their semantic monotonicity properties, and (2) whether these categories play a similar role in LMs as in human language understanding, using negative polarity item licensing as a case study. We introduce a series of experiments consisting of probing with diagnostic classifiers (DCs), linguistic acceptability tasks, as well as a novel DC ranking method that tightly connects the probing results to the inner workings of the LM. By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.", "venue": "Findings", "year": 2021, "referenceCount": 49, "citationCount": 18, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.439.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-28", "journal": {"pages": "4958-4969"}, "authors": [{"authorId": "51247075", "name": "Jaap Jumelet"}, {"authorId": "2106386033", "name": "Milica Deni'c"}, {"authorId": "2325493", "name": "Jakub Szymanik"}, {"authorId": "3449411", "name": "D. Hupkes"}, {"authorId": "1403904282", "name": "Shane Steinert-Threlkeld"}]}, {"paperId": "47bd2e1e3a56a2226649e7b912f5bcccdc978087", "externalIds": {"DBLP": "conf/acl/LuHYWLJW21", "ACL": "2021.findings-acl.440", "DOI": "10.18653/v1/2021.findings-acl.440", "CorpusId": 236478071}, "corpusId": 236478071, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/47bd2e1e3a56a2226649e7b912f5bcccdc978087", "title": "Slot Transferability for Cross-domain Slot Filling", "abstract": "Cross-domain slot \ufb01lling focuses on using labeled data from source domains to train a slot \ufb01lling model for target domains. It is of great signi\ufb01cance for transferring a dialogue system into new domains. Most of the existing work focused on building a cross-domain transfer model. From the perspective of slots them-selves, this paper proposes a model-agnostic Slot Transferability Measure (STM) for evalu-ating the transferability from a source slot to a target slot, speci\ufb01cally, the degree that labeled data of the source slot is helpful to train the slot \ufb01lling model for the target slot. We also give a STM-based method for a model to select helpful source slots and their labeled data for a given target slot. Experimental results on multiple existing models and datasets show that our method signi\ufb01cantly outperforms state-of-the-art baselines in cross-domain slot \ufb01lling. The code is available at https://github. com/luhengtong/STM-for-cdsf.git .", "venue": "Findings", "year": 2021, "referenceCount": 27, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.440.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4970-4979"}, "authors": [{"authorId": "150124800", "name": "Hengtong Lu"}, {"authorId": "2153092440", "name": "Zhuoxin Han"}, {"authorId": "2006373", "name": "Caixia Yuan"}, {"authorId": "2115450950", "name": "Xiaojie Wang"}, {"authorId": "2112834086", "name": "Shuyu Lei"}, {"authorId": "2309680", "name": "Huixing Jiang"}, {"authorId": "2118974140", "name": "Wei Wu"}]}, {"paperId": "008721e4f9cb9b2d3242bc31af48db6fb3f8727d", "externalIds": {"DBLP": "conf/acl/HuLCSSWC21", "ACL": "2021.findings-acl.441", "ArXiv": "2112.09925", "DOI": "10.18653/v1/2021.findings-acl.441", "CorpusId": 236477655}, "corpusId": 236477655, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/008721e4f9cb9b2d3242bc31af48db6fb3f8727d", "title": "Word Graph Guided Summarization for Radiology Findings", "abstract": "Radiology reports play a critical role in communicating medical findings to physicians. In each report, the impression section summarizes essential radiology findings. In clinical practice, writing impression is highly demanded yet time-consuming and prone to errors for radiologists. Therefore, automatic impression generation has emerged as an attractive research direction to facilitate such clinical practice. Existing studies mainly focused on introducing salient word information to the general text summarization framework to guide the selection of the key content in radiology findings. However, for this task, a model needs not only capture the important words in findings but also accurately describe their relations so as to generate high-quality impressions. In this paper, we propose a novel method for automatic impression generation, where a word graph is constructed from the findings to record the critical words and their relations, then a Word Graph guided Summarization model (WGSum) is designed to generate impressions with the help of the word graph. Experimental results on two datasets, OpenI and MIMIC-CXR, confirm the validity and effectiveness of our proposed approach, where the state-of-the-art results are achieved on both datasets. Further experiments are also conducted to analyze the impact of different graph designs to the performance of our method.", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 20, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.441.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-12-18", "journal": {"name": "ArXiv", "volume": "abs/2112.09925"}, "authors": [{"authorId": "49268110", "name": "Jinpeng Hu"}, {"authorId": "2144522886", "name": "Jianling Li"}, {"authorId": "46843171", "name": "Zhihong Chen"}, {"authorId": "2115437031", "name": "Yaling Shen"}, {"authorId": "1922182598", "name": "Yan Song"}, {"authorId": "2005096276", "name": "Xiang Wan"}, {"authorId": "2678812", "name": "Tsung-Hui Chang"}]}, {"paperId": "44e34bddc5f0bf794b40271f958159970ff973ef", "externalIds": {"DBLP": "conf/acl/LiuZZJZT21", "ACL": "2021.findings-acl.442", "DOI": "10.18653/v1/2021.findings-acl.442", "CorpusId": 236478264}, "corpusId": 236478264, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/44e34bddc5f0bf794b40271f958159970ff973ef", "title": "Generalized Supervised Attention for Text Generation", "abstract": "The attention-based encoder-decoder framework is widely used in many natural language generation tasks. The attention mechanism builds alignments between target words and source items that facilitate text generation. Previous work proposes supervised attention that uses human knowledge to guide the attention mechanism to learn better alignments. However, well-designed supervision built from ideal alignments can be costly or even infeasible. In this paper, we build a Generalized Supervised Attention method (GSA) based on quasi alignments, which specify candidate sets of alignments and are much easier to obtain than ideal alignments. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision.", "venue": "Findings", "year": 2021, "referenceCount": 35, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "4991-5003"}, "authors": [{"authorId": "35283208", "name": "Yixian Liu"}, {"authorId": "2107964289", "name": "Liwen Zhang"}, {"authorId": "2155543565", "name": "Xinyu Zhang"}, {"authorId": "50262192", "name": "Yong Jiang"}, {"authorId": "1591125925", "name": "Yue Zhang"}, {"authorId": "40341553", "name": "Kewei Tu"}]}, {"paperId": "c69a5ebbca886a0fa288ec9c60b9e31ee21edbd9", "externalIds": {"ACL": "2021.findings-acl.443", "DBLP": "conf/acl/FriedlRSHSHS21", "DOI": "10.18653/v1/2021.findings-acl.443", "CorpusId": 236477402}, "corpusId": 236477402, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/c69a5ebbca886a0fa288ec9c60b9e31ee21edbd9", "title": "Uncertainty Aware Review Hallucination for Science Article Classification", "abstract": "The high subjectivity and costs inherent in peer reviewing have recently motivated the preliminary design of machine learning-based acceptance decision methods. However, such approaches are limited in that they: a) do not explore the usage of both the reviewer and area chair recommendations, b) do not explicitly model subjectivity on a per submission basis, and c) are not applicable in realistic settings, by assuming that review texts are available at test time, when these are exactly the inputs that should be considered to be missing in this application. We propose to utilise methods that model the aleatory uncertainty of the submissions, while also exploring different loss importance interpolations between area chair and reviewers\u2019 recommendations. We also propose a modality hallucination approach to impute review representations at test time, providing the first realistic evaluation framework for this challenging task.", "venue": "Findings", "year": 2021, "referenceCount": 24, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.443.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "journal": {"pages": "5004-5009"}, "authors": [{"authorId": "2121343100", "name": "Korbinian Friedl"}, {"authorId": "40185455", "name": "Georgios Rizos"}, {"authorId": "113705775", "name": "Lukas Stappen"}, {"authorId": "33143239", "name": "Madina Hasan"}, {"authorId": "2065700826", "name": "Lucia Specia"}, {"authorId": "2067068771", "name": "T. Hain"}, {"authorId": "2090950337", "name": "B. Schuller"}]}, {"paperId": "34f9d7ffc2e76038c4af316c4776a2dbbeec564c", "externalIds": {"DBLP": "conf/acl/WenCYLS21", "ArXiv": "2106.15846", "ACL": "2021.findings-acl.444", "DOI": "10.18653/v1/2021.findings-acl.444", "CorpusId": 235683159}, "corpusId": 235683159, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/34f9d7ffc2e76038c4af316c4776a2dbbeec564c", "title": "Automatically Select Emotion for Response via Personality-affected Emotion Transition", "abstract": "To provide consistent emotional interaction with users, dialog systems should be capable to automatically select appropriate emotions for responses like humans. However, most existing works focus on rendering specified emotions in responses or empathetically respond to the emotion of users, yet the individual difference in emotion expression is overlooked. This may lead to inconsistent emotional expressions and disinterest users. To tackle this issue, we propose to equip the dialog system with personality and enable it to automatically select emotions in responses by simulating the emotion transition of humans in conversation. In detail, the emotion of the dialog system is transitioned from its preceding emotion in context. The transition is triggered by the preceding dialog context and affected by the specified personality trait. To achieve this, we first model the emotion transition in the dialog system as the variation between the preceding emotion and the response emotion in the Valence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks to encode the preceding dialog context and the specified personality traits to compose the variation. Finally, the emotion for response is selected from the sum of the preceding emotion and the variation. We construct a dialog dataset with emotion and personality labels and conduct emotion prediction tasks for evaluation. Experimental results validate the effectiveness of the personality-affected emotion transition.", "venue": "Findings", "year": 2021, "referenceCount": 59, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.444.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-30", "journal": {"name": "ArXiv", "volume": "abs/2106.15846"}, "authors": [{"authorId": "2000188918", "name": "Zhiyuan Wen"}, {"authorId": "2115870728", "name": "Jiannong Cao"}, {"authorId": "8092850", "name": "Ruosong Yang"}, {"authorId": "3344531", "name": "Shuaiqi Liu"}, {"authorId": "144980022", "name": "Jiaxing Shen"}]}, {"paperId": "e885c810fbe23bd730eb80631c488556c9fd1d85", "externalIds": {"DBLP": "conf/acl/LiuCYW21", "ACL": "2021.findings-acl.445", "DOI": "10.18653/v1/2021.findings-acl.445", "CorpusId": 236477957}, "corpusId": 236477957, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/e885c810fbe23bd730eb80631c488556c9fd1d85", "title": "Highlight-Transformer: Leveraging Key Phrase Aware Attention to Improve Abstractive Multi-Document Summarization", "abstract": "Abstractive multi-document summarization aims to generate a comprehensive summary covering salient content from multiple input documents. Compared with previous RNN-based models, the Transformer-based models employ the self-attention mechanism to capture the dependencies in input documents and can generate better summaries. Existing works have not considered key phrases in determin-ing attention weights of self-attention. Conse-quently, some of the tokens within key phrases only receive small attention weights. It can affect completely encoding key phrases that convey the salient ideas of input documents. In this paper, we introduce the Highlight-Transformer, a model with the highlighting mechanism in the encoder to assign greater attention weights for the tokens within key phrases. We propose two structures of highlighting attention for each head and the multi-head highlighting attention. The experimental results on the Multi-News dataset show that our proposed model signi\ufb01cantly outperforms the competitive baseline models.", "venue": "Findings", "year": 2021, "referenceCount": 33, "citationCount": 6, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "5021-5027"}, "authors": [{"authorId": "3344531", "name": "Shuaiqi Liu"}, {"authorId": "144115026", "name": "Jiannong Cao"}, {"authorId": "8092850", "name": "Ruosong Yang"}, {"authorId": "2000188918", "name": "Zhiyuan Wen"}]}, {"paperId": "d0babb55df48d544f4765a29fba7fd6c854d03ba", "externalIds": {"DBLP": "conf/acl/YamazakiA21", "ACL": "2021.findings-acl.446", "DOI": "10.18653/v1/2021.findings-acl.446", "CorpusId": 236477384}, "corpusId": 236477384, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/d0babb55df48d544f4765a29fba7fd6c854d03ba", "title": "Phrase-Level Action Reinforcement Learning for Neural Dialog Response Generation", "abstract": "Defining a sophisticated action space for a dialog agent is essential for efficient training with reinforcement learning (RL). Recent work introduces discrete latent variables to use as an action space; however, a limitation is that a global vector can contain entangled information such as dialog act, sentence structure, and content. This sacrifices the flexibility of the response generation. In this paper, we propose phrase-level action reinforcement learning (PHRASERL), which allows the model to flexibly alter the sentence structure and content with the sequential action selection. Our model first learns to generate useful phrases during the supervised pre-training, and then further trained to form a response by rearranging the phrases with reinforcement learning. Experiments on the MultiWOZ dataset show that our model achieves competitive results with state-of-the-art models on automatic evaluation metrics, indicating that our phrase-level action space has improved flexibility and is effective for solving task-oriented dialogs.", "venue": "Findings", "year": 2021, "referenceCount": 28, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "5028-5038"}, "authors": [{"authorId": "2067035285", "name": "T. Yamazaki"}, {"authorId": "1705519", "name": "Akiko Aizawa"}]}, {"paperId": "5fd1269f7258d33c06022553c1c983c9e9218ab4", "externalIds": {"DBLP": "journals/corr/abs-2106-05852", "ACL": "2021.findings-acl.447", "ArXiv": "2106.05852", "DOI": "10.18653/v1/2021.findings-acl.447", "CorpusId": 235390653}, "corpusId": 235390653, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/5fd1269f7258d33c06022553c1c983c9e9218ab4", "title": "Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights", "abstract": "Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the various linguistic peculiarities present in the language. The Sanskrit language is lexically productive, undergoes euphonic assimilation of phones at the word boundaries and exhibits variations in spelling conventions and in pronunciations. In this work, we propose the first large scale study of automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR dataset for Sanskrit, which faithfully captures several of the linguistic characteristics expressed by the language. We investigate the role of different acoustic model and language model units in ASR systems for Sanskrit. We also propose a newmodelling unit, inspired by the syllable level unit selection, that captures character sequences from one vowel in the word to the next vowel. We also highlight the importance of choosing graphemic representations for Sanskrit and show the impact of this choice on word error rates (WER). Finally, we extend these insights from Sanskrit ASR for building ASR systems in two other Indic languages, Gujarati and Telugu. For both these languages, our experimental results show that the use of phonetic based graphemic representations in ASR results in performance improvements as compared to ASR systems that use native scripts.1", "venue": "Findings", "year": 2021, "referenceCount": 34, "citationCount": 19, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.447.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"pages": "5039-5050"}, "authors": [{"authorId": "35487849", "name": "D. Adiga"}, {"authorId": "2107940118", "name": "Rishabh Kumar"}, {"authorId": "144612532", "name": "A. Krishna"}, {"authorId": "144859542", "name": "P. Jyothi"}, {"authorId": "145799547", "name": "Ganesh Ramakrishnan"}, {"authorId": "51130504", "name": "Pawan Goyal"}]}, {"paperId": "0201812947eff2f6ff08ccd33fcab781738e0305", "externalIds": {"DBLP": "journals/corr/abs-2109-07396", "ArXiv": "2109.07396", "ACL": "2021.findings-acl.448", "DOI": "10.18653/v1/2021.findings-acl.448", "CorpusId": 236477461}, "corpusId": 236477461, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0201812947eff2f6ff08ccd33fcab781738e0305", "title": "Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs", "abstract": "End-to-End task-oriented dialogue systems generate responses based on dialog history and an accompanying knowledge base (KB). Inferring those KB entities that are most relevant for an utterance is crucial for response generation. Existing state of the art scales to large KBs by softly filtering over irrelevant KB information. In this paper, we propose a novel filtering technique that consists of (1) a pairwise similarity based filter that identifies relevant information by respecting the n-ary structure in a KB record. and, (2) an auxiliary loss that helps in separating contextually unrelated KB information. We also propose a new metric -- multiset entity F1 which fixes a correctness issue in the existing entity F1 metric. Experimental results on three publicly available task-oriented dialog datasets show that our proposed approach outperforms existing state-of-the-art models.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 11, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.448.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-15", "journal": {"name": "ArXiv", "volume": "abs/2109.07396"}, "authors": [{"authorId": "1916865", "name": "Dinesh Raghu"}, {"authorId": "2110518661", "name": "Atishya Jain"}, {"authorId": "2674444", "name": "Mausam"}, {"authorId": "1703799", "name": "Sachindra Joshi"}]}, {"paperId": "0934d7cac5a86b02fc49852334051bde540b34bd", "externalIds": {"ArXiv": "2105.06762", "DBLP": "conf/acl/ChenLCZ21", "ACL": "2021.findings-acl.449", "DOI": "10.18653/v1/2021.findings-acl.449", "CorpusId": 234681504}, "corpusId": 234681504, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0934d7cac5a86b02fc49852334051bde540b34bd", "title": "DialogSum: A Real-Life Scenario Dialogue Summarization Dataset", "abstract": "Proposal of large-scale datasets has facilitated research on deep neural models for news summarization. Deep learning can also be potentially useful for spoken dialogue summarization, which can benefit a range of real-life scenarios including customer service management and medication tracking. To this end, we propose DialogSum, a large-scale labeled dialogue summarization dataset. We conduct empirical analysis on DialogSum using state-of-the-art neural summarizers. Experimental results show unique challenges in dialogue summarization, such as spoken terms, special discourse structures, coreferences and ellipsis, pragmatics and social common sense, which require specific representation learning technologies to better deal with.", "venue": "Findings", "year": 2021, "referenceCount": 37, "citationCount": 107, "influentialCitationCount": 34, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.449.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "5062-5074"}, "authors": [{"authorId": "2109404730", "name": "Yulong Chen"}, {"authorId": "39798499", "name": "Yang Liu"}, {"authorId": "2146034504", "name": "Liang Chen"}, {"authorId": "1591125925", "name": "Yue Zhang"}]}, {"paperId": "44b9391d660b319e9814ece1b3d9ac6f3a98e5f1", "externalIds": {"ACL": "2021.findings-acl.450", "DBLP": "conf/acl/ZhangZTZZL21", "DOI": "10.18653/v1/2021.findings-acl.450", "CorpusId": 236477368}, "corpusId": 236477368, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/44b9391d660b319e9814ece1b3d9ac6f3a98e5f1", "title": "What Did You Refer to? Evaluating Co-References in Dialogue", "abstract": "Existing neural end-to-end dialogue models have limitations on exactly interpreting the linguistic structures, such as ellipsis, anaphor and co-reference, etc., in dialogue history context. Therefore, it is hard to determine whether the dialogue models truly understand a dialogue or not, only depending on the coherence evaluation of their generated responses. To address these issues, in this paper, we proposed to directly measure the capability of dialogue models on understanding the entity-oriented structures via question answering and construct a new benchmark dataset, DEQA, including large-scale English and Chinese humanhuman dialogues. Experiments carried on representative dialogue models show that these models all face challenges on the proposed dialogue understanding task. The DEQA dataset will release for research use.", "venue": "Findings", "year": 2021, "referenceCount": 39, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.450.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "5075-5084"}, "authors": [{"authorId": "1806419", "name": "Weinan Zhang"}, {"authorId": "1591125925", "name": "Yue Zhang"}, {"authorId": "46666025", "name": "Hanlin Tang"}, {"authorId": "2152619962", "name": "Zhengyu Zhao"}, {"authorId": "2048147621", "name": "Caihai Zhu"}, {"authorId": "40282288", "name": "Ting Liu"}]}, {"paperId": "8f45b9abb6444495f10bd7123ee706aee0f4526d", "externalIds": {"ACL": "2021.findings-acl.451", "DBLP": "conf/acl/KolyadaPS21", "DOI": "10.18653/v1/2021.findings-acl.451", "CorpusId": 236477324}, "corpusId": 236477324, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/8f45b9abb6444495f10bd7123ee706aee0f4526d", "title": "Beyond Metadata: What Paper Authors Say About Corpora They Use", "abstract": "The growing ecosystem of data sharing in science has put dataset search into the focus. To make data sharing and reuse more feasible, new retrieval tools and services are being de-veloped. Currently, dataset retrieval relies al-most exclusively on metadata provided by the publishers. To extend this knowledge source our work studies the task of \u201cdataset review mining\u201d in scienti\ufb01c publications. For the \ufb01eld of Natural Language Processing we collect metadata about datasets from established resources such as the ELRA and LDC catalogs, and then extract review statements about the datasets from ACL Anthology Corpus publications, compiling the Webis-Dataset-Reviews-21 corpus. By analyzing the reviews we identify different categories of what paper authors write about data. To the best of our knowledge, this is the \ufb01rst analysis of this kind in the \ufb01eld of Natural Language Processing, albeit similar analyses have been carried out in the social and medical sciences. Our corpus and the underly-ing code are shared alongside this paper. 1,2,3", "venue": "Findings", "year": 2021, "referenceCount": 14, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "journal": {"pages": "5085-5090"}, "authors": [{"authorId": "50244788", "name": "Nikolay Kolyada"}, {"authorId": "2058609780", "name": "Martin Potthast"}, {"authorId": "1405867539", "name": "Benno Stein"}]}, {"paperId": "0f4197cb978525b300a764e7e62000c89ef0a23a", "externalIds": {"DBLP": "conf/acl/GajbhiyeFABOAS21", "ACL": "2021.findings-acl.452", "ArXiv": "2107.00411", "DOI": "10.18653/v1/2021.findings-acl.452", "CorpusId": 235694593}, "corpusId": 235694593, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0f4197cb978525b300a764e7e62000c89ef0a23a", "title": "Knowledge Distillation for Quality Estimation", "abstract": "Quality Estimation (QE) is the task of automatically predicting Machine Translation quality in the absence of reference translations, making it applicable in real-time settings, such as translating online social media conversations. Recent success in QE stems from the use of multilingual pre-trained representations, where very large models lead to impressive results. However, the inference time, disk and memory requirements of such models do not allow for wide usage in the real world. Models trained on distilled pre-trained representations remain prohibitively large for many usage scenarios. We instead propose to directly transfer knowledge from a strong QE teacher model to a much smaller model with a different, shallower architecture. We show that this approach, in combination with data augmentation, leads to light-weight QE models that perform competitively with distilled pre-trained representations with 8x fewer parameters.", "venue": "Findings", "year": 2021, "referenceCount": 26, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.452.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-01", "journal": {"name": "ArXiv", "volume": "abs/2107.00411"}, "authors": [{"authorId": "25703683", "name": "Amit Gajbhiye"}, {"authorId": "2006017", "name": "M. Fomicheva"}, {"authorId": "69930782", "name": "Fernando Alva-Manchego"}, {"authorId": "1948646", "name": "F. Blain"}, {"authorId": "22313325", "name": "A. Obamuyide"}, {"authorId": "3238627", "name": "Nikolaos Aletras"}, {"authorId": "2065700826", "name": "Lucia Specia"}]}, {"paperId": "3e009dabee818e761d580c94d4b960aa8c763bd9", "externalIds": {"DBLP": "journals/corr/abs-2106-01210", "ACL": "2021.findings-acl.453", "ArXiv": "2106.01210", "DOI": "10.18653/v1/2021.findings-acl.453", "CorpusId": 235294128}, "corpusId": 235294128, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/3e009dabee818e761d580c94d4b960aa8c763bd9", "title": "Cross-document Coreference Resolution over Predicted Mentions", "abstract": "Coreference resolution has been mostly investigated within a single document scope, showing impressive progress in recent years based on end-to-end models. However, the more challenging task of cross-document (CD) coreference resolution remained relatively under-explored, with the few recent models applied only to gold mentions. Here, we introduce the first end-to-end model for CD coreference resolution from raw text, which extends the prominent model for within-document coreference to the CD setting. Our model achieves competitive results for event and entity coreference resolution on gold mentions. More importantly, we set first baseline results, on the standard ECB+ dataset, for CD coreference resolution over predicted mentions. Further, our model is simpler and more efficient than recent CD coreference resolution systems, while not using any external resources.", "venue": "Findings", "year": 2021, "referenceCount": 28, "citationCount": 19, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.453.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-02", "journal": {"name": "ArXiv", "volume": "abs/2106.01210"}, "authors": [{"authorId": "1962331387", "name": "Arie Cattan"}, {"authorId": "51150806", "name": "Alon Eirew"}, {"authorId": "2157025", "name": "Gabriel Stanovsky"}, {"authorId": "144863691", "name": "Mandar Joshi"}, {"authorId": "7465342", "name": "Ido Dagan"}]}, {"paperId": "95085501ff72c296b2df3f12969e0f8a57c224fd", "externalIds": {"DBLP": "journals/corr/abs-2105-14064", "ArXiv": "2105.14064", "ACL": "2021.findings-acl.454", "DOI": "10.18653/v1/2021.findings-acl.454", "CorpusId": 235253875}, "corpusId": 235253875, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/95085501ff72c296b2df3f12969e0f8a57c224fd", "title": "Controllable Abstractive Dialogue Summarization with Sketch Supervision", "abstract": "In this paper, we aim to improve abstractive dialogue summarization quality and, at the same time, enable granularity control. Our model has two primary components and stages: 1) a two-stage generation strategy that generates a preliminary summary sketch serving as the basis for the final summary. This summary sketch provides a weakly supervised signal in the form of pseudo-labeled interrogative pronoun categories and key phrases extracted using a constituency parser. 2) A simple strategy to control the granularity of the final summary, in that our model can automatically determine or control the number of generated summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum, with as high as 50.79 in ROUGE-L score. In addition, we conduct a case study and show competitive human evaluation results and controllability to human-annotated summaries.", "venue": "Findings", "year": 2021, "referenceCount": 64, "citationCount": 31, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.454.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-28", "journal": {"name": "ArXiv", "volume": "abs/2105.14064"}, "authors": [{"authorId": "120930931", "name": "C. Wu"}, {"authorId": "2111591", "name": "Linqing Liu"}, {"authorId": "120639804", "name": "Wenhao Liu"}, {"authorId": "1918552", "name": "Pontus Stenetorp"}, {"authorId": "2054594326", "name": "Caiming Xiong"}]}, {"paperId": "0c7ff554c98435b398c0f8687c8bd6e7dbbfbf0d", "externalIds": {"DBLP": "journals/corr/abs-2010-10035", "ACL": "2021.findings-acl.455", "MAG": "3093832563", "ArXiv": "2010.10035", "DOI": "10.18653/v1/2021.findings-acl.455", "CorpusId": 224803326}, "corpusId": 224803326, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/0c7ff554c98435b398c0f8687c8bd6e7dbbfbf0d", "title": "Elaborative Simplification: Content Addition and Explanation Generation in Text Simplification", "abstract": "Much of modern day text simplification research focuses on sentence-level simplification, transforming original, more complex sentences to simplified versions. However, adding content can often be useful when difficult concepts and reasoning need to be explained. In this work, we present the first data-driven study of content addition in document simplification, which we call elaborative simplification. We introduce a new annotated dataset of 1.3K instances of elaborative simplification and analyze how entities, ideas, and concepts are elaborated through the lens of contextual specificity. We establish baselines for elaboration generation using large scale pre-trained language models, and illustrate that considering contextual specificity during generation can improve performance. Our results illustrate the complexities of elaborative simplification, suggesting many interesting directions for future work.", "venue": "Findings", "year": 2020, "referenceCount": 60, "citationCount": 20, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.455.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-10-20", "journal": {"pages": "5123-5137"}, "authors": [{"authorId": "2065955216", "name": "Neha Srikanth"}, {"authorId": "22319255", "name": "Junyi Jessy Li"}]}, {"paperId": "b9057dce43181a30aa3e0435c8ffc4c0b6f8f127", "externalIds": {"DBLP": "journals/corr/abs-2105-05418", "ArXiv": "2105.05418", "ACL": "2021.findings-acl.456", "DOI": "10.18653/v1/2021.findings-acl.456", "CorpusId": 234470030}, "corpusId": 234470030, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/b9057dce43181a30aa3e0435c8ffc4c0b6f8f127", "title": "Could you give me a hint ? Generating inference graphs for defeasible reasoning", "abstract": "Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. A commonly used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference graphs through transfer learning from another NLP task that shares the kind of reasoning that inference graphs support. Through automated metrics and human evaluation, we find that our method generates meaningful graphs for the defeasible inference task. Human accuracy on this task improves by 20% by consulting the generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning. (A dataset of 230,000 influence graphs for each defeasible query is located at: https://tinyurl.com/defeasiblegraphs.)", "venue": "Findings", "year": 2021, "referenceCount": 20, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.456.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-12", "journal": {"pages": "5138-5147"}, "authors": [{"authorId": "21626987", "name": "Aman Madaan"}, {"authorId": "1801149", "name": "Dheeraj Rajagopal"}, {"authorId": "1721168", "name": "Niket Tandon"}, {"authorId": "46286308", "name": "Yiming Yang"}, {"authorId": "144547315", "name": "E. Hovy"}]}, {"paperId": "59583454cef87dfee40ddb4db1ae67b277a5bacb", "externalIds": {"DBLP": "conf/acl/GiorgiUS21", "ACL": "2021.findings-acl.457", "DOI": "10.18653/v1/2021.findings-acl.457", "CorpusId": 236477504}, "corpusId": 236477504, "publicationVenue": {"id": "479d5605-51be-4346-b1d6-4334084504df", "name": "Findings", "type": "journal", "issn": "2652-8800", "url": "https://findingspress.org/"}, "url": "https://www.semanticscholar.org/paper/59583454cef87dfee40ddb4db1ae67b277a5bacb", "title": "Characterizing Social Spambots by their Human Traits", "abstract": "Social spambots, an emerging class of spam-mers attempting to emulate people, are dif-\ufb01cult for both human annotators and classic bot detection techniques to reliably distinguish from genuine accounts. We examine this human emulation through studying the human characteristics (personality, gender, age, emotions) exhibited by social spambots\u2019 language, hypothesizing the values for these attributes will be unhuman-like (e.g. unusually high or low). We found our hypothesis mostly discon-\ufb01rmed \u2014 individually, social bots exhibit very human-like attributes. However, a striking pattern emerged when consider the full distributions of these estimated human attributes: social bots were extremely similar and average in their expressed personality, demographics, and emotion (in contrast with traditional bots which we found to exhibit more variance and extreme values than genuine accounts). We thus consider how well social bots can be iden-ti\ufb01ed only using the 17 variables of these human attributes and ended up with a new state of the art in social spambot detection (e.g. F 1 = . 946 ). Further, simulating the situation of not knowing the bots a priori , we found that even an unsupervised clustering using the same 17 attributes could yield nearly as accurate of social bot identi\ufb01cation ( F 1 = 0 . 925 ).", "venue": "Findings", "year": 2021, "referenceCount": 54, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-acl.457.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "journal": {"pages": "5148-5158"}, "authors": [{"authorId": "50360470", "name": "Salvatore Giorgi"}, {"authorId": "1717822", "name": "Lyle Ungar"}, {"authorId": "145035129", "name": "H. A. Schwartz"}]}, {"paperId": "39369c2e5d6d6b403c444e72948ccf0dfdf59489", "externalIds": {"ACL": "2021.acl-demo.1", "DBLP": "conf/acl/LiuZJLZXSZZZFCY21", "MAG": "3173505468", "DOI": "10.18653/v1/2021.acl-demo.1", "CorpusId": 237561394}, "corpusId": 237561394, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/39369c2e5d6d6b403c444e72948ccf0dfdf59489", "title": "TexSmart: A System for Enhanced Natural Language Understanding", "abstract": "This paper introduces TexSmart, a text understanding system that supports fine-grained named entity recognition (NER) and enhanced semantic analysis functionalities. Compared to most previous publicly available text understanding systems and tools, TexSmart holds some unique features. First, the NER function of TexSmart supports over 1,000 entity types, while most other public tools typically support several to (at most) dozens of entity types. Second, TexSmart introduces new semantic analysis functions like semantic expansion and deep semantic representation, that are absent in most previous systems. Third, a spectrum of algorithms (from very fast algorithms to those that are relatively slow but more accurate) are implemented for one function in TexSmart, to fulfill the requirements of different academic and industrial applications. The adoption of unsupervised or weakly-supervised algorithms is especially emphasized, with the goal of easily updating our models to include fresh data with less human annotation efforts.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 50, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.1.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "1-10"}, "authors": [{"authorId": "2978364", "name": "Lemao Liu"}, {"authorId": "2108558798", "name": "Haisong Zhang"}, {"authorId": "48579460", "name": "Haiyun Jiang"}, {"authorId": "48514481", "name": "Yangming Li"}, {"authorId": "2065703096", "name": "Enbo Zhao"}, {"authorId": "2113451979", "name": "Kun Xu"}, {"authorId": "50258954", "name": "Linfeng Song"}, {"authorId": "37423160", "name": "Suncong Zheng"}, {"authorId": "94584519", "name": "Botong Zhou"}, {"authorId": "2148404387", "name": "Dick Zhu"}, {"authorId": "2149114863", "name": "Xiao Feng"}, {"authorId": "2118213045", "name": "Tao Chen"}, {"authorId": "2122901214", "name": "Tao Yang"}, {"authorId": "2111505433", "name": "Dong Yu"}, {"authorId": "1884418505", "name": "Feng Zhang"}, {"authorId": "2705857", "name": "Zhanhui Kang"}, {"authorId": "2072684668", "name": "Shuming Shi"}]}, {"paperId": "e94ae0f0640d3ca3e00c359edb5684bf3f54121a", "externalIds": {"DBLP": "journals/corr/abs-2105-12172", "ArXiv": "2105.12172", "ACL": "2021.acl-demo.2", "DOI": "10.18653/v1/2021.acl-demo.2", "CorpusId": 235195614}, "corpusId": 235195614, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e94ae0f0640d3ca3e00c359edb5684bf3f54121a", "title": "IntelliCAT: Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion", "abstract": "We present IntelliCAT, an interactive translation interface with neural models that streamline the post-editing process on machine translation output. We leverage two quality estimation (QE) models at different granularities: sentence-level QE, to predict the quality of each machine-translated sentence, and word-level QE, to locate the parts of the machine-translated sentence that need correction. Additionally, we introduce a novel translation suggestion model conditioned on both the left and right contexts, providing alternatives for specific words or phrases for correction. Finally, with word alignments, IntelliCAT automatically preserves the original document\u2019s styles in the translated document. The experimental results show that post-editing based on the proposed QE and translation suggestions can significantly improve translation quality. Furthermore, a user study reveals that three features provided in IntelliCAT significantly accelerate the post-editing task, achieving a 52.9% speedup in translation time compared to translating from scratch. The interface is publicly available at https://intellicat.beringlab.com/.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.2.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-25", "journal": {"pages": "11-19"}, "authors": [{"authorId": "2109519725", "name": "Dongjun Lee"}, {"authorId": "2111005006", "name": "Junhyeong Ahn"}, {"authorId": "2117003387", "name": "Heesoo Park"}, {"authorId": "1939362", "name": "Jaemin Jo"}]}, {"paperId": "4bec66ec7b21cc8ef2f7dd4cfc79d0fa96e5d417", "externalIds": {"MAG": "3174520822", "ACL": "2021.acl-demo.3", "DBLP": "conf/acl/JohnsonBSCBM21", "DOI": "10.18653/v1/2021.acl-demo.3", "CorpusId": 237732031}, "corpusId": 237732031, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4bec66ec7b21cc8ef2f7dd4cfc79d0fa96e5d417", "title": "The Classical Language Toolkit: An NLP Framework for Pre-Modern Languages", "abstract": "This paper announces version 1.0 of the Classical Language Toolkit (CLTK), an NLP framework for pre-modern languages. The vast majority of NLP, its algorithms and software, is created with assumptions particular to living languages, thus neglecting certain important characteristics of largely non-spoken historical languages. Further, scholars of pre-modern languages often have different goals than those of living-language researchers. To fill this void, the CLTK adapts ideas from several leading NLP frameworks to create a novel software architecture that satisfies the unique needs of pre-modern languages and their researchers. Its centerpiece is a modular processing pipeline that balances the competing demands of algorithmic diversity with pre-configured defaults. The CLTK currently provides pipelines, including models, for almost 20 languages.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 55, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "20-29"}, "authors": [{"authorId": "48296935", "name": "Kyle P. Johnson"}, {"authorId": "144770367", "name": "P. Burns"}, {"authorId": "2117149584", "name": "John Stewart"}, {"authorId": "2128344980", "name": "Todd Cook"}, {"authorId": "2029670166", "name": "Cl\u00e9ment Besnier"}, {"authorId": "2020374350", "name": "William Mattingly"}]}, {"paperId": "7d1cd398a9a035ccc2cba2cf79ea79943a758889", "externalIds": {"ACL": "2021.acl-demo.4", "DBLP": "conf/acl/LiTHJHXCYZW21", "ArXiv": "2101.02046", "DOI": "10.18653/v1/2021.acl-demo.4", "CorpusId": 230770247}, "corpusId": 230770247, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7d1cd398a9a035ccc2cba2cf79ea79943a758889", "title": "TextBox: A Unified, Modularized, and Extensible Framework for Text Generation", "abstract": "In this paper, we release an open-source library, called TextBox, to provide a unified, modularized, and extensible text generation framework. TextBox aims to support a broad set of text generation tasks and models. In our library, we implement 21 text generation models on 9 benchmark datasets, covering the categories of VAE, GAN, and pretrained language models. Meanwhile, our library maintains sufficient modularity and extensibility by properly decomposing the model architecture, inference, and learning process into highly reusable modules, which allows users to easily incorporate new models into our framework. The above features make TextBox especially suitable for researchers and practitioners to quickly reproduce baseline models and develop new models. TextBox is implemented based on PyTorch, and released under Apache License 2.0 at the link https://github.com/RUCAIBox/TextBox.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 54, "citationCount": 20, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.4.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-06", "journal": {"name": "ArXiv", "volume": "abs/2101.02046"}, "authors": [{"authorId": "2138220600", "name": "Junyi Li"}, {"authorId": "1997234792", "name": "Tianyi Tang"}, {"authorId": "51149404", "name": "Gaole He"}, {"authorId": "2118240359", "name": "Jinhao Jiang"}, {"authorId": "2109561982", "name": "Xiaoxuan Hu"}, {"authorId": "2045176461", "name": "Puzhao Xie"}, {"authorId": "2542603", "name": "Wayne Xin Zhao"}, {"authorId": "153693432", "name": "Ji-rong Wen"}]}, {"paperId": "489ffd70cb2afd550ab809bc90f5a766eb07aa80", "externalIds": {"DBLP": "journals/corr/abs-2105-13662", "ArXiv": "2105.13662", "ACL": "2021.acl-demo.5", "DOI": "10.18653/v1/2021.acl-demo.5", "CorpusId": 235247848}, "corpusId": 235247848, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/489ffd70cb2afd550ab809bc90f5a766eb07aa80", "title": "Inside ASCENT: Exploring a Deep Commonsense Knowledge Base and its Usage in Question Answering", "abstract": "ASCENT is a fully automated methodology for extracting and consolidating commonsense assertions from web contents (Nguyen et al., 2021). It advances traditional triple-based commonsense knowledge representation by capturing semantic facets like locations and purposes, and composite concepts, i.e., subgroups and related aspects of subjects. In this demo, we present a web portal that allows users to understand its construction process, explore its content, and observe its impact in the use case of question answering. The demo website (https://ascent.mpi-inf.mpg.de) and an introductory video (https://youtu.be/qMkJXqu_Yd4) are both available online.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 31, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.5.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-28", "journal": {"name": "ArXiv", "volume": "abs/2105.13662"}, "authors": [{"authorId": "8615485", "name": "Tuan-Phong Nguyen"}, {"authorId": "2499758", "name": "Simon Razniewski"}, {"authorId": "1751591", "name": "G. Weikum"}]}, {"paperId": "ca608192fc9d0c013915248c867656916b1059e8", "externalIds": {"DBLP": "conf/acl/ShenWMCW21", "ACL": "2021.acl-demo.6", "DOI": "10.18653/v1/2021.acl-demo.6", "CorpusId": 237250135}, "corpusId": 237250135, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ca608192fc9d0c013915248c867656916b1059e8", "title": "SciConceptMiner: A system for large-scale scientific concept discovery", "abstract": "Scientific knowledge is evolving at an unprecedented rate of speed, with new concepts constantly being introduced from millions of academic articles published every month. In this paper, we introduce a self-supervised end-to-end system, SciConceptMiner, for the automatic capture of emerging scientific concepts from both independent knowledge sources (semi-structured data) and academic publications (unstructured documents). First, we adopt a BERT-based sequence labeling model to predict candidate concept phrases with self-supervision data. Then, we incorporate rich Web content for synonym detection and concept selection via a web search API. This two-stage approach achieves highly accurate (94.7%) concept identification with more than 740K scientific concepts. These concepts are deployed in the Microsoft Academic production system and are the backbone for its semantic search capability.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 19, "citationCount": 1, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "48-54"}, "authors": [{"authorId": "3303634", "name": "Zhihong Shen"}, {"authorId": "153248481", "name": "Chieh-Han Wu"}, {"authorId": "2152166202", "name": "Li Ma"}, {"authorId": "2127333332", "name": "Chien-Pang Chen"}, {"authorId": "1748169", "name": "Kuansan Wang"}]}, {"paperId": "34fbca00589c77fca70db4962beb43a9094c49ef", "externalIds": {"ACL": "2021.acl-demo.7", "DBLP": "journals/corr/abs-2012-10018", "ArXiv": "2012.10018", "DOI": "10.18653/v1/2021.acl-demo.7", "CorpusId": 229331627}, "corpusId": 229331627, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/34fbca00589c77fca70db4962beb43a9094c49ef", "title": "NeurST: Neural Speech Translation Toolkit", "abstract": "NeurST is an open-source toolkit for neural speech translation. The toolkit mainly focuses on end-to-end speech translation, which is easy to use, modify, and extend to advanced speech translation research and products. NeurST aims at facilitating the speech translation research for NLP researchers and building reliable benchmarks for this field. It provides step-by-step recipes for feature extraction, data preprocessing, distributed training, and evaluation. In this paper, we will introduce the framework design of NeurST and show experimental results for different benchmark datasets, which can be regarded as reliable baselines for future research. The toolkit is publicly available at https://github.com/bytedance/neurst and we will continuously update the performance of with other counterparts and studies at https://st-benchmark.github.io/.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 42, "citationCount": 25, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.7.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-18", "journal": {"pages": "55-62"}, "authors": [{"authorId": "144562857", "name": "Chengqi Zhao"}, {"authorId": "50468534", "name": "Mingxuan Wang"}, {"authorId": "143900005", "name": "Lei Li"}]}, {"paperId": "59c6400221188cec53a676f980a0dc89120ee119", "externalIds": {"ArXiv": "2107.06632", "ACL": "2021.acl-demo.8", "DBLP": "journals/corr/abs-2107-06632", "DOI": "10.18653/v1/2021.acl-demo.8", "CorpusId": 235829552}, "corpusId": 235829552, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/59c6400221188cec53a676f980a0dc89120ee119", "title": "ParCourE: A Parallel Corpus Explorer for a Massively Multilingual Corpus", "abstract": "With more than 7000 languages worldwide, multilingual natural language processing (NLP) is essential both from an academic and commercial perspective. Researching typological properties of languages is fundamental for progress in multilingual NLP. Examples include assessing language similarity for effective transfer learning, injecting inductive biases into machine learning models or creating resources such as dictionaries and inflection tables. We provide ParCourE, an online tool that allows to browse a word-aligned parallel corpus, covering 1334 languages. We give evidence that this is useful for typological research. ParCourE can be set up for any parallel corpus and can thus be used for typological research on other corpora as well as for exploring their quality and properties.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.8.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-14", "journal": {"pages": "63-72"}, "authors": [{"authorId": "51894641", "name": "Ayyoob Imani"}, {"authorId": "33014152", "name": "Masoud Jalili Sabet"}, {"authorId": "35501453", "name": "Philipp Dufter"}, {"authorId": "1698077", "name": "Michael Cysouw"}, {"authorId": "144418438", "name": "Hinrich Sch\u00fctze"}]}, {"paperId": "46dac3c30478f76140d11969452784821b012a87", "externalIds": {"ACL": "2021.acl-demo.9", "DBLP": "conf/acl/ReiFSCL21", "DOI": "10.18653/v1/2021.acl-demo.9", "CorpusId": 237012292}, "corpusId": 237012292, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/46dac3c30478f76140d11969452784821b012a87", "title": "MT-Telescope: An interactive platform for contrastive evaluation of MT systems", "abstract": "We present MT-Telescope, a visualization platform designed to facilitate comparative analysis of the output quality of two Machine Translation (MT) systems. While automated MT evaluation metrics are commonly used to evaluate MT systems at a corpus-level, our platform supports fine-grained segment-level analysis and interactive visualisations that expose the fundamental differences in the performance of the compared systems. MT-Telescope also supports dynamic corpus filtering to enable focused analysis on specific phenomena such as; translation of named entities, handling of terminology, and the impact of input segment length on translation quality. Furthermore, the platform provides a bootstrapped t-test for statistical significance as a means of evaluating the rigor of the resulting system ranking. MT-Telescope is open source, written in Python, and is built around a user friendly and dynamic web interface. Complementing other existing tools, our platform is designed to facilitate and promote the broader adoption of more rigorous analysis practices in the evaluation of MT quality.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "73-80"}, "authors": [{"authorId": "15631652", "name": "Ricardo Rei"}, {"authorId": "50588992", "name": "Ana C. Farinha"}, {"authorId": "40163298", "name": "Craig Alan Stewart"}, {"authorId": "1771718", "name": "Lu\u00edsa Coheur"}, {"authorId": "1784914", "name": "A. Lavie"}]}, {"paperId": "e8761be78e9222495c63e54ae386b7ea023c72be", "externalIds": {"MAG": "3174911914", "DBLP": "conf/acl/Lertvittayakumjorn21", "ACL": "2021.acl-demo.10", "DOI": "10.18653/v1/2021.acl-demo.10", "CorpusId": 237649152}, "corpusId": 237649152, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e8761be78e9222495c63e54ae386b7ea023c72be", "title": "Supporting Complaints Investigation for Nursing and Midwifery Regulatory Agencies", "abstract": "Health professional regulators aim to protect the health and well-being of patients and the public by setting standards for scrutinising and overseeing the training and conduct of health and care professionals. A major task of such regulators is the investigation of complaints against practitioners. However, processing a complaint often lasts several months and is particularly costly. Hence, we worked with international regulators from different countries (the UK, US and Australia), to develop the first decision support tool that aims to help such regulators process complaints more efficiently. Our system uses state-of-the-art machine learning and natural language processing techniques to process complaints and predict their risk level. Our tool also provides additional useful information including explanations, to help the regulatory staff interpret the prediction results, and similar past cases as well as non-compliance to regulations, to support the decision making.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 56, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.10.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "81-91"}, "authors": [{"authorId": "9433771", "name": "Piyawat Lertvittayakumjorn"}, {"authorId": "2119023", "name": "I. Petej"}, {"authorId": "2145974284", "name": "Yang Gao"}, {"authorId": "2756087", "name": "Yamuna Krishnamurthy"}, {"authorId": "12565316", "name": "A. van der Gaag"}, {"authorId": "145575440", "name": "R. Jago"}, {"authorId": "1802398", "name": "Kostas Stathis"}]}, {"paperId": "d74aaa18d851e16a2f6e0284eea584bf8f52b27f", "externalIds": {"MAG": "3175102171", "ACL": "2021.acl-demo.11", "DBLP": "conf/acl/JinCSWXZ21", "DOI": "10.18653/v1/2021.acl-demo.11", "CorpusId": 237708230}, "corpusId": 237708230, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d74aaa18d851e16a2f6e0284eea584bf8f52b27f", "title": "CogIE: An Information Extraction Toolkit for Bridging Texts and CogNet", "abstract": "CogNet is a knowledge base that integrates three types of knowledge: linguistic knowledge, world knowledge and commonsense knowledge. In this paper, we propose an information extraction toolkit, called CogIE, which is a bridge connecting raw texts and CogNet. CogIE has three features: versatile, knowledge-grounded and extensible. First, CogIE is a versatile toolkit with a rich set of functional modules, including named entity recognition, entity typing, entity linking, relation extraction, event extraction and frame-semantic parsing. Second, as a knowledge-grounded toolkit, CogIE can ground the extracted facts to CogNet and leverage different types of knowledge to enrich extracted results. Third, for extensibility, owing to the design of three-tier architecture, CogIE is not only a plug-and-play toolkit for developers but also an extensible programming framework for researchers. We release an open-access online system to visually extract information from texts. Source code, datasets and pre-trained models are publicly available at GitHub, with a short instruction video.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "92-98"}, "authors": [{"authorId": "2152843772", "name": "Zhuoran Jin"}, {"authorId": "152829071", "name": "Yubo Chen"}, {"authorId": "1381062467", "name": "Dianbo Sui"}, {"authorId": "2135762532", "name": "Chenhao Wang"}, {"authorId": "2052284158", "name": "Zhipeng Xue"}, {"authorId": "11447228", "name": "Jun Zhao"}]}, {"paperId": "6d2263ddcbaf988aebc6930f741a3121918190e2", "externalIds": {"ArXiv": "2009.08633", "DBLP": "conf/acl/GengYQH21", "ACL": "2021.acl-demo.12", "DOI": "10.18653/v1/2021.acl-demo.12", "CorpusId": 235262785}, "corpusId": 235262785, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6d2263ddcbaf988aebc6930f741a3121918190e2", "title": "fastHan: A BERT-based Multi-Task Toolkit for Chinese NLP", "abstract": "We present fastHan, an open-source toolkit for four basic tasks in Chinese natural language processing: Chinese word segmentation (CWS), Part-of-Speech (POS) tagging, named entity recognition (NER), and dependency parsing. The backbone of fastHan is a multi-task model based on a pruned BERT, which uses the first 8 layers in BERT. We also provide a 4-layer base model compressed from the 8-layer model. The joint-model is trained and evaluated on 13 corpora of four tasks, yielding near state-of-the-art (SOTA) performance in dependency parsing and NER, achieving SOTA performance in CWS and POS. Besides, fastHan\u2019s transferability is also strong, performing much better than popular segmentation tools on a non-training corpus. To better meet the need of practical application, we allow users to use their own labeled data to further fine-tune fastHan. In addition to its small size and excellent performance, fastHan is user-friendly. Implemented as a python package, fastHan isolates users from the internal technical details and is convenient to use. The project is released on Github.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.12.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-18", "journal": {"pages": "99-106"}, "authors": [{"authorId": "2056612192", "name": "Zhichao Geng"}, {"authorId": "146948229", "name": "Hang Yan"}, {"authorId": "1767521", "name": "Xipeng Qiu"}, {"authorId": "1790227", "name": "Xuanjing Huang"}]}, {"paperId": "35133d6931d1710313b183c755bdcae6df95c4ed", "externalIds": {"ACL": "2021.acl-demo.13", "DBLP": "conf/acl/FrasnelliBA21", "DOI": "10.18653/v1/2021.acl-demo.13", "CorpusId": 237158776}, "corpusId": 237158776, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/35133d6931d1710313b183c755bdcae6df95c4ed", "title": "Erase and Rewind: Manual Correction of NLP Output through a Web Interface", "abstract": "In this paper, we present Tintful, an NLP annotation software that can be used both to manually annotate texts and to fix mistakes in NLP pipelines, such as Stanford CoreNLP. Using a paradigm similar to wiki-like systems, a user who notices some wrong annotation can easily fix it and submit the resulting (and right) entry back to the tool developers. Moreover, Tintful can be used to easily annotate data from scratch. The input documents do not need to be in a particular format: starting from the plain text, the sentences are first annotated with CoreNLP, then the user can edit the annotations and submit everything back through a user-friendly interface.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "107-113"}, "authors": [{"authorId": "2081605213", "name": "Valentino Frasnelli"}, {"authorId": "2097034371", "name": "Lorenzo Bocchi"}, {"authorId": "2179409", "name": "Alessio Palmero Aprosio"}]}, {"paperId": "bf37e581f9d2447bf732b8502c8aff5141a88f06", "externalIds": {"MAG": "3173621131", "ACL": "2021.acl-demo.14", "DBLP": "conf/acl/HongwimolKLLNLL21", "DOI": "10.18653/v1/2021.acl-demo.14", "CorpusId": 237650704}, "corpusId": 237650704, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bf37e581f9d2447bf732b8502c8aff5141a88f06", "title": "ESRA: Explainable Scientific Research Assistant", "abstract": "We introduce Explainable Scientific Research Assistant (ESRA), a literature discovery platform that augments search results with relevant details and explanations, aiding users in understanding more about their queries and the returned papers beyond existing literature search systems. Enabled by a knowledge graph we extracted from abstracts of 23k papers on the arXiv\u2019s cs.CL category, ESRA provides three main features: explanation (for why a paper is returned to the user), list of facts (that are relevant to the query), and graph visualization (drawing connections between the query and each paper with surrounding related entities). The experimental results with humans involved show that ESRA can accelerate the users\u2019 search process with paper explanations and helps them better explore the landscape of the topics of interest by exploiting the underlying knowledge graph. We provide the ESRA web application at http://esra.cp.eng.chula.ac.th/.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 25, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "114-121"}, "authors": [{"authorId": "2128264848", "name": "Pollawat Hongwimol"}, {"authorId": "2128281427", "name": "Peeranuth Kehasukcharoen"}, {"authorId": "2128282529", "name": "Pasit Laohawarutchai"}, {"authorId": "9433771", "name": "Piyawat Lertvittayakumjorn"}, {"authorId": "1557327062", "name": "Aik Beng Ng"}, {"authorId": "2072863482", "name": "Zhangsheng Lai"}, {"authorId": "2109992450", "name": "Timothy Liu"}, {"authorId": "1715851", "name": "P. Vateekul"}]}, {"paperId": "48a3a41dab9d9e13d193b9b3d6fa7fe9df261cb0", "externalIds": {"ACL": "2021.acl-demo.15", "DBLP": "conf/acl/Barbaresi21", "MAG": "3177057043", "DOI": "10.18653/v1/2021.acl-demo.15", "CorpusId": 237561567}, "corpusId": 237561567, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/48a3a41dab9d9e13d193b9b3d6fa7fe9df261cb0", "title": "Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction", "abstract": "An essential operation in web corpus construction consists in retaining the desired content while discarding the rest. Another challenge finding one\u2019s way through websites. This article introduces a text discovery and extraction tool published under open-source license. Its installation and use is straightforward, notably from Python and on the command-line. The software allows for main text, comments and metadata extraction, while also providing building blocks for web crawling tasks. A comparative evaluation on real-world data also shows its interest as well as the performance of other available solutions. The contributions of this paper are threefold: it references the software, features a benchmark, and provides a meaningful baseline for similar tasks. The tool performs significantly better than other open-source solutions in this evaluation and in external benchmarks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 55, "citationCount": 30, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.15.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "122-131"}, "authors": [{"authorId": "2551614", "name": "A. Barbaresi"}]}, {"paperId": "c96dbf796fdaf16e03896d38b24d25942c1a857e", "externalIds": {"ArXiv": "2103.14625", "ACL": "2021.acl-demo.16", "DBLP": "journals/corr/abs-2103-14625", "DOI": "10.18653/v1/2021.acl-demo.16", "CorpusId": 232380086}, "corpusId": 232380086, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c96dbf796fdaf16e03896d38b24d25942c1a857e", "title": "Dodrio: Exploring Transformer Models with Interactive Visualization", "abstract": "Why do large pre-trained transformer-based models perform so well across a wide variety of NLP tasks? Recent research suggests the key may lie in multi-headed attention mechanism\u2019s ability to learn and represent linguistic information. Understanding how these models represent both syntactic and semantic knowledge is vital to investigate why they succeed and fail, what they have learned, and how they can improve. We present Dodrio, an open-source interactive visualization tool to help NLP researchers and practitioners analyze attention mechanisms in transformer-based models with linguistic knowledge. Dodrio tightly integrates an overview that summarizes the roles of different attention heads, and detailed views that help users compare attention weights with the syntactic structure and semantic information in the input text. To facilitate the visual comparison of attention weights and linguistic knowledge, Dodrio applies different graph visualization techniques to represent attention weights scalable to longer input text. Case studies highlight how Dodrio provides insights into understanding the attention mechanism in transformer-based models. Dodrio is available at https://poloclub.github.io/dodrio/.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 31, "citationCount": 19, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.16.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-03-26", "journal": {"name": "ArXiv", "volume": "abs/2103.14625"}, "authors": [{"authorId": "1390877819", "name": "Zijie J. Wang"}, {"authorId": "1478596939", "name": "Robert Turko"}, {"authorId": "1793506", "name": "Duen Horng Chau"}]}, {"paperId": "fc6c921a246e92cee300cb3f00711635b784cc9e", "externalIds": {"ACL": "2021.acl-demo.17", "DBLP": "conf/acl/AndersenZM21", "DOI": "10.18653/v1/2021.acl-demo.17", "CorpusId": 235702860}, "corpusId": 235702860, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fc6c921a246e92cee300cb3f00711635b784cc9e", "title": "REM: Efficient Semi-Automated Real-Time Moderation of Online Forums", "abstract": "This paper presents REM, a novel tool for the semi-automated real-time moderation of large scale online forums. The growing demand for online participation and the increasing number of user comments raise challenges in filtering out harmful and undesirable content from public debates in online forums. Since a manual moderation does not scale well and pure automated approaches often lack the required level of accuracy, we suggest a semi-automated moderation approach. Our approach maximizes the efficiency of manual efforts by targeting only those comments for which human intervention is needed, e.g. due to high classification uncertainty. Our tool offers a rich visual interactive environment enabling the exploration of online debates. We conduct a preliminary evaluation experiment to demonstrate the suitability of our approach and publicly release the source code of REM.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.17.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "142-149"}, "authors": [{"authorId": "3459629", "name": "J. S. Andersen"}, {"authorId": "2478828", "name": "O. Zukunft"}, {"authorId": "1706506", "name": "W. Maalej"}]}, {"paperId": "7139a82d9cce629b6782c7c67202797da035a01d", "externalIds": {"DBLP": "conf/acl/VigKGR21", "ArXiv": "2104.07605", "ACL": "2021.acl-demo.18", "DOI": "10.18653/v1/2021.acl-demo.18", "CorpusId": 233241131}, "corpusId": 233241131, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7139a82d9cce629b6782c7c67202797da035a01d", "title": "SummVis: Interactive Visual Analysis of Models, Data, and Evaluation for Text Summarization", "abstract": "Novel neural architectures, training strategies, and the availability of large-scale corpora haven been the driving force behind recent progress in abstractive text summarization. However, due to the black-box nature of neural models, uninformative evaluation metrics, and scarce tooling for model and data analysis the true performance and failure modes of summarization models remain largely unknown. To address this limitation, we introduce SummVis, an open-source tool for visualizing abstractive summaries that enables fine-grained analysis of the models, data, and evaluation metrics associated with text summarization. Through its lexical and semantic visualizations, the tools offers an easy entry point for in-depth model prediction exploration across important dimensions such as factual consistency or abstractiveness. The tool together with several pre-computed model outputs is available at https://summvis.com.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 34, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.18.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-04-15", "journal": {"pages": "150-158"}, "authors": [{"authorId": "2056908", "name": "Jesse Vig"}, {"authorId": "51232396", "name": "Wojciech Kryscinski"}, {"authorId": "1822288", "name": "Karan Goel"}, {"authorId": "8937909", "name": "Nazneen Rajani"}]}, {"paperId": "e95e0caa59f006777f633860f105cf139e5d8611", "externalIds": {"DBLP": "conf/acl/MishraMBPK21", "MAG": "3175874586", "ACL": "2021.acl-demo.19", "DOI": "10.18653/v1/2021.acl-demo.19", "CorpusId": 237344345}, "corpusId": 237344345, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e95e0caa59f006777f633860f105cf139e5d8611", "title": "A Graphical Interface for Curating Schemas", "abstract": "Much past work has focused on extracting information like events, entities, and relations from documents. Very little work has focused on analyzing these results for better model understanding. In this paper, we introduce a curation interface that takes an Information Extraction (IE) system\u2019s output in a pre-defined format and generates a graphical representation of its elements. The interface supports editing while curating schemas for complex events like Improvised Explosive Device (IED) based scenarios. We identify various schemas that either have linear event chains or contain parallel events with complicated temporal ordering. We iteratively update an induced schema to uniquely identify events specific to it, add optional events around them, and prune unnecessary events. The resulting schemas are improved and enriched versions of the machine-induced versions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 19, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "159-166"}, "authors": [{"authorId": "51234098", "name": "Piyush Mishra"}, {"authorId": "144963080", "name": "Akanksha Malhotra"}, {"authorId": "1783500", "name": "S. Brown"}, {"authorId": "145755155", "name": "Martha Palmer"}, {"authorId": "51203051", "name": "Ghazaleh Kazeminejad"}]}, {"paperId": "9569703963cbaa618b7a3b59e32a036d76c706ab", "externalIds": {"ACL": "2021.acl-demo.20", "DBLP": "conf/acl/ZhangLXZZG21", "MAG": "3174620475", "ArXiv": "2110.15063", "DOI": "10.18653/v1/2021.acl-demo.20", "CorpusId": 237729646}, "corpusId": 237729646, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9569703963cbaa618b7a3b59e32a036d76c706ab", "title": "TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition", "abstract": "TEXTOIR is the first integrated and visualized platform for text open intent recognition. It is composed of two main modules: open intent detection and open intent discovery. Each module integrates most of the state-of-the-art algorithms and benchmark intent datasets. It also contains an overall framework connecting the two modules in a pipeline scheme. In addition, this platform has visualized tools for data and model management, training, evaluation and analysis of the performance from different aspects. TEXTOIR provides useful toolkits and convenient visualized interfaces for each sub-module, and designs a framework to implement a complete process to both identify known intents and discover open intents.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 18, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.20.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"name": "ArXiv", "volume": "abs/2110.15063"}, "authors": [{"authorId": "13390595", "name": "Hanlei Zhang"}, {"authorId": "2108689482", "name": "Xiaoteng Li"}, {"authorId": "2904643", "name": "Hua Xu"}, {"authorId": "2110350972", "name": "Panpan Zhang"}, {"authorId": "2074107495", "name": "K. Zhao"}, {"authorId": "49834418", "name": "Kai Gao"}]}, {"paperId": "19003e5f5623c9d778aeaa580a946cae9b473841", "externalIds": {"ACL": "2021.acl-demo.21", "MAG": "3173295223", "DBLP": "conf/acl/XiMLLCYCTLlJLZH21", "DOI": "10.18653/v1/2021.acl-demo.21", "CorpusId": 237700337}, "corpusId": 237700337, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/19003e5f5623c9d778aeaa580a946cae9b473841", "title": "KuiLeiXi: a Chinese Open-Ended Text Adventure Game", "abstract": "There is a long history of research related to automated story generation, dating back as far as the 1970s. Recently, the rapid development of pre-trained language models has spurred great progresses in this field. Equipped with GPT-2 and the latest GPT-3, AI Dungeon has been seen as a famous example of the powerful text generation capabilities of large-scale pre-trained language models, and a possibility for future games. However, as a game, AI Dungeon lacks incentives to players and relies entirely on players to explore on their own. This makes players\u2019 enthusiasm decline rapidly. In this paper, we present an open-ended text adventure game in Chinese, named as KuiLeiXi. In KuiLeiXi, players need to interact with the AI until the pre-determined plot goals are reached. By introducing the plot goals, players have a stronger incentive to explore ways to reach plot goals, while the AI\u2019s abilities are not abused to generate harmful contents. This limited freedom allows this game to be integrated as a part of a romance simulation mobile game, Yu Jian Love. Since KuiLeiXi was launched, it has received a lot of positive feedbacks from more than 100,000 players. A demo video is available at https://youtu.be/DyYZhxMRrkk.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 20, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Education", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "175-184"}, "authors": [{"authorId": "90827036", "name": "Xing Yadong"}, {"authorId": "29422474", "name": "Xiaoxi Mao"}, {"authorId": "2149866335", "name": "Li Le"}, {"authorId": "2150077978", "name": "Lin Lei"}, {"authorId": "2149865010", "name": "Chen Yanjiang"}, {"authorId": "2155665735", "name": "Shuhan Yang"}, {"authorId": "2149866142", "name": "Chen Xuhan"}, {"authorId": "2128352351", "name": "Kailun Tao"}, {"authorId": "2149967344", "name": "Li Zhi"}, {"authorId": "2108422147", "name": "GongZheng Li"}, {"authorId": "2151881294", "name": "Jiang Lin"}, {"authorId": "67051008", "name": "Liu Siyan"}, {"authorId": "2150468833", "name": "Zhao Zeng"}, {"authorId": "1730108", "name": "Minlie Huang"}, {"authorId": "3120655", "name": "Changjie Fan"}, {"authorId": "2159237053", "name": "Hu Zhipeng"}]}, {"paperId": "f5ec998b44beb4bd743204e77f564a62e9df46b2", "externalIds": {"MAG": "3120182083", "ArXiv": "2101.00939", "DBLP": "conf/acl/ZhouWZSCZLW21", "ACL": "2021.acl-demo.22", "DOI": "10.18653/v1/2021.acl-demo.22", "CorpusId": 230435757}, "corpusId": 230435757, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f5ec998b44beb4bd743204e77f564a62e9df46b2", "title": "CRSLab: An Open-Source Toolkit for Building Conversational Recommender System", "abstract": "In recent years, conversational recommender systems (CRSs) have drawn a wide attention in the research community, which focus on providing high-quality recommendations to users via natural language conversations. However, due to diverse scenarios and data formats, existing studies on CRSs lack unified and standardized implementation or comparison. To tackle this challenge, we release an open-source toolkit CRSLab, which provides a unified and extensible framework with highly-decoupled modules to develop CRSs. Based on this framework, we collect 6 commonly used human-annotated CRS datasets and implement 19 models that include advanced techniques such as graph neural networks and pre-training models. Besides, our toolkit provides a series of automatic evaluation protocols and a human-machine interaction interface to evaluate and compare different CRS methods. The project and documents are released at https://github.com/RUCAIBox/CRSLab.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 35, "citationCount": 38, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.22.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-04", "journal": {"name": "ArXiv", "volume": "abs/2101.00939"}, "authors": [{"authorId": "1423651904", "name": "Kun Zhou"}, {"authorId": "72541556", "name": "Xiaolei Wang"}, {"authorId": "2116568362", "name": "Yuanhang Zhou"}, {"authorId": "2214470070", "name": "Chenzhang Shang"}, {"authorId": "145591949", "name": "Yuan Cheng"}, {"authorId": "2542603", "name": "Wayne Xin Zhao"}, {"authorId": "2110479359", "name": "Yaliang Li"}, {"authorId": "153693432", "name": "Ji-rong Wen"}]}, {"paperId": "491d193b32dfdf4604bf48e494fa1fda96c60cfe", "externalIds": {"ArXiv": "2104.05807", "DBLP": "conf/acl/FerreiraRTVF21", "ACL": "2021.acl-demo.23", "DOI": "10.18653/v1/2021.acl-demo.23", "CorpusId": 233219699}, "corpusId": 233219699, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/491d193b32dfdf4604bf48e494fa1fda96c60cfe", "title": "Does My Representation Capture X? Probe-Ably", "abstract": "Probing (or diagnostic classification) has become a popular strategy for investigating whether a given set of intermediate features is present in the representations of neural models. Naive probing studies may have misleading results, but various recent works have suggested more reliable methodologies that compensate for the possible pitfalls of probing. However, these best practices are numerous and fast-evolving. To simplify the process of running a set of probing experiments in line with suggested methodologies, we introduce Probe-Ably: an extendable probing framework which supports and automates the application of probing methods to the user\u2019s inputs.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 20, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.23.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-04-12", "journal": {"pages": "194-201"}, "authors": [{"authorId": "2058261256", "name": "Deborah Ferreira"}, {"authorId": "8471045", "name": "Julia Rozanova"}, {"authorId": "102669988", "name": "Mokanarangan Thayaparan"}, {"authorId": "34102057", "name": "Marco Valentino"}, {"authorId": "145528474", "name": "A. Freitas"}]}, {"paperId": "016c171611237518dd8f167f9195c984cfb482c4", "externalIds": {"ACL": "2021.acl-demo.24", "DBLP": "conf/acl/PanCGGF21", "ArXiv": "2106.04441", "DOI": "10.18653/v1/2021.acl-demo.24", "CorpusId": 235368270}, "corpusId": 235368270, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/016c171611237518dd8f167f9195c984cfb482c4", "title": "CLTR: An End-to-End, Transformer-Based System for Cell-Level Table Retrieval and Table Question Answering", "abstract": "We present the first end-to-end, transformer-based table question answering (QA) system that takes natural language questions and massive table corpora as inputs to retrieve the most relevant tables and locate the correct table cells to answer the question. Our system, CLTR, extends the current state-of-the-art QA over tables model to build an end-to-end table QA architecture. This system has successfully tackled many real-world table QA problems with a simple, unified pipeline. Our proposed system can also generate a heatmap of candidate columns and rows over complex tables and allow users to quickly identify the correct cells to answer questions. In addition, we introduce two new open domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 natural language questions over 76,242 tables. The benchmarks are designed to validate CLTR as well as accommodate future table retrieval and end-to-end table QA research and experiments. Our experiments demonstrate that our system is the current state-of-the-art model on the table retrieval task and produces promising results for end-to-end table QA.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.24.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"name": "ArXiv", "volume": "abs/2106.04441"}, "authors": [{"authorId": "2068792892", "name": "Feifei Pan"}, {"authorId": "1888104", "name": "Mustafa Canim"}, {"authorId": "143742133", "name": "Michael R. Glass"}, {"authorId": "1711133", "name": "A. Gliozzo"}, {"authorId": "2143727333", "name": "Peter Fox"}]}, {"paperId": "7006eb1ed218921745a3a598d0bc809f2bf4d7b6", "externalIds": {"DBLP": "journals/corr/abs-2106-04612", "ArXiv": "2106.04612", "ACL": "2021.acl-demo.25", "DOI": "10.18653/v1/2021.acl-demo.25", "CorpusId": 235376742}, "corpusId": 235376742, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7006eb1ed218921745a3a598d0bc809f2bf4d7b6", "title": "Neural Extractive Search", "abstract": "Domain experts often need to extract structured information from large corpora. We advocate for a search paradigm called \u201cextractive search\u201d, in which a search query is enriched with capture-slots, to allow for such rapid extraction. Such an extractive search system can be built around syntactic structures, resulting in high-precision, low-recall results. We show how the recall can be improved using neural retrieval and alignment. The goals of this paper are to concisely introduce the extractive-search paradigm; and to demonstrate a prototype neural retrieval system for extractive search and its benefits and potential. Our prototype is available at https://spike.neural-sim.apps.allenai.org/ and a video demonstration is available at https://vimeo.com/559586687.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 13, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.25.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"name": "ArXiv", "volume": "abs/2106.04612"}, "authors": [{"authorId": "2143278592", "name": "Shauli Ravfogel"}, {"authorId": "1409499701", "name": "Hillel Taub-Tabib"}, {"authorId": "79775260", "name": "Yoav Goldberg"}]}, {"paperId": "2384c92bbde47f5dbc8d8f175aa67e0f95c413d4", "externalIds": {"DBLP": "journals/corr/abs-2106-04718", "ACL": "2021.acl-demo.26", "ArXiv": "2106.04718", "DOI": "10.18653/v1/2021.acl-demo.26", "CorpusId": 235376968}, "corpusId": 235376968, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2384c92bbde47f5dbc8d8f175aa67e0f95c413d4", "title": "FastSeq: Make Sequence Generation Faster", "abstract": "Transformer-based models have made tremendous impacts in natural language generation. However the inference speed is a bottleneck due to large model size and intensive computing involved in auto-regressive decoding process. We develop FastSeq framework to accelerate sequence generation without accuracy loss. The proposed optimization techniques include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I/O. These optimizations are general enough to be applicable to Transformer-based models (e.g., T5, GPT2, and UniLM). Our benchmark results on a set of widely used and diverse models demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use with a simple one-line code change. The source code is available at https://github.com/microsoft/fastseq.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.26.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"pages": "218-226"}, "authors": [{"authorId": "145967727", "name": "Yu Yan"}, {"authorId": "2064992833", "name": "Fei Hu"}, {"authorId": "2108323525", "name": "Jiusheng Chen"}, {"authorId": "29769330", "name": "Nikhil Bhendawade"}, {"authorId": "2108746950", "name": "Ting Ye"}, {"authorId": "2171182", "name": "Yeyun Gong"}, {"authorId": "46429989", "name": "Nan Duan"}, {"authorId": "2108851526", "name": "Desheng Cui"}, {"authorId": "2109492308", "name": "Bingyu Chi"}, {"authorId": "2109962816", "name": "Ruifei Zhang"}]}, {"paperId": "9ca19acce35fd440cb9ffa504907f36a2e176bbc", "externalIds": {"DBLP": "journals/corr/abs-2110-10973", "ACL": "2021.acl-demo.27", "ArXiv": "2110.10973", "DOI": "10.18653/v1/2021.acl-demo.27", "CorpusId": 237057231}, "corpusId": 237057231, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9ca19acce35fd440cb9ffa504907f36a2e176bbc", "title": "LOA: Logical Optimal Actions for Text-based Interaction Games", "abstract": "We present Logical Optimal Actions (LOA), an action decision architecture of reinforcement learning applications with a neuro-symbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games. The demonstration for LOA experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowledge for improving interpretability for trained rules. This demonstration also provides a comparison module with other neuro-symbolic approaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our LOA also provides open-sourced implementation in Python for the reinforcement learning environment to facilitate an experiment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 14, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.27.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-10-21", "journal": {"name": "ArXiv", "volume": "abs/2110.10973"}, "authors": [{"authorId": "40433860", "name": "Daiki Kimura"}, {"authorId": "34597365", "name": "Subhajit Chaudhury"}, {"authorId": "2054340207", "name": "Masaki Ono"}, {"authorId": "3305985", "name": "Michiaki Tatsubori"}, {"authorId": "3352943", "name": "Don Joven Agravante"}, {"authorId": "1688057", "name": "Asim Munawar"}, {"authorId": "41036308", "name": "Akifumi Wachi"}, {"authorId": "22312240", "name": "Ryosuke Kohita"}, {"authorId": "1703070", "name": "Alexander G. Gray"}]}, {"paperId": "26e3d58181724f9ef77973ff0f65bac06e499fec", "externalIds": {"ACL": "2021.acl-demo.28", "DBLP": "journals/corr/abs-2104-08006", "ArXiv": "2104.08006", "DOI": "10.18653/v1/2021.acl-demo.28", "CorpusId": 233289543}, "corpusId": 233289543, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/26e3d58181724f9ef77973ff0f65bac06e499fec", "title": "ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation", "abstract": "Now, the pre-training technique is ubiquitous in natural language processing field. ProphetNet is a pre-training based natural language generation method which shows powerful performance on English text summarization and question generation tasks. In this paper, we extend ProphetNet into other domains and languages, and present the ProphetNet family pre-training models, named ProphetNet-X, where X can be English, Chinese, Multi-lingual, and so on. We pre-train a cross-lingual generation model ProphetNet-Multi, a Chinese generation model ProphetNet-Zh, two open-domain dialog generation models ProphetNet-Dialog-En and ProphetNet-Dialog-Zh. And also, we provide a PLG (Programming Language Generation) model ProphetNet-Code to show the generation performance besides NLG (Natural Language Generation) tasks. In our experiments, ProphetNet-X models achieve new state-of-the-art performance on 10 benchmarks. All the models of ProphetNet-X share the same model structure, which allows users to easily switch between different models. We make the code and models publicly available, and we will keep updating more pre-training models and finetuning scripts.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 46, "citationCount": 49, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.28.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-04-16", "journal": {"pages": "232-239"}, "authors": [{"authorId": "15629561", "name": "Weizhen Qi"}, {"authorId": "2171182", "name": "Yeyun Gong"}, {"authorId": "145967727", "name": "Yu Yan"}, {"authorId": "46747953", "name": "Can Xu"}, {"authorId": "2119658391", "name": "Bolun Yao"}, {"authorId": "2109061043", "name": "Bartuer Zhou"}, {"authorId": "2055922979", "name": "Biao Cheng"}, {"authorId": "71790825", "name": "Daxin Jiang"}, {"authorId": "2108323525", "name": "Jiusheng Chen"}, {"authorId": "2124601065", "name": "Ruofei Zhang"}, {"authorId": "2144406784", "name": "Houqiang Li"}, {"authorId": "46429989", "name": "Nan Duan"}]}, {"paperId": "1bd035b551175ac8bbaf6c443de5aa668d5d91a0", "externalIds": {"ACL": "2021.acl-demo.29", "DBLP": "conf/acl/GongHSFSZWL21", "DOI": "10.18653/v1/2021.acl-demo.29", "CorpusId": 237393105}, "corpusId": 237393105, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1bd035b551175ac8bbaf6c443de5aa668d5d91a0", "title": "IFlyEA: A Chinese Essay Assessment System with Automated Rating, Review Generation, and Recommendation", "abstract": "Automated Essay Assessment (AEA) aims to judge students\u2019 writing proficiency in an automatic way. This paper presents a Chinese AEA system IFlyEssayAssess (IFlyEA), targeting on evaluating essays written by native Chinese students from primary and junior schools. IFlyEA provides multi-level and multi-dimension analytical modules for essay assessment. It has state-of-the-art grammar level analysis techniques, and also integrates components for rhetoric and discourse level analysis, which are important for evaluating native speakers\u2019 writing ability, but still challenging and less studied in previous work. Based on the comprehensive analysis, IFlyEA provides application services for essay scoring, review generation, recommendation, and explainable analytical visualization. These services can benefit both teachers and students during the process of writing teaching and learning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 38, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.29.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Education", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": null, "journal": {"pages": "240-248"}, "authors": [{"authorId": "112863312", "name": "Jiefu Gong"}, {"authorId": "2109788314", "name": "Xiao Hu"}, {"authorId": "145273601", "name": "Wei Song"}, {"authorId": "2884436", "name": "Ruiji Fu"}, {"authorId": "2125340023", "name": "Zhichao Sheng"}, {"authorId": "2087090943", "name": "Bolin Zhu"}, {"authorId": "2108620507", "name": "Shijin Wang"}, {"authorId": "40282288", "name": "Ting Liu"}]}, {"paperId": "ac34c70ee85b048ad97328713c790f389656e4eb", "externalIds": {"MAG": "3176390156", "ACL": "2021.acl-demo.30", "DBLP": "conf/acl/Alammar21", "DOI": "10.18653/v1/2021.acl-demo.30", "CorpusId": 237730643}, "corpusId": 237730643, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ac34c70ee85b048ad97328713c790f389656e4eb", "title": "Ecco: An Open Source Library for the Explainability of Transformer Language Models", "abstract": "Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the transparency of Transformer-based language models, we present Ecco \u2013 an open-source library for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and interactively explore the inner mechanics of these models. This includes (1) gradient-based feature attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and examination tools for neuron activations in the under-explored Feed-Forward Neural Network sublayer of Transformer layers. (4) convenient examination of activation vectors via canonical correlation analysis (CCA), non-negative matrix factorization (NMF), and probing classifiers. We find that syntactic information can be retrieved from BERT\u2019s FFNN representations in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntactic information. Ecco is available at https://www.eccox.io/", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 51, "citationCount": 32, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.30.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "249-257"}, "authors": [{"authorId": "2086023612", "name": "J. Alammar"}]}, {"paperId": "d4f95365a0c1aec74333f50a996c62bfad4a8478", "externalIds": {"DBLP": "conf/acl/NeumannSS21", "ArXiv": "2101.10281", "ACL": "2021.acl-demo.31", "DOI": "10.18653/v1/2021.acl-demo.31", "CorpusId": 231698598}, "corpusId": 231698598, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d4f95365a0c1aec74333f50a996c62bfad4a8478", "title": "PAWLS: PDF Annotation With Labels and Structure", "abstract": "Adobe\u2019s Portable Document Format (PDF) is a popular way of distributing view-only documents with a rich visual markup. This presents a challenge to NLP practitioners who wish to use the information contained within PDF documents for training models or data analysis, because annotating these documents is difficult. In this paper, we present PDF Annotation with Labels and Structure (PAWLS), a new annotation tool designed specifically for the PDF document format. PAWLS is particularly suited for mixed-mode annotation and scenarios in which annotators require extended context to annotate accurately. PAWLS supports span-based textual annotation, N-ary relations and freeform, non-textual bounding boxes, all of which can be exported in convenient formats for training multi-modal machine learning models. A read-only PAWLS server is available at https://pawls.apps.allenai.org/, and the source code is available at https://github.com/allenai/pawls.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 27, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.31.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-25", "journal": {"pages": "258-264"}, "authors": [{"authorId": "2060376981", "name": "Mark Neumann"}, {"authorId": "101568984", "name": "Zejiang Shen"}, {"authorId": "46181683", "name": "Sam Skjonsberg"}]}, {"paperId": "591db6a777fb744046715eed9997b889114d3c64", "externalIds": {"DBLP": "conf/acl/ShahSS21", "ACL": "2021.acl-demo.32", "ArXiv": "2106.10512", "DOI": "10.18653/v1/2021.acl-demo.32", "CorpusId": 235489949}, "corpusId": 235489949, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/591db6a777fb744046715eed9997b889114d3c64", "title": "TweeNLP: A Twitter Exploration Portal for Natural Language Processing", "abstract": "We present TweeNLP, a one-stop portal that organizes Twitter\u2019s natural language processing (NLP) data and builds a visualization and exploration platform. It curates 19,395 tweets (as of April 2021) from various NLP conferences and general NLP discussions. It supports multiple features such as TweetExplorer to explore tweets by topics, visualize insights from Twitter activity throughout the organization cycle of conferences, discover popular research papers and researchers. It also builds a timeline of conference and workshop submission deadlines. We envision TweeNLP to function as a collective memory unit for the NLP community by integrating the tweets pertaining to research papers with the NLPExplorer scientific literature search engine. The current system is hosted at http://nlpexplorer.org/twitter/CFP.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 16, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.32.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-19", "journal": {"name": "ArXiv", "volume": "abs/2106.10512"}, "authors": [{"authorId": "2069609675", "name": "Viraj Shah"}, {"authorId": "14918655", "name": "Shruti Singh"}, {"authorId": "1400331254", "name": "M. Singh"}]}, {"paperId": "bb53946c7da617a05bbeef47fff74012db27ee78", "externalIds": {"ACL": "2021.acl-demo.33", "DBLP": "conf/acl/ZhangFB21", "ArXiv": "2107.14800", "DOI": "10.18653/v1/2021.acl-demo.33", "CorpusId": 236635031}, "corpusId": 236635031, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bb53946c7da617a05bbeef47fff74012db27ee78", "title": "ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback", "abstract": "We introduce ChrEnTranslate, an online machine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as provides quality estimation to inform users of reliability, two user feedback interfaces for experts and common users respectively, example inputs to collect human translations for monolingual data, word alignment visualization, and relevant terms from the Cherokee English dictionary. The quantitative evaluation demonstrates that our backbone translation models achieve state-of-the-art translation performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable because it copies less than SMT, and, in general, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert-corrected parallel texts into the training set and retrain models, equal or slightly better performance is observed, which demonstrates indicates the potential of human-in-the-loop learning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 31, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.33.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-30", "journal": {"pages": "272-279"}, "authors": [{"authorId": "7670835", "name": "Shiyue Zhang"}, {"authorId": "46689115", "name": "B. Frey"}, {"authorId": "143977268", "name": "Mohit Bansal"}]}, {"paperId": "1cf2e9e198feef3893da2800a7949f6880ddc084", "externalIds": {"DBLP": "conf/acl/LiuFXYCDLYN21", "ACL": "2021.acl-demo.34", "ArXiv": "2104.06387", "DOI": "10.18653/v1/2021.acl-demo.34", "CorpusId": 233219948}, "corpusId": 233219948, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1cf2e9e198feef3893da2800a7949f6880ddc084", "title": "ExplainaBoard: An Explainable Leaderboard for NLP", "abstract": "With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B and C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, ExplainaBoard covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. We not only released an online platform at the website but also make our evaluation tool an API with MIT Licence at Github and PyPi that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate \u201coutput-driven\u201d research in the future.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 57, "citationCount": 48, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.34.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-04-13", "journal": {"pages": "280-289"}, "authors": [{"authorId": "144118452", "name": "Pengfei Liu"}, {"authorId": "41037252", "name": "Jinlan Fu"}, {"authorId": "2116642640", "name": "Yanghua Xiao"}, {"authorId": "30300197", "name": "Weizhe Yuan"}, {"authorId": "46923811", "name": "Shuaichen Chang"}, {"authorId": "2087363104", "name": "Junqi Dai"}, {"authorId": "2108176413", "name": "Yixin Liu"}, {"authorId": "1796245019", "name": "Zihuiwen Ye"}, {"authorId": "14199369", "name": "Zi-Yi Dou"}, {"authorId": "1700325", "name": "Graham Neubig"}]}, {"paperId": "127ee5675b3d36151a840c4b55b2e5946c9bd8d7", "externalIds": {"ACL": "2021.acl-demo.35", "DBLP": "conf/acl/Horn21", "DOI": "10.18653/v1/2021.acl-demo.35", "CorpusId": 237368935}, "corpusId": 237368935, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/127ee5675b3d36151a840c4b55b2e5946c9bd8d7", "title": "Exploring Word Usage Change with Continuously Evolving Embeddings", "abstract": "The usage of individual words can change over time, for example, when words experience a semantic shift. As text datasets generally comprise documents that were collected over a longer period of time, examining word usage changes in a corpus can often reveal interesting patterns. In this paper, we introduce a simple and intuitive way to track word usage changes via continuously evolving embeddings, computed as a weighted running average of transformer-based contextualized embeddings. We demonstrate our approach on a corpus of recent New York Times article snippets and provide code for an easy to use web app to conveniently explore semantic shifts with interactive plots.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 43, "citationCount": 2, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "290-297"}, "authors": [{"authorId": "2125297534", "name": "Franziska Horn"}]}, {"paperId": "4f05aba9dea39063c77f3f186aab4547dde2993e", "externalIds": {"ACL": "2021.acl-demo.36", "DBLP": "journals/corr/abs-2106-04559", "ArXiv": "2106.04559", "DOI": "10.18653/v1/2021.acl-demo.36", "CorpusId": 235368091}, "corpusId": 235368091, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4f05aba9dea39063c77f3f186aab4547dde2993e", "title": "TURING: an Accurate and Interpretable Multi-Hypothesis Cross-Domain Natural Language Database Interface", "abstract": "A natural language database interface (NLDB) can democratize data-driven insights for non-technical users. However, existing Text-to-SQL semantic parsers cannot achieve high enough accuracy in the cross-database setting to allow good usability in practice. This work presents TURING, a NLDB system toward bridging this gap. The cross-domain semantic parser of TURING with our novel value prediction method achieves 75.1% execution accuracy, and 78.3% top-5 beam execution accuracy on the Spider validation set (Yu et al., 2018b). To benefit from the higher beam accuracy, we design an interactive system where the SQL hypotheses in the beam are explained step-by-step in natural language, with their differences highlighted. The user can then compare and judge the hypotheses to select which one reflects their intention if any. The English explanations of SQL queries in TURING are produced by our high-precision natural language generation system based on synchronous grammars.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 24, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.36.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-08", "journal": {"pages": "298-305"}, "authors": [{"authorId": "145612634", "name": "Peng Xu"}, {"authorId": "2088910563", "name": "Wenjie Zi"}, {"authorId": "1387982258", "name": "H. Shahidi"}, {"authorId": "1400413081", "name": "'Akos K'ad'ar"}, {"authorId": "1995851674", "name": "Keyi Tang"}, {"authorId": "144205313", "name": "Wei Yang"}, {"authorId": "8467956", "name": "Jawad Ateeq"}, {"authorId": "82258487", "name": "Harsh Barot"}, {"authorId": "2107696510", "name": "Meidan Alon"}, {"authorId": "2902068", "name": "Yanshuai Cao"}]}, {"paperId": "a49b0997efaec2db61109a6deed1512672c3cd0c", "externalIds": {"ArXiv": "2104.00290", "DBLP": "conf/acl/GowdaZMM21", "ACL": "2021.acl-demo.37", "DOI": "10.18653/v1/2021.acl-demo.37", "CorpusId": 232478372}, "corpusId": 232478372, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a49b0997efaec2db61109a6deed1512672c3cd0c", "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models", "abstract": "While there are more than 7000 languages in the world, most translation research efforts have targeted a few high resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTData, NLCodec and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 60, "citationCount": 15, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.37.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-04-01", "journal": {"pages": "306-316"}, "authors": [{"authorId": "145845766", "name": "Thamme Gowda"}, {"authorId": "2156121274", "name": "Zhao Zhang"}, {"authorId": "1725551", "name": "C. Mattmann"}, {"authorId": "143823227", "name": "Jonathan May"}]}, {"paperId": "a1fb0ebf04a929cf7748bb2b6b09f391dc63b9dc", "externalIds": {"ArXiv": "2105.01992", "ACL": "2021.acl-demo.38", "DBLP": "conf/acl/LiAYSY21", "DOI": "10.18653/v1/2021.acl-demo.38", "CorpusId": 233740100}, "corpusId": 233740100, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a1fb0ebf04a929cf7748bb2b6b09f391dc63b9dc", "title": "LEGOEval: An Open-Source Toolkit for Dialogue System Evaluation via Crowdsourcing", "abstract": "We present LEGOEval, an open-source toolkit that enables researchers to easily evaluate dialogue systems in a few lines of code using the online crowdsource platform, Amazon Mechanical Turk. Compared to existing toolkits, LEGOEval features a flexible task design by providing a Python API that maps to commonly used React.js interface components. Researchers can personalize their evaluation procedures easily with our built-in pages as if playing with LEGO blocks. Thus, LEGOEval provides a fast, consistent method for reproducing human evaluation results. Besides the flexible task design, LEGOEval also offers an easy API to review collected data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 29, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.38.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-05-05", "journal": {"name": "ArXiv", "volume": "abs/2105.01992"}, "authors": [{"authorId": "40058381", "name": "Yu Li"}, {"authorId": "2054031596", "name": "Josh Arnold"}, {"authorId": "2088384643", "name": "Feifan Yan"}, {"authorId": "8299781", "name": "Weiyan Shi"}, {"authorId": "1564034697", "name": "Zhou Yu"}]}, {"paperId": "ebc64974e9e0021984a0158b3c04b60327730a88", "externalIds": {"MAG": "3175795662", "ACL": "2021.acl-demo.39", "DBLP": "conf/acl/ChenLYLLJ21", "DOI": "10.18653/v1/2021.acl-demo.39", "CorpusId": 237698810}, "corpusId": 237698810, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ebc64974e9e0021984a0158b3c04b60327730a88", "title": "ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering", "abstract": "We present Retriever-Transducer-Checker (ReTraCk), a neural semantic parsing framework for large scale knowledge base question answering (KBQA). ReTraCk is designed as a modular framework to maintain high flexibility. It includes a retriever to retrieve relevant KB items efficiently, a transducer to generate logical form with syntax correctness guarantees and a checker to improve transduction procedure. ReTraCk is ranked at top1 overall performance on the GrailQA leaderboard and obtains highly competitive performance on the typical WebQuestionsSP benchmark. Our system can interact with users timely, demonstrating the efficiency of the proposed framework.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 33, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.39.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-08-01", "journal": {"pages": "325-336"}, "authors": [{"authorId": "23548372", "name": "Shuang Chen"}, {"authorId": "1409707585", "name": "Qian Liu"}, {"authorId": "2139425204", "name": "Zhiwei Yu"}, {"authorId": "50554693", "name": "Chin-Yew Lin"}, {"authorId": "153249455", "name": "Jian-Guang Lou"}, {"authorId": "144999037", "name": "F. Jiang"}]}, {"paperId": "1a64ccfa07381895b278b9e85173d2825f3331c3", "externalIds": {"ACL": "2021.acl-demo.40", "DBLP": "conf/acl/LisonBH21", "ArXiv": "2104.09683", "DOI": "10.18653/v1/2021.acl-demo.40", "CorpusId": 233307206}, "corpusId": 233307206, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1a64ccfa07381895b278b9e85173d2825f3331c3", "title": "skweak: Weak Supervision Made Easy for NLP", "abstract": "We present skweak, a versatile, Python-based software toolkit enabling NLP developers to apply weak supervision to a wide range of NLP tasks. Weak supervision is an emerging machine learning paradigm based on a simple idea: instead of labelling data points by hand, we use labelling functions derived from domain knowledge to automatically obtain annotations for a given dataset. The resulting labels are then aggregated with a generative model that estimates the accuracy (and possible confusions) of each labelling function. The skweak toolkit makes it easy to implement a large spectrum of labelling functions (such as heuristics, gazetteers, neural models or linguistic constraints) on text data, apply them on a corpus, and aggregate their results in a fully unsupervised fashion. skweak is especially designed to facilitate the use of weak supervision for NLP tasks such as text classification and sequence labelling. We illustrate the use of skweak for NER and sentiment analysis. skweak is released under an open-source license and is available at https://github.com/NorskRegnesentral/skweak", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 41, "citationCount": 26, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.40.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-04-19", "journal": {"pages": "337-346"}, "authors": [{"authorId": "1746165", "name": "Pierre Lison"}, {"authorId": "144435436", "name": "Jeremy Barnes"}, {"authorId": "51116121", "name": "A. Hubin"}]}, {"paperId": "37c4ec8d4fcaaf7f2c3f01a2fb30e9e41d8865a2", "externalIds": {"ACL": "2021.acl-demo.41", "DBLP": "conf/acl/WangLGZZZYZZPWL21", "ArXiv": "2103.11441", "DOI": "10.18653/v1/2021.acl-demo.41", "CorpusId": 232307242}, "corpusId": 232307242, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/37c4ec8d4fcaaf7f2c3f01a2fb30e9e41d8865a2", "title": "TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing", "abstract": "TextFlint is a multilingual robustness evaluation toolkit for NLP tasks that incorporates universal text transformation, task-specific transformation, adversarial attack, subpopulation, and their combinations to provide comprehensive robustness analyses. This enables practitioners to automatically evaluate their models from various aspects or to customize their evaluations as desired with just a few lines of code. TextFlint also generates complete analytical reports as well as targeted augmented data to address the shortcomings of the model in terms of its robustness. To guarantee acceptability, all the text transformations are linguistically based and all the transformed data selected (up to 100,000 texts) scored highly under human evaluation. To validate the utility, we performed large-scale empirical evaluations (over 67,000) on state-of-the-art deep learning models, classic supervised methods, and real-world systems. The toolkit is already available at https://github.com/textflint with all the evaluation results demonstrated at textflint.io.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 80, "citationCount": 78, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.41.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-03-21", "journal": {"name": "ArXiv", "volume": "abs/2103.11441"}, "authors": [{"authorId": "2067331064", "name": "Tao Gui"}, {"authorId": "2118451107", "name": "Xiao Wang"}, {"authorId": "1409702669", "name": "Qi Zhang"}, {"authorId": "2109185819", "name": "Qin Liu"}, {"authorId": "51192034", "name": "Yicheng Zou"}, {"authorId": "2148927523", "name": "Xin Zhou"}, {"authorId": "2058585152", "name": "Rui Zheng"}, {"authorId": "2111387504", "name": "Chong Zhang"}, {"authorId": "1490734800", "name": "Qinzhuo Wu"}, {"authorId": "65846898", "name": "Jiacheng Ye"}, {"authorId": "2056769063", "name": "Zexiong Pang"}, {"authorId": "2154716854", "name": "Yongxin Zhang"}, {"authorId": "2145371018", "name": "Zhengyan Li"}, {"authorId": "151482614", "name": "Ruotian Ma"}, {"authorId": "1411255713", "name": "Zichu Fei"}, {"authorId": "2057920426", "name": "Ruijian Cai"}, {"authorId": "2145804572", "name": "Jun Zhao"}, {"authorId": "4549353", "name": "Xinwu Hu"}, {"authorId": "113087283", "name": "Zhiheng Yan"}, {"authorId": "2110747493", "name": "Yiding Tan"}, {"authorId": "2189586856", "name": "Yuan Hu"}, {"authorId": "1380102745", "name": "Qiyuan Bian"}, {"authorId": "2109408939", "name": "Zhihua Liu"}, {"authorId": "2087090943", "name": "Bolin Zhu"}, {"authorId": "49841830", "name": "Shan Qin"}, {"authorId": "2151054", "name": "Xiaoyu Xing"}, {"authorId": "41037252", "name": "Jinlan Fu"}, {"authorId": "1591125925", "name": "Yue Zhang"}, {"authorId": "24859244", "name": "Minlong Peng"}, {"authorId": "2152196565", "name": "Xiaoqing Zheng"}, {"authorId": "2110347068", "name": "Yaqian Zhou"}, {"authorId": "2118602528", "name": "Zhongyu Wei"}, {"authorId": "1767521", "name": "Xipeng Qiu"}, {"authorId": "1790227", "name": "Xuanjing Huang"}]}, {"paperId": "84e5815125b8c5b81a5e6554e1044b057020cf89", "externalIds": {"ACL": "2021.acl-demo.42", "DBLP": "conf/acl/HsuCYHK21", "DOI": "10.18653/v1/2021.acl-demo.42", "CorpusId": 237263087}, "corpusId": 237263087, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/84e5815125b8c5b81a5e6554e1044b057020cf89", "title": "Stretch-VST: Getting Flexible With Visual Stories", "abstract": "In visual storytelling, a short story is generated based on a given image sequence. Despite years of work, most visual storytelling models remain limited in terms of the generated stories\u2019 fixed length: most models produce stories with exactly five sentences because five-sentence stories dominate the training data. The fix-length stories carry limited details and provide ambiguous textual information to the readers. Therefore, we propose to \u201cstretch\u201d the stories, which create the potential to present in-depth visual details. This paper presents Stretch-VST, a visual storytelling framework that enables the generation of prolonged stories by adding appropriate knowledge, which is selected by the proposed scoring function. We propose a length-controlled Transformer to generate long stories. This model introduces novel positional encoding methods to maintain story quality with lengthy inputs. Experiments confirm that long stories are generated without deteriorating the quality. The human evaluation further shows that Stretch-VST can provide better focus and detail when stories are prolonged compared to state of the art. We create a webpage to demonstrate our prolonged capability.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 28, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "356-362"}, "authors": [{"authorId": "48162772", "name": "Chi-Yang Hsu"}, {"authorId": "9628638", "name": "Yun-Wei Chu"}, {"authorId": "2109191163", "name": "Tsai-Lun Yang"}, {"authorId": "144188081", "name": "Ting-Hao 'Kenneth' Huang"}, {"authorId": "1746959", "name": "Lun-Wei Ku"}]}, {"paperId": "95968b89040146cb015827aee8ff6f77d67bbaf1", "externalIds": {"ACL": "2021.acl-demo.43", "ArXiv": "2009.09191", "MAG": "3087231533", "DBLP": "journals/corr/abs-2009-09191", "DOI": "10.18653/v1/2021.acl-demo.43", "CorpusId": 221819315}, "corpusId": 221819315, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/95968b89040146cb015827aee8ff6f77d67bbaf1", "title": "OpenAttack: An Open-source Textual Adversarial Attack Toolkit", "abstract": "Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at https://github.com/thunlp/OpenAttack.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 45, "citationCount": 76, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-demo.43.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-19", "journal": {"pages": "363-371"}, "authors": [{"authorId": "1398454307", "name": "Guoyang Zeng"}, {"authorId": "51466208", "name": "Fanchao Qi"}, {"authorId": "1955831254", "name": "Qianrui Zhou"}, {"authorId": "143691153", "name": "Ting Zhang"}, {"authorId": "1955614986", "name": "Bairu Hou"}, {"authorId": "49253441", "name": "Yuan Zang"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "1753344", "name": "Maosong Sun"}]}, {"paperId": "8c689903f0d82b5e7a71e0d4c78b78e13faee68b", "externalIds": {"ACL": "2020.acl-main.1", "DBLP": "journals/corr/abs-2005-02721", "MAG": "3023983208", "ArXiv": "2005.02721", "DOI": "10.18653/v1/2020.acl-main.1", "CorpusId": 218516666}, "corpusId": 218516666, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8c689903f0d82b5e7a71e0d4c78b78e13faee68b", "title": "Learning to Understand Child-directed and Adult-directed Speech", "abstract": "Speech directed to children differs from adult-directed speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.02721", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-06", "journal": {"pages": "1-6"}, "authors": [{"authorId": "7805500", "name": "Lieke Gelderloos"}, {"authorId": "2756960", "name": "Grzegorz Chrupa\u0142a"}, {"authorId": "103538973", "name": "A. Alishahi"}]}, {"paperId": "91b19bc20d18e9691e4c4ba5e1a02918db01bc1f", "externalIds": {"DBLP": "conf/acl/RinaldiTC20", "MAG": "3035250245", "ACL": "2020.acl-main.2", "DOI": "10.18653/v1/2020.acl-main.2", "CorpusId": 220047307}, "corpusId": 220047307, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/91b19bc20d18e9691e4c4ba5e1a02918db01bc1f", "title": "Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts", "abstract": "Accurately diagnosing depression is difficult\u2013 requiring time-intensive interviews, assessments, and analysis. Hence, automated methods that can assess linguistic patterns in these interviews could help psychiatric professionals make faster, more informed decisions about diagnosis. We propose JLPC, a model that analyzes interview transcripts to identify depression while jointly categorizing interview prompts into latent categories. This latent categorization allows the model to define high-level conversational contexts that influence patterns of language in depressed individuals. We show that the proposed model not only outperforms competitive baselines, but that its latent prompt categories provide psycholinguistic insights about depression.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 49, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.2.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "7-18"}, "authors": [{"authorId": "19320780", "name": "Alex Rinaldi"}, {"authorId": "2457504", "name": "J. E. Tree"}, {"authorId": "37202877", "name": "Snigdha Chaturvedi"}]}, {"paperId": "894009cd79adab9d32132ea7ea79c8c028d68d3b", "externalIds": {"ArXiv": "2004.11727", "ACL": "2020.acl-main.3", "DBLP": "conf/acl/LiuWXF20", "MAG": "3020629500", "DOI": "10.18653/v1/2020.acl-main.3", "CorpusId": 216144770}, "corpusId": 216144770, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/894009cd79adab9d32132ea7ea79c8c028d68d3b", "title": "Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling", "abstract": "As an essential task in task-oriented dialog systems, slot filling requires extensive training data in a certain domain. However, such data are not always available. Hence, cross-domain slot filling has naturally arisen to cope with this data scarcity problem. In this paper, we propose a Coarse-to-fine approach (Coach) for cross-domain slot filling. Our model first learns the general pattern of slot entities by detecting whether the tokens are slot entities or not. It then predicts the specific types for the slot entities. In addition, we propose a template regularization approach to improve the adaptation robustness by regularizing the representation of utterances based on utterance templates. Experimental results show that our model significantly outperforms state-of-the-art approaches in slot filling. Furthermore, our model can also be applied to the cross-domain named entity recognition task, and it achieves better adaptation performance than other existing baselines. The code is available at https://github.com/zliucr/coach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 74, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.3.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.11727"}, "authors": [{"authorId": "152613855", "name": "Zihan Liu"}, {"authorId": "9162688", "name": "Genta Indra Winata"}, {"authorId": "145011005", "name": "Peng Xu"}, {"authorId": "40539650", "name": "Pascale Fung"}]}, {"paperId": "5ca5c40661eeec6dda1d80d65dca1b3d8a5f4132", "externalIds": {"DBLP": "conf/acl/ZhaoLK20", "MAG": "3034950505", "ACL": "2020.acl-main.4", "ArXiv": "2004.04908", "DOI": "10.18653/v1/2020.acl-main.4", "CorpusId": 215737016}, "corpusId": 215737016, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5ca5c40661eeec6dda1d80d65dca1b3d8a5f4132", "title": "Designing Precise and Robust Dialogue Response Evaluators", "abstract": "Automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. However, existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. In this work, we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. Experimental results demonstrate that the proposed evaluator achieves a strong correlation (> 0.6) with human judgement and generalizes robustly to diverse responses and corpora. We open-source the code and data in https://github.com/ZHAOTING/dialog-processing.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 39, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.4.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-10", "journal": {"name": "ArXiv", "volume": "abs/2004.04908"}, "authors": [{"authorId": "46887780", "name": "Tianyu Zhao"}, {"authorId": "1939089", "name": "Divesh Lala"}, {"authorId": "1717105", "name": "Tatsuya Kawahara"}]}, {"paperId": "93cdca34fa4b3efa937e4ec374c4ab9cd4270c95", "externalIds": {"ACL": "2020.acl-main.5", "DBLP": "conf/acl/OuyangCDZHC20", "MAG": "3035594326", "DOI": "10.18653/v1/2020.acl-main.5", "CorpusId": 220050380}, "corpusId": 220050380, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/93cdca34fa4b3efa937e4ec374c4ab9cd4270c95", "title": "Dialogue State Tracking with Explicit Slot Connection Modeling", "abstract": "Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to explicitly consider slot correlations across different domains. Given a target slot, the slot connecting mechanism in DST-SC can infer its source slot and copy the source slot value directly, thus significantly reducing the difficulty of learning and reasoning. Experimental results verify the benefits of explicit slot connection modeling, and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 22, "citationCount": 34, "influentialCitationCount": 4, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "34-40"}, "authors": [{"authorId": "2070558572", "name": "Yawen Ouyang"}, {"authorId": "1753948811", "name": "Moxin Chen"}, {"authorId": "3035069", "name": "Xinyu Dai"}, {"authorId": "2651937", "name": "Yinggong Zhao"}, {"authorId": "2046010", "name": "Shujian Huang"}, {"authorId": "1838162", "name": "Jiajun Chen"}]}, {"paperId": "44b0af43ab04d92bcce98d975a7d448d27c78673", "externalIds": {"ACL": "2020.acl-main.6", "DBLP": "conf/acl/LinJHWC20", "MAG": "3034758256", "DOI": "10.18653/v1/2020.acl-main.6", "CorpusId": 220046799}, "corpusId": 220046799, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/44b0af43ab04d92bcce98d975a7d448d27c78673", "title": "Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy", "abstract": "Knowledge-driven conversation approaches have achieved remarkable research attention recently. However, generating an informative response with multiple relevant knowledge without losing fluency and coherence is still one of the main challenges. To address this issue, this paper proposes a method that uses recurrent knowledge interaction among response decoding steps to incorporate appropriate knowledge. Furthermore, we introduce a knowledge copy mechanism using a knowledge-aware pointer network to copy words from external knowledge according to knowledge attention distribution. Our joint neural conversation model which integrates recurrent Knowledge-Interaction and knowledge Copy (KIC) performs well on generating informative responses. Experiments demonstrate that our model with fewer parameters yields significant improvements over competitive baselines on two datasets Wizard-of-Wikipedia(average Bleu +87%; abs.: 0.034) and DuConv(average Bleu +20%; abs.: 0.047)) with different knowledge formats (textual & structured) and different languages (English & Chinese).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 42, "influentialCitationCount": 7, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "41-52"}, "authors": [{"authorId": "2109496214", "name": "X. Lin"}, {"authorId": "2070542993", "name": "Weiyu Jian"}, {"authorId": "51235263", "name": "Jianshan He"}, {"authorId": "1799672", "name": "Taifeng Wang"}, {"authorId": "2057047939", "name": "Wei Chu"}]}, {"paperId": "ea77490525d6e354f77d74e3f3a085ff7e7eb264", "externalIds": {"DBLP": "journals/corr/abs-1911-02390", "ArXiv": "1911.02390", "ACL": "2020.acl-main.7", "MAG": "3034999754", "DOI": "10.18653/v1/2020.acl-main.7", "CorpusId": 207881153}, "corpusId": 207881153, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ea77490525d6e354f77d74e3f3a085ff7e7eb264", "title": "Guiding Variational Response Generator to Exploit Persona", "abstract": "Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of the promising progress achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via End-to-End learning. This paper proposes to adopt the personality-related characteristics of human conversations into variational response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant responses. Besides, to reasonably evaluate the performances of various persona modeling approaches, this paper further presents three direct persona-oriented metrics from different perspectives. The experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation, and the metrics are reasonable to evaluate the results.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 36, "citationCount": 36, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.7.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-06", "journal": {"pages": "53-65"}, "authors": [{"authorId": "152365288", "name": "Bowen Wu"}, {"authorId": "2145712176", "name": "Mengyuan Li"}, {"authorId": "2108183338", "name": "Zongsheng Wang"}, {"authorId": "1519052027", "name": "Yifu Chen"}, {"authorId": "1758353", "name": "Derek F. Wong"}, {"authorId": "2068041503", "name": "Qihang Feng"}, {"authorId": "2118226003", "name": "Junhong Huang"}, {"authorId": "2806178", "name": "Baoxun Wang"}]}, {"paperId": "87362e75d0c428cefffb28d7b175269dc8e1f068", "externalIds": {"MAG": "3034614576", "ACL": "2020.acl-main.8", "ArXiv": "2005.06114", "DBLP": "journals/corr/abs-2005-06114", "DOI": "10.18653/v1/2020.acl-main.8", "CorpusId": 218613714}, "corpusId": 218613714, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/87362e75d0c428cefffb28d7b175269dc8e1f068", "title": "Large Scale Multi-Actor Generative Dialog Modeling", "abstract": "Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper addresses these issues by controlling an agent\u2019s persona upon generation via conditioning on prior conversations of a target actor. In doing so, we are able to utilize more abstract patterns within a person\u2019s speech and better emulate them in generated responses. This work introduces the Generative Conversation Control model, an augmented and fine-tuned GPT-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor\u2019s persona. We introduce an accompanying data collection procedure to obtain 10.3M conversations from 6 months worth of Reddit comments. We demonstrate that scaling model sizes from 117M to 8.3B parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held out Reddit conversations. Increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism (31% increased to 37% preference), style matching (37% to 42%), grammar and content quality (29% to 42%), and conversation coherency (32% to 40%). We find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. Through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 21, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.06114", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-13", "journal": {"name": "ArXiv", "volume": "abs/2005.06114"}, "authors": [{"authorId": "2074330244", "name": "Alex Boyd"}, {"authorId": "41158993", "name": "Raul Puri"}, {"authorId": "1911755", "name": "M. Shoeybi"}, {"authorId": "66870756", "name": "M. Patwary"}, {"authorId": "2301680", "name": "Bryan Catanzaro"}]}, {"paperId": "6ebfbc954b9975d2f2651f380b9bdf46ae963178", "externalIds": {"ArXiv": "1910.07931", "MAG": "2980661038", "DBLP": "journals/corr/abs-1910-07931", "ACL": "2020.acl-main.9", "DOI": "10.18653/v1/2020.acl-main.9", "CorpusId": 204744108}, "corpusId": 204744108, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6ebfbc954b9975d2f2651f380b9bdf46ae963178", "title": "PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable", "abstract": "Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 38, "citationCount": 215, "influentialCitationCount": 31, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.9.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-17", "journal": {"pages": "85-96"}, "authors": [{"authorId": "2026806395", "name": "Siqi Bao"}, {"authorId": "46350360", "name": "H. He"}, {"authorId": "2145903238", "name": "Fan Wang"}, {"authorId": "40354707", "name": "Hua Wu"}]}, {"paperId": "e2ac8131af3fe175d5f83e968dae9fcf6be64d72", "externalIds": {"DBLP": "conf/acl/LiYQCLL20", "ACL": "2020.acl-main.10", "MAG": "3034643843", "DOI": "10.18653/v1/2020.acl-main.10", "CorpusId": 220050117}, "corpusId": 220050117, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e2ac8131af3fe175d5f83e968dae9fcf6be64d72", "title": "Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network", "abstract": "Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 14, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "97-106"}, "authors": [{"authorId": "48514481", "name": "Yangming Li"}, {"authorId": "39922478", "name": "K. Yao"}, {"authorId": null, "name": "Libo Qin"}, {"authorId": "2256319", "name": "Wanxiang Che"}, {"authorId": "2108672800", "name": "Xiaolong Li"}, {"authorId": "40282288", "name": "Ting Liu"}]}, {"paperId": "9dddb9a2ef2d380866c19de2830403c4fa9f48b5", "externalIds": {"MAG": "3034861927", "ACL": "2020.acl-main.11", "ArXiv": "2005.08866", "DBLP": "journals/corr/abs-2005-08866", "DOI": "10.18653/v1/2020.acl-main.11", "CorpusId": 218674499}, "corpusId": 218674499, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9dddb9a2ef2d380866c19de2830403c4fa9f48b5", "title": "Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations", "abstract": "We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task. This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT (Henderson et al., 2019). We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a BERT-based span extractor. In order to inspire more work on span extraction for the slot-filling task, we also release RESTAURANTS-8K, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 53, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.11.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-18", "journal": {"name": "ArXiv", "volume": "abs/2005.08866"}, "authors": [{"authorId": "22223875", "name": "Sam Coope"}, {"authorId": "1703120788", "name": "Tyler Farghly"}, {"authorId": "51176335", "name": "D. Gerz"}, {"authorId": "1747849", "name": "Ivan Vulic"}, {"authorId": "145133553", "name": "Matthew Henderson"}]}, {"paperId": "ec3d5bdfda2c5c841c2481f5da123b2c086e6f5c", "externalIds": {"MAG": "3034284249", "ACL": "2020.acl-main.12", "DBLP": "journals/corr/abs-2005-00891", "ArXiv": "2005.00891", "DOI": "10.18653/v1/2020.acl-main.12", "CorpusId": 214816511}, "corpusId": 214816511, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ec3d5bdfda2c5c841c2481f5da123b2c086e6f5c", "title": "Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking", "abstract": "Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21%.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 50, "citationCount": 78, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.12.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00891"}, "authors": [{"authorId": "1382048113", "name": "Giovanni Campagna"}, {"authorId": "1491516389", "name": "Agata Foryciarz"}, {"authorId": "40879549", "name": "M. Moradshahi"}, {"authorId": "39682108", "name": "M. Lam"}]}, {"paperId": "294ac606a8ac47e337706e20bfa195ba6d049394", "externalIds": {"DBLP": "conf/acl/HungHC20", "MAG": "3034849249", "ACL": "2020.acl-main.13", "DOI": "10.18653/v1/2020.acl-main.13", "CorpusId": 220047296}, "corpusId": 220047296, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/294ac606a8ac47e337706e20bfa195ba6d049394", "title": "A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle", "abstract": "This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 16, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "133-138"}, "authors": [{"authorId": "1723401722", "name": "Shyh-Shiun Hung"}, {"authorId": "2611607", "name": "Hen-Hsen Huang"}, {"authorId": "153924342", "name": "Hsin-Hsi Chen"}]}, {"paperId": "583887ae83f056f8a7a73e4acd9a40466ab7b6f4", "externalIds": {"MAG": "3035563811", "DBLP": "conf/acl/HeWGH20", "ACL": "2020.acl-main.14", "DOI": "10.18653/v1/2020.acl-main.14", "CorpusId": 220047183}, "corpusId": 220047183, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/583887ae83f056f8a7a73e4acd9a40466ab7b6f4", "title": "TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition", "abstract": "Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 19, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.14.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "139-148"}, "authors": [{"authorId": "1724097", "name": "Ruifang He"}, {"authorId": "144469308", "name": "Jian Wang"}, {"authorId": "34049807", "name": "Fengyu Guo"}, {"authorId": "2112896208", "name": "Yugui Han"}]}, {"paperId": "bed87e8fb3e7e9bc87e1c2ee459ae405a35d3267", "externalIds": {"ACL": "2020.acl-main.15", "ArXiv": "2004.10454", "DBLP": "journals/corr/abs-2004-10454", "MAG": "3018753103", "DOI": "10.18653/v1/2020.acl-main.15", "CorpusId": 216056470}, "corpusId": 216056470, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bed87e8fb3e7e9bc87e1c2ee459ae405a35d3267", "title": "A Study of Non-autoregressive Model for Sequence Generation", "abstract": "Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge distillation and source-target alignment can help NAR models. Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks. We have several interesting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least. 2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 49, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.15.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-22", "journal": {"name": "ArXiv", "volume": "abs/2004.10454"}, "authors": [{"authorId": "1500435161", "name": "Yi Ren"}, {"authorId": "48211720", "name": "Jinglin Liu"}, {"authorId": "48391466", "name": "Xu Tan"}, {"authorId": "47601191", "name": "Sheng Zhao"}, {"authorId": "47122432", "name": "Zhou Zhao"}, {"authorId": "2110264337", "name": "Tie-Yan Liu"}]}, {"paperId": "b88c52150995965a66dc774603444ad570c3941d", "externalIds": {"MAG": "3034180655", "ArXiv": "2005.00246", "ACL": "2020.acl-main.16", "DBLP": "journals/corr/abs-2005-00246", "DOI": "10.18653/v1/2020.acl-main.16", "CorpusId": 218470234}, "corpusId": 218470234, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b88c52150995965a66dc774603444ad570c3941d", "title": "Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage", "abstract": "Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations. We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage. We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages directly at training time both existing English annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it generates first an English caption and then a corresponding target-language caption. We show that PLuGS models outperform other candidate solutions in evaluations performed over 5 different target languages, under a large-domain testset using images from the Open Images dataset. Furthermore, we find an interesting effect where the English captions generated by the PLuGS models are better than the captions generated by the original, monolingual English model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.16.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00246"}, "authors": [{"authorId": "2904055", "name": "Ashish V. Thapliyal"}, {"authorId": "1737285", "name": "Radu Soricut"}]}, {"paperId": "ad47d109e7e5edfd2f6938f2e0e2cb54bbbf9f98", "externalIds": {"MAG": "3040613747", "DBLP": "conf/acl/IsoQL20", "ACL": "2020.acl-main.17", "ArXiv": "2007.00916", "DOI": "10.18653/v1/2020.acl-main.17", "CorpusId": 220047315}, "corpusId": 220047315, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ad47d109e7e5edfd2f6938f2e0e2cb54bbbf9f98", "title": "Fact-based Text Editing", "abstract": "We propose a novel text editing task, referred to as fact-based text editing, in which the goal is to revise a given document to better describe the facts in a knowledge base (e.g., several triples). The task is important in practice because reflecting the truth is a common requirement in text editing. First, we propose a method for automatically generating a dataset for research on fact-based text editing, where each instance consists of a draft text, a revised text, and several facts represented in triples. We apply the method into two public table-to-text datasets, obtaining two new datasets consisting of 233k and 37k instances, respectively. Next, we propose a new neural network architecture for fact-based text editing, called FactEditor, which edits a draft text by referring to given facts using a buffer, a stream, and a memory. A straightforward approach to address the problem would be to employ an encoder-decoder model. Our experimental results on the two datasets show that FactEditor outperforms the encoder-decoder approach in terms of fidelity and fluency. The results also show that FactEditor conducts inference faster than the encoder-decoder approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 42, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.17.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"name": "ArXiv", "volume": "abs/2007.00916"}, "authors": [{"authorId": "7782351", "name": "Hayate Iso"}, {"authorId": "2055528529", "name": "Chao Qiao"}, {"authorId": "2145571830", "name": "Hang Li"}]}, {"paperId": "4095018e41da90f623af8be7c6f56f597b9cc136", "externalIds": {"MAG": "2939363422", "DBLP": "journals/corr/abs-1904-09521", "ACL": "2020.acl-main.18", "ArXiv": "1904.09521", "DOI": "10.18653/v1/2020.acl-main.18", "CorpusId": 128345548}, "corpusId": 128345548, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4095018e41da90f623af8be7c6f56f597b9cc136", "title": "Few-Shot NLG with Pre-Trained Language Model", "abstract": "Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of few-shot natural language generation. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement. Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 46, "citationCount": 120, "influentialCitationCount": 24, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1904.09521", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-04-21", "journal": {"name": "ArXiv", "volume": "abs/1904.09521"}, "authors": [{"authorId": "48354180", "name": "Zhiyu Chen"}, {"authorId": "2289246", "name": "H. Eavani"}, {"authorId": "3097158", "name": "Yinyin Liu"}, {"authorId": "1682479", "name": "William Yang Wang"}]}, {"paperId": "c8249315df50cda85ef76bb149e852ea2adfbaf0", "externalIds": {"ArXiv": "2005.10464", "MAG": "3026805444", "ACL": "2020.acl-main.19", "DBLP": "journals/corr/abs-2005-10464", "DOI": "10.18653/v1/2020.acl-main.19", "CorpusId": 218763640}, "corpusId": 218763640, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c8249315df50cda85ef76bb149e852ea2adfbaf0", "title": "Fluent Response Generation for Conversational Question Answering", "abstract": "Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our model\u2019s scalability by conducting tests on the CoQA dataset. The code and data are available at https://github.com/abaheti95/QADialogSystem.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 45, "citationCount": 21, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.19.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-21", "journal": {"pages": "191-207"}, "authors": [{"authorId": "3458166", "name": "Ashutosh Baheti"}, {"authorId": "1863425", "name": "Alan Ritter"}, {"authorId": "50044599", "name": "Kevin Small"}]}, {"paperId": "14988524c74fe0467571033a31c16d36d25f52bd", "externalIds": {"ArXiv": "2005.13837", "MAG": "3035500185", "DBLP": "conf/acl/LeeLJKH20", "ACL": "2020.acl-main.20", "DOI": "10.18653/v1/2020.acl-main.20", "CorpusId": 218971632}, "corpusId": 218971632, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/14988524c74fe0467571033a31c16d36d25f52bd", "title": "Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs", "abstract": "One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation. An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia). In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency. We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models. The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 69, "citationCount": 49, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.20.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.13837"}, "authors": [{"authorId": "2115673552", "name": "Dong Bok Lee"}, {"authorId": "1472875852", "name": "Seanie Lee"}, {"authorId": "2097273505", "name": "Woo Tae Jeong"}, {"authorId": "2145184318", "name": "Donghwan Kim"}, {"authorId": "35788904", "name": "Sung Ju Hwang"}]}, {"paperId": "2456acf35a627f4aceaf79f1ad055d7fd772db96", "externalIds": {"ACL": "2020.acl-main.21", "MAG": "3034239264", "DBLP": "conf/acl/ChaiW20", "DOI": "10.18653/v1/2020.acl-main.21", "CorpusId": 220045394}, "corpusId": 220045394, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2456acf35a627f4aceaf79f1ad055d7fd772db96", "title": "Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction", "abstract": "Traditional Question Generation (TQG) aims to generate a question given an input passage and an answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to produce a series of interconnected questions. Since the frequently occurred information omission and coreference between questions, SQG is rather challenging. Prior works regarded SQG as a dialog generation task and recurrently produced each question. However, they suffered from problems caused by error cascades and could only capture limited context dependencies. To this end, we generate questions in a semi-autoregressive way. Our model divides questions into different groups and generates each group of them in parallel. During this process, it builds two graphs focusing on information from passages, answers respectively and performs dual-graph interaction to get information for generation. Besides, we design an answer-aware attention mechanism and the coarse-to-fine generation scenario. Experiments on our new dataset containing 81.9K questions show that our model substantially outperforms prior works.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 12, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.21.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "225-237"}, "authors": [{"authorId": "114904655", "name": "Zi Chai"}, {"authorId": "145078589", "name": "Xiaojun Wan"}]}, {"paperId": "0c5480ba0e07da4995006bf7959e6a2e7b3e279d", "externalIds": {"MAG": "3034531294", "ArXiv": "2005.02013", "ACL": "2020.acl-main.22", "DBLP": "journals/corr/abs-2005-02013", "DOI": "10.18653/v1/2020.acl-main.22", "CorpusId": 218502606}, "corpusId": 218502606, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0c5480ba0e07da4995006bf7959e6a2e7b3e279d", "title": "Neural Syntactic Preordering for Controlled Paraphrase Generation", "abstract": "Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly \u201creorder\u201d the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 54, "citationCount": 80, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.22.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"name": "ArXiv", "volume": "abs/2005.02013"}, "authors": [{"authorId": "9531287", "name": "Tanya Goyal"}, {"authorId": "1814094", "name": "Greg Durrett"}]}, {"paperId": "d5a2638856d2428610fbda7dd4aafe7f5c92edb0", "externalIds": {"ACL": "2020.acl-main.23", "MAG": "2984715914", "DBLP": "journals/corr/abs-1911-03882", "ArXiv": "1911.03882", "DOI": "10.18653/v1/2020.acl-main.23", "CorpusId": 207853283}, "corpusId": 207853283, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d5a2638856d2428610fbda7dd4aafe7f5c92edb0", "title": "Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders", "abstract": "Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents. Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion. When a new condition added, these techniques require full retraining. In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation. PPVAE decouples the text generation module from the condition representation module to allow \u201cone-to-many\u201d conditional generation. When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications. Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 42, "citationCount": 37, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.23.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-10", "journal": {"pages": "253-262"}, "authors": [{"authorId": "2087104526", "name": "Yu Duan"}, {"authorId": "51136528", "name": "Jiaxin Pei"}, {"authorId": "66247317", "name": "Canwen Xu"}, {"authorId": "2829009", "name": "Chenliang Li"}]}, {"paperId": "f02bb4bc0b711fd90d32bd7dadd505465d6b45e1", "externalIds": {"MAG": "3019688513", "ACL": "2020.acl-main.24", "DBLP": "journals/corr/abs-2004-11579", "ArXiv": "2004.11579", "DOI": "10.18653/v1/2020.acl-main.24", "CorpusId": 216144416}, "corpusId": 216144416, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f02bb4bc0b711fd90d32bd7dadd505465d6b45e1", "title": "Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order", "abstract": "Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 27, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.24.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-24", "journal": {"pages": "263-274"}, "authors": [{"authorId": "2048004675", "name": "Yi Liao"}, {"authorId": "2110310493", "name": "Xin Jiang"}, {"authorId": "1688015", "name": "Qun Liu"}]}, {"paperId": "2961ba3cffb20b1f1d6db0ecd2d632b46735c2ab", "externalIds": {"MAG": "3034470897", "DBLP": "conf/acl/TayBZBMT20", "ArXiv": "2004.06201", "ACL": "2020.acl-main.25", "DOI": "10.18653/v1/2020.acl-main.25", "CorpusId": 215754413}, "corpusId": 215754413, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2961ba3cffb20b1f1d6db0ecd2d632b46735c2ab", "title": "Reverse Engineering Configurations of Neural Text Generation Models", "abstract": "Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways. It is therefore desirable to develop a deeper understanding of the fundamental properties of such models. The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area. To this end, the extent and degree to which these artifacts surface in generated text is still unclear. In the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated some piece of text. Specifically, we conduct an extensive suite of diagnostic tests to observe whether modeling choices (e.g., sampling methods, top-k probabilities, model architectures, etc.) leave detectable artifacts in the text they generate. Our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone. This suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 23, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.25.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-13", "journal": {"name": "ArXiv", "volume": "abs/2004.06201"}, "authors": [{"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "11774695", "name": "Dara Bahri"}, {"authorId": "72117196", "name": "Che Zheng"}, {"authorId": "3202382", "name": "Clifford Brunk"}, {"authorId": "1680617", "name": "Donald Metzler"}, {"authorId": "49365095", "name": "A. Tomkins"}]}, {"paperId": "2d38837919d84a322884f3ef3c433995f3e5d519", "externalIds": {"DBLP": "conf/acl/YuBZLS20", "MAG": "3034818681", "ACL": "2020.acl-main.26", "ArXiv": "1911.01556", "DOI": "10.18653/v1/2020.acl-main.26", "CorpusId": 207779873}, "corpusId": 207779873, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2d38837919d84a322884f3ef3c433995f3e5d519", "title": "Review-based Question Generation with Adaptive Instance Transfer and Augmentation", "abstract": "While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need. We propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences. One major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review sentences. To obtain proper training instances for the generation model, we propose an iterative learning framework with adaptive instance transfer and augmentation. To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 44, "citationCount": 16, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.26.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2019-11-05", "journal": {"name": "ArXiv", "volume": "abs/1911.01556"}, "authors": [{"authorId": "144873018", "name": "Qian Yu"}, {"authorId": "1996394", "name": "Lidong Bing"}, {"authorId": "2115485", "name": "Qiong Zhang"}, {"authorId": "144594306", "name": "Wai Lam"}, {"authorId": "2059080424", "name": "Luo Si"}]}, {"paperId": "eccd7060c4f81e92d65601f5c7ac7cade2f68807", "externalIds": {"MAG": "3034716028", "DBLP": "conf/acl/CaiLXLHC20", "ACL": "2020.acl-main.27", "ArXiv": "2005.02835", "DOI": "10.18653/v1/2020.acl-main.27", "CorpusId": 218516704}, "corpusId": 218516704, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/eccd7060c4f81e92d65601f5c7ac7cade2f68807", "title": "TAG : Type Auxiliary Guiding for Code Comment Generation", "abstract": "Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node. Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code. We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework. Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 20, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.27.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-06", "journal": {"pages": "291-301"}, "authors": [{"authorId": "39913331", "name": "Ruichu Cai"}, {"authorId": "50437563", "name": "Zhihao Liang"}, {"authorId": "48309993", "name": "Boyan Xu"}, {"authorId": "46947549", "name": "Zijian Li"}, {"authorId": "2106141565", "name": "Yuexing Hao"}, {"authorId": "2118360502", "name": "Yao Chen"}]}, {"paperId": "aec95d1124378f46dc17ef1d4ad07ae37ee72ffe", "externalIds": {"MAG": "2972136110", "ACL": "2020.acl-main.28", "DBLP": "journals/corr/abs-1909-03588", "ArXiv": "1909.03588", "DOI": "10.18653/v1/2020.acl-main.28", "CorpusId": 202537332}, "corpusId": 202537332, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/aec95d1124378f46dc17ef1d4ad07ae37ee72ffe", "title": "Unsupervised Paraphrasing by Simulated Annealing", "abstract": "We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases. UPSA searches the sentence space towards this objective by performing a sequence of local editing. We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter. Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations. Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 55, "citationCount": 73, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.28.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-09", "journal": {"pages": "302-312"}, "authors": [{"authorId": "9137409", "name": "Xianggen Liu"}, {"authorId": "38956216", "name": "Lili Mou"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "2111826330", "name": "Hao Zhou"}, {"authorId": "2108485135", "name": "Jie Zhou"}, {"authorId": "15168128", "name": "Sen Song"}]}, {"paperId": "8ebbe4f00b3d28a51e1a6d5c5dcf63062e295f84", "externalIds": {"ACL": "2020.acl-main.29", "MAG": "3034327408", "DBLP": "conf/acl/BarrowJMMOR20", "DOI": "10.18653/v1/2020.acl-main.29", "CorpusId": 220045894}, "corpusId": 220045894, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8ebbe4f00b3d28a51e1a6d5c5dcf63062e295f84", "title": "A Joint Model for Document Segmentation and Segment Labeling", "abstract": "Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 38, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.29.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "313-322"}, "authors": [{"authorId": "40080808", "name": "Joe Barrow"}, {"authorId": "39878379", "name": "R. Jain"}, {"authorId": "2852035", "name": "Vlad I. Morariu"}, {"authorId": "1977256", "name": "Varun Manjunatha"}, {"authorId": "1737250", "name": "Douglas W. Oard"}, {"authorId": "1680292", "name": "P. Resnik"}]}, {"paperId": "4fa4e39ade763085a75146392b997b7d4da49725", "externalIds": {"ACL": "2020.acl-main.30", "DBLP": "conf/acl/MekalaS20", "MAG": "3034588688", "DOI": "10.18653/v1/2020.acl-main.30", "CorpusId": 220045126}, "corpusId": 220045126, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4fa4e39ade763085a75146392b997b7d4da49725", "title": "Contextualized Weak Supervision for Text Classification", "abstract": "Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 77, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.30.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "323-333"}, "authors": [{"authorId": "7565696", "name": "Dheeraj Mekala"}, {"authorId": "2884976", "name": "Jingbo Shang"}]}, {"paperId": "23ce8950b9360158c04ab0c1dcf9a73022b60673", "externalIds": {"MAG": "3023462908", "ACL": "2020.acl-main.31", "ArXiv": "2004.13826", "DBLP": "journals/corr/abs-2004-13826", "DOI": "10.18653/v1/2020.acl-main.31", "CorpusId": 216641848}, "corpusId": 216641848, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/23ce8950b9360158c04ab0c1dcf9a73022b60673", "title": "Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks", "abstract": "Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 22, "citationCount": 188, "influentialCitationCount": 31, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.31.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-22", "journal": {"pages": "334-339"}, "authors": [{"authorId": "2145056386", "name": "Yufeng Zhang"}, {"authorId": "2116329807", "name": "Xueli Yu"}, {"authorId": "72723327", "name": "Zeyu Cui"}, {"authorId": "50425438", "name": "Shu Wu"}, {"authorId": "2152090246", "name": "Zhongzheng Wen"}, {"authorId": "123865558", "name": "Liang Wang"}]}, {"paperId": "584f7c1216198e6db25927adce45033d8c0e6903", "externalIds": {"MAG": "3035332461", "DBLP": "journals/corr/abs-2004-12331", "ACL": "2020.acl-main.32", "ArXiv": "2004.12331", "DOI": "10.18653/v1/2020.acl-main.32", "CorpusId": 216553903}, "corpusId": 216553903, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/584f7c1216198e6db25927adce45033d8c0e6903", "title": "Neural Topic Modeling with Bidirectional Adversarial Training", "abstract": "Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 63, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.32.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.12331"}, "authors": [{"authorId": "40601625", "name": "Rui Wang"}, {"authorId": "153004050", "name": "Xuemeng Hu"}, {"authorId": "1725992", "name": "Deyu Zhou"}, {"authorId": "1704133", "name": "Yulan He"}, {"authorId": "20942909", "name": "Yuxuan Xiong"}, {"authorId": "2064449321", "name": "Chencheng Ye"}, {"authorId": "153194420", "name": "Haiyang Xu"}]}, {"paperId": "f84f519a63d1557fbbe2d0d8133bbd7ea5be0e2d", "externalIds": {"MAG": "3034693764", "DBLP": "conf/acl/OhashiTKCA20", "ACL": "2020.acl-main.33", "DOI": "10.18653/v1/2020.acl-main.33", "CorpusId": 220048538}, "corpusId": 220048538, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f84f519a63d1557fbbe2d0d8133bbd7ea5be0e2d", "title": "Text Classification with Negative Supervision", "abstract": "Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks. However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts. To address this problem, we propose a simple multitask learning model that uses negative supervision. Specifically, our model encourages texts with different labels to have distinct representations. Comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications, sentence and document classifications, and classifications in three different languages.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.33.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "351-357"}, "authors": [{"authorId": "1755134768", "name": "Sora Ohashi"}, {"authorId": "67032534", "name": "Junya Takayama"}, {"authorId": "1981103", "name": "Tomoyuki Kajiwara"}, {"authorId": "2427516", "name": "Chenhui Chu"}, {"authorId": "3043844", "name": "Yuki Arase"}]}, {"paperId": "9dee20d97d6c43fe48ebd8c3df47ffa6483d53f7", "externalIds": {"MAG": "3034330464", "ACL": "2020.acl-main.34", "DBLP": "conf/acl/ChenWUS20", "DOI": "10.18653/v1/2020.acl-main.34", "CorpusId": 220050045}, "corpusId": 220050045, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9dee20d97d6c43fe48ebd8c3df47ffa6483d53f7", "title": "Content Word Aware Neural Machine Translation", "abstract": "Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance. Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 20, "citationCount": 16, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.34.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "358-364"}, "authors": [{"authorId": "2849740", "name": "Kehai Chen"}, {"authorId": "108085542", "name": "Rui Wang"}, {"authorId": "1802277", "name": "M. Utiyama"}, {"authorId": "1698363", "name": "E. Sumita"}]}, {"paperId": "19dcc665dee21a8148f9bd07066332783d212126", "externalIds": {"DBLP": "journals/corr/abs-2005-01672", "MAG": "3034324324", "ACL": "2020.acl-main.35", "ArXiv": "2005.01672", "DOI": "10.18653/v1/2020.acl-main.35", "CorpusId": 218487295}, "corpusId": 218487295, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/19dcc665dee21a8148f9bd07066332783d212126", "title": "Evaluating Explanation Methods for Neural Machine Translation", "abstract": "Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model. As the exact computation for this metric is intractable, we employ an efficient approach as its approximation. On six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 21, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.35.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"name": "ArXiv", "volume": "abs/2005.01672"}, "authors": [{"authorId": "9073069", "name": "Jierui Li"}, {"authorId": "2978364", "name": "Lemao Liu"}, {"authorId": "91956362", "name": "Huayang Li"}, {"authorId": "151485795", "name": "Guanlin Li"}, {"authorId": "47752841", "name": "Guoping Huang"}, {"authorId": "34720053", "name": "Shuming Shi"}]}, {"paperId": "9e96657d76ce74173b658cf933b4bd74c13f8b8d", "externalIds": {"ACL": "2020.acl-main.36", "DBLP": "conf/acl/GuoXC20", "MAG": "3035289598", "DOI": "10.18653/v1/2020.acl-main.36", "CorpusId": 220046693}, "corpusId": 220046693, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9e96657d76ce74173b658cf933b4bd74c13f8b8d", "title": "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation", "abstract": "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 54, "influentialCitationCount": 5, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "376-385"}, "authors": [{"authorId": "13838086", "name": "Junliang Guo"}, {"authorId": "2230211", "name": "Linli Xu"}, {"authorId": "2227868312", "name": "Enhong Chen"}]}, {"paperId": "3ca61d43874caec1a4e18e5c6ca1976d93083bd4", "externalIds": {"ACL": "2020.acl-main.37", "MAG": "3037900071", "DBLP": "conf/acl/XuGXLZ20", "ArXiv": "2006.14405", "DOI": "10.18653/v1/2020.acl-main.37", "CorpusId": 220047350}, "corpusId": 220047350, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3ca61d43874caec1a4e18e5c6ca1976d93083bd4", "title": "Learning Source Phrase Representations for Neural Machine Translation", "abstract": "The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT). Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018). Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks (\u201cphrases\u201d) and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships. In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships. In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.37.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-25", "journal": {"pages": "386-396"}, "authors": [{"authorId": "49507285", "name": "Hongfei Xu"}, {"authorId": "7519068", "name": "Josef van Genabith"}, {"authorId": "2694222", "name": "Deyi Xiong"}, {"authorId": "14147919", "name": "Qiuhui Liu"}, {"authorId": "2108122973", "name": "Jingyi Zhang"}]}, {"paperId": "633d727ec370e729c0e398cd92032ab05ef20b35", "externalIds": {"MAG": "3035747971", "ArXiv": "1911.03179", "DBLP": "conf/acl/XuLGXZ20", "ACL": "2020.acl-main.38", "DOI": "10.18653/v1/2020.acl-main.38", "CorpusId": 218502632}, "corpusId": 218502632, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/633d727ec370e729c0e398cd92032ab05ef20b35", "title": "Lipschitz Constrained Parameter Initialization for Deep Transformers", "abstract": "The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers. We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence. In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers. In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 21, "citationCount": 25, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1911.03179", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-08", "journal": {"pages": "397-402"}, "authors": [{"authorId": "49507285", "name": "Hongfei Xu"}, {"authorId": "14147919", "name": "Qiuhui Liu"}, {"authorId": "7519068", "name": "Josef van Genabith"}, {"authorId": "2694222", "name": "Deyi Xiong"}, {"authorId": "2108122973", "name": "Jingyi Zhang"}]}, {"paperId": "d715b4a9282562b9d84fb66e04ee70e66b12e86d", "externalIds": {"DBLP": "conf/acl/DuboisDHB20", "MAG": "2984272647", "ArXiv": "1911.03872", "ACL": "2020.acl-main.39", "DOI": "10.18653/v1/2020.acl-main.39", "CorpusId": 207852603}, "corpusId": 207852603, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d715b4a9282562b9d84fb66e04ee70e66b12e86d", "title": "Location Attention for Extrapolation to Longer Sequences", "abstract": "Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle it. We then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization to sequences that are longer than the training ones. We hypothesize that models with a separate content- and location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 40, "citationCount": 27, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.39.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2019-11-10", "journal": {"name": "ArXiv", "volume": "abs/1911.03872"}, "authors": [{"authorId": "47367640", "name": "Yann Dubois"}, {"authorId": "67175437", "name": "Gautier Dagan"}, {"authorId": "3449411", "name": "D. Hupkes"}, {"authorId": "2552871", "name": "Elia Bruni"}]}, {"paperId": "9e9ec3dba37f2b1a47b84a7ad7d6fef11ef34b6b", "externalIds": {"ArXiv": "2004.14021", "DBLP": "conf/acl/WeiYHZWL20", "ACL": "2020.acl-main.40", "MAG": "3021993108", "DOI": "10.18653/v1/2020.acl-main.40", "CorpusId": 216641810}, "corpusId": 216641810, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9e9ec3dba37f2b1a47b84a7ad7d6fef11ef34b6b", "title": "Multiscale Collaborative Deep Models for Neural Machine Translation", "abstract": "Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 49, "citationCount": 28, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.14021", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.14021"}, "authors": [{"authorId": "46252591", "name": "Xiangpeng Wei"}, {"authorId": "1712236217", "name": "Heng Yu"}, {"authorId": "2108954687", "name": "Yue Hu"}, {"authorId": "1591125925", "name": "Yue Zhang"}, {"authorId": "24009282", "name": "Rongxiang Weng"}, {"authorId": "48244817", "name": "Weihua Luo"}]}, {"paperId": "8c67be2c975d3373f09920fbccdddef1a1d6b5f7", "externalIds": {"ArXiv": "2006.02014", "DBLP": "journals/corr/abs-2006-02014", "MAG": "3033360591", "ACL": "2020.acl-main.41", "DOI": "10.18653/v1/2020.acl-main.41", "CorpusId": 219260306}, "corpusId": 219260306, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8c67be2c975d3373f09920fbccdddef1a1d6b5f7", "title": "Norm-Based Curriculum Learning for Neural Machine Translation", "abstract": "A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT\u201914 English-German and WMT\u201917 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 78, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2006.02014", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-03", "journal": {"pages": "427-436"}, "authors": [{"authorId": "1390611971", "name": "Xuebo Liu"}, {"authorId": "1736553736", "name": "Houtim Lai"}, {"authorId": "1758353", "name": "Derek F. Wong"}, {"authorId": "1774304", "name": "Lidia S. Chao"}]}, {"paperId": "931514d517731f9d925511e60f0594e5b61cd47b", "externalIds": {"DBLP": "conf/acl/ZhengMZLH20", "ACL": "2020.acl-main.42", "MAG": "3034982693", "ArXiv": "2005.00675", "DOI": "10.18653/v1/2020.acl-main.42", "CorpusId": 218487602}, "corpusId": 218487602, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/931514d517731f9d925511e60f0594e5b61cd47b", "title": "Opportunistic Decoding with Timely Correction for Simultaneous Translation", "abstract": "Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 20, "citationCount": 17, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.42.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00675"}, "authors": [{"authorId": "40223399", "name": "Renjie Zheng"}, {"authorId": "1847848", "name": "Mingbo Ma"}, {"authorId": "20712300", "name": "Baigong Zheng"}, {"authorId": "66057453", "name": "Kaibo Liu"}, {"authorId": "48545084", "name": "Liang Huang"}]}, {"paperId": "45e5d7637a585a87d967a4a357d17c5d89aecea2", "externalIds": {"DBLP": "conf/acl/MerrillWGSSY20", "MAG": "3017116062", "ArXiv": "2004.08500", "ACL": "2020.acl-main.43", "DOI": "10.18653/v1/2020.acl-main.43", "CorpusId": 215828261}, "corpusId": 215828261, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/45e5d7637a585a87d967a4a357d17c5d89aecea2", "title": "A Formal Hierarchy of RNN Architectures", "abstract": "We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNN\u2019s memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how these models\u2019 expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions. Our results build on the theory of \u201csaturated\u201d RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. We provide empirical results to support this conjecture. Experimental findings from training unsaturated networks on formal languages support this conjecture.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 46, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.08500", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.08500"}, "authors": [{"authorId": "143696607", "name": "William Cooper Merrill"}, {"authorId": "145909798", "name": "Gail Weiss"}, {"authorId": "79775260", "name": "Yoav Goldberg"}, {"authorId": "4671928", "name": "Roy Schwartz"}, {"authorId": "1685669", "name": "Noah A. Smith"}, {"authorId": "1743232", "name": "Eran Yahav"}]}, {"paperId": "2f3f8d5538c950ae83a64c51d838485e62ee8d76", "externalIds": {"DBLP": "conf/acl/DingUS20", "ACL": "2020.acl-main.44", "MAG": "3034938221", "DOI": "10.18653/v1/2020.acl-main.44", "CorpusId": 220046225}, "corpusId": 220046225, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2f3f8d5538c950ae83a64c51d838485e62ee8d76", "title": "A Three-Parameter Rank-Frequency Relation in Natural Languages", "abstract": "We present that, the rank-frequency relation in textual data follows f \\propto r^{-\\alpha}(r+\\gamma)^{-\\beta}, where f is the token frequency and r is the rank by frequency, with (\\alpha, \\beta, \\gamma) as parameters. The formulation is derived based on the empirical observation that d^2 (x+y)/dx^2 is a typical impulse function, where (x,y)=(\\log r, \\log f). The formulation is the power law when \\beta=0 and the Zipf\u2013Mandelbrot law when \\alpha=0. We illustrate that \\alpha is related to the analytic features of syntax and \\beta+\\gamma to those of morphology in natural languages from an investigation of multilingual corpora.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 11, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.44.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "460-464"}, "authors": [{"authorId": "2126835", "name": "Chenchen Ding"}, {"authorId": "1802277", "name": "M. Utiyama"}, {"authorId": "1698363", "name": "E. Sumita"}]}, {"paperId": "185e7d2a761594451b02ace240356dadad2aef78", "externalIds": {"DBLP": "conf/acl/LiSMLWL20", "ACL": "2020.acl-main.45", "MAG": "3034328552", "ArXiv": "1911.02855", "DOI": "10.18653/v1/2020.acl-main.45", "CorpusId": 207847250}, "corpusId": 207847250, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/185e7d2a761594451b02ace240356dadad2aef78", "title": "Dice Loss for Data-imbalanced NLP Tasks", "abstract": "Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the S\u00f8rensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 64, "citationCount": 275, "influentialCitationCount": 25, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1911.02855", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-07", "journal": {"name": "ArXiv", "volume": "abs/1911.02855"}, "authors": [{"authorId": "2845020", "name": "Xiaoya Li"}, {"authorId": "2109329406", "name": "Xiaofei Sun"}, {"authorId": "65844131", "name": "Yuxian Meng"}, {"authorId": "2115780454", "name": "Junjun Liang"}, {"authorId": "144894837", "name": "Fei Wu"}, {"authorId": "49298465", "name": "Jiwei Li"}]}, {"paperId": "f7f20e163cba2ec5ca74fd1c4352a987d67ef7b7", "externalIds": {"ArXiv": "2005.01119", "MAG": "3035035964", "ACL": "2020.acl-main.46", "DBLP": "conf/acl/BaillyG20", "DOI": "10.18653/v1/2020.acl-main.46", "CorpusId": 218487842}, "corpusId": 218487842, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f7f20e163cba2ec5ca74fd1c4352a987d67ef7b7", "title": "Emergence of Syntax Needs Minimal Supervision", "abstract": "This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance. Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information. We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 47, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.46.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"name": "ArXiv", "volume": "abs/2005.01119"}, "authors": [{"authorId": "1844132", "name": "Rapha\u00ebl Bailly"}, {"authorId": "49365084", "name": "Kata G\u00e1bor"}]}, {"paperId": "55701719f1d7f3559e2c755044386334df8c1ea9", "externalIds": {"DBLP": "journals/corr/abs-2005-00842", "ArXiv": "2005.00842", "MAG": "3111743899", "ACL": "2020.acl-main.47", "DOI": "10.18653/v1/2020.acl-main.47", "CorpusId": 218487721}, "corpusId": 218487721, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/55701719f1d7f3559e2c755044386334df8c1ea9", "title": "Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese", "abstract": "We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods. In this study, we explore whether the LM-based method is valid for analyzing the word order. As a case study, this study focuses on Japanese due to its complex and flexible word order. To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies. Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool. Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 49, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00842"}, "authors": [{"authorId": "83446147", "name": "Tatsuki Kuribayashi"}, {"authorId": "119804885", "name": "Takumi Ito"}, {"authorId": "144042991", "name": "Jun Suzuki"}, {"authorId": "3040648", "name": "Kentaro Inui"}]}, {"paperId": "7b2ab7a828a6ae5cadffe79b1b3aa8bfbe3ae577", "externalIds": {"ACL": "2020.acl-main.48", "MAG": "3034254906", "DBLP": "conf/acl/LuL20", "ArXiv": "2004.11648", "DOI": "10.18653/V1/2020.ACL-MAIN.48", "CorpusId": 216144503}, "corpusId": 216144503, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7b2ab7a828a6ae5cadffe79b1b3aa8bfbe3ae577", "title": "GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media", "abstract": "This paper solves the fake news detection problem under a more realistic scenario on social media. Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average. In addition, the case studies also show that GCAN can produce reasonable explanations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 239, "influentialCitationCount": 22, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.48.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-24", "journal": {"pages": "505-514"}, "authors": [{"authorId": "2141540961", "name": "Yi-Ju Lu"}, {"authorId": "2169355", "name": "Cheng-te Li"}]}, {"paperId": "7e82015c386726f4b8f6f686b6e6bb7d1e7564bb", "externalIds": {"ACL": "2020.acl-main.49", "DBLP": "conf/acl/ZhongCSGW20", "MAG": "3025602092", "ArXiv": "2005.07886", "DOI": "10.18653/v1/2020.acl-main.49", "CorpusId": 218674458}, "corpusId": 218674458, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7e82015c386726f4b8f6f686b6e6bb7d1e7564bb", "title": "Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection", "abstract": "Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views. However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set. To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection. As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically. Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods. Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 75, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.49.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-16", "journal": {"pages": "515-526"}, "authors": [{"authorId": "39223973", "name": "L. Zhong"}, {"authorId": "144089410", "name": "Juan Cao"}, {"authorId": "46630548", "name": "Qiang Sheng"}, {"authorId": "2031845", "name": "Junbo Guo"}, {"authorId": "2000280978", "name": "Ziang Wang"}]}, {"paperId": "ecb00a6e83b35e7e05cb8ae341a2fa88d2e6016a", "externalIds": {"DBLP": "conf/acl/StefanovDAN20", "MAG": "3035238981", "ACL": "2020.acl-main.50", "DOI": "10.18653/v1/2020.acl-main.50", "CorpusId": 220047716}, "corpusId": 220047716, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ecb00a6e83b35e7e05cb8ae341a2fa88d2e6016a", "title": "Predicting the Topical Stance and Political Leaning of Media using Tweets", "abstract": "Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers. Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly. In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic. We evaluate the model by comparing its predictions to gold labels from the Media Bias/Fact Check website, achieving 82.6% accuracy.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 65, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.50.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Political Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "527-537"}, "authors": [{"authorId": "152860987", "name": "P. Stefanov"}, {"authorId": "143758717", "name": "Kareem Darwish"}, {"authorId": "2063953501", "name": "Atanas G. Atanasov"}, {"authorId": "1683562", "name": "Preslav Nakov"}]}, {"paperId": "56b82813d8203929615ebb589bdfc55adc7eb960", "externalIds": {"DBLP": "journals/corr/abs-2112-14330", "ArXiv": "2112.14330", "MAG": "3035345764", "ACL": "2020.acl-main.51", "DOI": "10.18653/v1/2020.acl-main.51", "CorpusId": 220047871}, "corpusId": 220047871, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/56b82813d8203929615ebb589bdfc55adc7eb960", "title": "Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora", "abstract": "The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results. We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. The method is simple, interpretable and stable. We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 46, "citationCount": 54, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.51.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"name": "ArXiv", "volume": "abs/2112.14330"}, "authors": [{"authorId": "1821892", "name": "Hila Gonen"}, {"authorId": "145123979", "name": "Ganesh Jawahar"}, {"authorId": "1679170", "name": "Djam\u00e9 Seddah"}, {"authorId": "79775260", "name": "Yoav Goldberg"}]}, {"paperId": "0ad447542a3fba88f6860fcfb43a86df5042079c", "externalIds": {"ACL": "2020.acl-main.52", "MAG": "3034242983", "DBLP": "journals/corr/abs-2005-00329", "ArXiv": "2005.00329", "DOI": "10.18653/v1/2020.acl-main.52", "CorpusId": 218470266}, "corpusId": 218470266, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0ad447542a3fba88f6860fcfb43a86df5042079c", "title": "CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation", "abstract": "Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. CDL utilizes two rewards focusing on emotion and content to improve the duality. Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 77, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.00329", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "556-566"}, "authors": [{"authorId": "80439483", "name": "Lei Shen"}, {"authorId": "49771779", "name": "Yang Feng"}]}, {"paperId": "7b82190043903e240b3003f174603ddc7d3a299e", "externalIds": {"MAG": "2987442863", "ArXiv": "1911.03906", "DBLP": "conf/acl/KimYKL20", "ACL": "2020.acl-main.53", "DOI": "10.18653/v1/2020.acl-main.53", "CorpusId": 207852597}, "corpusId": 207852597, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7b82190043903e240b3003f174603ddc7d3a299e", "title": "Efficient Dialogue State Tracking by Selectively Overwriting Memory", "abstract": "Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 28, "citationCount": 150, "influentialCitationCount": 37, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.53.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-10", "journal": {"name": "ArXiv", "volume": "abs/1911.03906"}, "authors": [{"authorId": "2829848", "name": "Sungdong Kim"}, {"authorId": "16110760", "name": "Sohee Yang"}, {"authorId": "2276429", "name": "Gyuwan Kim"}, {"authorId": "3226948", "name": "Sang-Woo Lee"}]}, {"paperId": "d0839b9e64d95e72cedb245b58053db791d9ba98", "externalIds": {"DBLP": "conf/acl/HamLJK20", "ACL": "2020.acl-main.54", "MAG": "3034533785", "DOI": "10.18653/v1/2020.acl-main.54", "CorpusId": 219719687}, "corpusId": 219719687, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d0839b9e64d95e72cedb245b58053db791d9ba98", "title": "End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2", "abstract": "The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system. On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response. In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above. In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 147, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.54.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "583-592"}, "authors": [{"authorId": "1405505217", "name": "Dong-hyun Ham"}, {"authorId": "2220888708", "name": "Jeong-Gwan Lee"}, {"authorId": "48595109", "name": "Youngsoo Jang"}, {"authorId": "97531959", "name": "Kyungmin Kim"}]}, {"paperId": "b3fa3cd0a6e8a06e409048975bdc746d8eda14cf", "externalIds": {"ACL": "2020.acl-main.55", "ArXiv": "2004.14302", "DBLP": "conf/acl/SatoAOSI20", "MAG": "3113148688", "DOI": "10.18653/v1/2020.acl-main.55", "CorpusId": 216642097}, "corpusId": 216642097, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b3fa3cd0a6e8a06e409048975bdc746d8eda14cf", "title": "Evaluating Dialogue Generation Systems via Response Selection", "abstract": "Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. We focus on evaluating response generation systems via response selection. To evaluate systems properly via response selection, we propose a method to construct response selection test sets with well-chosen false candidates. Specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as appropriate responses. Through experiments, we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation, compared with widely used automatic evaluation metrics such as BLEU.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-29", "journal": {"pages": "593-599"}, "authors": [{"authorId": "1661093124", "name": "Shiki Sato"}, {"authorId": "31487889", "name": "Reina Akama"}, {"authorId": "33516663", "name": "Hiroki Ouchi"}, {"authorId": "144042991", "name": "Jun Suzuki"}, {"authorId": "3040648", "name": "Kentaro Inui"}]}, {"paperId": "958354f3f336902c78b3adb19387f49d8fd124e8", "externalIds": {"DBLP": "conf/acl/ZhaLL20", "ACL": "2020.acl-main.56", "ArXiv": "2004.09036", "MAG": "3017372818", "DOI": "10.18653/V1/2020.ACL-MAIN.56", "CorpusId": 215828113}, "corpusId": 215828113, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/958354f3f336902c78b3adb19387f49d8fd124e8", "title": "Gated Convolutional Bidirectional Attention-based Model for Off-topic Spoken Response Detection", "abstract": "Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system. In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training. In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts. We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts. Moreover, a new negative sampling method is proposed to augment training data. Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 25, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.56.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.09036"}, "authors": [{"authorId": "9197467", "name": "Yefei Zha"}, {"authorId": "2112079580", "name": "Ruobing Li"}, {"authorId": "46933410", "name": "Hui-Ching Lin"}]}, {"paperId": "d408dd6b382f0c6d7ea92cfa8e02b4438c97dd25", "externalIds": {"DBLP": "conf/acl/DaiLTLSZ20", "MAG": "3034879520", "ACL": "2020.acl-main.57", "DOI": "10.18653/v1/2020.acl-main.57", "CorpusId": 220047814}, "corpusId": 220047814, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d408dd6b382f0c6d7ea92cfa8e02b4438c97dd25", "title": "Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment", "abstract": "Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 26, "citationCount": 25, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "609-618"}, "authors": [{"authorId": "30087809", "name": "Yinpei Dai"}, {"authorId": "2118386175", "name": "Hangyu Li"}, {"authorId": "1672552269", "name": "Chengguang Tang"}, {"authorId": "1527090216", "name": "Yongbin Li"}, {"authorId": "95879618", "name": "Jian Sun"}, {"authorId": "150345740", "name": "Xiaodan Zhu"}]}, {"paperId": "b5d26e5cd0f219b96cb50ddede897be5a3d73c16", "externalIds": {"DBLP": "conf/acl/HeYX20", "MAG": "3035568869", "ACL": "2020.acl-main.58", "DOI": "10.18653/v1/2020.acl-main.58", "CorpusId": 220046230}, "corpusId": 220046230, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b5d26e5cd0f219b96cb50ddede897be5a3d73c16", "title": "Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge", "abstract": "Neural-based context-aware models for slot tagging have achieved state-of-the-art performance. However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multi-level graph attention to explicitly model lexical relations. The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 18, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.58.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "619-624"}, "authors": [{"authorId": "2058349088", "name": "Keqing He"}, {"authorId": "1500528818", "name": "Yuanmeng Yan"}, {"authorId": "1753096", "name": "Weiran Xu"}]}, {"paperId": "c4e8f111b4cb6cdfe0f718d0d81ca138c0e9464e", "externalIds": {"MAG": "3015216216", "ArXiv": "2004.03809", "DBLP": "journals/corr/abs-2004-03809", "ACL": "2020.acl-main.59", "DOI": "10.18653/v1/2020.acl-main.59", "CorpusId": 215415923}, "corpusId": 215415923, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c4e8f111b4cb6cdfe0f718d0d81ca138c0e9464e", "title": "Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition", "abstract": "Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents. Two agents interact with each other and are jointly learned simultaneously. The method uses the actor-critic framework to facilitate pretraining and improve scalability. We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 57, "citationCount": 39, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.59.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.03809"}, "authors": [{"authorId": "51055574", "name": "Ryuichi Takanobu"}, {"authorId": "93513536", "name": "Runze Liang"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "4d1142f2ad96e4e8319f1cf2ef3144c91d4cf926", "externalIds": {"ArXiv": "2004.07462", "MAG": "3017342174", "DBLP": "journals/corr/abs-2004-07462", "ACL": "2020.acl-main.60", "DOI": "10.18653/v1/2020.acl-main.60", "CorpusId": 215786013}, "corpusId": 215786013, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4d1142f2ad96e4e8319f1cf2ef3144c91d4cf926", "title": "Paraphrase Augmented Task-Oriented Dialog Generation", "abstract": "Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings. We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance. We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels. PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019). Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 72, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.60.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-16", "journal": {"name": "ArXiv", "volume": "abs/2004.07462"}, "authors": [{"authorId": "1492154932", "name": "Silin Gao"}, {"authorId": "46868553", "name": "Yichi Zhang"}, {"authorId": "1717830", "name": "Zhijian Ou"}, {"authorId": "1564034697", "name": "Zhou Yu"}]}, {"paperId": "343673b1dcff42f92c279e55f773f5f3a8f9aaaf", "externalIds": {"MAG": "3035755323", "ArXiv": "2005.06128", "DBLP": "conf/acl/TianBLXSLZ20", "ACL": "2020.acl-main.61", "DOI": "10.18653/v1/2020.acl-main.61", "CorpusId": 218613702}, "corpusId": 218613702, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/343673b1dcff42f92c279e55f773f5f3a8f9aaaf", "title": "Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation", "abstract": "Neural conversation models are known to generate appropriate but non-informative responses in general. A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a given external document. In previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory. In this paper, we propose to create the document memory with some anticipated responses in mind. This is achieved using a teacher-student framework. The teacher is given the external document, the context, and the ground-truth response, and learns how to build a response-aware document memory from three sources of information. The student learns to construct a response-anticipated document memory from the first two sources, and teacher\u2019s insight on memory creation. Empirical results show that our model outperforms the previous state-of-the-art for the CbR task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 23, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.61.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-13", "journal": {"name": "ArXiv", "volume": "abs/2005.06128"}, "authors": [{"authorId": "33992653", "name": "Zhiliang Tian"}, {"authorId": "145001706", "name": "Wei Bi"}, {"authorId": "2109477294", "name": "Dongkyu Lee"}, {"authorId": "1438952297", "name": "Lanqing Xue"}, {"authorId": "8281265", "name": "Yiping Song"}, {"authorId": "3028405", "name": "Xiaojiang Liu"}, {"authorId": "3587277", "name": "N. Zhang"}]}, {"paperId": "f2e95bef0b7a7ae1701dcf97ac6fa12801b783e8", "externalIds": {"MAG": "3022486066", "DBLP": "conf/acl/HuangQSZ20", "ACL": "2020.acl-main.62", "ArXiv": "2005.04379", "DOI": "10.18653/v1/2020.acl-main.62", "CorpusId": 218581530}, "corpusId": 218581530, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f2e95bef0b7a7ae1701dcf97ac6fa12801b783e8", "title": "Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation", "abstract": "Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems. This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues. To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards. This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive. To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning. The proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations. The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations. We further propose to learn action embeddings for a better generalization of the reward function. The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 15, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.62.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-09", "journal": {"pages": "660-670"}, "authors": [{"authorId": "14799547", "name": "Xinting Huang"}, {"authorId": "39899794", "name": "Jianzhong Qi"}, {"authorId": "2117103785", "name": "Yu Sun"}, {"authorId": "2118403427", "name": "Rui Zhang"}]}, {"paperId": "7c6bf6e62accb82bdb0a3ca9d9f2e60bb8c23ea2", "externalIds": {"MAG": "3021172362", "ArXiv": "2004.14710", "DBLP": "conf/acl/SuHC20", "ACL": "2020.acl-main.63", "DOI": "10.18653/v1/2020.acl-main.63", "CorpusId": 216869072}, "corpusId": 216869072, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7c6bf6e62accb82bdb0a3ca9d9f2e60bb8c23ea2", "title": "Towards Unsupervised Language Understanding and Generation by Joint Dual Learning", "abstract": "In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG. The source code is available at: https://github.com/MiuLab/DuaLUG.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 20, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.63.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.14710"}, "authors": [{"authorId": "27629426", "name": "Shang-Yu Su"}, {"authorId": "47396497", "name": "Chao-Wei Huang"}, {"authorId": "2144862809", "name": "Yun-Nung Chen"}]}, {"paperId": "8d668b0161db3f0769194c999e7743ae1b0538e5", "externalIds": {"MAG": "3023366413", "DBLP": "journals/corr/abs-2005-00456", "ArXiv": "2005.00456", "ACL": "2020.acl-main.64", "DOI": "10.18653/v1/2020.acl-main.64", "CorpusId": 218470058}, "corpusId": 218470058, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8d668b0161db3f0769194c999e7743ae1b0538e5", "title": "USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation", "abstract": "The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces interpretable measures for several desirable properties of dialog.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 131, "influentialCitationCount": 26, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.64.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "681-707"}, "authors": [{"authorId": "32251567", "name": "Shikib Mehri"}, {"authorId": "1716325", "name": "M. Esk\u00e9nazi"}]}, {"paperId": "b74937a4408fc1cd2d01cdeef633373923bfe991", "externalIds": {"ACL": "2020.acl-main.65", "DBLP": "conf/acl/LiBHDC20", "MAG": "3034459399", "DOI": "10.18653/v1/2020.acl-main.65", "CorpusId": 220046855}, "corpusId": 220046855, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b74937a4408fc1cd2d01cdeef633373923bfe991", "title": "Explicit Semantic Decomposition for Definition Generation", "abstract": "Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts. However, previous works hardly consider explicitly modeling the \u201ccomponents\u201d of definitions, leading to under-specific generation results. In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation. Experimental results show that achieves top results on WordNet and Oxford benchmarks, outperforming strong previous baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 70, "citationCount": 15, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "708-717"}, "authors": [{"authorId": "2108959287", "name": "Jiahuan Li"}, {"authorId": "145854784", "name": "Yu Bao"}, {"authorId": "2046010", "name": "Shujian Huang"}, {"authorId": "3035069", "name": "Xinyu Dai"}, {"authorId": "1838162", "name": "Jiajun Chen"}]}, {"paperId": "4b52fd45a52aa84b2b1bb1c8cf36cc2884d7df7a", "externalIds": {"MAG": "3022543146", "ACL": "2020.acl-main.66", "DBLP": "journals/corr/abs-2004-14589", "ArXiv": "2004.14589", "DOI": "10.18653/v1/2020.acl-main.66", "CorpusId": 216869577}, "corpusId": 216869577, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4b52fd45a52aa84b2b1bb1c8cf36cc2884d7df7a", "title": "Improved Natural Language Generation via Loss Truncation", "abstract": "Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss. While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts). Even a small fraction of noisy data can degrade the performance of log loss. As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references. However, distinguishability has not been used in practice due to challenges in optimization and estimation. We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability. Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task. Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 56, "citationCount": 74, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.66.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"name": "ArXiv", "volume": "abs/2004.14589"}, "authors": [{"authorId": "35342489", "name": "Daniel Kang"}, {"authorId": "3056528", "name": "Tatsunori B. Hashimoto"}]}, {"paperId": "894db7570ed652673a18248375714aaa839a5434", "externalIds": {"DBLP": "conf/acl/ZhaoCCCZY20", "MAG": "3035521632", "ACL": "2020.acl-main.67", "DOI": "10.18653/v1/2020.acl-main.67", "CorpusId": 220047360}, "corpusId": 220047360, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/894db7570ed652673a18248375714aaa839a5434", "title": "Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks", "abstract": "Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation \u2013 A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR). Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) The message propagation process in AMR graphs is only guided by the first-order adjacency information. 2) The relationships between labeled edges are not fully considered. In this work, we propose a novel graph encoding framework which can effectively explore the edge relations. We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs. Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets. The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 23, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.67.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "732-741"}, "authors": [{"authorId": "2117889541", "name": "Yanbin Zhao"}, {"authorId": "1390833791", "name": "Lu Chen"}, {"authorId": "2117098991", "name": "Zhi Chen"}, {"authorId": "150273321", "name": "Ruisheng Cao"}, {"authorId": "144110100", "name": "Su Zhu"}, {"authorId": "1736727", "name": "Kai Yu"}]}, {"paperId": "18f3bdb40650f6c6268b2b108e67cdb8939741f9", "externalIds": {"MAG": "3034589622", "DBLP": "conf/acl/LiZLS20", "ArXiv": "2004.08022", "ACL": "2020.acl-main.68", "DOI": "10.18653/v1/2020.acl-main.68", "CorpusId": 215814127}, "corpusId": 215814127, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/18f3bdb40650f6c6268b2b108e67cdb8939741f9", "title": "Rigid Formats Controlled Text Generation", "abstract": "Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 43, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.68.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-17", "journal": {"name": "ArXiv", "volume": "abs/2004.08022"}, "authors": [{"authorId": "2193560", "name": "Piji Li"}, {"authorId": "5520376", "name": "Haisong Zhang"}, {"authorId": "3028405", "name": "Xiaojiang Liu"}, {"authorId": "34720053", "name": "Shuming Shi"}]}, {"paperId": "34f0d911c5219d947683a38c52405ac89e0c1e39", "externalIds": {"ArXiv": "2004.08694", "MAG": "3017356804", "DBLP": "conf/acl/DholeM20", "ACL": "2020.acl-main.69", "DOI": "10.18653/v1/2020.acl-main.69", "CorpusId": 215828433}, "corpusId": 215828433, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/34f0d911c5219d947683a38c52405ac89e0c1e39", "title": "Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation", "abstract": "Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems. In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules. A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 44, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.69.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-18", "journal": {"pages": "752-765"}, "authors": [{"authorId": "4834571", "name": "Kaustubh D. Dhole"}, {"authorId": "144783904", "name": "Christopher D. Manning"}]}, {"paperId": "4cb982400dfc13f6af5677b2524564992e29223d", "externalIds": {"DBLP": "conf/acl/KumarSUA20", "ACL": "2020.acl-main.70", "MAG": "3034719632", "DOI": "10.18653/v1/2020.acl-main.70", "CorpusId": 220047816}, "corpusId": 220047816, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4cb982400dfc13f6af5677b2524564992e29223d", "title": "An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering", "abstract": "Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution. Existing approaches often exploit short text streams in a batch way. However, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve. In addition, traditional independent word representation in graphical model tends to cause \u201cterm ambiguity\u201d problem in short text clustering. Therefore, in this paper, we propose an Online Semantic-enhanced Dirichlet Model for short sext stream clustering, called OSDM, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and clusters each arriving short text automatically in an online way. Extensive results have demonstrated that OSDM has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 22, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.70.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "766-776"}, "authors": [{"authorId": "144296649", "name": "J. Kumar"}, {"authorId": "40161233", "name": "Junming Shao"}, {"authorId": "2066940465", "name": "Salah Uddin"}, {"authorId": "115681118", "name": "Wazir Ali"}]}, {"paperId": "4b09d8b6952a294f86ab0b369e7f8af6d8c13ac2", "externalIds": {"DBLP": "journals/corr/abs-2006-08858", "ACL": "2020.acl-main.71", "MAG": "3035623165", "ArXiv": "2006.08858", "DOI": "10.18653/v1/2020.acl-main.71", "CorpusId": 219708250}, "corpusId": 219708250, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4b09d8b6952a294f86ab0b369e7f8af6d8c13ac2", "title": "Generative Semantic Hashing Enhanced via Boltzmann Machines", "abstract": "Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.71.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-16", "journal": {"name": "ArXiv", "volume": "abs/2006.08858"}, "authors": [{"authorId": "1633166807", "name": "Lin Zheng"}, {"authorId": "2482836", "name": "Qinliang Su"}, {"authorId": "19178763", "name": "Dinghan Shen"}, {"authorId": "1752041", "name": "Changyou Chen"}]}, {"paperId": "cdcf5dc58af574d3a8e3f3a0c48864ee764570dc", "externalIds": {"DBLP": "conf/acl/KohitaYKN20", "MAG": "3035760427", "ACL": "2020.acl-main.72", "DOI": "10.18653/v1/2020.acl-main.72", "CorpusId": 220047249}, "corpusId": 220047249, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cdcf5dc58af574d3a8e3f3a0c48864ee764570dc", "title": "Interactive Construction of User-Centric Dictionary for Text Analytics", "abstract": "We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis. This paper introduces the first formulation of interactive dictionary construction to address this issue. To optimize the interaction, we propose a new algorithm that effectively captures an analyst\u2019s intention starting from only a small number of sample terms. Along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task. Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.72.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "789-799"}, "authors": [{"authorId": "22312240", "name": "Ryosuke Kohita"}, {"authorId": "3313754", "name": "Issei Yoshida"}, {"authorId": "35562751", "name": "H. Kanayama"}, {"authorId": "2261082", "name": "Tetsuya Nasukawa"}]}, {"paperId": "4ab013772b898b56a2c2e290887d2a4694be5b5e", "externalIds": {"DBLP": "conf/acl/IsonumaMBS20", "ACL": "2020.acl-main.73", "MAG": "3034735823", "DOI": "10.18653/v1/2020.acl-main.73", "CorpusId": 220045470}, "corpusId": 220045470, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4ab013772b898b56a2c2e290887d2a4694be5b5e", "title": "Tree-Structured Neural Topic Model", "abstract": "This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010). This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 28, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.73.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "800-806"}, "authors": [{"authorId": "24905917", "name": "Masaru Isonuma"}, {"authorId": "49010536", "name": "Junichiro Mori"}, {"authorId": "2720656", "name": "Danushka Bollegala"}, {"authorId": "2850710", "name": "I. Sakata"}]}, {"paperId": "0f350ed24fda3a50671bb654f4c246f22c4f98a9", "externalIds": {"ACL": "2020.acl-main.74", "DBLP": "conf/acl/MassCRK20", "MAG": "3034937228", "DOI": "10.18653/v1/2020.acl-main.74", "CorpusId": 220047252}, "corpusId": 220047252, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0f350ed24fda3a50671bb654f4c246f22c4f98a9", "title": "Unsupervised FAQ Retrieval with Question Generation and BERT", "abstract": "We focus on the task of Frequently Asked Questions (FAQ) retrieval. A given user query can be matched against the questions and/or the answers in the FAQ. We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models. The two models match user queries to FAQ answers and questions, respectively. We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases. We show that our model is on par and even outperforms supervised models on existing datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 26, "citationCount": 34, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.74.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "807-812"}, "authors": [{"authorId": "1727535", "name": "Y. Mass"}, {"authorId": "1719299", "name": "B. Carmeli"}, {"authorId": "1799955", "name": "Haggai Roitman"}, {"authorId": "1775524", "name": "D. Konopnicki"}]}, {"paperId": "57de240193f3317d6b1b1bd94c95249759611e4e", "externalIds": {"ArXiv": "2004.14457", "DBLP": "journals/corr/abs-2004-14457", "ACL": "2020.acl-main.75", "MAG": "3022989070", "DOI": "10.18653/v1/2020.acl-main.75", "CorpusId": 216867302}, "corpusId": 216867302, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/57de240193f3317d6b1b1bd94c95249759611e4e", "title": "\u201cThe Boating Store Had Its Best Sail Ever\u201d: Pronunciation-attentive Contextualized Pun Recognition", "abstract": "Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity. However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence. PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols. Extensive experiments are conducted on two benchmark datasets. Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks. In-depth analyses verify the effectiveness and robustness of PCPR.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 57, "citationCount": 9, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.14457", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-29", "journal": {"pages": "813-822"}, "authors": [{"authorId": "3022124", "name": "Yichao Zhou"}, {"authorId": "3165179", "name": "Jyun-Yu Jiang"}, {"authorId": "33524946", "name": "Jieyu Zhao"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}, {"authorId": "46315104", "name": "Wei Wang"}]}, {"paperId": "ec9b897fc0db476c922b802375b268266f26a9c4", "externalIds": {"MAG": "3035229572", "ACL": "2020.acl-main.76", "DBLP": "conf/acl/ShinLYJ20", "ArXiv": "2004.08097", "DOI": "10.18653/v1/2020.acl-main.76", "CorpusId": 215814272}, "corpusId": 215814272, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ec9b897fc0db476c922b802375b268266f26a9c4", "title": "Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning", "abstract": "Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA). The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT. In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task. Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks. Code is available at https://github.com/joongbo/tta.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 43, "citationCount": 7, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.76.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-17", "journal": {"name": "ArXiv", "volume": "abs/2004.08097"}, "authors": [{"authorId": "27582486", "name": "Joongbo Shin"}, {"authorId": "30485896", "name": "Yoonhyung Lee"}, {"authorId": "2110654003", "name": "Seunghyun Yoon"}, {"authorId": "1731707", "name": "Kyomin Jung"}]}, {"paperId": "ba7b9e6fa9716fae0fa3f74bccb803c6741cd0b8", "externalIds": {"ACL": "2020.acl-main.77", "DBLP": "conf/acl/WangWLX20", "MAG": "3034236656", "DOI": "10.18653/v1/2020.acl-main.77", "CorpusId": 220045915}, "corpusId": 220045915, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ba7b9e6fa9716fae0fa3f74bccb803c6741cd0b8", "title": "Fine-grained Interest Matching for Neural News Recommendation", "abstract": "Personalized news recommendation is a critical technology to improve users\u2019 online news reading experience. The core of news recommendation is accurate matching between user\u2019s interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Fine-grained Interest Matching method for neural news recommendation. Instead of aggregating user\u2019s all historical browsed news into a unified vector, we hierarchically construct multi-level representations for each news via stacked dilated convolutions. Then we perform fine-grained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 86, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.77.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-25", "journal": {"pages": "836-845"}, "authors": [{"authorId": "2149695447", "name": "Heyuan Wang"}, {"authorId": "2397264", "name": "Fangzhao Wu"}, {"authorId": "2145976175", "name": "Zheng Liu"}, {"authorId": "144076239", "name": "Xing Xie"}]}, {"paperId": "c1ba3f8e4a52ed598eedb7c1fd2e288dc2da755d", "externalIds": {"DBLP": "conf/acl/ZhouZY20", "ACL": "2020.acl-main.78", "MAG": "3034208549", "DOI": "10.18653/v1/2020.acl-main.78", "CorpusId": 220046708}, "corpusId": 220046708, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c1ba3f8e4a52ed598eedb7c1fd2e288dc2da755d", "title": "Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder", "abstract": "Operational risk management is one of the biggest challenges nowadays faced by financial institutions. There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability. To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC). We empirically evaluate the framework on a real-world dataset. The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations. SemiORC also outperforms other baseline methods on operational risk classification.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "846-852"}, "authors": [{"authorId": "144315732", "name": "Fan Zhou"}, {"authorId": "2145524391", "name": "Shengming Zhang"}, {"authorId": "2143686211", "name": "Yi Yang"}]}, {"paperId": "1889b6e4270c966adfa0253b89b57a549b64609a", "externalIds": {"ACL": "2020.acl-main.79", "DBLP": "conf/acl/ZhongWZTZY20", "MAG": "3034966773", "DOI": "10.18653/v1/2020.acl-main.79", "CorpusId": 220045415}, "corpusId": 220045415, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1889b6e4270c966adfa0253b89b57a549b64609a", "title": "Interpreting Twitter User Geolocation", "abstract": "Identifying user geolocation in online social networks is an essential task in many location-based applications. Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior. In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users. This methodology helps with providing meaningful explanations on prediction results. Furthermore, it also initiates an attempt to uncover the so-called \u201cblack-box\u201d GNN-based models by investigating the effect of individual nodes.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.79.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "853-859"}, "authors": [{"authorId": "46456474", "name": "Ting Zhong"}, {"authorId": "1737495902", "name": "Tianliang Wang"}, {"authorId": "144315732", "name": "Fan Zhou"}, {"authorId": "1776969", "name": "Goce Trajcevski"}, {"authorId": "2737830", "name": "Kunpeng Zhang"}, {"authorId": "2143686211", "name": "Yi Yang"}]}, {"paperId": "b2d6e9dc8785d6c1433019820b7ef4b69eea7fc3", "externalIds": {"DBLP": "conf/acl/LeeL20", "ACL": "2020.acl-main.80", "MAG": "3034430025", "DOI": "10.18653/v1/2020.acl-main.80", "CorpusId": 220050403}, "corpusId": 220050403, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b2d6e9dc8785d6c1433019820b7ef4b69eea7fc3", "title": "Modeling Code-Switch Languages Using Bilingual Parallel Corpus", "abstract": "Language modeling is the technique to estimate the probability of a sequence of words. A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages. We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency. The attention mechanism learns the bilingual context from a parallel corpus. BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result. We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 24, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.80.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "860-870"}, "authors": [{"authorId": "35346402", "name": "Grandee Lee"}, {"authorId": "2108493009", "name": "Haizhou Li"}]}, {"paperId": "bc4b6bc216a9748aa5a37e0bf7b4ca0876f57e4b", "externalIds": {"ACL": "2020.acl-main.81", "DBLP": "journals/corr/abs-2004-14166", "MAG": "3035309733", "ArXiv": "2004.14166", "DOI": "10.18653/v1/2020.acl-main.81", "CorpusId": 216642230}, "corpusId": 216642230, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bc4b6bc216a9748aa5a37e0bf7b4ca0876f57e4b", "title": "SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check", "abstract": "Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters. However, they take the similarity knowledge as either an external input resource or just heuristic rules. This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN). The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers. These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable. Experiments are conducted on three human-annotated datasets. Our method achieves superior performance against previous models by a large margin.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 81, "influentialCitationCount": 30, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.14166", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "871-881"}, "authors": [{"authorId": "26382255", "name": "Xingyi Cheng"}, {"authorId": "1930234", "name": "Weidi Xu"}, {"authorId": "2110722041", "name": "Kunlong Chen"}, {"authorId": "2119329074", "name": "Shaohua Jiang"}, {"authorId": "2145758048", "name": "Feng Wang"}, {"authorId": null, "name": "Taifeng Wang"}, {"authorId": "2057047939", "name": "Wei Chu"}, {"authorId": "145475951", "name": "Yuan Qi"}]}, {"paperId": "93bf0b32b17297d94f6a2cc35bce7c396eb17b36", "externalIds": {"ACL": "2020.acl-main.82", "MAG": "3024680312", "DBLP": "journals/corr/abs-2005-07421", "ArXiv": "2005.07421", "DOI": "10.18653/v1/2020.acl-main.82", "CorpusId": 218665566}, "corpusId": 218665566, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/93bf0b32b17297d94f6a2cc35bce7c396eb17b36", "title": "Spelling Error Correction with Soft-Masked BERT", "abstract": "Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability. Without loss of generality we consider Chinese spelling error correction (CSC) in this paper. A state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of BERT, the language representation model. The accuracy of the method can be sub-optimal, however, because BERT does not have sufficient capability to detect whether there is an error at each position, apparently due to the way of pre-training it using mask language modeling. In this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what we call soft-masking technique. Our method of using \u2018Soft-Masked BERT\u2019 is general, and it may be employed in other language detection-correction problems. Experimental results on two datasets, including one large dataset which we create and plan to release, demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 17, "citationCount": 127, "influentialCitationCount": 25, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.82.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-15", "journal": {"pages": "882-890"}, "authors": [{"authorId": "2116577338", "name": "Shaohua Zhang"}, {"authorId": "2039788", "name": "Haoran Huang"}, {"authorId": "2145471219", "name": "Jicong Liu"}, {"authorId": "2125544106", "name": "Hang Li"}]}, {"paperId": "0bcdf7573ff07e9ada5ac6224465c0cff7acfb7d", "externalIds": {"ACL": "2020.acl-main.83", "DBLP": "conf/acl/GuoLTLGZZ20", "MAG": "3035025198", "DOI": "10.18653/v1/2020.acl-main.83", "CorpusId": 220047836}, "corpusId": 220047836, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0bcdf7573ff07e9ada5ac6224465c0cff7acfb7d", "title": "A Frame-based Sentence Representation for Machine Reading Comprehension", "abstract": "Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC). MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge. To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling. Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema. Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations. Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 23, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.83.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "891-896"}, "authors": [{"authorId": "3111734", "name": "Shaoru Guo"}, {"authorId": "2109380034", "name": "Ru Li"}, {"authorId": "3075017", "name": "Hongye Tan"}, {"authorId": "39952499", "name": "Xiaoli Li"}, {"authorId": "2069570219", "name": "Yong Guan"}, {"authorId": "2146230140", "name": "Hongyan Zhao"}, {"authorId": "2145913942", "name": "Yueping Zhang"}]}, {"paperId": "15026f39f20463ba2c3137203968a5d0c904eab4", "externalIds": {"MAG": "3034750657", "DBLP": "journals/corr/abs-2004-07633", "ACL": "2020.acl-main.84", "ArXiv": "2004.07633", "DOI": "10.18653/v1/2020.acl-main.84", "CorpusId": 215786544}, "corpusId": 215786544, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/15026f39f20463ba2c3137203968a5d0c904eab4", "title": "A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation", "abstract": "In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT). This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of the tokens to the operations. Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens. We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.84.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-16", "journal": {"name": "ArXiv", "volume": "abs/2004.07633"}, "authors": [{"authorId": "145116511", "name": "Jan Deriu"}, {"authorId": "1637256642", "name": "Katsiaryna Mlynchyk"}, {"authorId": "1637257248", "name": "Philippe Schlapfer"}, {"authorId": "47489866", "name": "\u00c1lvaro Rodrigo"}, {"authorId": "1637268942", "name": "D. V. Grunigen"}, {"authorId": "2066949444", "name": "Nicolas Kaiser"}, {"authorId": "2113917675", "name": "Kurt Stockinger"}, {"authorId": "1733049", "name": "Eneko Agirre"}, {"authorId": "2648584", "name": "Mark Cieliebak"}]}, {"paperId": "b88a74bed2d0c8c438d5d19a4743c35bf6c9d4d9", "externalIds": {"ACL": "2020.acl-main.85", "MAG": "3035099133", "DBLP": "conf/acl/LeeSHK20", "ArXiv": "1911.02896", "DOI": "10.18653/v1/2020.acl-main.85", "CorpusId": 219058995}, "corpusId": 219058995, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b88a74bed2d0c8c438d5d19a4743c35bf6c9d4d9", "title": "Contextualized Sparse Representations for Real-Time Open-Domain Question Answering", "abstract": "Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space. By augmenting the previous phrase retrieval model (Seo et al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open. Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 26, "citationCount": 20, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.85.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-07", "journal": {"pages": "912-919"}, "authors": [{"authorId": "46664096", "name": "Jinhyuk Lee"}, {"authorId": "4418074", "name": "Minjoon Seo"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}, {"authorId": "144323862", "name": "Jaewoo Kang"}]}, {"paperId": "6b34507e185ead4c482754774d9d87a974dcb169", "externalIds": {"DBLP": "conf/acl/GottumukkalaDSG20", "ACL": "2020.acl-main.86", "MAG": "3034407524", "DOI": "10.18653/v1/2020.acl-main.86", "CorpusId": 220047193}, "corpusId": 220047193, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6b34507e185ead4c482754774d9d87a974dcb169", "title": "Dynamic Sampling Strategies for Multi-Task Reading Comprehension", "abstract": "Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community. Prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up. We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task model\u2019s current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning. We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level. Our final model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 17, "citationCount": 15, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.86.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "920-924"}, "authors": [{"authorId": "1471885977", "name": "Ananth Gottumukkala"}, {"authorId": "33546336", "name": "Dheeru Dua"}, {"authorId": "34650964", "name": "Sameer Singh"}, {"authorId": "40642935", "name": "Matt Gardner"}]}, {"paperId": "708138f692062efb656447531649076d54472b60", "externalIds": {"MAG": "3021483992", "DBLP": "conf/acl/YuanSBGLDFJ20", "ArXiv": "2004.14069", "ACL": "2020.acl-main.87", "DOI": "10.18653/v1/2020.acl-main.87", "CorpusId": 216641748}, "corpusId": 216641748, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/708138f692062efb656447531649076d54472b60", "title": "Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension", "abstract": "Multilingual pre-trained models could leverage the training data from a rich source language (such as English) to improve performance on low resource languages. However, the transfer quality for multilingual Machine Reading Comprehension (MRC) is significantly worse than sentence classification tasks mainly due to the requirement of MRC to detect the word level answer boundary. In this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision: (1) A mixed MRC task, which translates the question or passage to other languages and builds cross-lingual question-passage pairs; (2) A language-agnostic knowledge masking task by leveraging knowledge phrases mined from web. Besides, extensive experiments on two cross-lingual MRC datasets show the effectiveness of our proposed approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 21, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.87.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-29", "journal": {"name": "ArXiv", "volume": "abs/2004.14069"}, "authors": [{"authorId": "40247395", "name": "Fei Yuan"}, {"authorId": "24962156", "name": "Linjun Shou"}, {"authorId": "2114140208", "name": "X. Bai"}, {"authorId": "50175330", "name": "Ming Gong"}, {"authorId": "3887469", "name": "Yaobo Liang"}, {"authorId": "46429989", "name": "Nan Duan"}, {"authorId": "1832664242", "name": "Yan Fu"}, {"authorId": "71790825", "name": "Daxin Jiang"}]}, {"paperId": "5f410910c3cf8b1affd189f0e1664997f13d39e1", "externalIds": {"MAG": "3029217317", "ACL": "2020.acl-main.88", "DBLP": "conf/acl/GaoWJXSKLH20", "ArXiv": "2005.12484", "DOI": "10.18653/v1/2020.acl-main.88", "CorpusId": 218889485}, "corpusId": 218889485, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5f410910c3cf8b1affd189f0e1664997f13d39e1", "title": "Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading", "abstract": "The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions. Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them. In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision. Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions. On the ShARC benchmark (blind, held-out) testset, EMT achieves new state-of-the-art results of 74.6% micro-averaged decision accuracy and 49.5 BLEU4. We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows. Code and models are released at https://github.com/Yifan-Gao/explicit_memory_tracker.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 18, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.88.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-26", "journal": {"name": "ArXiv", "volume": "abs/2005.12484"}, "authors": [{"authorId": "1921742", "name": "Yifan Gao"}, {"authorId": "30340989", "name": "Chien-Sheng Wu"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "2228109", "name": "Caiming Xiong"}, {"authorId": "2166511", "name": "R. Socher"}, {"authorId": "145310663", "name": "Irwin King"}, {"authorId": "145609003", "name": "M. Lyu"}, {"authorId": "1741126", "name": "S. Hoi"}]}, {"paperId": "3dd61d97827e3f380bf9304101149a3f865051fc", "externalIds": {"ACL": "2020.acl-main.89", "DBLP": "conf/acl/GevaGB20", "MAG": "3015339775", "ArXiv": "2004.04487", "DOI": "10.18653/v1/2020.acl-main.89", "CorpusId": 215548225}, "corpusId": 215548225, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3dd61d97827e3f380bf9304101149a3f865051fc", "title": "Injecting Numerical Reasoning Skills into Language Models", "abstract": "Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 \u2013> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 162, "influentialCitationCount": 25, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.89.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "946-958"}, "authors": [{"authorId": "22245981", "name": "Mor Geva"}, {"authorId": "145520321", "name": "Ankit Gupta"}, {"authorId": "1750652", "name": "Jonathan Berant"}]}, {"paperId": "2ec308198dfa8dd4f349ce54e23345f800b28e2c", "externalIds": {"ACL": "2020.acl-main.90", "MAG": "3035359051", "DBLP": "conf/acl/KunduLN20", "DOI": "10.18653/v1/2020.acl-main.90", "CorpusId": 220046984}, "corpusId": 220046984, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2ec308198dfa8dd4f349ce54e23345f800b28e2c", "title": "Learning to Identify Follow-Up Questions in Conversational Question Answering", "abstract": "Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions. Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently. In this paper, we introduce a new follow-up question identification task. We propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage, the conversation history, and a candidate follow-up question. It enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question. Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 25, "citationCount": 9, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "959-968"}, "authors": [{"authorId": "2103994667", "name": "Souvik Kundu"}, {"authorId": "2114096968", "name": "Qian Lin"}, {"authorId": "34789794", "name": "H. Ng"}]}, {"paperId": "065691db44b3b1ae48ffb64559e7a98e5b80306f", "externalIds": {"DBLP": "conf/acl/LanJ20", "MAG": "3034273250", "ACL": "2020.acl-main.91", "DOI": "10.18653/v1/2020.acl-main.91", "CorpusId": 220047976}, "corpusId": 220047976, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/065691db44b3b1ae48ffb64559e7a98e5b80306f", "title": "Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases", "abstract": "Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations. In this paper, we handle both types of complexity at the same time. Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs. Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 22, "citationCount": 122, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.91.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "969-974"}, "authors": [{"authorId": "3458560", "name": "Yunshi Lan"}, {"authorId": "144924128", "name": "Jing Jiang"}]}, {"paperId": "f13e41d24e5d0a68ca662c1b49de398a6fb68251", "externalIds": {"DBLP": "conf/acl/MiaoLS20", "ArXiv": "2106.15772", "MAG": "3034643750", "ACL": "2020.acl-main.92", "DOI": "10.18653/v1/2020.acl-main.92", "CorpusId": 220047831}, "corpusId": 220047831, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f13e41d24e5d0a68ca662c1b49de398a6fb68251", "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers", "abstract": "We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types. We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level (for indicating the level of difficulty). Furthermore, we propose a metric to measure the lexicon usage diversity of a given MWP corpus, and demonstrate that ASDiv is more diverse than existing corpora. Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 20, "citationCount": 121, "influentialCitationCount": 29, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.92.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"name": "ArXiv", "volume": "abs/2106.15772"}, "authors": [{"authorId": "2631051", "name": "Shen-Yun Miao"}, {"authorId": "3092327", "name": "Chao-Chun Liang"}, {"authorId": "145718668", "name": "Keh-Yih Su"}]}, {"paperId": "0b4d5b7cef06b66182db80803f783d077e3637b6", "externalIds": {"ACL": "2020.acl-main.93", "DBLP": "conf/acl/YiDH20", "MAG": "3034417909", "DOI": "10.18653/v1/2020.acl-main.93", "CorpusId": 220045419}, "corpusId": 220045419, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0b4d5b7cef06b66182db80803f783d077e3637b6", "title": "Improving Image Captioning Evaluation by Considering Inter References Variance", "abstract": "Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image. Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions. It usually leads to over-penalization and thus a bad correlation to human judgment. Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance. In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation. The experimental results show that our metric achieves state-of-the-art human judgment correlation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 30, "influentialCitationCount": 8, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "985-994"}, "authors": [{"authorId": "2066645440", "name": "Yanzhi Yi"}, {"authorId": "1486063321", "name": "Hangyu Deng"}, {"authorId": "37065012", "name": "Jinglu Hu"}]}, {"paperId": "74692442286c8b967e253c02f451b1ddef794150", "externalIds": {"MAG": "3019287770", "ArXiv": "2004.10813", "DBLP": "journals/corr/abs-2004-10813", "ACL": "2020.acl-main.94", "DOI": "10.18653/v1/2020.acl-main.94", "CorpusId": 216080450}, "corpusId": 216080450, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/74692442286c8b967e253c02f451b1ddef794150", "title": "Revisiting the Context Window for Cross-lingual Word Embeddings", "abstract": "Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar. The structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines. Despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work. In this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual embeddings trained with different context windows. The highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction, especially the performance on frequent nouns.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.94.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "995-1005"}, "authors": [{"authorId": "1466451143", "name": "Ryokan Ri"}, {"authorId": "143946906", "name": "Yoshimasa Tsuruoka"}]}, {"paperId": "9a25609275bb1113aaf7c92b28477ed7ff0677a8", "externalIds": {"DBLP": "journals/corr/abs-2005-02590", "MAG": "3034675880", "ArXiv": "2005.02590", "ACL": "2020.acl-main.95", "DOI": "10.18653/v1/2020.acl-main.95", "CorpusId": 218517044}, "corpusId": 218517044, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9a25609275bb1113aaf7c92b28477ed7ff0677a8", "title": "Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders", "abstract": "A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training. We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense. The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding. Our system outperforms previous state-of-the-art models on English all-words WSD; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work. This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 120, "influentialCitationCount": 24, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.95.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-06", "journal": {"name": "ArXiv", "volume": "abs/2005.02590"}, "authors": [{"authorId": "3443287", "name": "Terra Blevins"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}]}, {"paperId": "7fef7784aa65a2419a05b773a4aaa8ae1a688647", "externalIds": {"DBLP": "conf/acl/BansalGSPM20", "ACL": "2020.acl-main.96", "ArXiv": "2005.02295", "MAG": "3022021376", "DOI": "10.18653/v1/2020.acl-main.96", "CorpusId": 218502471}, "corpusId": 218502471, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7fef7784aa65a2419a05b773a4aaa8ae1a688647", "title": "Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream NLP Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection", "abstract": "In this paper, we demonstrate how code-switching patterns can be utilised to improve various downstream NLP applications. In particular, we encode various switching features to improve humour, sarcasm and hate speech detection tasks. We believe that this simple linguistic observation can also be potentially helpful in improving other similar NLP applications.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 17, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.96.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"pages": "1018-1023"}, "authors": [{"authorId": "67152985", "name": "Srijan Bansal"}, {"authorId": "1672775068", "name": "Vishal Garimella"}, {"authorId": "117312289", "name": "Ayush Suhane"}, {"authorId": "9942598", "name": "Jasabanta Patro"}, {"authorId": "46405816", "name": "Animesh Mukherjee"}]}, {"paperId": "945d4f09698e5a925bdcc8c2f99d43d5b612c01b", "externalIds": {"DBLP": "conf/acl/WuRZLN20", "ArXiv": "2004.13455", "ACL": "2020.acl-main.97", "MAG": "3023772852", "DOI": "10.18653/v1/2020.acl-main.97", "CorpusId": 216562850}, "corpusId": 216562850, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/945d4f09698e5a925bdcc8c2f99d43d5b612c01b", "title": "DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification", "abstract": "Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. However, in these methods, the discovery process of evidence is nontransparent and unexplained. Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification. Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way. Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by more than 3.11%, 2.41%, respectively.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 48, "citationCount": 38, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.97.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-28", "journal": {"pages": "1024-1035"}, "authors": [{"authorId": "49279687", "name": "Lianwei Wu"}, {"authorId": "1492126621", "name": "Y. Rao"}, {"authorId": "2151271026", "name": "Yongqiang Zhao"}, {"authorId": "2111207435", "name": "Hao Liang"}, {"authorId": "51293593", "name": "Ambreen Nazir"}]}, {"paperId": "80b2d5233fc2d4b3b9bdcfdfea3e66c79a767a95", "externalIds": {"DBLP": "conf/acl/LiuWNWCL20", "MAG": "3035355914", "ACL": "2020.acl-main.98", "ArXiv": "2005.03954", "DOI": "10.18653/v1/2020.acl-main.98", "CorpusId": 218571035}, "corpusId": 218571035, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/80b2d5233fc2d4b3b9bdcfdfea3e66c79a767a95", "title": "Towards Conversational Recommendation over Multi-Type Dialogs", "abstract": "We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user\u2019s interests and feedback. To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), where there are multiple sequential dialogs for a pair of a recommendation seeker (user) and a recommender (bot). In each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 134, "influentialCitationCount": 27, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.98.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-08", "journal": {"pages": "1036-1049"}, "authors": [{"authorId": "2109371626", "name": "Zeming Liu"}, {"authorId": "144270731", "name": "Haifeng Wang"}, {"authorId": "2715551", "name": "Zheng-Yu Niu"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "2256319", "name": "Wanxiang Che"}, {"authorId": "40282288", "name": "Ting Liu"}]}, {"paperId": "b387b19b8c02f3087bacd8514ea31e55e494ccf7", "externalIds": {"ACL": "2020.acl-main.99", "DBLP": "conf/acl/YanFLLZWL20", "MAG": "3035371495", "DOI": "10.18653/v1/2020.acl-main.99", "CorpusId": 220046205}, "corpusId": 220046205, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b387b19b8c02f3087bacd8514ea31e55e494ccf7", "title": "Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification", "abstract": "User intent classification plays a vital role in dialogue systems. Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun. This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection. In particular, we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means, which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection. Coupled with a density-based outlier detection algorithm, SEG achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection. On top of that, we propose to integrate SEG as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance. A case study on a state-of-the-art method, ReCapsNet, shows that SEG can push the classification performance to a significantly higher level.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 57, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.99.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1050-1060"}, "authors": [{"authorId": "2074120147", "name": "Guangfeng Yan"}, {"authorId": "145451508", "name": "Lu Fan"}, {"authorId": "35692225", "name": "Qimai Li"}, {"authorId": "73623714", "name": "\u5218\u6657"}, {"authorId": "2137337360", "name": "Xiaotong Zhang"}, {"authorId": "19195265", "name": "Xiao-Ming Wu"}, {"authorId": "1902169", "name": "Albert Y. S. Lam"}]}, {"paperId": "a2a378153c1f09e23c2c13a81be89af2ca0d383e", "externalIds": {"ArXiv": "2005.00701", "ACL": "2020.acl-main.100", "MAG": "3021923363", "DBLP": "conf/acl/CaoSPKLC20", "DOI": "10.18653/v1/2020.acl-main.100", "CorpusId": 218487448}, "corpusId": 218487448, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a2a378153c1f09e23c2c13a81be89af2ca0d383e", "title": "Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen", "abstract": "The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. The results demonstrate a significant gap between machine and human performance. We also discuss the challenges of automatic evaluation, to provide insights into future research directions. The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 60, "citationCount": 51, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.100.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"pages": "1061-1071"}, "authors": [{"authorId": "2112867078", "name": "Yixin Cao"}, {"authorId": "1667839719", "name": "Ruihao Shui"}, {"authorId": "3470231", "name": "Liangming Pan"}, {"authorId": "37596605", "name": "Min-Yen Kan"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "144078686", "name": "Tat-Seng Chua"}]}, {"paperId": "29e86cbeacf1e2235cc320ad240956012b294646", "externalIds": {"ACL": "2020.acl-main.101", "MAG": "3035543325", "DBLP": "conf/acl/WangWAYC20", "ArXiv": "2005.00969", "DOI": "10.18653/v1/2020.acl-main.101", "CorpusId": 218487237}, "corpusId": 218487237, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/29e86cbeacf1e2235cc320ad240956012b294646", "title": "Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints", "abstract": "Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table. In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal. The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model. Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem. We also provide detailed analysis on each component of our model in our experiments. Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 63, "influentialCitationCount": 17, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.101.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"name": "ArXiv", "volume": "abs/2005.00969"}, "authors": [{"authorId": "2920297", "name": "Zhenyi Wang"}, {"authorId": "48631781", "name": "Xiaoyang Wang"}, {"authorId": "49640821", "name": "Bang An"}, {"authorId": "144580027", "name": "Dong Yu"}, {"authorId": "1752041", "name": "Changyou Chen"}]}, {"paperId": "e88ea611d2c644138ee9507ea508937e1af58bee", "externalIds": {"MAG": "3025397699", "DBLP": "journals/corr/abs-2005-05727", "ArXiv": "2005.05727", "ACL": "2020.acl-main.102", "DOI": "10.18653/v1/2020.acl-main.102", "CorpusId": 218596002}, "corpusId": 218596002, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e88ea611d2c644138ee9507ea508937e1af58bee", "title": "Dynamic Memory Induction Networks for Few-Shot Text Classification", "abstract": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets. Detailed analysis is further performed to show how the proposed network achieves the new performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 50, "citationCount": 62, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.102.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-12", "journal": {"name": "ArXiv", "volume": "abs/2005.05727"}, "authors": [{"authorId": "9706609", "name": "Ruiying Geng"}, {"authorId": "66200440", "name": "Binhua Li"}, {"authorId": "1527090216", "name": "Yongbin Li"}, {"authorId": null, "name": "Jian Sun"}, {"authorId": "150345740", "name": "Xiaodan Zhu"}]}, {"paperId": "ba46ece6feba34c408d081a8dce66f0ecf4b7a60", "externalIds": {"MAG": "3035328829", "DBLP": "conf/acl/ChenCLK20", "ArXiv": "2004.08511", "ACL": "2020.acl-main.103", "DOI": "10.18653/v1/2020.acl-main.103", "CorpusId": 215828384}, "corpusId": 215828384, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ba46ece6feba34c408d081a8dce66f0ecf4b7a60", "title": "Exclusive Hierarchical Decoding for Deep Keyphrase Generation", "abstract": "Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases. A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce. Previous work in this setting employs a sequential decoding process to generate keyphrases. However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document. Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources. To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set. Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases. Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 57, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.103.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "1095-1105"}, "authors": [{"authorId": "2111898210", "name": "Wang Chen"}, {"authorId": "23181435", "name": "Hou Pong Chan"}, {"authorId": "2193560", "name": "Piji Li"}, {"authorId": "145310663", "name": "Irwin King"}]}, {"paperId": "25467372b8517c6431319aef9d65ae1fa87c8581", "externalIds": {"ACL": "2020.acl-main.104", "DBLP": "conf/acl/ZhouMLXDZXL20", "MAG": "3035690777", "DOI": "10.18653/v1/2020.acl-main.104", "CorpusId": 220044881}, "corpusId": 220044881, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/25467372b8517c6431319aef9d65ae1fa87c8581", "title": "Hierarchy-Aware Global Model for Hierarchical Text Classification", "abstract": "Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy. Existing methods have difficulties in modeling the hierarchical label structure in a global view. Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space. In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies. Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (HiAGM) with two variants. A multi-label attention variant (HiAGM-LA) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features. A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders. Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 72, "influentialCitationCount": 24, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.104.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1106-1117"}, "authors": [{"authorId": "144535458", "name": "Jie Zhou"}, {"authorId": "48168083", "name": "Chunping Ma"}, {"authorId": "8427191", "name": "Dingkun Long"}, {"authorId": "2149131512", "name": "Guangwei Xu"}, {"authorId": "46649145", "name": "Ning Ding"}, {"authorId": "3108945", "name": "Haoyu Zhang"}, {"authorId": "35930962", "name": "Pengjun Xie"}, {"authorId": "150112803", "name": "Gongshen Liu"}]}, {"paperId": "0077fd6a31cf638a316d35d50cb5e7d26c63dfc6", "externalIds": {"MAG": "3034218205", "ArXiv": "2106.14726", "DBLP": "conf/acl/BoudinGA20", "ACL": "2020.acl-main.105", "DOI": "10.18653/v1/2020.acl-main.105", "CorpusId": 220047513}, "corpusId": 220047513, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0077fd6a31cf638a316d35d50cb5e7d26c63dfc6", "title": "Keyphrase Generation for Scientific Document Retrieval", "abstract": "Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval. This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models. Using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains. Our code is available at https://github.com/boudinfl/ir-using-kg", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 49, "citationCount": 23, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.105.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1118-1126"}, "authors": [{"authorId": "2346578", "name": "Florian Boudin"}, {"authorId": "1438301862", "name": "Ygor Gallina"}, {"authorId": "1705519", "name": "Akiko Aizawa"}]}, {"paperId": "dcd72571e380eacf3ddccf225f3324eb8c51ece0", "externalIds": {"MAG": "3034786567", "DBLP": "conf/acl/HofmannSP20", "ACL": "2020.acl-main.106", "DOI": "10.18653/v1/2020.acl-main.106", "CorpusId": 220047777}, "corpusId": 220047777, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dcd72571e380eacf3ddccf225f3324eb8c51ece0", "title": "A Graph Auto-encoder Model of Derivational Morphology", "abstract": "There has been little work on modeling the morphological well-formedness (MWF) of derivatives, a problem judged to be complex and difficult in linguistics. We present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation. The auto-encoder models MWF in English surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 54, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.106.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-06", "journal": {"pages": "1127-1138"}, "authors": [{"authorId": "1667898858", "name": "Valentin Hofmann"}, {"authorId": "144418438", "name": "Hinrich Sch\u00fctze"}, {"authorId": "1970864", "name": "J. Pierrehumbert"}]}, {"paperId": "bc3c662d2b6f5159202d703d201a9283687d43b9", "externalIds": {"ACL": "2020.acl-main.107", "DBLP": "conf/acl/SeddahEFFMSSS20", "MAG": "3035283133", "DOI": "10.18653/v1/2020.acl-main.107", "CorpusId": 220045406}, "corpusId": 220045406, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bc3c662d2b6f5159202d703d201a9283687d43b9", "title": "Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell", "abstract": "We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African Arabic dialect known for its frequent usage of code-switching. Made of 1500 sentences, fully annotated in morpho-syntax and Universal Dependency syntax, with full translation at both the word and the sentence levels, this treebank is made freely available. It is supplemented with 50k unlabeled sentences collected from Common Crawl and web-crawled data using intensive data-mining techniques. Preliminary experiments demonstrate its usefulness for POS tagging and dependency parsing. We believe that what we present in this paper is useful beyond the low-resource language community. This is the first time that enough unlabeled and annotated data is provided for an emerging user-generated content dialectal language with rich morphology and code switching, making it an challenging test-bed for most recent NLP approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 33, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.107.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-05", "journal": {"pages": "1139-1150"}, "authors": [{"authorId": "1679170", "name": "Djam\u00e9 Seddah"}, {"authorId": "1754432886", "name": "Farah Essaidi"}, {"authorId": "1738571716", "name": "Amal Fethi"}, {"authorId": "1754383720", "name": "Matthieu Futeral"}, {"authorId": "2067886843", "name": "Benjamin M\u00fcller"}, {"authorId": "147846651", "name": "Pedro Ortiz Suarez"}, {"authorId": "2075036", "name": "B. Sagot"}, {"authorId": "153257123", "name": "Abhishek Srivastava"}]}, {"paperId": "d15d9f119af4b02a6d3a2757ececbfd055c37af8", "externalIds": {"ACL": "2020.acl-main.108", "DBLP": "conf/acl/BevendorffKPS20", "MAG": "3034655460", "DOI": "10.18653/v1/2020.acl-main.108", "CorpusId": 220046692}, "corpusId": 220046692, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d15d9f119af4b02a6d3a2757ececbfd055c37af8", "title": "Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis", "abstract": "This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully preprocessed email corpus to date. We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model. With 96% accuracy on 15 classes of email segments, our model achieves state-of-the-art performance while being more efficient to train than previous ones. All data, code, and trained models are made freely available alongside the paper.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1151-1158"}, "authors": [{"authorId": "9537652", "name": "Janek Bevendorff"}, {"authorId": "2248209", "name": "Khalid Al Khatib"}, {"authorId": "3046200", "name": "Martin Potthast"}, {"authorId": "1405867539", "name": "Benno Stein"}]}, {"paperId": "0741b49f86dda6ca7eb2fabe4172dffa59a1d7e0", "externalIds": {"DBLP": "conf/acl/NikolaevAKKMSA20", "ArXiv": "2005.03436", "ACL": "2020.acl-main.109", "MAG": "3022778924", "DOI": "10.18653/v1/2020.acl-main.109", "CorpusId": 218537924}, "corpusId": 218537924, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0741b49f86dda6ca7eb2fabe4172dffa59a1d7e0", "title": "Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences", "abstract": "The patterns in which the syntax of different languages converges and diverges are often used to inform work on cross-lingual transfer. Nevertheless, little empirical work has been done on quantifying the prevalence of different syntactic divergences across language pairs. We propose a framework for extracting divergence patterns for any language pair from a parallel corpus, building on Universal Dependencies. We show that our framework provides a detailed picture of cross-language divergences, generalizes previous approaches, and lends itself to full automation. We further present a novel dataset, a manually word-aligned subset of the Parallel UD corpus in five languages, and use it to perform a detailed corpus study. We demonstrate the usefulness of the resulting analysis by showing that it can help account for performance patterns of a cross-lingual parser.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.109.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-07", "journal": {"name": "ArXiv", "volume": "abs/2005.03436"}, "authors": [{"authorId": "2064191779", "name": "D. Nikolaev"}, {"authorId": "1454512761", "name": "Ofir Arviv"}, {"authorId": "82585013", "name": "Taelin Karidi"}, {"authorId": "1679969772", "name": "Neta Kenneth"}, {"authorId": "1679970381", "name": "Veronika Mitnik"}, {"authorId": "1679979742", "name": "Lilja Saeboe"}, {"authorId": "2769805", "name": "Omri Abend"}]}, {"paperId": "a2617e990e735f0efe6700afbdadcf19d30376dd", "externalIds": {"ArXiv": "2004.04216", "MAG": "3016257480", "DBLP": "conf/acl/TekirougluCG20", "ACL": "2020.acl-main.110", "DOI": "10.18653/v1/2020.acl-main.110", "CorpusId": 215548382}, "corpusId": 215548382, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a2617e990e735f0efe6700afbdadcf19d30376dd", "title": "Generating Counter Narratives against Online Hate Speech: Data and Strategies", "abstract": "Recently research has started focusing on avoiding undesired effects that come with content moderation, such as censorship and overblocking, when dealing with hatred online. The core idea is to directly intervene in the discussion with textual responses that are meant to counter the hate content and prevent it from further spreading. Accordingly, automation strategies, such as natural language generation, are beginning to be investigated. Still, they suffer from the lack of sufficient amount of quality data and tend to produce generic/repetitive responses. Being aware of the aforementioned limitations, we present a study on how to collect responses to hate effectively, employing large scale unsupervised language models such as GPT-2 for the generation of silver data, and the best annotation strategies/neural architectures that can be used for data filtering before expert validation/post-editing.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 53, "citationCount": 65, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.110.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-08", "journal": {"name": "ArXiv", "volume": "abs/2004.04216"}, "authors": [{"authorId": "2034636", "name": "Serra Sinem Tekiro\u011flu"}, {"authorId": "3365740", "name": "Yi-Ling Chung"}, {"authorId": "1912357", "name": "Marco Guerini"}]}, {"paperId": "f527bcef68aeda601aae314fe5c75185c716e579", "externalIds": {"ACL": "2020.acl-main.111", "MAG": "3021457404", "DBLP": "journals/corr/abs-2005-00630", "ArXiv": "2005.00630", "DOI": "10.18653/v1/2020.acl-main.111", "CorpusId": 218487823}, "corpusId": 218487823, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f527bcef68aeda601aae314fe5c75185c716e579", "title": "KLEJ: Comprehensive Benchmark for Polish Language Understanding", "abstract": "In recent years, a series of Transformer-based models unlocked major improvements in general natural language understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR). To ensure a common evaluation scheme and promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language, which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 42, "citationCount": 54, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.111.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00630"}, "authors": [{"authorId": "52078780", "name": "Piotr Rybak"}, {"authorId": "1667962595", "name": "Robert Mroczkowski"}, {"authorId": "1668105331", "name": "Janusz Tracz"}, {"authorId": "40530061", "name": "Ireneusz Gawlik"}]}, {"paperId": "bcd35ec9bda0cfa306cae6201fd3e50c543e232c", "externalIds": {"ACL": "2020.acl-main.112", "DBLP": "conf/acl/BuechelRH20", "MAG": "3024540984", "ArXiv": "2005.05672", "DOI": "10.18653/v1/2020.acl-main.112", "CorpusId": 218596120}, "corpusId": 218596120, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bcd35ec9bda0cfa306cae6201fd3e50c543e232c", "title": "Learning and Evaluating Emotion Lexicons for 91 Languages", "abstract": "Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 90, "citationCount": 23, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.112.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-12", "journal": {"pages": "1202-1217"}, "authors": [{"authorId": "3449424", "name": "Sven Buechel"}, {"authorId": "1698101293", "name": "Susanna R\u00fccker"}, {"authorId": "1744669", "name": "U. Hahn"}]}, {"paperId": "7b1a7cee3b5e5eac796856b737b9ec7bf896c791", "externalIds": {"MAG": "3034325332", "DBLP": "conf/acl/FomichevaSG20", "ACL": "2020.acl-main.113", "DOI": "10.18653/v1/2020.acl-main.113", "CorpusId": 220049291}, "corpusId": 220049291, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7b1a7cee3b5e5eac796856b737b9ec7bf896c791", "title": "Multi-Hypothesis Machine Translation Evaluation", "abstract": "Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 48, "citationCount": 16, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.113.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1218-1232"}, "authors": [{"authorId": "2006017", "name": "M. Fomicheva"}, {"authorId": "1702974", "name": "Lucia Specia"}, {"authorId": "144204682", "name": "Francisco Guzm\u00e1n"}]}, {"paperId": "b43ad46c0731ac2f5334bc47eff9f012091f3c07", "externalIds": {"DBLP": "conf/acl/OkabeBS20", "ACL": "2020.acl-main.114", "MAG": "3034489970", "DOI": "10.18653/v1/2020.acl-main.114", "CorpusId": 220046446}, "corpusId": 220046446, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b43ad46c0731ac2f5334bc47eff9f012091f3c07", "title": "Multimodal Quality Estimation for Machine Translation", "abstract": "We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE. We compare various multimodality integration and fusion strategies. For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 20, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://wlv.openrepository.com/bitstream/2436/623554/3/Blain_Multimodal_Quality_Estimation_2020.pdf", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1233-1240"}, "authors": [{"authorId": "11712793", "name": "Shu Okabe"}, {"authorId": "1948646", "name": "F. Blain"}, {"authorId": "1702974", "name": "Lucia Specia"}]}, {"paperId": "2e0fab543b49b315d29b0d81e8f958707456f69e", "externalIds": {"ACL": "2020.acl-main.115", "MAG": "3020890980", "DBLP": "conf/acl/SahinKRG20", "ArXiv": "2004.13161", "DOI": "10.18653/v1/2020.acl-main.115", "CorpusId": 216562282}, "corpusId": 216562282, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2e0fab543b49b315d29b0d81e8f958707456f69e", "title": "PuzzLing Machines: A Challenge on Learning From Small Data", "abstract": "Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks. They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge. To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students. These puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions. Solving them does not require external information (e.g., knowledge bases, visual signals) or linguistic expertise, but meta-linguistic awareness and deductive skills. Our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages. We show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge, as expected. We hope that this benchmark, available at https://ukplab.github.io/PuzzLing-Machines/, inspires further efforts towards a new paradigm in NLP\u2014one that is grounded in human-like reasoning and understanding.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 6, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.115.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "1241-1254"}, "authors": [{"authorId": "7655033", "name": "G\u00f6zde G\u00fcl \u015eahin"}, {"authorId": "51208524", "name": "Yova Kementchedjhieva"}, {"authorId": "1660797358", "name": "Phillip Rust"}, {"authorId": "1730400", "name": "Iryna Gurevych"}]}, {"paperId": "595e215e2e96f52f4e617447b60cbee35ec8297f", "externalIds": {"ACL": "2020.acl-main.116", "MAG": "3033204164", "DBLP": "conf/acl/FriedrichATHBML20", "ArXiv": "2006.03039", "DOI": "10.18653/v1/2020.acl-main.116", "CorpusId": 219304992}, "corpusId": 219304992, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/595e215e2e96f52f4e617447b60cbee35ec8297f", "title": "The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain", "abstract": "This paper presents a new challenging information extraction task in the domain of materials science. We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involved materials and measurement conditions. With this paper, we publish our annotation guidelines, as well as our SOFC-Exp corpus consisting of 45 open-access scholarly articles annotated by domain experts. A corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality. We also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set. On all tasks, using BERT embeddings leads to large performance gains, but with increasing task complexity, adding a recurrent neural network on top seems beneficial. Our models will serve as competitive baselines in future work, and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 47, "citationCount": 47, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.116.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-04", "journal": {"pages": "1255-1268"}, "authors": [{"authorId": "33985877", "name": "Annemarie Friedrich"}, {"authorId": "145793834", "name": "Heike Adel"}, {"authorId": "1395444239", "name": "F. Tomazic"}, {"authorId": "1736741346", "name": "Johannes Hingerl"}, {"authorId": "1736759674", "name": "Renou Benteau"}, {"authorId": "1736741341", "name": "Anika Maruscyk"}, {"authorId": "47665464", "name": "Lukas Lange"}]}, {"paperId": "366669ad13ba9cc7c01796739196e34dc4eb5563", "externalIds": {"MAG": "3034478991", "ArXiv": "1911.02984", "DBLP": "journals/corr/abs-1911-02984", "ACL": "2020.acl-main.117", "DOI": "10.18653/v1/2020.acl-main.117", "CorpusId": 207847581}, "corpusId": 207847581, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/366669ad13ba9cc7c01796739196e34dc4eb5563", "title": "The TechQA Dataset", "abstract": "We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size \u2013 600 training, 310 dev, and 490 evaluation question/answer pairs \u2013 thus reflecting the cost of creating large labeled datasets with actual data. Hence, TECHQA is meant to stimulate research in domain adaptation rather than as a resource to build QA systems from scratch. TECHQA was obtained by crawling the IBMDeveloper and DeveloperWorks forums for questions with accepted answers provided in an IBM Technote\u2014a technical document that addresses a specific technical issue. We also release a collection of the 801,998 Technotes available on the web as of April 4, 2019 as a companion resource that can be used to learn representations of the IT domain language.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 16, "citationCount": 30, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.117.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-08", "journal": {"name": "ArXiv", "volume": "abs/1911.02984"}, "authors": [{"authorId": "2879453", "name": "Vittorio Castelli"}, {"authorId": "38725355", "name": "Rishav Chakravarti"}, {"authorId": "2066641453", "name": "Saswati Dana"}, {"authorId": "1388016369", "name": "Anthony Ferritto"}, {"authorId": "1707117", "name": "Radu Florian"}, {"authorId": "39038065", "name": "M. Franz"}, {"authorId": "50252087", "name": "Dinesh Garg"}, {"authorId": "50564082", "name": "Dinesh Khandelwal"}, {"authorId": "40454673", "name": "J. Scott McCarley"}, {"authorId": "1404350473", "name": "Mike McCawley"}, {"authorId": "2056258384", "name": "Mohamed Nasr"}, {"authorId": "2101328894", "name": "Lin Pan"}, {"authorId": "1999108", "name": "Cezar Pendus"}, {"authorId": "1993357", "name": "J. Pitrelli"}, {"authorId": "1403641392", "name": "Saurabh Pujar"}, {"authorId": "1781292", "name": "S. Roukos"}, {"authorId": "1389541398", "name": "Andrzej Sakrajda"}, {"authorId": "2707234", "name": "Avirup Sil"}, {"authorId": "1400349389", "name": "Rosario A. Uceda-Sosa"}, {"authorId": "144582029", "name": "T. Ward"}, {"authorId": "2119062987", "name": "Rong Zhang"}]}, {"paperId": "8c0747409831117ff0ca3836a429d3702ee32a1b", "externalIds": {"MAG": "2983902164", "DBLP": "conf/acl/OpreaM20", "ArXiv": "1911.03123", "ACL": "2020.acl-main.118", "DOI": "10.18653/v1/2020.acl-main.118", "CorpusId": 207847660}, "corpusId": 207847660, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8c0747409831117ff0ca3836a429d3702ee32a1b", "title": "iSarcasm: A Dataset of Intended Sarcasm", "abstract": "We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience. We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors. Examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets, which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far. By providing the iSarcasm dataset, we aim to encourage future NLP research to develop methods for detecting sarcasm in text as intended by the authors of the text, not as labeled under assumptions that we demonstrate to be sub-optimal.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 52, "citationCount": 62, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.118.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-08", "journal": {"pages": "1279-1289"}, "authors": [{"authorId": "2066295029", "name": "Silviu Vlad Oprea"}, {"authorId": "1745226", "name": "Walid Magdy"}]}, {"paperId": "b2075a1ce0e9bea9a6b6c0448c67de067471e885", "externalIds": {"MAG": "3035586188", "ACL": "2020.acl-main.119", "DBLP": "conf/acl/CaiL20", "ArXiv": "2004.05572", "DOI": "10.18653/v1/2020.acl-main.119", "CorpusId": 215744806}, "corpusId": 215744806, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b2075a1ce0e9bea9a6b6c0448c67de067471e885", "title": "AMR Parsing via Graph-Sequence Iterative Inference", "abstract": "We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 100, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.05572", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.05572"}, "authors": [{"authorId": "46689291", "name": "Deng Cai"}, {"authorId": "144594306", "name": "Wai Lam"}]}, {"paperId": "6e6a2fe517b33e1f29d761ae31fb37ddccb9a213", "externalIds": {"DBLP": "journals/corr/abs-2005-10070", "MAG": "3027372696", "ACL": "2020.acl-main.120", "ArXiv": "2005.10070", "DOI": "10.18653/v1/2020.acl-main.120", "CorpusId": 218076648}, "corpusId": 218076648, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6e6a2fe517b33e1f29d761ae31fb37ddccb9a213", "title": "A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal", "abstract": "Multi-document summarization (MDS) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds, presentation of search results, and timeline generation. However, there is a lack of datasets that realistically address such use cases at a scale large enough for training supervised models for this task. This work presents a new dataset for MDS that is large both in the total number of document clusters and in the size of individual clusters. We build this dataset by leveraging the Wikipedia Current Events Portal (WCEP), which provides concise and neutral human-written summaries of news events, with links to external source articles. We also automatically extend these source articles by looking for related articles in the Common Crawl archive. We provide a quantitative analysis of the dataset and empirical results for several state-of-the-art MDS techniques.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 71, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.120.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "1302-1308"}, "authors": [{"authorId": "23997890", "name": "D. Ghalandari"}, {"authorId": "2140238", "name": "Chris Hokamp"}, {"authorId": "2702980", "name": "N. Pham"}, {"authorId": "2054483918", "name": "John Glover"}, {"authorId": "2203428", "name": "Georgiana Ifrim"}]}, {"paperId": "3f4018f26161d6ff8943012d418f338e9246556d", "externalIds": {"ACL": "2020.acl-main.121", "MAG": "3035691816", "DBLP": "conf/acl/ZhuZZZ20", "DOI": "10.18653/v1/2020.acl-main.121", "CorpusId": 220045357}, "corpusId": 220045357, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3f4018f26161d6ff8943012d418f338e9246556d", "title": "Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization", "abstract": "Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English). In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary. We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary. Specifically, we first employ the encoder-decoder attention distribution to attend to the source words. Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word. Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words. Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 37, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.121.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1309-1321"}, "authors": [{"authorId": "24925751", "name": "Junnan Zhu"}, {"authorId": "2110631853", "name": "Yu Zhou"}, {"authorId": "38358352", "name": "Jiajun Zhang"}, {"authorId": "2423168", "name": "Chengqing Zong"}]}, {"paperId": "d02d0edf09227b4a26b1081250ca015ced3005c1", "externalIds": {"MAG": "3026292559", "ACL": "2020.acl-main.122", "ArXiv": "2005.10107", "DBLP": "journals/corr/abs-2005-10107", "DOI": "10.18653/v1/2020.acl-main.122", "CorpusId": 218718559}, "corpusId": 218718559, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d02d0edf09227b4a26b1081250ca015ced3005c1", "title": "Examining the State-of-the-Art in News Timeline Summarization", "abstract": "Previous work on automatic news timeline summarization (TLS) leaves an unclear picture about how this task can generally be approached and how well it is currently solved. This is mostly due to the focus on individual subtasks, such as date selection and date summarization, and to the previous lack of appropriate evaluation metrics for the full TLS task. In this paper, we compare different TLS strategies using appropriate evaluation frameworks, and propose a simple and effective combination of methods that improves over the stateof-the-art on all tested benchmarks. For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 26, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.122.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-20", "journal": {"pages": "1322-1334"}, "authors": [{"authorId": "23997890", "name": "D. Ghalandari"}, {"authorId": "2203428", "name": "Georgiana Ifrim"}]}, {"paperId": "890644102d9c5577c3c2bcb488748ad0e1c1930d", "externalIds": {"DBLP": "journals/corr/abs-2005-00882", "MAG": "3034407283", "ACL": "2020.acl-main.123", "ArXiv": "2005.00882", "DOI": "10.18653/v1/2020.acl-main.123", "CorpusId": 218487340}, "corpusId": 218487340, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/890644102d9c5577c3c2bcb488748ad0e1c1930d", "title": "Improving Truthfulness of Headline Generation", "abstract": "Most studies on abstractive summarization report ROUGE scores between system and reference summaries. However, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text. This paper explores improving the truthfulness in headline generation on two popular datasets. Analyzing headlines generated by the state-of-the-art encoder-decoder model, we show that the model sometimes generates untruthful headlines. We conjecture that one of the reasons lies in untruthful supervision data used for training the model. In order to quantify the truthfulness of article-headline pairs, we consider the textual entailment of whether an article entails its headline. After confirming quite a few untruthful instances in the datasets, this study hypothesizes that removing untruthful instances from the supervision data may remedy the problem of the untruthful behaviors of the model. Building a binary classifier that predicts an entailment relation between an article and its headline, we filter out untruthful instances from the supervision data. Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 41, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.123.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00882"}, "authors": [{"authorId": "1668029722", "name": "Kazuki Matsumaru"}, {"authorId": "33544449", "name": "Sho Takase"}, {"authorId": "1764004", "name": "Naoaki Okazaki"}]}, {"paperId": "4c5c21f424042f77ea3fc809b3f9365f50ef6126", "externalIds": {"ArXiv": "2005.03724", "MAG": "3021557422", "DBLP": "journals/corr/abs-2005-03724", "ACL": "2020.acl-main.124", "DOI": "10.18653/v1/2020.acl-main.124", "CorpusId": 218571152}, "corpusId": 218571152, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4c5c21f424042f77ea3fc809b3f9365f50ef6126", "title": "SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization", "abstract": "We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39%. Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 90, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.03724", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.03724"}, "authors": [{"authorId": "152673773", "name": "Yang Gao"}, {"authorId": "98902616", "name": "Wei Zhao"}, {"authorId": "2620186", "name": "Steffen Eger"}]}, {"paperId": "fe5b999305bdaee6e3fc0868df77e99adde39b3a", "externalIds": {"DBLP": "conf/acl/XuLYWHZ20", "MAG": "3034682120", "ACL": "2020.acl-main.125", "DOI": "10.18653/v1/2020.acl-main.125", "CorpusId": 220046828}, "corpusId": 220046828, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fe5b999305bdaee6e3fc0868df77e99adde39b3a", "title": "Self-Attention Guided Copy Mechanism for Abstractive Summarization", "abstract": "Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary. Generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge. In this work, we propose a Transformer-based model to enhance the copy mechanism. Specifically, we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer. We use the centrality of each source word to guide the copy process explicitly. Experimental results show that the self-attention graph provides useful guidance for the copy distribution. Our proposed models significantly outperform the baseline methods on the CNN/Daily Mail dataset and the Gigaword dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 43, "citationCount": 65, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.125.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1355-1362"}, "authors": [{"authorId": "1500656515", "name": "Song Xu"}, {"authorId": "47893252", "name": "Haoran Li"}, {"authorId": "2069399205", "name": "Peng Yuan"}, {"authorId": "1768799", "name": "Youzheng Wu"}, {"authorId": "144137069", "name": "Xiaodong He"}, {"authorId": "150048906", "name": "Bowen Zhou"}]}, {"paperId": "b8dfffe8073badb3a8d5fdeb639dc5a4ec2f4b99", "externalIds": {"ACL": "2020.acl-main.126", "MAG": "3035729454", "DBLP": "journals/corr/abs-2005-10716", "ArXiv": "2005.10716", "DOI": "10.18653/v1/2020.acl-main.126", "CorpusId": 218763500}, "corpusId": 218763500, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b8dfffe8073badb3a8d5fdeb639dc5a4ec2f4b99", "title": "Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation", "abstract": "Open Domain dialog system evaluation is one of the most important challenges in dialog research. Existing automatic evaluation metrics, such as BLEU are mostly reference-based. They calculate the difference between the generated response and a limited number of available references. Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers from bias and variance among different users. To alleviate this problem, we formulate dialog evaluation as a comparison task. We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them. Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog comparison task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 69, "citationCount": 27, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.126.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-21", "journal": {"pages": "1363-1374"}, "authors": [{"authorId": "151253861", "name": "Weixin Liang"}, {"authorId": "145085305", "name": "James Y. Zou"}, {"authorId": "1564034697", "name": "Zhou Yu"}]}, {"paperId": "941481fbf02654d90751d8fa1eff1d1b4ca079e8", "externalIds": {"MAG": "3021756887", "ArXiv": "2004.13249", "DBLP": "journals/corr/abs-2004-13249", "ACL": "2020.acl-main.127", "DOI": "10.18653/V1/2020.ACL-MAIN.127", "CorpusId": 216562635}, "corpusId": 216562635, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/941481fbf02654d90751d8fa1eff1d1b4ca079e8", "title": "Conversational Word Embedding for Retrieval-Based Dialog System", "abstract": "Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs  to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.The experiment results show that PR-Embedding can improve the quality of the selected response.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 25, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.127.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.13249"}, "authors": [{"authorId": "145910725", "name": "Wentao Ma"}, {"authorId": "3043830", "name": "Yiming Cui"}, {"authorId": "40282288", "name": "Ting Liu"}, {"authorId": "2152689328", "name": "Dong Wang"}, {"authorId": "2108620507", "name": "Shijin Wang"}, {"authorId": "40936264", "name": "Guoping Hu"}]}, {"paperId": "602c62938bfbb0b81aa7d01cceae1e2c0b434791", "externalIds": {"MAG": "3041209438", "DBLP": "journals/corr/abs-2006-05702", "ACL": "2020.acl-main.128", "ArXiv": "2006.05702", "DOI": "10.18653/v1/2020.acl-main.128", "CorpusId": 219558508}, "corpusId": 219558508, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/602c62938bfbb0b81aa7d01cceae1e2c0b434791", "title": "Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network", "abstract": "In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word\u2019s similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model \u2013 TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 59, "citationCount": 141, "influentialCitationCount": 38, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.128.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-10", "journal": {"pages": "1381-1393"}, "authors": [{"authorId": "8471176", "name": "Yutai Hou"}, {"authorId": "2256319", "name": "Wanxiang Che"}, {"authorId": "30538619", "name": "Y. Lai"}, {"authorId": "50251585", "name": "Zhihan Zhou"}, {"authorId": "3317638", "name": "Yijia Liu"}, {"authorId": "2118960667", "name": "Han Liu"}, {"authorId": "40282288", "name": "Ting Liu"}]}, {"paperId": "75242a19646ab0dbfd4e69d64b3a3b984121b8c1", "externalIds": {"DBLP": "journals/corr/abs-2004-11054", "MAG": "3035597485", "ArXiv": "2004.11054", "ACL": "2020.acl-main.129", "DOI": "10.18653/v1/2020.acl-main.129", "CorpusId": 216080844}, "corpusId": 216080844, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/75242a19646ab0dbfd4e69d64b3a3b984121b8c1", "title": "Learning Dialog Policies from Weak Demonstrations", "abstract": "Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user\u2019s requests. We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators. We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment. Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on out-of-domain data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 49, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.129.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "1394-1405"}, "authors": [{"authorId": "1641479429", "name": "Gabriel Gordon-Hall"}, {"authorId": "2108320", "name": "P. Gorinski"}, {"authorId": "40146204", "name": "Shay B. Cohen"}]}, {"paperId": "07ded4cf00095d91e8689a0a52d9e20eb64aca0b", "externalIds": {"DBLP": "journals/corr/abs-2004-04494", "ACL": "2020.acl-main.130", "MAG": "3016194643", "ArXiv": "2004.04494", "DOI": "10.18653/v1/2020.acl-main.130", "CorpusId": 215548215}, "corpusId": 215548215, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/07ded4cf00095d91e8689a0a52d9e20eb64aca0b", "title": "MuTual: A Dataset for Multi-Turn Dialogue Reasoning", "abstract": "Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities. To facilitate the conversation reasoning research, we introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams. Compared to previous benchmarks for non-task oriented dialogue systems, MuTual is much more challenging since it requires a model that be able to handle various reasoning problems. Empirical results show that state-of-the-art methods only reach 71%, which is far behind human performance of 94%, indicating that there is ample room for improving reasoning ability.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 111, "influentialCitationCount": 25, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.130.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-09", "journal": {"name": "ArXiv", "volume": "abs/2004.04494"}, "authors": [{"authorId": "152496687", "name": "Leyang Cui"}, {"authorId": "49176273", "name": "Yu Wu"}, {"authorId": "2107983441", "name": "Shujie Liu"}, {"authorId": "1591125925", "name": "Yue Zhang"}, {"authorId": "92660691", "name": "Ming Zhou"}]}, {"paperId": "298a68153859303ee70b3ef1525ee9c7031e32f5", "externalIds": {"DBLP": "conf/acl/LiuCCLCZZ20", "ArXiv": "2004.05388", "MAG": "3035044096", "ACL": "2020.acl-main.131", "DOI": "10.18653/v1/2020.acl-main.131", "CorpusId": 215745354}, "corpusId": 215745354, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/298a68153859303ee70b3ef1525ee9c7031e32f5", "title": "You Impress Me: Dialogue Generation via Mutual Persona Perception", "abstract": "Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose P\u02c62 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, P\u02c62 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 108, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.131.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-11", "journal": {"pages": "1417-1427"}, "authors": [{"authorId": "1409707585", "name": "Qian Liu"}, {"authorId": "2116613401", "name": "Yihong Chen"}, {"authorId": "143876723", "name": "B. Chen"}, {"authorId": "4648762", "name": "Jian-Guang Lou"}, {"authorId": "2144251677", "name": "Zixuan Chen"}, {"authorId": "152108463", "name": "Bin Zhou"}, {"authorId": "1485159990", "name": "Dongmei Zhang"}]}, {"paperId": "175d5bd966e874ec36e93c880a74164af8da6c84", "externalIds": {"DBLP": "journals/corr/abs-2004-07898", "ACL": "2020.acl-main.132", "MAG": "3034928924", "ArXiv": "2004.07898", "DOI": "10.18653/v1/2020.acl-main.132", "CorpusId": 215814427}, "corpusId": 215814427, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/175d5bd966e874ec36e93c880a74164af8da6c84", "title": "Bridging Anaphora Resolution as Question Answering", "abstract": "Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of \u201cquasi-bridging\u201d training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (Ro \u0308siger, 2018)).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 35, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.07898", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-16", "journal": {"name": "ArXiv", "volume": "abs/2004.07898"}, "authors": [{"authorId": "39517968", "name": "Yufang Hou"}]}, {"paperId": "33168a640106bbb2d48cf2064a61e69f354ac5e7", "externalIds": {"MAG": "3033475175", "DBLP": "conf/acl/MesgarBG20", "ACL": "2020.acl-main.133", "DOI": "10.18653/v1/2020.acl-main.133", "CorpusId": 219260210}, "corpusId": 219260210, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/33168a640106bbb2d48cf2064a61e69f354ac5e7", "title": "Dialogue Coherence Assessment Without Explicit Dialogue Act Labels", "abstract": "Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment. We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 40, "citationCount": 26, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.133.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-08-22", "journal": {"pages": "1439-1450"}, "authors": [{"authorId": "2568628", "name": "Mohsen Mesgar"}, {"authorId": "1753296844", "name": "Sebastian Bucker"}, {"authorId": "1730400", "name": "Iryna Gurevych"}]}, {"paperId": "c72bbe97e0892dac3c3b09e98b762d729dd64902", "externalIds": {"ACL": "2020.acl-main.134", "MAG": "3034214743", "DBLP": "conf/acl/YuTVK20", "DOI": "10.18653/v1/2020.acl-main.134", "CorpusId": 220045928}, "corpusId": 220045928, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c72bbe97e0892dac3c3b09e98b762d729dd64902", "title": "Fast and Accurate Non-Projective Dependency Tree Linearization", "abstract": "We propose a graph-based method to tackle the dependency tree linearization task. We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1451-1462"}, "authors": [{"authorId": "2003808127", "name": "Xiang Yu"}, {"authorId": "87863676", "name": "Simon Tannert"}, {"authorId": "4160376", "name": "Ngoc Thang Vu"}, {"authorId": "1716963", "name": "Jonas Kuhn"}]}, {"paperId": "6e516a4fb5bed1306bdcf10482904f6c7ba8a52c", "externalIds": {"ACL": "2020.acl-main.135", "MAG": "3034383728", "DBLP": "conf/acl/PanXFCK20", "ArXiv": "2004.12704", "DOI": "10.18653/v1/2020.acl-main.135", "CorpusId": 216553210}, "corpusId": 216553210, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6e516a4fb5bed1306bdcf10482904f6c7ba8a52c", "title": "Semantic Graphs for Generating Deep Questions", "abstract": "This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 69, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.135.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-27", "journal": {"pages": "1463-1475"}, "authors": [{"authorId": "3470231", "name": "Liangming Pan"}, {"authorId": "46268944", "name": "Yuxi Xie"}, {"authorId": "1717629", "name": "Yansong Feng"}, {"authorId": "144078686", "name": "Tat-Seng Chua"}, {"authorId": "37596605", "name": "Min-Yen Kan"}]}, {"paperId": "0070d0cc816f7d20d7a07524ee9966d1e77ca58c", "externalIds": {"ACL": "2020.acl-main.136", "MAG": "3034617555", "DBLP": "conf/acl/WeiSWTC20", "DOI": "10.18653/v1/2020.acl-main.136", "CorpusId": 216562386}, "corpusId": 216562386, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0070d0cc816f7d20d7a07524ee9966d1e77ca58c", "title": "A Novel Cascade Binary Tagging Framework for Relational Triple Extraction", "abstract": "Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework. It enjoys further performance boost when employing a pre-trained BERT encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG, respectively. In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios. The source code and data are released online.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 34, "citationCount": 286, "influentialCitationCount": 54, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.136.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-07", "journal": {"pages": "1476-1488"}, "authors": [{"authorId": "122835755", "name": "Zhepei Wei"}, {"authorId": "51111230", "name": "Jianlin Su"}, {"authorId": "3189266", "name": "Yue Wang"}, {"authorId": "2152948229", "name": "Yuan Tian"}, {"authorId": "2118858577", "name": "Yi Chang"}]}, {"paperId": "cf6b6a55e3e10fdb5590c66ef6056abe40c867e7", "externalIds": {"DBLP": "conf/acl/KruiperVCDK20", "ACL": "2020.acl-main.137", "MAG": "3025620128", "ArXiv": "2005.07751", "DOI": "10.18653/v1/2020.acl-main.137", "CorpusId": 218674072}, "corpusId": 218674072, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cf6b6a55e3e10fdb5590c66ef6056abe40c867e7", "title": "In Layman\u2019s Terms: Semi-Open Relation Extraction from Scientific Texts", "abstract": "Information Extraction (IE) from scientific texts can be used to guide readers to the central information in scientific documents. But narrow IE systems extract only a fraction of the information captured, and Open IE systems do not perform well on the long and complex sentences encountered in scientific texts. In this work we combine the output of both types of systems to achieve Semi-Open Relation Extraction, a new task that we explore in the Biology domain. First, we present the Focused Open Biological Information Extraction (FOBIE) dataset and use FOBIE to train a state-of-the-art narrow scientific IE system to extract trade-off relations and arguments that are central to biology texts. We then run both the narrow IE system and a state-of-the-art Open IE system on a corpus of 10K open-access scientific biological texts. We show that a significant amount (65%) of erroneous and uninformative Open IE extractions can be filtered using narrow IE extractions. Furthermore, we show that the retained extractions are significantly more often informative to a reader.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 66, "citationCount": 13, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.137.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-15", "journal": {"name": "ArXiv", "volume": "abs/2005.07751"}, "authors": [{"authorId": "3430563", "name": "Ruben Kruiper"}, {"authorId": "145655241", "name": "J. Vincent"}, {"authorId": "1403798853", "name": "Jessica Chen-Burger"}, {"authorId": "144057272", "name": "M. Desmulliez"}, {"authorId": "2621022", "name": "Ioannis Konstas"}]}, {"paperId": "99314a532a3358cb86064fc8917ed2c283227539", "externalIds": {"MAG": "3025877323", "DBLP": "journals/corr/abs-2005-07162", "ACL": "2020.acl-main.138", "ArXiv": "2005.07162", "DOI": "10.18653/v1/2020.acl-main.138", "CorpusId": 218628841}, "corpusId": 218628841, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/99314a532a3358cb86064fc8917ed2c283227539", "title": "NAT: Noise-Aware Training for Robust Neural Sequence Labeling", "abstract": "Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs\u2014as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. We make our code and data publicly available for the research community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 55, "citationCount": 13, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.138.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-14", "journal": {"pages": "1501-1517"}, "authors": [{"authorId": "134442417", "name": "Marcin Namysl"}, {"authorId": "1699019", "name": "Sven Behnke"}, {"authorId": "152162939", "name": "Joachim Kohler"}]}, {"paperId": "bedb476a4a691e27169563f2fd275c2f44efd187", "externalIds": {"ArXiv": "2004.14723", "MAG": "3035097673", "DBLP": "journals/corr/abs-2004-14723", "ACL": "2020.acl-main.139", "DOI": "10.18653/v1/2020.acl-main.139", "CorpusId": 216867841}, "corpusId": 216867841, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bedb476a4a691e27169563f2fd275c2f44efd187", "title": "Named Entity Recognition without Labelled Data: A Weak Supervision Approach", "abstract": "Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain. But what should one do when there is no hand-labelled data for the target domain? This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision. The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain. These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions. A sequence labelling model can finally be trained on the basis of this unified annotation. We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 65, "citationCount": 88, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.139.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"pages": "1518-1533"}, "authors": [{"authorId": "1746165", "name": "Pierre Lison"}, {"authorId": "51116121", "name": "A. Hubin"}, {"authorId": "144435436", "name": "Jeremy Barnes"}, {"authorId": "3083347", "name": "Samia Touileb"}]}, {"paperId": "98a25ab6f929583138e526f8dccea4b343865c0d", "externalIds": {"DBLP": "conf/acl/AltGH20", "ACL": "2020.acl-main.140", "MAG": "3034760557", "DOI": "10.18653/v1/2020.acl-main.140", "CorpusId": 220047815}, "corpusId": 220047815, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/98a25ab6f929583138e526f8dccea4b343865c0d", "title": "Probing Linguistic Features of Sentence-Level Representations in Relation Extraction", "abstract": "Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder. We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find that the bias induced by the architecture and the inclusion of linguistic features are clearly expressed in the probing task performance. For example, adding contextualized word representations greatly increases performance on probing tasks with a focus on named entity and part-of-speech information, and yields better results in RE. In contrast, entity masking improves RE, but considerably lowers performance on entity type related probing tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.140.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-17", "journal": {"pages": "1534-1545"}, "authors": [{"authorId": "3413117", "name": "Christoph Alt"}, {"authorId": "3449621", "name": "Aleksandra Gabryszak"}, {"authorId": "36943315", "name": "Leonhard Hennig"}]}, {"paperId": "013faec0400d315935e71a2bdfeb22cc83752b3e", "externalIds": {"MAG": "3035053871", "DBLP": "conf/acl/NanGSL20", "ACL": "2020.acl-main.141", "ArXiv": "2005.06312", "DOI": "10.18653/v1/2020.acl-main.141", "CorpusId": 218613850}, "corpusId": 218613850, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/013faec0400d315935e71a2bdfeb22cc83752b3e", "title": "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction", "abstract": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 66, "citationCount": 193, "influentialCitationCount": 44, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.141.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-13", "journal": {"pages": "1546-1557"}, "authors": [{"authorId": "2056582888", "name": "Guoshun Nan"}, {"authorId": "2681038", "name": "Zhijiang Guo"}, {"authorId": "3305422", "name": "Ivan Sekulic"}, {"authorId": "2153424287", "name": "Wei Lu"}]}, {"paperId": "85bbec78eb692d78020f3fbcca70d34f279485af", "externalIds": {"MAG": "3034891697", "ACL": "2020.acl-main.142", "ArXiv": "2004.14855", "DBLP": "conf/acl/AltGH20a", "DOI": "10.18653/v1/2020.acl-main.142", "CorpusId": 216869183}, "corpusId": 216869183, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/85bbec78eb692d78020f3fbcca70d34f279485af", "title": "TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task", "abstract": "TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 101, "influentialCitationCount": 28, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.142.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "1558-1569"}, "authors": [{"authorId": "3413117", "name": "Christoph Alt"}, {"authorId": "3449621", "name": "Aleksandra Gabryszak"}, {"authorId": "36943315", "name": "Leonhard Hennig"}]}, {"paperId": "4a3249107c35eeebdba75fdd18a4387cb92de221", "externalIds": {"MAG": "3034428794", "DBLP": "conf/acl/DuanJJTZCLZ20", "ArXiv": "2007.02671", "ACL": "2020.acl-main.143", "DOI": "10.18653/v1/2020.acl-main.143", "CorpusId": 220046231}, "corpusId": 220046231, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4a3249107c35eeebdba75fdd18a4387cb92de221", "title": "Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences", "abstract": "In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training (AT) to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 24, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2007.02671", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1570-1579"}, "authors": [{"authorId": "2109002", "name": "Xiangyu Duan"}, {"authorId": "1441095620", "name": "Baijun Ji"}, {"authorId": "2113238072", "name": "Hao Jia"}, {"authorId": "145035830", "name": "Min Tan"}, {"authorId": "1390813134", "name": "Min Zhang"}, {"authorId": "2229601", "name": "Boxing Chen"}, {"authorId": "48244817", "name": "Weihua Luo"}, {"authorId": "1591125925", "name": "Yue Zhang"}]}, {"paperId": "b390df1a35c2a493ad2a796a6ac68f49be8a43d2", "externalIds": {"ACL": "2020.acl-main.144", "DBLP": "conf/acl/XuCS20", "MAG": "3035531963", "DOI": "10.18653/v1/2020.acl-main.144", "CorpusId": 220045828}, "corpusId": 220045828, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b390df1a35c2a493ad2a796a6ac68f49be8a43d2", "title": "Boosting Neural Machine Translation with Similar Translations", "abstract": "This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with \u201ccopy\u201d information while translations based on embedding similarities tend to extend the translation \u201ccontext\u201d. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 62, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.144.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1580-1590"}, "authors": [{"authorId": "1925543", "name": "Jitao Xu"}, {"authorId": "2209023", "name": "J. Crego"}, {"authorId": "3053934", "name": "Jean Senellart"}]}, {"paperId": "9d92905edc1a5d027b0827d1309bc6ac03dd94a0", "externalIds": {"MAG": "3034407863", "DBLP": "conf/acl/GaoNHH20", "ArXiv": "2004.14788", "ACL": "2020.acl-main.145", "DOI": "10.18653/v1/2020.acl-main.145", "CorpusId": 209319301}, "corpusId": 209319301, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9d92905edc1a5d027b0827d1309bc6ac03dd94a0", "title": "Character-Level Translation with Self-attention", "abstract": "We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 20, "citationCount": 20, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.145.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"pages": "1591-1604"}, "authors": [{"authorId": "1663413126", "name": "Yingqiang Gao"}, {"authorId": "32160741", "name": "Nikola I. Nikolov"}, {"authorId": "2175899", "name": "Yuhuang Hu"}, {"authorId": "2066973317", "name": "Richard H. R. Hahnloser"}]}, {"paperId": "2eee73090484b836de98b1e6cc8f6eba5ec8248f", "externalIds": {"MAG": "3022467813", "ACL": "2020.acl-main.146", "ArXiv": "2004.14675", "DBLP": "conf/acl/ZenkelWD20", "DOI": "10.18653/v1/2020.acl-main.146", "CorpusId": 216869436}, "corpusId": 216869436, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2eee73090484b836de98b1e6cc8f6eba5ec8248f", "title": "End-to-End Neural Word Alignment Outperforms GIZA++", "abstract": "Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems. Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training. We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 49, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.146.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"pages": "1605-1617"}, "authors": [{"authorId": "24055426", "name": "Thomas Zenkel"}, {"authorId": "2046393", "name": "Joern Wuebker"}, {"authorId": "3069584", "name": "John DeNero"}]}, {"paperId": "49f1525c78d42037ffe4cd5a0c451bb7f5eb27b2", "externalIds": {"MAG": "3035589854", "ACL": "2020.acl-main.147", "DBLP": "conf/acl/BugliarelloO20", "ArXiv": "1909.03149", "DOI": "10.18653/v1/2020.acl-main.147", "CorpusId": 214802912}, "corpusId": 214802912, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/49f1525c78d42037ffe4cd5a0c451bb7f5eb27b2", "title": "Enhancing Machine Translation with Dependency-Aware Self-Attention", "abstract": "Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 50, "citationCount": 44, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.147.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-06", "journal": {"pages": "1618-1627"}, "authors": [{"authorId": "83574123", "name": "Emanuele Bugliarello"}, {"authorId": "1764004", "name": "Naoaki Okazaki"}]}, {"paperId": "7600bc8922c225b299658417f83d54e450a28642", "externalIds": {"DBLP": "conf/acl/ZhangWTS20", "ArXiv": "2004.11867", "ACL": "2020.acl-main.148", "MAG": "3017454464", "DOI": "10.18653/v1/2020.acl-main.148", "CorpusId": 216144650}, "corpusId": 216144650, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7600bc8922c225b299658417f83d54e450a28642", "title": "Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation", "abstract": "Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 46, "citationCount": 254, "influentialCitationCount": 66, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.148.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-24", "journal": {"name": "ArXiv", "volume": "abs/2004.11867"}, "authors": [{"authorId": "48335426", "name": "Biao Zhang"}, {"authorId": "6177629", "name": "P. Williams"}, {"authorId": "144889265", "name": "Ivan Titov"}, {"authorId": "2082372", "name": "Rico Sennrich"}]}, {"paperId": "fd9e0fc8753f5b2c7af5f5ce03d819e48332925c", "externalIds": {"ArXiv": "2005.02354", "DBLP": "conf/acl/BugliarelloMACO20", "MAG": "3022822895", "ACL": "2020.acl-main.149", "DOI": "10.18653/v1/2020.acl-main.149", "CorpusId": 218502235}, "corpusId": 218502235, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fd9e0fc8753f5b2c7af5f5ce03d819e48332925c", "title": "It\u2019s Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information", "abstract": "The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 50, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.research-collection.ethz.ch/bitstream/20.500.11850/462891/1/2020.acl-main.149.pdf", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"name": "ArXiv", "volume": "abs/2005.02354"}, "authors": [{"authorId": "83574123", "name": "Emanuele Bugliarello"}, {"authorId": "27689253", "name": "Sabrina J. Mielke"}, {"authorId": "49513989", "name": "Antonios Anastasopoulos"}, {"authorId": "1750769", "name": "Ryan Cotterell"}, {"authorId": "1764004", "name": "Naoaki Okazaki"}]}, {"paperId": "dc5fa49b08942cff64ed39b10adafd07e0fd6b3b", "externalIds": {"DBLP": "conf/acl/ZhuYCL20", "ACL": "2020.acl-main.150", "MAG": "3034719878", "DOI": "10.18653/v1/2020.acl-main.150", "CorpusId": 220045819}, "corpusId": 220045819, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dc5fa49b08942cff64ed39b10adafd07e0fd6b3b", "title": "Language-aware Interlingua for Multilingual Neural Machine Translation", "abstract": "Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 20, "citationCount": 28, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1650-1655"}, "authors": [{"authorId": "2594744", "name": "Changfeng Zhu"}, {"authorId": "1712236217", "name": "Heng Yu"}, {"authorId": "3456696", "name": "Shanbo Cheng"}, {"authorId": "48244817", "name": "Weihua Luo"}]}, {"paperId": "4a9addd883124c90906df58d74dc520ddb14a9b4", "externalIds": {"MAG": "3020922140", "ArXiv": "2005.01196", "DBLP": "journals/corr/abs-2005-01196", "ACL": "2020.acl-main.151", "DOI": "10.18653/v1/2020.acl-main.151", "CorpusId": 218487791}, "corpusId": 218487791, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4a9addd883124c90906df58d74dc520ddb14a9b4", "title": "On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation", "abstract": "Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish \u201ctranslationese\u201d, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 64, "citationCount": 55, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.151.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "1656-1671"}, "authors": [{"authorId": "98902616", "name": "Wei Zhao"}, {"authorId": "1666177566", "name": "Goran Glavavs"}, {"authorId": "35512303", "name": "Maxime Peyrard"}, {"authorId": null, "name": "Yang Gao"}, {"authorId": "145781376", "name": "Robert West"}, {"authorId": "2620186", "name": "Steffen Eger"}]}, {"paperId": "bcfbc82291a81ac88516b2aa042d71e9de3cfd49", "externalIds": {"DBLP": "conf/acl/ChenBHK20", "MAG": "3035308166", "ACL": "2020.acl-main.152", "DOI": "10.18653/v1/2020.acl-main.152", "CorpusId": 219690743}, "corpusId": 219690743, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bcfbc82291a81ac88516b2aa042d71e9de3cfd49", "title": "Parallel Sentence Mining by Constrained Decoding", "abstract": "We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation. Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus. We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search. When benchmarked on the BUCC shared task, our method achieves results comparable to other submissions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.152.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1672-1678"}, "authors": [{"authorId": "143616669", "name": "Pinzhen Chen"}, {"authorId": "3444222", "name": "Nikolay Bogoychev"}, {"authorId": "1702066", "name": "Kenneth Heafield"}, {"authorId": "1388337832", "name": "Faheem Kirefu"}]}, {"paperId": "39283c3d6262b24bd61c88038353f3ed0145b6e4", "externalIds": {"ArXiv": "2004.13310", "DBLP": "conf/acl/DingWT20", "MAG": "3034998639", "ACL": "2020.acl-main.153", "DOI": "10.18653/v1/2020.acl-main.153", "CorpusId": 216562331}, "corpusId": 216562331, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/39283c3d6262b24bd61c88038353f3ed0145b6e4", "title": "Self-Attention with Cross-Lingual Position Representation", "abstract": "Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently. Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem. In this paper, we augment SANs with cross-lingual position representations to model the bilingually aware latent structure for the input sentence. Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments. Experimental results on WMT\u201914 English\\RightarrowGerman, WAT\u201917 Japanese\\RightarrowEnglish, and WMT\u201917 Chinese\\LeftrightarrowEnglish translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines. Extensive analyses confirm that the performance gains come from the cross-lingual information.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 32, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.153.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.13310"}, "authors": [{"authorId": "46573238", "name": "Liang Ding"}, {"authorId": "1800190", "name": "Longyue Wang"}, {"authorId": "143719920", "name": "D. Tao"}]}, {"paperId": "a5ccd107c08c5a73ee89aee00a15bc4a0c8f7397", "externalIds": {"MAG": "3034987021", "DBLP": "conf/acl/HovyBF20", "ACL": "2020.acl-main.154", "DOI": "10.18653/v1/2020.acl-main.154", "CorpusId": 220047833}, "corpusId": 220047833, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a5ccd107c08c5a73ee89aee00a15bc4a0c8f7397", "title": "\u201cYou Sound Just Like Your Father\u201d Commercial Machine Translation Systems Include Stylistic Biases", "abstract": "The main goal of machine translation has been to convey the correct content. Stylistic considerations have been at best secondary. We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make demographically diverse samples from five languages \u201csound\u201d older and more male than the original. Our findings suggest that translation models reflect demographic bias in the training data. This opens up interesting new research avenues in machine translation to take stylistic considerations into account.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 10, "citationCount": 38, "influentialCitationCount": 2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1686-1690"}, "authors": [{"authorId": "2022288", "name": "Dirk Hovy"}, {"authorId": "49224924", "name": "Federico Bianchi"}, {"authorId": "2223077", "name": "Tommaso Fornaciari"}]}, {"paperId": "dc9222f00b4eed684b4015d7b2d28938f995ceee", "externalIds": {"ACL": "2020.acl-main.155", "MAG": "3034673518", "DBLP": "conf/acl/HerbigDPMMKG20", "DOI": "10.18653/v1/2020.acl-main.155", "CorpusId": 220046466}, "corpusId": 220046466, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dc9222f00b4eed684b4015d7b2d28938f995ceee", "title": "MMPE: A Multi-Modal Interface for Post-Editing Machine Translation", "abstract": "Current advances in machine translation (MT) increase the need for translators to switch from traditional translation to post-editing (PE) of machine-translated text, a process that saves time and reduces errors. This affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT. The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while they are of limited use for longer insertions. On the other hand, speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions but offer less potential for deletion and reordering. Overall, participants were enthusiastic about the new modalities and saw them as good extensions to mouse & keyboard, but not as a complete substitute.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 42, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1691-1702"}, "authors": [{"authorId": "3181259", "name": "Nico Herbig"}, {"authorId": "1666644166", "name": "Tim D\u00fcwel"}, {"authorId": "145244784", "name": "Santanu Pal"}, {"authorId": "1754625305", "name": "Kalliopi Meladaki"}, {"authorId": "1754554603", "name": "Mahsa Monshizadeh"}, {"authorId": "145263612", "name": "A. Kr\u00fcger"}, {"authorId": "7519068", "name": "Josef van Genabith"}]}, {"paperId": "528dd0da358b4939d99eeb92548deccfeac48bd6", "externalIds": {"DBLP": "journals/corr/abs-2006-06202", "ACL": "2020.acl-main.156", "ArXiv": "2006.06202", "MAG": "3034617741", "DOI": "10.18653/v1/2020.acl-main.156", "CorpusId": 219573244}, "corpusId": 219573244, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/528dd0da358b4939d99eeb92548deccfeac48bd6", "title": "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages", "abstract": "We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 47, "citationCount": 151, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.156.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-11", "journal": {"pages": "1703-1714"}, "authors": [{"authorId": "147846651", "name": "Pedro Ortiz Suarez"}, {"authorId": "49799441", "name": "L. Romary"}, {"authorId": "68990982", "name": "Beno\u00eet Sagot"}]}, {"paperId": "57778c196aca71a82c48a58522ac535c1ce53a06", "externalIds": {"ACL": "2020.acl-main.157", "MAG": "3023183123", "DBLP": "conf/acl/ConfortiBPGTC20", "ArXiv": "2005.00388", "DOI": "10.18653/v1/2020.acl-main.157", "CorpusId": 218470042}, "corpusId": 218470042, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/57778c196aca71a82c48a58522ac535c1ce53a06", "title": "Will-They-Won\u2019t-They: A Very Large Dataset for Stance Detection on Twitter", "abstract": "We present a new challenging stance detection dataset, called Will-They-Won\u2019t-They (WT--WT), which contains 51,284 tweets in English, making it by far the largest available dataset of the type. All the annotations are carried out by experts; therefore, the dataset constitutes a high-quality and reliable benchmark for future research in stance detection. Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 59, "citationCount": 69, "influentialCitationCount": 19, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.157.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "1715-1724"}, "authors": [{"authorId": "6736769", "name": "Costanza Conforti"}, {"authorId": "1666182796", "name": "Jakob Berndt"}, {"authorId": "1717641", "name": "Mohammad Taher Pilehvar"}, {"authorId": "101958625", "name": "Chryssi Giannitsarou"}, {"authorId": "2766988", "name": "Flavio Toxvaerd"}, {"authorId": "50638196", "name": "Nigel Collier"}]}, {"paperId": "427973cbf535187c95cd174adce64c20292a0c78", "externalIds": {"DBLP": "conf/acl/HuGQWL20", "ACL": "2020.acl-main.158", "ArXiv": "2005.03692", "MAG": "3022853932", "DOI": "10.18653/v1/2020.acl-main.158", "CorpusId": 218571018}, "corpusId": 218571018, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/427973cbf535187c95cd174adce64c20292a0c78", "title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models", "abstract": "While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 92, "citationCount": 135, "influentialCitationCount": 23, "isOpenAccess": true, "openAccessPdf": {"url": "https://dspace.mit.edu/bitstream/1721.1/130402/2/2020.acl-main.158.pdf", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-07", "journal": {"name": "ArXiv", "volume": "abs/2005.03692"}, "authors": [{"authorId": "2112327566", "name": "Jennifer Hu"}, {"authorId": "24339276", "name": "Jon Gauthier"}, {"authorId": "1483502658", "name": "Peng Qian"}, {"authorId": "51445267", "name": "Ethan Gotlieb Wilcox"}, {"authorId": "50007746", "name": "R. Levy"}]}, {"paperId": "02d3b7c6e887391a38dd4b2839f998fa02ee324a", "externalIds": {"ACL": "2020.acl-main.159", "DBLP": "conf/acl/McCurdyGL20", "ArXiv": "2005.08826", "MAG": "3024934151", "DOI": "10.18653/v1/2020.acl-main.159", "CorpusId": 218674568}, "corpusId": 218674568, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/02d3b7c6e887391a38dd4b2839f998fa02ee324a", "title": "Inflecting When There\u2019s No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals", "abstract": "Can artificial neural networks learn to represent inflectional morphology and generalize to new words as human speakers do? Kirov and Cotterell (2018) argue that the answer is yes: modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words. However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class \u2014 and thus fail on tasks like German number inflection, where infrequent suffixes like /-s/ can still be productively generalized. To investigate this question, we first collect a new dataset from German speakers (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ED model. The speaker data show high variability, and two suffixes evince \u2018regular\u2019 behavior, appearing more often with phonologically atypical inputs. Encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or \u2018regular\u2019 extension of these other plural markers. We conclude that modern neural models may still struggle with minority-class generalization.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 20, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.159.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-18", "journal": {"pages": "1745-1756"}, "authors": [{"authorId": "14310569", "name": "Kate McCurdy"}, {"authorId": "1991315", "name": "S. Goldwater"}, {"authorId": "144871732", "name": "Adam Lopez"}]}, {"paperId": "1baea2e07501fe81bc828bd91314d04cf6bca7d0", "externalIds": {"ACL": "2020.acl-main.160", "DBLP": "conf/acl/KodnerG20", "MAG": "3016191525", "ArXiv": "2004.05067", "DOI": "10.18653/v1/2020.acl-main.160", "CorpusId": 215737063}, "corpusId": 215737063, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1baea2e07501fe81bc828bd91314d04cf6bca7d0", "title": "Overestimation of Syntactic Representation in Neural Language Models", "abstract": "With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful. Several testing methodologies have been developed to probe models\u2019 syntactic representations. One popular method for determining a model\u2019s ability to induce syntactic structure trains a model on strings generated according to a template then tests the model\u2019s ability to distinguish such strings from superficially similar ones with different syntax. We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.160.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-10", "journal": {"name": "ArXiv", "volume": "abs/2004.05067"}, "authors": [{"authorId": "31906633", "name": "Jordan Kodner"}, {"authorId": "2285178", "name": "Nitish Gupta"}]}, {"paperId": "98736c3274ad89c48947af92af36cd881048839c", "externalIds": {"MAG": "3023136100", "DBLP": "journals/corr/abs-2004-14905", "ACL": "2020.acl-main.161", "ArXiv": "2004.14905", "DOI": "10.18653/v1/2020.acl-main.161", "CorpusId": 216867371}, "corpusId": 216867371, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/98736c3274ad89c48947af92af36cd881048839c", "title": "Modelling Suspense in Short Stories as Uncertainty Reduction over Neural Representation", "abstract": "Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling. While there is a vast theoretical literature on suspense, it is computationally not well understood. We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is. Both can be computed either directly over story representations or over their probability distributions. We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction. Evaluating against short stories annotated with human suspense judgements, we find that uncertainty reduction over representations is the best predictor, resulting in near human accuracy. We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 229, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.161.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"name": "ArXiv", "volume": "abs/2004.14905"}, "authors": [{"authorId": "1666944787", "name": "David Wilmot"}, {"authorId": "1393020635", "name": "Frank Keller"}]}, {"paperId": "dcc19e0d3cb394fefacba1cd244f6ca0d2d620e4", "externalIds": {"ACL": "2020.acl-main.162", "DBLP": "conf/acl/WellerHRCTSS20", "MAG": "3034837209", "DOI": "10.18653/v1/2020.acl-main.162", "CorpusId": 220047305}, "corpusId": 220047305, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dcc19e0d3cb394fefacba1cd244f6ca0d2d620e4", "title": "You Don\u2019t Have Time to Read This: An Exploration of Document Reading Time Prediction", "abstract": "Predicting reading time has been a subject of much previous work, focusing on how different words affect human processing, measured by reading time. However, previous work has dealt with a limited number of participants as well as word level only predictions (i.e. predicting the time to read a single word). We seek to extend these works by examining whether or not document level predictions are effective, given additional information such as subject matter, font characteristics, and readability metrics. We perform a novel experiment to examine how different features of text contribute to the time it takes to read, distributing and collecting data from over a thousand participants. We then employ a large number of machine learning methods to predict a user\u2019s reading time. We find that despite extensive research showing that word level reading time can be most effectively predicted by neural networks, larger scale text can be easily and most accurately predicted by one factor, the number of words.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1789-1794"}, "authors": [{"authorId": "47433471", "name": "Orion Weller"}, {"authorId": "29499305", "name": "J. Hildebrandt"}, {"authorId": "2065527902", "name": "Ilya Reznik"}, {"authorId": "72607805", "name": "Christopher Challis"}, {"authorId": "104209321", "name": "E. Tass"}, {"authorId": "1743210", "name": "Q. Snell"}, {"authorId": "31819161", "name": "Kevin Seppi"}]}, {"paperId": "471516b38a44c317197b1b14866efdf4dabd078b", "externalIds": {"MAG": "3034800807", "DBLP": "journals/corr/abs-2006-07499", "ACL": "2020.acl-main.163", "ArXiv": "2006.07499", "DOI": "10.18653/v1/2020.acl-main.163", "CorpusId": 219686949}, "corpusId": 219686949, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/471516b38a44c317197b1b14866efdf4dabd078b", "title": "A Generative Model for Joint Natural Language Understanding and Generation", "abstract": "Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse. A key to success in either task is parallel training data which is expensive to obtain at a large scale. In this work, we propose a generative model which couples NLU and NLG through a shared latent variable. This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG. Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations. We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 18, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.163.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-12", "journal": {"name": "ArXiv", "volume": "abs/2006.07499"}, "authors": [{"authorId": "33870107", "name": "Bo-Hsiang Tseng"}, {"authorId": "1941442", "name": "Jianpeng Cheng"}, {"authorId": "2213149", "name": "Yimai Fang"}, {"authorId": "92480907", "name": "David Vandyke"}]}, {"paperId": "821a4aedd40596d4a6a95b3b9246baa109193a08", "externalIds": {"ArXiv": "1911.00650", "MAG": "3034287667", "DBLP": "conf/acl/IppolitoDCE20", "ACL": "2020.acl-main.164", "DOI": "10.18653/v1/2020.acl-main.164", "CorpusId": 218560609}, "corpusId": 218560609, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/821a4aedd40596d4a6a95b3b9246baa109193a08", "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled", "abstract": "Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies\u2014top-_k_, nucleus sampling, and untruncated random sampling\u2014and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 44, "citationCount": 173, "influentialCitationCount": 30, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.164.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-02", "journal": {"pages": "1808-1822"}, "authors": [{"authorId": "7975935", "name": "Daphne Ippolito"}, {"authorId": "40620532", "name": "Daniel Duckworth"}, {"authorId": "1763608", "name": "Chris Callison-Burch"}, {"authorId": "2396681", "name": "D. Eck"}]}, {"paperId": "976c35cddeb331fbd238fdadd44b807ab633760a", "externalIds": {"MAG": "3034771276", "DBLP": "journals/corr/abs-1911-02692", "ACL": "2020.acl-main.165", "ArXiv": "1911.02692", "DOI": "10.18653/v1/2020.acl-main.165", "CorpusId": 207847476}, "corpusId": 207847476, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/976c35cddeb331fbd238fdadd44b807ab633760a", "title": "Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing", "abstract": "Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 43, "citationCount": 21, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1911.02692", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-07", "journal": {"name": "ArXiv", "volume": "abs/1911.02692"}, "authors": [{"authorId": "5795999", "name": "Haoming Jiang"}, {"authorId": "98703980", "name": "Chen Liang"}, {"authorId": "2146309022", "name": "Chong Wang"}, {"authorId": "36345161", "name": "T. Zhao"}]}, {"paperId": "a4789304d4f5337d7c1e18dcc0f13049f2b5dbc5", "externalIds": {"ACL": "2020.acl-main.166", "DBLP": "conf/acl/XuWNWCL20", "MAG": "3034548376", "DOI": "10.18653/v1/2020.acl-main.166", "CorpusId": 220045840}, "corpusId": 220045840, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a4789304d4f5337d7c1e18dcc0f13049f2b5dbc5", "title": "Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation", "abstract": "To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog. To this end, we first construct a conversational graph (CG) from dialog corpora, in which there are vertices to represent \u201cwhat to say\u201d and \u201chow to say\u201d, and edges to represent natural transition between a message (the last utterance in a dialog context) and its response. We then present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal, which learns to identify a what-vertex and a how-vertex from the CG at each turn to guide response generation. In this way, we effectively leverage the CG to facilitate policy learning as follows: (1) it enables more effective long-term reward design, (2) it provides high-quality candidate actions, and (3) it gives us more control over the policy. Results on two benchmark corpora demonstrate the effectiveness of this framework.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 38, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.166.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1835-1845"}, "authors": [{"authorId": "49394471", "name": "J. Xu"}, {"authorId": "144270731", "name": "Haifeng Wang"}, {"authorId": "2715551", "name": "Zheng-Yu Niu"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "2256319", "name": "Wanxiang Che"}, {"authorId": "40282288", "name": "Ting Liu"}]}, {"paperId": "72ccc300dd13d180ac8328e40b0f24e8df8a2722", "externalIds": {"DBLP": "journals/corr/abs-2005-09123", "ACL": "2020.acl-main.167", "ArXiv": "2005.09123", "MAG": "3027327463", "DOI": "10.18653/v1/2020.acl-main.167", "CorpusId": 218684947}, "corpusId": 218684947, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/72ccc300dd13d180ac8328e40b0f24e8df8a2722", "title": "GPT-too: A Language-Model-First Approach for AMR-to-Text Generation", "abstract": "Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 80, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.167.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-18", "journal": {"pages": "1846-1852"}, "authors": [{"authorId": "153151470", "name": "Manuel Mager"}, {"authorId": "3394760", "name": "Ram\u00f3n Fern\u00e1ndez Astudillo"}, {"authorId": "2524647", "name": "Tahira Naseem"}, {"authorId": "2937809", "name": "Md Arafat Sultan"}, {"authorId": "2110153639", "name": "Young-suk Lee"}, {"authorId": "1707117", "name": "Radu Florian"}, {"authorId": "46924970", "name": "Salim Roukos"}]}, {"paperId": "a266b37f98928f27fddd863d11b38a5563043315", "externalIds": {"DBLP": "conf/acl/PanthaplackelNG20", "MAG": "3020347567", "ACL": "2020.acl-main.168", "ArXiv": "2004.12169", "DOI": "10.18653/v1/2020.acl-main.168", "CorpusId": 216553060}, "corpusId": 216553060, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a266b37f98928f27fddd863d11b38a5563043315", "title": "Learning to Update Natural Language Comments Based on Code Changes", "abstract": "We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies. We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications. We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment. We compare our approach against multiple baselines using both automatic metrics and human evaluation. Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 45, "citationCount": 50, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.168.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-25", "journal": {"name": "ArXiv", "volume": "abs/2004.12169"}, "authors": [{"authorId": "1468751197", "name": "Sheena Panthaplackel"}, {"authorId": "5890416", "name": "Pengyu Nie"}, {"authorId": "2020926", "name": "Milo\u0161 Gligori\u0107"}, {"authorId": "22319255", "name": "Junyi Jessy Li"}, {"authorId": "1797655", "name": "R. Mooney"}]}, {"paperId": "80257b7d02ad4d6a762ebc0d7f1560e0ef182354", "externalIds": {"ArXiv": "2004.14257", "DBLP": "journals/corr/abs-2004-14257", "ACL": "2020.acl-main.169", "MAG": "3021491028", "DOI": "10.18653/v1/2020.acl-main.169", "CorpusId": 215811473}, "corpusId": 215811473, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/80257b7d02ad4d6a762ebc0d7f1560e0ef182354", "title": "Politeness Transfer: A Tag and Generate Approach", "abstract": "This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https://github.com/tag-and-generate.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 45, "citationCount": 117, "influentialCitationCount": 17, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.169.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-29", "journal": {"pages": "1869-1881"}, "authors": [{"authorId": "21626987", "name": "Aman Madaan"}, {"authorId": "80366270", "name": "Amrith Rajagopal Setlur"}, {"authorId": "46719088", "name": "Tanmay Parekh"}, {"authorId": "1719347", "name": "B. P\u00f3czos"}, {"authorId": "1700325", "name": "Graham Neubig"}, {"authorId": "35729970", "name": "Yiming Yang"}, {"authorId": "145124475", "name": "R. Salakhutdinov"}, {"authorId": "1690706", "name": "A. Black"}, {"authorId": "9358910", "name": "Shrimai Prabhumoye"}]}, {"paperId": "d023e4c652dc21b2068a8527203a70d9eaf195d9", "externalIds": {"MAG": "3035207248", "ArXiv": "1910.13267", "ACL": "2020.acl-main.170", "DBLP": "journals/corr/abs-1910-13267", "DOI": "10.18653/v1/2020.acl-main.170", "CorpusId": 204949631}, "corpusId": 204949631, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d023e4c652dc21b2068a8527203a70d9eaf195d9", "title": "BPE-Dropout: Simple and Effective Subword Regularization", "abstract": "Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 31, "citationCount": 192, "influentialCitationCount": 51, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1910.13267", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-29", "journal": {"name": "ArXiv", "volume": "abs/1910.13267"}, "authors": [{"authorId": "1390051629", "name": "Ivan Provilkov"}, {"authorId": "1390051705", "name": "Dmitrii Emelianenko"}, {"authorId": "46235299", "name": "Elena Voita"}]}, {"paperId": "699b29745ec527e4d3b08ea38adf34888a85469f", "externalIds": {"DBLP": "conf/acl/ZhouK20", "ArXiv": "2005.00932", "ACL": "2020.acl-main.171", "MAG": "3022593918", "DOI": "10.18653/v1/2020.acl-main.171", "CorpusId": 218486806}, "corpusId": 218486806, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/699b29745ec527e4d3b08ea38adf34888a85469f", "title": "Improving Non-autoregressive Neural Machine Translation with Monolingual Data", "abstract": "Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model\u2019s performance, with the goal of transferring the AR model\u2019s generalization ability while preventing overfitting. On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model\u2019s performance, yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 21, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.00932", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00932"}, "authors": [{"authorId": "46618542", "name": "Jiawei Zhou"}, {"authorId": "9921376", "name": "Phillip Keung"}]}, {"paperId": "4c3b5f8db4f44ed4d24e15227a0da30f7c20a665", "externalIds": {"MAG": "3034356621", "DBLP": "journals/corr/abs-2005-00163", "ACL": "2020.acl-main.172", "ArXiv": "2005.00163", "DOI": "10.18653/v1/2020.acl-main.172", "CorpusId": 218470160}, "corpusId": 218470160, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4c3b5f8db4f44ed4d24e15227a0da30f7c20a665", "title": "Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization", "abstract": "Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics (with improvements: 2.9% RG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of improvement impacts patients\u2019 welfare.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 40, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.00163", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00163"}, "authors": [{"authorId": "119240402", "name": "Sajad Sotudeh"}, {"authorId": "1685063", "name": "Nazli Goharian"}, {"authorId": "3005117", "name": "Ross W. Filice"}]}, {"paperId": "dbeeca8466e0c177ec67c60d529899232415ca87", "externalIds": {"DBLP": "conf/acl/MaynezNBM20", "MAG": "3021338988", "ArXiv": "2005.00661", "ACL": "2020.acl-main.173", "DOI": "10.18653/v1/2020.acl-main.173", "CorpusId": 218487034}, "corpusId": 218487034, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dbeeca8466e0c177ec67c60d529899232415ca87", "title": "On Faithfulness and Factuality in Abstractive Summarization", "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 61, "citationCount": 608, "influentialCitationCount": 91, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.173.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00661"}, "authors": [{"authorId": "46219105", "name": "Joshua Maynez"}, {"authorId": "143790499", "name": "Shashi Narayan"}, {"authorId": "1678747", "name": "Bernd Bohnet"}, {"authorId": "143957226", "name": "Ryan T. McDonald"}]}, {"paperId": "979a4428bde364415d21c6231609eb4564df84a8", "externalIds": {"DBLP": "journals/corr/abs-2004-12727", "ArXiv": "2004.12727", "ACL": "2020.acl-main.174", "MAG": "3020656123", "DOI": "10.18653/v1/2020.acl-main.174", "CorpusId": 216553636}, "corpusId": 216553636, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/979a4428bde364415d21c6231609eb4564df84a8", "title": "Screenplay Summarization Using Latent Narrative Structure", "abstract": "Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 56, "citationCount": 29, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.174.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-27", "journal": {"name": "ArXiv", "volume": "abs/2004.12727"}, "authors": [{"authorId": "22220222", "name": "Pinelopi Papalampidi"}, {"authorId": "1393020635", "name": "Frank Keller"}, {"authorId": "2875615", "name": "Lea Frermann"}, {"authorId": "1747893", "name": "Mirella Lapata"}]}, {"paperId": "cbdb0d682a59933fb144124f1bfaec0ee3f3b04c", "externalIds": {"DBLP": "journals/corr/abs-2004-10150", "MAG": "3017806159", "ACL": "2020.acl-main.175", "ArXiv": "2004.10150", "DOI": "10.18653/v1/2020.acl-main.175", "CorpusId": 216036020}, "corpusId": 216036020, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cbdb0d682a59933fb144124f1bfaec0ee3f3b04c", "title": "Unsupervised Opinion Summarization with Noising and Denoising", "abstract": "The supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. In this paper we enable the use of supervised learning for the setting where there are only documents available (e.g., product or business reviews) without ground truth summaries. We create a synthetic dataset from a corpus of user reviews by sampling a review, pretending it is a summary, and generating noisy versions thereof which we treat as pseudo-review input. We introduce several linguistically motivated noise generation functions and a summarization model which learns to denoise the input and generate the original review. At test time, the model accepts genuine reviews and generates a summary containing salient opinions, treating those that do not reach consensus as noise. Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 56, "citationCount": 53, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.175.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-04-21", "journal": {"name": "ArXiv", "volume": "abs/2004.10150"}, "authors": [{"authorId": "23181472", "name": "Reinald Kim Amplayo"}, {"authorId": "1747893", "name": "Mirella Lapata"}]}, {"paperId": "99979db50a6f2cdb43225240e44f1928e778554d", "externalIds": {"ArXiv": "2005.03593", "DBLP": "conf/acl/CohenP20", "MAG": "3022165804", "ACL": "2020.acl-main.176", "DOI": "10.18653/v1/2020.acl-main.176", "CorpusId": 218538138}, "corpusId": 218538138, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/99979db50a6f2cdb43225240e44f1928e778554d", "title": "A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia of the Alzheimer\u2019s Type", "abstract": "In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls. The difference between perplexity estimates from two neural language models (LMs) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance. However, little is known about why this approach is effective, and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts (DementiaBank), it is unclear if these approaches are truly diagnostic, or are sensitive to other variables. In this paper, we interrogate neural LMs trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency. We find that perplexity of neural LMs is strongly and differentially associated with lexical frequency, and that using a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art for models trained on transcript text exclusively.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 63, "citationCount": 14, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.176.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-07", "journal": {"name": "ArXiv", "volume": "abs/2005.03593"}, "authors": [{"authorId": "2696466", "name": "T. Cohen"}, {"authorId": "143847074", "name": "Serguei V. S. Pakhomov"}]}, {"paperId": "4dc005ea288c50d57222122903edf87f21689781", "externalIds": {"ACL": "2020.acl-main.177", "DBLP": "journals/corr/abs-2005-04315", "ArXiv": "2005.04315", "MAG": "3022745277", "DOI": "10.18653/v1/2020.acl-main.177", "CorpusId": 218581294}, "corpusId": 218581294, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4dc005ea288c50d57222122903edf87f21689781", "title": "Probing Linguistic Systematicity", "abstract": "Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 59, "citationCount": 44, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.177.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-08", "journal": {"pages": "1958-1969"}, "authors": [{"authorId": "2001353013", "name": "Emily Goodwin"}, {"authorId": "40910779", "name": "Koustuv Sinha"}, {"authorId": "1400877758", "name": "T. O\u2019Donnell"}]}, {"paperId": "e9e08833787d823dcbec537a04e930f184741471", "externalIds": {"DBLP": "conf/acl/SapHCSP20", "MAG": "3034514149", "ACL": "2020.acl-main.178", "DOI": "10.18653/v1/2020.acl-main.178", "CorpusId": 218498322}, "corpusId": 218498322, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e9e08833787d823dcbec537a04e930f184741471", "title": "Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models", "abstract": "We investigate the use of NLP as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release Hippocorpus, a dataset of 7,000 stories about imagined and recalled events. We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events. Our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected. In addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory. Finally, our measures reveal the effect of narrativization of memories in stories (e.g., stories about frequently recalled memories flow more linearly; Bartlett, 1932). Our findings highlight the potential of using NLP tools to study the traces of human cognition in language.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 42, "citationCount": 23, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.178.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1970-1978"}, "authors": [{"authorId": "2729164", "name": "Maarten Sap"}, {"authorId": "145479841", "name": "E. Horvitz"}, {"authorId": "1699545", "name": "Yejin Choi"}, {"authorId": "144365876", "name": "Noah A. Smith"}, {"authorId": "1854783", "name": "J. Pennebaker"}]}, {"paperId": "7a6385947507c8187043ced97c87480134543a44", "externalIds": {"MAG": "3022407266", "DBLP": "conf/acl/DavisS20", "ACL": "2020.acl-main.179", "ArXiv": "2005.00165", "DOI": "10.18653/v1/2020.acl-main.179", "CorpusId": 218470598}, "corpusId": 218470598, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7a6385947507c8187043ced97c87480134543a44", "title": "Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment", "abstract": "A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 48, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.179.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00165"}, "authors": [{"authorId": "40109754", "name": "Forrest Davis"}, {"authorId": "3220165", "name": "Marten van Schijndel"}]}, {"paperId": "12fe17daa6a00631449b24231d650066fefd368c", "externalIds": {"DBLP": "conf/acl/MeinhardtBB20", "MAG": "3035691952", "ACL": "2020.acl-main.180", "DOI": "10.18653/v1/2020.acl-main.180", "CorpusId": 220048243}, "corpusId": 220048243, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/12fe17daa6a00631449b24231d650066fefd368c", "title": "Speakers enhance contextually confusable words", "abstract": "Recent work has found evidence that natural languages are shaped by pressures for efficient communication \u2014 e.g. the more contextually predictable a word is, the fewer speech sounds or syllables it has (Piantadosi et al. 2011). Research on the degree to which speech and language are shaped by pressures for effective communication \u2014 robustness in the face of noise and uncertainty \u2014 has been more equivocal. We develop a measure of contextual confusability during word recognition based on psychoacoustic data. Applying this measure to naturalistic speech corpora, we find evidence suggesting that speakers alter their productions to make contextually more confusable words easier to understand.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 60, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Physics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "1991-2002"}, "authors": [{"authorId": "12167112", "name": "Eric Meinhardt"}, {"authorId": "52106370", "name": "Eric Bakovic"}, {"authorId": "2819425", "name": "Leon Bergen"}]}, {"paperId": "e54d05c8cae02b304a015ea9b1ceeb2e0b25f2bc", "externalIds": {"DBLP": "conf/acl/FutrellDS20", "MAG": "3034568351", "ACL": "2020.acl-main.181", "DOI": "10.18653/v1/2020.acl-main.181", "CorpusId": 220044838}, "corpusId": 220044838, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e54d05c8cae02b304a015ea9b1ceeb2e0b25f2bc", "title": "What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks", "abstract": "We take up the scientific question of what determines the preferred order of adjectives in English, in phrases such as big blue box where multiple adjectives modify a following noun. We implement and test four quantitative theories, all of which are theoretically motivated in terms of efficiency in human language production and comprehension. The four theories we test are subjectivity (Scontras et al., 2017), information locality (Futrell, 2019), integration cost (Dyer, 2017), and information gain, which we introduce. We evaluate theories based on their ability to predict orders of unseen adjectives in hand-parsed and automatically-parsed dependency treebanks. We find that subjectivity, information locality, and information gain are all strong predictors, with some evidence for a two-factor account, where subjectivity and information gain reflect a factor involving semantics, and information locality reflects collocational preferences.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 49, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.181.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Economics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2003-2012"}, "authors": [{"authorId": "2585394", "name": "Richard Futrell"}, {"authorId": "119054303", "name": "William Dyer"}, {"authorId": "2040994", "name": "Gregory Scontras"}]}, {"paperId": "1a4d57aea8352fcf871ca6cfd7f51bb3a5f8d664", "externalIds": {"DBLP": "journals/corr/abs-2004-01926", "ACL": "2020.acl-main.182", "MAG": "3035082574", "ArXiv": "2004.01926", "DOI": "10.18653/v1/2020.acl-main.182", "CorpusId": 214802016}, "corpusId": 214802016, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1a4d57aea8352fcf871ca6cfd7f51bb3a5f8d664", "title": "\u201cNone of the Above\u201d: Measure Uncertainty in Dialog Response Retrieval", "abstract": "This paper discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus. We show that instead of retraining models for this specific purpose, we can capture the original retrieval model\u2019s underlying confidence concerning the best prediction using trivial additional computation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 18, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.182.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-04", "journal": {"pages": "2013-2020"}, "authors": [{"authorId": "46749396", "name": "Yulan Feng"}, {"authorId": "32251567", "name": "Shikib Mehri"}, {"authorId": "1716325", "name": "M. Esk\u00e9nazi"}, {"authorId": "8200875", "name": "Tiancheng Zhao"}]}, {"paperId": "71017cc6d270d28d9edcd47550450dc05edd65f4", "externalIds": {"DBLP": "conf/acl/SmithWSWB20", "MAG": "3016952972", "ArXiv": "2004.08449", "ACL": "2020.acl-main.183", "DOI": "10.18653/v1/2020.acl-main.183", "CorpusId": 215827653}, "corpusId": 215827653, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/71017cc6d270d28d9edcd47550450dc05edd65f4", "title": "Can You Put it All Together: Evaluating Conversational Agents\u2019 Ability to Blend Skills", "abstract": "Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them. But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow. In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages. We further propose a new dataset, BlendedSkillTalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 179, "influentialCitationCount": 20, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.183.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-17", "journal": {"pages": "2021-2030"}, "authors": [{"authorId": "51324296", "name": "Eric Michael Smith"}, {"authorId": "2066769956", "name": "Mary Williamson"}, {"authorId": "35752280", "name": "Kurt Shuster"}, {"authorId": "145183709", "name": "J. Weston"}, {"authorId": "90841478", "name": "Y-Lan Boureau"}]}, {"paperId": "04ef00b711990004b9c4440b1e3aa59b28701cfc", "externalIds": {"MAG": "3034569646", "ACL": "2020.acl-main.184", "DBLP": "conf/acl/ZhangLXL20", "ArXiv": "1911.02707", "DOI": "10.18653/v1/2020.acl-main.184", "CorpusId": 214802401}, "corpusId": 214802401, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/04ef00b711990004b9c4440b1e3aa59b28701cfc", "title": "Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs", "abstract": "Human conversations naturally evolve around related concepts and hop to distant concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow\u2019s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 50, "citationCount": 108, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.184.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-07", "journal": {"pages": "2031-2043"}, "authors": [{"authorId": "92156482", "name": "Houyu Zhang"}, {"authorId": "49047064", "name": "Zhenghao Liu"}, {"authorId": "144628574", "name": "Chenyan Xiong"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}]}, {"paperId": "e5515b3befb4b4e61aebc0f3f6017056631acafa", "externalIds": {"MAG": "3035239386", "ACL": "2020.acl-main.185", "ArXiv": "1903.02134", "DBLP": "journals/corr/abs-1903-02134", "DOI": "10.18653/v1/2020.acl-main.185", "CorpusId": 70349945}, "corpusId": 70349945, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e5515b3befb4b4e61aebc0f3f6017056631acafa", "title": "Negative Training for Neural Dialogue Response Generation", "abstract": "Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses. In this work, we propose a framework named \u201cNegative Training\u201d to minimize such behaviors. Given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training signals for fine-tuning the model. Our experiments show that negative training can significantly reduce the hit rate of malicious responses, or discourage frequent responses and improve response diversity.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 53, "citationCount": 46, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.185.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-03-06", "journal": {"name": "ArXiv", "volume": "abs/1903.02134"}, "authors": [{"authorId": "3083253", "name": "Tianxing He"}, {"authorId": "145898106", "name": "James R. Glass"}]}, {"paperId": "c47128d85d2d2f28395ba565f4471cb1b8fb9e4f", "externalIds": {"ACL": "2020.acl-main.186", "MAG": "3032502206", "DBLP": "conf/acl/GangadharaiahN20", "DOI": "10.18653/V1/2020.ACL-MAIN.186", "CorpusId": 219682425}, "corpusId": 219682425, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c47128d85d2d2f28395ba565f4471cb1b8fb9e4f", "title": "Recursive Template-based Frame Generation for Task Oriented Dialog", "abstract": "The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user\u2019s request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST). This information is typically represented as a semantic frame that captures the intent and slot-labels provided by the user. We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains. We propose a recursive, hierarchical frame-based representation and show how to learn it from data. We formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template. We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end. We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.186.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2059-2064"}, "authors": [{"authorId": "3208169", "name": "Rashmi Gangadharaiah"}, {"authorId": "145541831", "name": "Balakrishnan Narayanaswamy"}]}, {"paperId": "4c8bdcf2ece019a72bbbf226a726dc8067f148d1", "externalIds": {"MAG": "3035715371", "DBLP": "journals/corr/abs-2005-02539", "ArXiv": "2005.02539", "ACL": "2020.acl-main.187", "DOI": "10.18653/v1/2020.acl-main.187", "CorpusId": 218516925}, "corpusId": 218516925, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4c8bdcf2ece019a72bbbf226a726dc8067f148d1", "title": "Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback", "abstract": "We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. We focus on natural language to SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback. We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction. While we estimated human correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research. SPLASH is publicly available at https://aka.ms/Splash_dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 61, "citationCount": 48, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.187.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-14", "journal": {"pages": "2065-2077"}, "authors": [{"authorId": "143718836", "name": "Ahmed Elgohary"}, {"authorId": "2195458", "name": "Saghar Hosseini"}, {"authorId": "1977489", "name": "Ahmed Hassan Awadallah"}]}, {"paperId": "37710e4b0dd851bd0d9bc4994c3b32cb10279151", "externalIds": {"ACL": "2020.acl-main.188", "DBLP": "journals/corr/abs-2004-04361", "MAG": "3034257552", "ArXiv": "2004.04361", "DOI": "10.18653/v1/2020.acl-main.188", "CorpusId": 215548393, "PubMed": "33612961"}, "corpusId": 215548393, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/37710e4b0dd851bd0d9bc4994c3b32cb10279151", "title": "Calibrating Structured Output Predictors for Natural Language Processing", "abstract": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 24, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.188.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-09", "journal": {"name": "Proceedings of the conference. Association for Computational Linguistics. Meeting", "pages": "\n          2078-2092\n        ", "volume": "2020"}, "authors": [{"authorId": "3422909", "name": "Abhyuday N. Jagannatha"}, {"authorId": "2119120474", "name": "Hong Yu"}]}, {"paperId": "811407c76cf46955ddae71808ea7f746f84df433", "externalIds": {"DBLP": "conf/acl/BrantleyDS20", "ArXiv": "2005.12801", "ACL": "2020.acl-main.189", "MAG": "3037588750", "DOI": "10.18653/v1/2020.acl-main.189", "CorpusId": 218889823}, "corpusId": 218889823, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/811407c76cf46955ddae71808ea7f746f84df433", "title": "Active Imitation Learning with Noisy Guidance", "abstract": "Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies. Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical. To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance. Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary. We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 47, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.189.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-26", "journal": {"pages": "2093-2105"}, "authors": [{"authorId": "11963742", "name": "Kiant\u00e9 Brantley"}, {"authorId": "143816740", "name": "Amr Sharaf"}, {"authorId": "2065041692", "name": "Hal Daum'e"}]}, {"paperId": "bd49e66af9755e6138967eba6aeb37d8190d2b4f", "externalIds": {"MAG": "3023332645", "DBLP": "journals/corr/abs-2005-01932", "ArXiv": "2005.01932", "ACL": "2020.acl-main.190", "DOI": "10.18653/v1/2020.acl-main.190", "CorpusId": 218502145}, "corpusId": 218502145, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bd49e66af9755e6138967eba6aeb37d8190d2b4f", "title": "ExpBERT: Representation Engineering with Natural Language Explanations", "abstract": "Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to \u201cinterpret\u201d these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3\u201320x less labeled data and improves on the baseline by 3\u201310 F1 points with the same amount of labeled data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 37, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.190.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"name": "ArXiv", "volume": "abs/2005.01932"}, "authors": [{"authorId": "37824171", "name": "Shikhar Murty"}, {"authorId": "2572525", "name": "Pang Wei Koh"}, {"authorId": "145419642", "name": "Percy Liang"}]}, {"paperId": "f46f9683925e32177802933b494d6f93835c69cc", "externalIds": {"DBLP": "conf/acl/CroceCB20", "MAG": "3035454789", "ACL": "2020.acl-main.191", "DOI": "10.18653/v1/2020.acl-main.191", "CorpusId": 220047251}, "corpusId": 220047251, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f46f9683925e32177802933b494d6f93835c69cc", "title": "GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples", "abstract": "Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks. However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples. In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected. One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks. In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting. Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 111, "influentialCitationCount": 24, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.191.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2114-2119"}, "authors": [{"authorId": "1784172", "name": "D. Croce"}, {"authorId": "3285152", "name": "Giuseppe Castellucci"}, {"authorId": "48949093", "name": "R. Basili"}]}, {"paperId": "7f1a6c67d03de88b898271d52dd2e51907d5b615", "externalIds": {"ACL": "2020.acl-main.192", "DBLP": "journals/corr/abs-1911-03822", "MAG": "3034694091", "ArXiv": "1911.03822", "DOI": "10.18653/v1/2020.acl-main.192", "CorpusId": 207853026}, "corpusId": 207853026, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7f1a6c67d03de88b898271d52dd2e51907d5b615", "title": "Generalizing Natural Language Analysis through Span-relation Representations", "abstract": "Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 69, "citationCount": 49, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.192.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-10", "journal": {"pages": "2120-2133"}, "authors": [{"authorId": "2669515", "name": "Zhengbao Jiang"}, {"authorId": "40515617", "name": "W. Xu"}, {"authorId": "50007145", "name": "J. Araki"}, {"authorId": "1700325", "name": "Graham Neubig"}]}, {"paperId": "efd81977f1e74138cf2ac3e9a42112b95f648c66", "externalIds": {"DBLP": "conf/acl/LanHLJLR20", "ACL": "2020.acl-main.193", "ArXiv": "1910.04289", "MAG": "3034718094", "DOI": "10.18653/v1/2020.acl-main.193", "CorpusId": 204009164}, "corpusId": 204009164, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/efd81977f1e74138cf2ac3e9a42112b95f648c66", "title": "Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling", "abstract": "Sequence labeling is a fundamental task for a range of natural language processing problems. When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly. In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible. In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data). It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module. Finally, it leads to a model reflecting the agreement (consensus) among multiple sources. We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings. We also demonstrate that the method can apply to various tasks and cope with different encoders.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 46, "citationCount": 25, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1910.04289", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-09", "journal": {"name": "ArXiv", "volume": "abs/1910.04289"}, "authors": [{"authorId": "51310039", "name": "Ouyu Lan"}, {"authorId": "145047108", "name": "Xiao Huang"}, {"authorId": "51583409", "name": "Bill Yuchen Lin"}, {"authorId": "2116480752", "name": "He Jiang"}, {"authorId": "46458310", "name": "Liyuan Liu"}, {"authorId": "1384550891", "name": "Xiang Ren"}]}, {"paperId": "ae2c03cbe6162dadf65edd2ff7dfc5333524dca5", "externalIds": {"MAG": "3035542229", "ArXiv": "2004.12239", "DBLP": "journals/corr/abs-2004-12239", "ACL": "2020.acl-main.194", "DOI": "10.18653/v1/2020.acl-main.194", "CorpusId": 216553182}, "corpusId": 216553182, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ae2c03cbe6162dadf65edd2ff7dfc5333524dca5", "title": "MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification", "abstract": "This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. The improvement is especially prominent when supervision is extremely limited. We have publicly released our code at https://github.com/GT-SALT/MixText.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 49, "citationCount": 264, "influentialCitationCount": 57, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.194.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-25", "journal": {"name": "ArXiv", "volume": "abs/2004.12239"}, "authors": [{"authorId": "47739850", "name": "Jiaao Chen"}, {"authorId": "8387085", "name": "Zichao Yang"}, {"authorId": "2022168", "name": "Diyi Yang"}]}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "externalIds": {"DBLP": "conf/acl/SunYSLYZ20", "MAG": "3015298864", "ACL": "2020.acl-main.195", "ArXiv": "2004.02984", "DOI": "10.18653/v1/2020.acl-main.195", "CorpusId": 215238853}, "corpusId": 215238853, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "abstract": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 67, "citationCount": 554, "influentialCitationCount": 98, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.195.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-06", "journal": {"name": "ArXiv", "volume": "abs/2004.02984"}, "authors": [{"authorId": "48064856", "name": "Zhiqing Sun"}, {"authorId": "40244451", "name": "Hongkun Yu"}, {"authorId": "1718192", "name": "Xiaodan Song"}, {"authorId": "2112354219", "name": "Renjie Liu"}, {"authorId": "35729970", "name": "Yiming Yang"}, {"authorId": "65855107", "name": "Denny Zhou"}]}, {"paperId": "ff20f19ffd6783d480ab249c893d1235e8a3780b", "externalIds": {"DBLP": "conf/acl/LoganGS20", "ACL": "2020.acl-main.196", "MAG": "3034542747", "DOI": "10.18653/v1/2020.acl-main.196", "CorpusId": 220045846}, "corpusId": 220045846, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ff20f19ffd6783d480ab249c893d1235e8a3780b", "title": "On Importance Sampling-Based Evaluation of Latent Language Models", "abstract": "Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. Existing works avoid this issue by using importance sampling. Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates. In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM. In addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 14, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.196.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2171-2176"}, "authors": [{"authorId": "30083905", "name": "Robert L Logan IV"}, {"authorId": "40642935", "name": "Matt Gardner"}, {"authorId": "34650964", "name": "Sameer Singh"}]}, {"paperId": "ab70853cd5912c470f6ff95e95481980f0a2a41b", "externalIds": {"DBLP": "conf/acl/JiangHCLGZ20", "MAG": "2985884876", "ArXiv": "1911.03437", "ACL": "2020.acl-main.197", "DOI": "10.18653/v1/2020.acl-main.197", "CorpusId": 207847598}, "corpusId": 207847598, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ab70853cd5912c470f6ff95e95481980f0a2a41b", "title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization", "abstract": "Transfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 69, "citationCount": 358, "influentialCitationCount": 62, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.197.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-08", "journal": {"name": "ArXiv", "volume": "abs/1911.03437"}, "authors": [{"authorId": "5795999", "name": "Haoming Jiang"}, {"authorId": "50462546", "name": "Pengcheng He"}, {"authorId": "2109136147", "name": "Weizhu Chen"}, {"authorId": "46522098", "name": "Xiaodong Liu"}, {"authorId": "1800422", "name": "Jianfeng Gao"}, {"authorId": "36345161", "name": "T. Zhao"}]}, {"paperId": "17293893e0ef799fc038b5da01fc6baf3b08abdd", "externalIds": {"ACL": "2020.acl-main.198", "DBLP": "journals/corr/abs-2005-02433", "MAG": "3022096528", "ArXiv": "2005.02433", "DOI": "10.18653/v1/2020.acl-main.198", "CorpusId": 218517141}, "corpusId": 218517141, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/17293893e0ef799fc038b5da01fc6baf3b08abdd", "title": "Stolen Probability: A Structural Weakness of Neural Language Models", "abstract": "Neural Network Language Models (NNLMs) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space. The dot-product distance metric forms part of the inductive bias of NNLMs. Although NNLMs optimize well with this inductive bias, we show that this results in a sub-optimal ordering of the embedding space that structurally impoverishes some words at the expense of others when assigning probability. We present numerical, theoretical and empirical analyses which show that words on the interior of the convex hull in the embedding space have their probability bounded by the probabilities of the words on the hull.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 14, "citationCount": 21, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.198.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"name": "ArXiv", "volume": "abs/2005.02433"}, "authors": [{"authorId": "48418509", "name": "David Demeter"}, {"authorId": "29012058", "name": "Gregory J. Kimmel"}, {"authorId": "145612610", "name": "Doug Downey"}]}, {"paperId": "154fe9eba41c43f43ed2e41cac3f5959dc358fb7", "externalIds": {"DBLP": "conf/acl/ShangDCMG20", "MAG": "3034444248", "ACL": "2020.acl-main.199", "DOI": "10.18653/v1/2020.acl-main.199", "CorpusId": 220046485}, "corpusId": 220046485, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/154fe9eba41c43f43ed2e41cac3f5959dc358fb7", "title": "Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer", "abstract": "Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications. Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks. However, there has been no attempt to exploit GNN to create taxonomies. In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task. Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain. We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction. Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training. The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain. Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 47, "citationCount": 17, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.199.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2198-2208"}, "authors": [{"authorId": "2062997349", "name": "Chao Shang"}, {"authorId": "3457412", "name": "Sarthak Dash"}, {"authorId": "8576392", "name": "Md. Faisal Mahbub Chowdhury"}, {"authorId": "2689774", "name": "Nandana Mihindukulasooriya"}, {"authorId": "1711133", "name": "A. Gliozzo"}]}, {"paperId": "81815d9a847e406f8d49fb5051e2ae1055e13208", "externalIds": {"ACL": "2020.acl-main.200", "ArXiv": "2006.08671", "DBLP": "conf/acl/WangKM20", "MAG": "3034993698", "DOI": "10.18653/v1/2020.acl-main.200", "CorpusId": 219708650}, "corpusId": 219708650, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/81815d9a847e406f8d49fb5051e2ae1055e13208", "title": "To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks", "abstract": "Pretraining NLP models with variants of Masked Language Model (MLM) objectives has recently led to a significant improvements on many tasks. This paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task. On several text classification tasks, we show that as the number of training examples grow into the millions, the accuracy gap between finetuning BERT-based model and training vanilla LSTM from scratch narrows to within 1%. Our findings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases significantly.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 18, "citationCount": 14, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.200.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-15", "journal": {"pages": "2209-2213"}, "authors": [{"authorId": "2096387", "name": "Sinong Wang"}, {"authorId": "2072010", "name": "Madian Khabsa"}, {"authorId": "2110815489", "name": "Hao Ma"}]}, {"paperId": "601b438b4a7129641e314b4ad731834e81d7dd30", "externalIds": {"DBLP": "journals/corr/abs-2005-00524", "ACL": "2020.acl-main.201", "ArXiv": "2005.00524", "MAG": "3035537076", "DOI": "10.18653/v1/2020.acl-main.201", "CorpusId": 218049673}, "corpusId": 218049673, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/601b438b4a7129641e314b4ad731834e81d7dd30", "title": "Why Overfitting Isn\u2019t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries", "abstract": "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 42, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.201.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "2214-2220"}, "authors": [{"authorId": "12554337", "name": "Mozhi Zhang"}, {"authorId": "31482660", "name": "Yoshinari Fujinuma"}, {"authorId": "143946641", "name": "Michael J. Paul"}, {"authorId": "1389036863", "name": "Jordan L. Boyd-Graber"}]}, {"paperId": "9c5a239b75bade55c830b164e2fadc424e879137", "externalIds": {"MAG": "3022128899", "ArXiv": "2004.05686", "ACL": "2020.acl-main.202", "DBLP": "conf/acl/MukherjeeA20", "DOI": "10.18653/v1/2020.acl-main.202", "CorpusId": 218502458}, "corpusId": 218502458, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9c5a239b75bade55c830b164e2fadc424e879137", "title": "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models", "abstract": "Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. However, the huge size of these models could be a deterrent to using them in practice. Some recent works use knowledge distillation to compress these huge models into shallow ones. In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER). In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works. Additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few. We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its F1-score for NER over 41 languages.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 49, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.202.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-04", "journal": {"pages": "2221-2234"}, "authors": [{"authorId": "1777140", "name": "Subhabrata Mukherjee"}, {"authorId": "1977489", "name": "Ahmed Hassan Awadallah"}]}, {"paperId": "318bf5ed287c15db135c00c74e2f6c00970abe82", "externalIds": {"MAG": "3035225773", "DBLP": "conf/acl/MahmoodSS20", "ACL": "2020.acl-main.203", "ArXiv": "2005.00702", "DOI": "10.18653/v1/2020.acl-main.203", "CorpusId": 218486757}, "corpusId": 218486757, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/318bf5ed287c15db135c00c74e2f6c00970abe82", "title": "A Girl Has A Name: Detecting Authorship Obfuscation", "abstract": "Authorship attribution aims to identify the author of a text based on the stylometric analysis. Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text\u2019s style. In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model. An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated \u2013 a decision that is key to the adversary interested in authorship attribution. We show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average F1 score of 0.87. The reason for the lack of stealthiness is that these obfuscators degrade text smoothness, as ascertained by neural language models, in a detectable manner. Our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.203.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00702"}, "authors": [{"authorId": "2052991657", "name": "Asad Mahmood"}, {"authorId": "34616778", "name": "Zubair Shafiq"}, {"authorId": "144684950", "name": "P. Srinivasan"}]}, {"paperId": "90a1491ac32e732c93773354e4e665794ed4d490", "externalIds": {"MAG": "3035038672", "DBLP": "conf/acl/XinTLYL20", "ArXiv": "2004.12993", "ACL": "2020.acl-main.204", "DOI": "10.18653/v1/2020.acl-main.204", "CorpusId": 216552850}, "corpusId": 216552850, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/90a1491ac32e732c93773354e4e665794ed4d490", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference", "abstract": "Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 224, "influentialCitationCount": 41, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.204.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-27", "journal": {"pages": "2246-2251"}, "authors": [{"authorId": "2059016789", "name": "Ji Xin"}, {"authorId": "26917433", "name": "Raphael Tang"}, {"authorId": "2108395397", "name": "Jaejun Lee"}, {"authorId": "40508553", "name": "Yaoliang Yu"}, {"authorId": "145580839", "name": "Jimmy J. Lin"}]}, {"paperId": "a63c14aa07dab3d87d188d5227bfebed6ebbcb2a", "externalIds": {"MAG": "3034880265", "DBLP": "journals/corr/abs-2005-02473", "ACL": "2020.acl-main.205", "ArXiv": "2005.02473", "DOI": "10.18653/v1/2020.acl-main.205", "CorpusId": 218517070}, "corpusId": 218517070, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a63c14aa07dab3d87d188d5227bfebed6ebbcb2a", "title": "Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks", "abstract": "In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy. Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model. We first define the task as a sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic task of bottom-up-classification. Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy\u2019s layers, and map them into the word vector space. We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search. Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy. With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.205.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"name": "ArXiv", "volume": "abs/2005.02473"}, "authors": [{"authorId": "1412643178", "name": "Kervy Rivas Rojas"}, {"authorId": "108809027", "name": "Gina Bustamante"}, {"authorId": "35664665", "name": "Marco Antonio Sobrevilla Cabezudo"}, {"authorId": "65775345", "name": "Arturo Oncevay"}]}, {"paperId": "b85bd7167771cec4b239a2e0a8ce90155143100c", "externalIds": {"ACL": "2020.acl-main.206", "MAG": "3034738758", "DBLP": "conf/acl/CraigheadCBY20", "DOI": "10.18653/v1/2020.acl-main.206", "CorpusId": 220047185}, "corpusId": 220047185, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b85bd7167771cec4b239a2e0a8ce90155143100c", "title": "Investigating the effect of auxiliary objectives for the automated grading of learner English speech transcriptions", "abstract": "We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts. Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (L1). We encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers, compare against a feature-rich baseline, and analyse performance at different proficiency levels and with transcriptions of varying error rates. Our best performance comes from a transformer encoder with L1 prediction as an auxiliary task. We discuss areas for improvement and potential applications for text-only speech scoring.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 55, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2258-2269"}, "authors": [{"authorId": "1651189042", "name": "Hannah Craighead"}, {"authorId": "143726824", "name": "Andrew Caines"}, {"authorId": "33490976", "name": "P. Buttery"}, {"authorId": "2169553", "name": "H. Yannakoudakis"}]}, {"paperId": "a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031", "externalIds": {"MAG": "3035324702", "DBLP": "journals/corr/abs-2004-07180", "ACL": "2020.acl-main.207", "ArXiv": "2004.07180", "DOI": "10.18653/v1/2020.acl-main.207", "CorpusId": 215768677}, "corpusId": 215768677, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031", "title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers", "abstract": "Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that Specter outperforms a variety of competitive baselines on the benchmark.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 56, "citationCount": 300, "influentialCitationCount": 64, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.207.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-15", "journal": {"name": "ArXiv", "volume": "abs/2004.07180"}, "authors": [{"authorId": "2527954", "name": "Arman Cohan"}, {"authorId": "46411828", "name": "Sergey Feldman"}, {"authorId": "46181066", "name": "Iz Beltagy"}, {"authorId": "145612610", "name": "Doug Downey"}, {"authorId": "1780531", "name": "Daniel S. Weld"}]}, {"paperId": "25d5a834eee27178bf3338d20612d6f18dbf286e", "externalIds": {"MAG": "3025107432", "DBLP": "journals/corr/abs-2005-05927", "ArXiv": "2005.05927", "ACL": "2020.acl-main.208", "DOI": "10.18653/V1/2020.ACL-MAIN.208", "CorpusId": 218596282}, "corpusId": 218596282, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/25d5a834eee27178bf3338d20612d6f18dbf286e", "title": "Semantic Scaffolds for Pseudocode-to-Code Generation", "abstract": "We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.208.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-12", "journal": {"name": "ArXiv", "volume": "abs/2005.05927"}, "authors": [{"authorId": "51011000", "name": "Ruiqi Zhong"}, {"authorId": "144872294", "name": "Mitchell Stern"}, {"authorId": "38666915", "name": "D. Klein"}]}, {"paperId": "7d4f6dbd77bc49a26e3d991b27b3a6fb62f036ab", "externalIds": {"MAG": "3035168168", "DBLP": "conf/acl/BroscheitGWG20", "ACL": "2020.acl-main.209", "DOI": "10.18653/v1/2020.acl-main.209", "CorpusId": 218610721}, "corpusId": 218610721, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7d4f6dbd77bc49a26e3d991b27b3a6fb62f036ab", "title": "Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction", "abstract": "Open Information Extraction systems extract (\u201csubject text\u201d, \u201crelation text\u201d, \u201cobject text\u201d) triples from raw text. Some triples are textual versions of facts, i.e., non-canonicalized mentions of entities and relations. In this paper, we investigate whether it is possible to infer new facts directly from the open knowledge graph without any canonicalization or any supervision from curated knowledge. For this purpose, we propose the open link prediction task,i.e., predicting test facts by completing (\u201csubject text\u201d, \u201crelation text\u201d, ?) questions. An evaluation in such a setup raises the question if a correct prediction is actually a new fact that was induced by reasoning over the open knowledge graph or if it can be trivially explained. For example, facts can appear in different paraphrased textual variants, which can lead to test leakage. To this end, we propose an evaluation protocol and a methodology for creating the open link prediction benchmark OlpBench. We performed experiments with a prototypical knowledge graph embedding model for openlink prediction. While the task is very challenging, our results suggests that it is possible to predict genuinely new facts, which can not be trivially explained.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 26, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.209.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2296-2308"}, "authors": [{"authorId": "2966239", "name": "Samuel Broscheit"}, {"authorId": "24868638", "name": "Kiril Gashteovski"}, {"authorId": "2108945437", "name": "Yanjie Wang"}, {"authorId": "1777107", "name": "Rainer Gemulla"}]}, {"paperId": "07c1c2429b63fefdae41eb546c31b40de2a880f7", "externalIds": {"MAG": "3035275890", "ArXiv": "2005.06117", "ACL": "2020.acl-main.210", "DBLP": "conf/acl/GuptaMNS20", "DOI": "10.18653/v1/2020.acl-main.210", "CorpusId": 218614095}, "corpusId": 218614095, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/07c1c2429b63fefdae41eb546c31b40de2a880f7", "title": "INFOTABS: Inference on Tables as Semi-structured Data", "abstract": "In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. We argue that such data can prove as a testing ground for understanding how we reason about information. To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes. Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning. Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 65, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.210.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-13", "journal": {"pages": "2309-2324"}, "authors": [{"authorId": "46346053", "name": "Vivek Gupta"}, {"authorId": "41016174", "name": "Maitrey Mehta"}, {"authorId": "35296383", "name": "Pegah Nokhiz"}, {"authorId": "3052879", "name": "Vivek Srikumar"}]}, {"paperId": "c0dd6f4ad8210f959a5fc50049a2da960165f741", "externalIds": {"ACL": "2020.acl-main.211", "ArXiv": "1908.10449", "DBLP": "journals/corr/abs-1908-10449", "MAG": "3034300400", "DOI": "10.18653/v1/2020.acl-main.211", "CorpusId": 201657804}, "corpusId": 201657804, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c0dd6f4ad8210f959a5fc50049a2da960165f741", "title": "Interactive Machine Comprehension with Information Seeking Agents", "abstract": "Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA). We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed. In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments. Specifically, we \u201cocclude\u201d the majority of a document\u2019s text and add context-sensitive commands that reveal \u201cglimpses\u201d of the hidden text to a model. We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making. We believe that this setting can contribute in scaling models to web-level QA scenarios.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 35, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.211.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-08-27", "journal": {"name": "ArXiv", "volume": "abs/1908.10449"}, "authors": [{"authorId": "2854297", "name": "Xingdi Yuan"}, {"authorId": "49252800", "name": "Jie Fu"}, {"authorId": "40638665", "name": "Marc-Alexandre C\u00f4t\u00e9"}, {"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "1972076", "name": "C. Pal"}, {"authorId": "3382568", "name": "A. Trischler"}]}, {"paperId": "84059eba69c02bd57b6b227710ba62168ade827e", "externalIds": {"MAG": "3035352537", "DBLP": "journals/corr/abs-2004-11999", "ArXiv": "2004.11999", "ACL": "2020.acl-main.212", "DOI": "10.18653/v1/2020.acl-main.212", "CorpusId": 216553149}, "corpusId": 216553149, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/84059eba69c02bd57b6b227710ba62168ade827e", "title": "Syntactic Data Augmentation Increases Robustness to Inference Heuristics", "abstract": "Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model\u2019s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus. The best-performing augmentation method, subject/object inversion, improved BERT\u2019s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 127, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.212.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-24", "journal": {"pages": "2339-2352"}, "authors": [{"authorId": "2067798455", "name": "Junghyun Min"}, {"authorId": "145534175", "name": "R. Thomas McCoy"}, {"authorId": "143790066", "name": "Dipanjan Das"}, {"authorId": "2585932", "name": "Emily Pitler"}, {"authorId": "2467508", "name": "Tal Linzen"}]}, {"paperId": "da6776e26e65c22d9fe79acbf6bff9dba26c0b37", "externalIds": {"MAG": "3015214239", "DBLP": "conf/acl/ChungG20", "ArXiv": "2004.05274", "ACL": "2020.acl-main.213", "DOI": "10.18653/v1/2020.acl-main.213", "CorpusId": 215744873}, "corpusId": 215744873, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/da6776e26e65c22d9fe79acbf6bff9dba26c0b37", "title": "Improved Speech Representations with Multi-Target Autoregressive Predictive Coding", "abstract": "Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 52, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.213.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-11", "journal": {"name": "ArXiv", "volume": "abs/2004.05274"}, "authors": [{"authorId": "2815804", "name": "Yu-An Chung"}, {"authorId": "145898106", "name": "James R. Glass"}]}, {"paperId": "0ecaddd3df8793c1b7050cb4c6b359c041bedc57", "externalIds": {"DBLP": "conf/acl/RahmanHLZMMH20", "ACL": "2020.acl-main.214", "MAG": "3034266838", "DOI": "10.18653/v1/2020.acl-main.214", "CorpusId": 219571176, "PubMed": "33782629"}, "corpusId": 219571176, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0ecaddd3df8793c1b7050cb4c6b359c041bedc57", "title": "Integrating Multimodal Information in Large Pretrained Transformers", "abstract": "Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). More specifically, this is due to the fact that pre-trained models don\u2019t have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 223, "influentialCitationCount": 42, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.214.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"name": "Proceedings of the conference. Association for Computational Linguistics. Meeting", "pages": "\n          2359-2369\n        ", "volume": "2020"}, "authors": [{"authorId": "153515321", "name": "Wasifur Rahman"}, {"authorId": "2811524", "name": "M. Hasan"}, {"authorId": "1459934320", "name": "Sangwu Lee"}, {"authorId": "144802290", "name": "Amir Zadeh"}, {"authorId": "1429197894", "name": "Chengfeng Mao"}, {"authorId": "49933077", "name": "Louis-Philippe Morency"}, {"authorId": "1491348598", "name": "E. Hoque"}]}, {"paperId": "866a02613e1d4b91aba5ef96ae511541fe2f9b30", "externalIds": {"ACL": "2020.acl-main.215", "MAG": "3021378543", "DBLP": "journals/corr/abs-2005-00812", "ArXiv": "2005.00812", "DOI": "10.18653/v1/2020.acl-main.215", "CorpusId": 218487819}, "corpusId": 218487819, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/866a02613e1d4b91aba5ef96ae511541fe2f9b30", "title": "MultiQT: Multimodal learning for real-time question tracking in speech", "abstract": "We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers. We propose a novel multimodal approach to real-time sequence labeling in speech. Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition. Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data. The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.00812", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00812"}, "authors": [{"authorId": "101654431", "name": "Jakob Drachmann Havtorn"}, {"authorId": "15027824", "name": "Jan Latko"}, {"authorId": "1414146093", "name": "Joakim Edin"}, {"authorId": "1938001", "name": "Lasse Borgholt"}, {"authorId": "7435149", "name": "Lars Maal\u00f8e"}, {"authorId": "1667962591", "name": "Lorenzo Belgrano"}, {"authorId": "1667186545", "name": "Nicolai Frost Jakobsen"}, {"authorId": "147970624", "name": "R. Sdun"}, {"authorId": "1806948", "name": "Zeljko Agic"}]}, {"paperId": "0612582490c445b1859f7feea0c04daf17598014", "externalIds": {"ArXiv": "2004.14840", "DBLP": "conf/acl/Paraskevopoulos20", "MAG": "3035299099", "ACL": "2020.acl-main.216", "DOI": "10.18653/v1/2020.acl-main.216", "CorpusId": 220045916}, "corpusId": 220045916, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0612582490c445b1859f7feea0c04daf17598014", "title": "Multimodal and Multiresolution Speech Recognition with Transformers", "abstract": "This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions. Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models. Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models. Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 25, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.216.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-29", "journal": {"name": "ArXiv", "volume": "abs/2004.14840"}, "authors": [{"authorId": "3397911", "name": "Georgios Paraskevopoulos"}, {"authorId": "2739353", "name": "Srinivas Parthasarathy"}, {"authorId": "40038450", "name": "Aparna Khare"}, {"authorId": "1734989", "name": "Shiva Sundaram"}]}, {"paperId": "15fb586993d1b269a72e61cfcebb69a56de6a3f1", "externalIds": {"MAG": "3028712640", "DBLP": "conf/acl/SaleskyB20", "ACL": "2020.acl-main.217", "ArXiv": "2005.13681", "DOI": "10.18653/v1/2020.acl-main.217", "CorpusId": 218971763}, "corpusId": 218971763, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/15fb586993d1b269a72e61cfcebb69a56de6a3f1", "title": "Phone Features Improve Speech Translation", "abstract": "End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two methods to incorporate phone features into ST models. We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work \u2013 by up to 9 BLEU on our low-resource setting.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 24, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.13681", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-27", "journal": {"name": "ArXiv", "volume": "abs/2005.13681"}, "authors": [{"authorId": "3448427", "name": "Elizabeth Salesky"}, {"authorId": "1690706", "name": "A. Black"}]}, {"paperId": "72bc8f3351229c0648142b29d90497ebf6d2f556", "externalIds": {"ArXiv": "2004.09544", "DBLP": "conf/acl/ChoM20", "MAG": "3019915663", "ACL": "2020.acl-main.218", "DOI": "10.18653/v1/2020.acl-main.218", "CorpusId": 216036372}, "corpusId": 216036372, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/72bc8f3351229c0648142b29d90497ebf6d2f556", "title": "Grounding Conversations with Improvised Dialogues", "abstract": "Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication. Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality. We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier. We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 52, "citationCount": 21, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.218.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-20", "journal": {"name": "ArXiv", "volume": "abs/2004.09544"}, "authors": [{"authorId": "91009922", "name": "Hyundong Justin Cho"}, {"authorId": "143823227", "name": "Jonathan May"}]}, {"paperId": "3abd405560daf61fa6a0a49168656a1d2fb805b0", "externalIds": {"MAG": "3035448310", "ArXiv": "1811.00945", "ACL": "2020.acl-main.219", "DBLP": "conf/acl/ShusterHBW20", "DOI": "10.18653/v1/2020.acl-main.219", "CorpusId": 216914409}, "corpusId": 216914409, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3abd405560daf61fa6a0a49168656a1d2fb805b0", "title": "Image-Chat: Engaging Grounded Conversations", "abstract": "To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners. Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014). In this work we study large-scale architectures and datasets for this goal. We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components. To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019). Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits. Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2018, "referenceCount": 37, "citationCount": 84, "influentialCitationCount": 24, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.219.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-11-02", "journal": {"pages": "2414-2429"}, "authors": [{"authorId": "35752280", "name": "Kurt Shuster"}, {"authorId": "2795882", "name": "Samuel Humeau"}, {"authorId": "1713934", "name": "Antoine Bordes"}, {"authorId": "145183709", "name": "J. Weston"}]}, {"paperId": "bc1f4c4e4839bf64158d599dce7d2bc95df58c55", "externalIds": {"MAG": "3035656139", "DBLP": "conf/acl/SinhaPWLHP20", "ArXiv": "2005.00583", "ACL": "2020.acl-main.220", "DOI": "10.18653/v1/2020.acl-main.220", "CorpusId": 218487696}, "corpusId": 218487696, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bc1f4c4e4839bf64158d599dce7d2bc95df58c55", "title": "Learning an Unreferenced Metric for Online Dialogue Evaluation", "abstract": "Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 72, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.220.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "2430-2441"}, "authors": [{"authorId": "40910779", "name": "Koustuv Sinha"}, {"authorId": "32899078", "name": "Prasanna Parthasarathi"}, {"authorId": "2116439939", "name": "Jasmine Wang"}, {"authorId": "49407415", "name": "Ryan J. Lowe"}, {"authorId": "49437682", "name": "William L. Hamilton"}, {"authorId": "145134886", "name": "Joelle Pineau"}]}, {"paperId": "901406579187e6e743ac6f376a4cb4a1afc523c4", "externalIds": {"ArXiv": "2005.09128", "ACL": "2020.acl-main.221", "DBLP": "conf/acl/RoddyH20", "MAG": "3035364442", "DOI": "10.18653/v1/2020.acl-main.221", "CorpusId": 218684475}, "corpusId": 218684475, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/901406579187e6e743ac6f376a4cb4a1afc523c4", "title": "Neural Generation of Dialogue Response Timings", "abstract": "The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue. We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn. The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS). We evaluate our models using offline experiments as well as human listening tests. We show that human listeners consider certain response timings to be more natural based on the dialogue context. The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.221.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-18", "journal": {"pages": "2442-2452"}, "authors": [{"authorId": "39534492", "name": "Matthew Roddy"}, {"authorId": "144686633", "name": "N. Harte"}]}, {"paperId": "11abce981e90585c142078b5c64b2cb8331b8794", "externalIds": {"MAG": "2985799198", "DBLP": "conf/acl/ShusterJRDBW20", "ACL": "2020.acl-main.222", "ArXiv": "1911.03768", "DOI": "10.18653/v1/2020.acl-main.222", "CorpusId": 207853069}, "corpusId": 207853069, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/11abce981e90585c142078b5c64b2cb8331b8794", "title": "The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents", "abstract": "We introduce dodecaDialogue: a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and situations, and perceive and converse about images. By multi-tasking on such a broad large-scale set of data, we hope to both move towards and measure progress in producing a single unified agent that can perceive, reason and converse with humans in an open-domain setting. We show that such multi-tasking improves over a BERT pre-trained baseline, largely due to multi-tasking with very large dialogue datasets in a similar domain, and that the multi-tasking in general provides gains to both text and image-based tasks using several metrics in both the fine-tune and task transfer settings. We obtain state-of-the-art results on many of the tasks, providing a strong baseline for this challenge.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 56, "citationCount": 68, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.222.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-09", "journal": {"pages": "2453-2470"}, "authors": [{"authorId": "35752280", "name": "Kurt Shuster"}, {"authorId": "3092435", "name": "Da Ju"}, {"authorId": "3849208", "name": "Stephen Roller"}, {"authorId": "31461304", "name": "Emily Dinan"}, {"authorId": "90841478", "name": "Y-Lan Boureau"}, {"authorId": "145183709", "name": "J. Weston"}]}, {"paperId": "f4b33eda909c873c519fce390c3fd66a568c2e27", "externalIds": {"MAG": "3034807061", "DBLP": "conf/acl/Cruys20", "DOI": "10.18653/v1/2020.acl-main.223", "CorpusId": 220046853}, "corpusId": 220046853, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f4b33eda909c873c519fce390c3fd66a568c2e27", "title": "Automatic Poetry Generation from Prosaic Text", "abstract": "In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions. In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation. The system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse. The framework is applied to the generation of poems in both English and French, and is equally evaluated for both languages. Even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 37, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.223.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2471-2480"}, "authors": [{"authorId": "1885890", "name": "T. V. D. Cruys"}]}, {"paperId": "83fb274ca565544743c4cdc7abe58db88a163ae2", "externalIds": {"ACL": "2020.acl-main.224", "MAG": "3034987089", "DBLP": "conf/acl/ZhaoWC20", "DOI": "10.18653/v1/2020.acl-main.224", "CorpusId": 215761208}, "corpusId": 215761208, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/83fb274ca565544743c4cdc7abe58db88a163ae2", "title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation", "abstract": "Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 73, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.224.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2481-2491"}, "authors": [{"authorId": "151480423", "name": "Chao Zhao"}, {"authorId": "2234088162", "name": "M. Walker"}, {"authorId": "37202877", "name": "Snigdha Chaturvedi"}]}, {"paperId": "94551d326be51a57434659093904524c39b877cd", "externalIds": {"MAG": "3035233162", "ACL": "2020.acl-main.225", "ArXiv": "2005.05339", "DBLP": "journals/corr/abs-2005-05339", "DOI": "10.18653/V1/2020.ACL-MAIN.225", "CorpusId": 218595915}, "corpusId": 218595915, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/94551d326be51a57434659093904524c39b877cd", "title": "Enabling Language Models to Fill in the Blanks", "abstract": "We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling\u2014a special case of infilling where text is predicted at the end of a document. In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling. To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked. We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics. Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 145, "influentialCitationCount": 29, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.225.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-11", "journal": {"name": "ArXiv", "volume": "abs/2005.05339"}, "authors": [{"authorId": "1872307", "name": "Chris Donahue"}, {"authorId": "49316195", "name": "Mina Lee"}, {"authorId": "145419642", "name": "Percy Liang"}]}, {"paperId": "da9ed8fcf82961562fee91a7ffe4b9da1f676345", "externalIds": {"MAG": "3034783931", "DBLP": "conf/acl/HuangZEC20", "ACL": "2020.acl-main.226", "DOI": "10.18653/v1/2020.acl-main.226", "CorpusId": 219916949}, "corpusId": 219916949, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/da9ed8fcf82961562fee91a7ffe4b9da1f676345", "title": "INSET: Sentence Infilling with INter-SEntential Transformer", "abstract": "Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion. This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context. Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation. In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2. We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 60, "citationCount": 26, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.226.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-10", "journal": {"pages": "2502-2515"}, "authors": [{"authorId": "2144325497", "name": "Yichen Huang"}, {"authorId": "48378494", "name": "Yizhe Zhang"}, {"authorId": "147733481", "name": "Oussama Elachqar"}, {"authorId": "145215470", "name": "Yu Cheng"}]}, {"paperId": "6675640a8ad5e180a3a68fb5e8b34386df28c68f", "externalIds": {"MAG": "3034629641", "DBLP": "journals/corr/abs-2005-01279", "ArXiv": "2005.01279", "ACL": "2020.acl-main.227", "DOI": "10.18653/v1/2020.acl-main.227", "CorpusId": 218486925}, "corpusId": 218486925, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6675640a8ad5e180a3a68fb5e8b34386df28c68f", "title": "Improving Adversarial Text Generation by Modeling the Distant Future", "abstract": "Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply. We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization. Extensive experiments demonstrate that the proposed method leads to improved performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 45, "citationCount": 12, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.227.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"pages": "2516-2531"}, "authors": [{"authorId": "1390533012", "name": "Ruiyi Zhang"}, {"authorId": "1752041", "name": "Changyou Chen"}, {"authorId": "144702900", "name": "Zhe Gan"}, {"authorId": "2900282", "name": "Wenlin Wang"}, {"authorId": "19178763", "name": "Dinghan Shen"}, {"authorId": "2107926840", "name": "Guoyin Wang"}, {"authorId": "39761651", "name": "Zheng Wen"}, {"authorId": "145006560", "name": "L. Carin"}]}, {"paperId": "34da205f20edba2428f7bba6f3ea0c553a1ae8ec", "externalIds": {"DBLP": "conf/acl/HossainGZ20", "ACL": "2020.acl-main.228", "MAG": "3034881347", "DOI": "10.18653/v1/2020.acl-main.228", "CorpusId": 219545197}, "corpusId": 219545197, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/34da205f20edba2428f7bba6f3ea0c553a1ae8ec", "title": "Simple and Effective Retrieve-Edit-Rerank Text Generation", "abstract": "Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output. We propose to extend this framework with a simple and effective post-generation ranking approach. Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output. We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies. Experiments on two machine translation (MT) datasets show new state-of-art results. We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 27, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.228.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Economics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2532-2538"}, "authors": [{"authorId": "2588850", "name": "Nabil Hossain"}, {"authorId": "2320509", "name": "Marjan Ghazvininejad"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}]}, {"paperId": "077dbc662622cb3419415320d06de0602c50a8d0", "externalIds": {"MAG": "3034253961", "ArXiv": "2005.04625", "DBLP": "conf/acl/ZhuHCDJIS20", "ACL": "2020.acl-main.229", "DOI": "10.18653/v1/2020.acl-main.229", "CorpusId": 218581602}, "corpusId": 218581602, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/077dbc662622cb3419415320d06de0602c50a8d0", "title": "BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps", "abstract": "Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk\u2019s generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 54, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.229.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-10", "journal": {"name": "ArXiv", "volume": "abs/2005.04625"}, "authors": [{"authorId": "143750633", "name": "Wang Zhu"}, {"authorId": "2804000", "name": "Hexiang Hu"}, {"authorId": "1918424", "name": "Jiacheng Chen"}, {"authorId": null, "name": "Zhiwei Deng"}, {"authorId": "20048351", "name": "Vihan Jain"}, {"authorId": "2042413", "name": "Eugene Ie"}, {"authorId": "145757665", "name": "Fei Sha"}]}, {"paperId": "04f7834936bf8f455f804c4d84b52fcffc6784ee", "externalIds": {"ACL": "2020.acl-main.230", "DBLP": "journals/corr/abs-2005-02472", "MAG": "3022137941", "ArXiv": "2005.02472", "DOI": "10.18653/v1/2020.acl-main.230", "CorpusId": 218501728}, "corpusId": 218501728, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/04f7834936bf8f455f804c4d84b52fcffc6784ee", "title": "Cross-media Structured Common Space for Multimedia Event Extraction", "abstract": "We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 70, "citationCount": 63, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.230.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"pages": "2557-2568"}, "authors": [{"authorId": "3361240", "name": "Manling Li"}, {"authorId": "2778637", "name": "Alireza Zareian"}, {"authorId": "145653969", "name": "Qi Zeng"}, {"authorId": "153188991", "name": "Spencer Whitehead"}, {"authorId": "152347526", "name": "Di Lu"}, {"authorId": "2113323573", "name": "Heng Ji"}, {"authorId": "9546964", "name": "Shih-Fu Chang"}]}, {"paperId": "23edba4188492f39a7aefebbd7267a6a8d9ddb74", "externalIds": {"MAG": "3035218028", "ACL": "2020.acl-main.231", "DBLP": "journals/corr/abs-2005-03684", "ArXiv": "2005.03684", "DOI": "10.18653/v1/2020.acl-main.231", "CorpusId": 218571360}, "corpusId": 218571360, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/23edba4188492f39a7aefebbd7267a6a8d9ddb74", "title": "Learning to Segment Actions from Observation and Narration", "abstract": "We apply a generative segmental model of task structure, guided by narration, to action segmentation in video. We focus on unsupervised and weakly-supervised settings where no action labels are known during training. Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos. Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 61, "citationCount": 19, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.231.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-07", "journal": {"pages": "2569-2588"}, "authors": [{"authorId": "153825694", "name": "Daniel Fried"}, {"authorId": "2285263", "name": "Jean-Baptiste Alayrac"}, {"authorId": "1685771", "name": "Phil Blunsom"}, {"authorId": "1745899", "name": "Chris Dyer"}, {"authorId": "144523372", "name": "S. Clark"}, {"authorId": "3208081", "name": "Aida Nematzadeh"}]}, {"paperId": "15a1727324216573f8859b748e65d6ac5458822b", "externalIds": {"DBLP": "conf/acl/JayannavarNH20", "MAG": "3034912332", "ACL": "2020.acl-main.232", "DOI": "10.18653/v1/2020.acl-main.232", "CorpusId": 220046510}, "corpusId": 220046510, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/15a1727324216573f8859b748e65d6ac5458822b", "title": "Learning to execute instructions in a Minecraft dialogue", "abstract": "The Minecraft Collaborative Building Task is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment. We define the subtask of predicting correct action sequences (block placements and removals) in a given game context, and show that capturing B\u2019s past actions as well as B\u2019s perspective leads to a significant improvement in performance on this challenging language understanding problem.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 48, "citationCount": 27, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.232.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2589-2602"}, "authors": [{"authorId": "3457456", "name": "Prashant Jayannavar"}, {"authorId": "35526556", "name": "Anjali Narayan-Chen"}, {"authorId": "3118681", "name": "J. Hockenmaier"}]}, {"paperId": "70557ea6b65846fc30729ceed224acd4ac64ca5d", "externalIds": {"DBLP": "journals/corr/abs-2005-05402", "MAG": "3035237998", "ACL": "2020.acl-main.233", "ArXiv": "2005.05402", "DOI": "10.18653/v1/2020.acl-main.233", "CorpusId": 218595710}, "corpusId": 218595710, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/70557ea6b65846fc30729ceed224acd4ac64ca5d", "title": "MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning", "abstract": "Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture. The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation. Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 122, "influentialCitationCount": 29, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.233.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-11", "journal": {"pages": "2603-2614"}, "authors": [{"authorId": "46665218", "name": "Jie Lei"}, {"authorId": "2130339621", "name": "Liwei Wang"}, {"authorId": "1752875", "name": "Yelong Shen"}, {"authorId": "144580027", "name": "Dong Yu"}, {"authorId": "1685538", "name": "Tamara L. Berg"}, {"authorId": "143977268", "name": "Mohit Bansal"}]}, {"paperId": "ff80f56fe0977836fdb232a058fbebc1c2d5bbac", "externalIds": {"DBLP": "conf/acl/KojimaARA20", "ACL": "2020.acl-main.234", "ArXiv": "2005.01678", "MAG": "3035720671", "DOI": "10.18653/v1/2020.acl-main.234", "CorpusId": 218487245}, "corpusId": 218487245, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ff80f56fe0977836fdb232a058fbebc1c2d5bbac", "title": "What is Learned in Visually Grounded Neural Syntax Acquisition", "abstract": "Visual features are a promising signal for learning bootstrap textual models. However, blackbox learning models make it difficult to isolate the specific contribution of visual components. In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal. By constructing simplified versions of the model, we isolate the core factors that yield the model\u2019s strong performance. Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better. We also find that a simple lexical signal of noun concreteness plays the main role in the model\u2019s predictions as opposed to more complex syntactic reasoning.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 18, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.234.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"name": "ArXiv", "volume": "abs/2005.01678"}, "authors": [{"authorId": "31686229", "name": "Noriyuki Kojima"}, {"authorId": "1388323535", "name": "Hadar Averbuch-Elor"}, {"authorId": "2531268", "name": "Alexander M. Rush"}, {"authorId": "3167681", "name": "Yoav Artzi"}]}, {"paperId": "ba6fa598e2fbf533300ae3cf7d8b2782c0fcfdfe", "externalIds": {"MAG": "3035100081", "DBLP": "journals/corr/abs-2004-12585", "ArXiv": "2004.12585", "ACL": "2020.acl-main.235", "DOI": "10.18653/v1/2020.acl-main.235", "CorpusId": 216552854}, "corpusId": 216552854, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ba6fa598e2fbf533300ae3cf7d8b2782c0fcfdfe", "title": "A Batch Normalized Inference Network Keeps the KL Vanishing Away", "abstract": "Variational Autoencoder (VAE) is widely used as a generative model to approximate a model\u2019s posterior on latent variables by combining the amortized variational inference and deep neural networks. However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as \u201cposterior collapse\u201d. Previous approaches consider the Kullback\u2013Leibler divergence (KL) individual for each datapoint. We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL\u2019s distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior\u2019s parameters. Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently. We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE). Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 46, "citationCount": 41, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.235.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-27", "journal": {"pages": "2636-2649"}, "authors": [{"authorId": "22317545", "name": "Qile Zhu"}, {"authorId": "145001706", "name": "Wei Bi"}, {"authorId": "3028405", "name": "Xiaojiang Liu"}, {"authorId": "152249495", "name": "Xiyao Ma"}, {"authorId": "2108672703", "name": "Xiaolin Li"}, {"authorId": "144953181", "name": "D. Wu"}]}, {"paperId": "75edd2aa6f72f3e4a129832f4754ea5c4d676d39", "externalIds": {"ArXiv": "2005.09117", "MAG": "3026043465", "ACL": "2020.acl-main.236", "DBLP": "conf/acl/AroraMZR20", "DOI": "10.18653/v1/2020.acl-main.236", "CorpusId": 215540397}, "corpusId": 215540397, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/75edd2aa6f72f3e4a129832f4754ea5c4d676d39", "title": "Contextual Embeddings: When Are They Worth It?", "abstract": "We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline\u2014random word embeddings\u2014focusing on the impact of the training set size and the linguistic properties of the task. Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks. Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 22, "citationCount": 37, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.236.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-18", "journal": {"pages": "2650-2663"}, "authors": [{"authorId": "47038321", "name": "Simran Arora"}, {"authorId": "40353015", "name": "Avner May"}, {"authorId": "2151810148", "name": "Jian Zhang"}, {"authorId": "1803218", "name": "Christopher R\u00e9"}]}, {"paperId": "2dafbae148700fc62663b3775cd36c6451ff5ba2", "externalIds": {"DBLP": "conf/acl/YuCWLA20", "ArXiv": "1911.03598", "ACL": "2020.acl-main.237", "MAG": "2986878711", "DOI": "10.18653/v1/2020.acl-main.237", "CorpusId": 207852454}, "corpusId": 207852454, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2dafbae148700fc62663b3775cd36c6451ff5ba2", "title": "Interactive Classification by Asking Informative Questions", "abstract": "We study the potential for interaction in natural language classification. We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions. At each turn, our system decides between asking the most informative question or making the final classification pre-diction. The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks. We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 47, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.237.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-09", "journal": {"name": "ArXiv", "volume": "abs/1911.03598"}, "authors": [{"authorId": "49297123", "name": "L. Yu"}, {"authorId": "27936309", "name": "Howard Chen"}, {"authorId": "16230274", "name": "Sida Wang"}, {"authorId": "3167681", "name": "Yoav Artzi"}, {"authorId": "49986267", "name": "Tao Lei"}]}, {"paperId": "33d469c6d9fc09b59522d91b7696b15dc60a9a93", "externalIds": {"MAG": "3034225346", "DBLP": "conf/akbc/Sachan20", "ACL": "2020.acl-main.238", "DOI": "10.18653/v1/2020.acl-main.238", "CorpusId": 212409690}, "corpusId": 212409690, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/33d469c6d9fc09b59522d91b7696b15dc60a9a93", "title": "Knowledge Graph Embedding Compression", "abstract": "Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes. The approach can be trained end-to-end with simple modifications to any existing KG embedding technique. We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 52, "citationCount": 25, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.238.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2681-2691"}, "authors": [{"authorId": "2790926", "name": "Mrinmaya Sachan"}]}, {"paperId": "5b2080ea6748bf4cfdb2415e1c1671225fb6337c", "externalIds": {"ACL": "2020.acl-main.239", "DBLP": "conf/acl/PerlCG20", "MAG": "3035513747", "DOI": "10.18653/v1/2020.acl-main.239", "CorpusId": 220047102}, "corpusId": 220047102, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5b2080ea6748bf4cfdb2415e1c1671225fb6337c", "title": "Low Resource Sequence Tagging using Sentence Reconstruction", "abstract": "This work revisits the task of training sequence tagging models with limited resources using transfer learning. We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings. Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines. We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 5, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.239.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2692-2698"}, "authors": [{"authorId": "1754707248", "name": "T. Perl"}, {"authorId": "2715333", "name": "Sriram Chaudhury"}, {"authorId": "2711839", "name": "R. Giryes"}]}, {"paperId": "4099c4d272c12081b562392606e6d567e4ae7031", "externalIds": {"DBLP": "conf/acl/SalazarLNK20", "ACL": "2020.acl-main.240", "MAG": "3034775979", "DOI": "10.18653/v1/2020.acl-main.240", "CorpusId": 218628872}, "corpusId": 218628872, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4099c4d272c12081b562392606e6d567e4ae7031", "title": "Masked Language Model Scoring", "abstract": "Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model\u2019s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL\u2019s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 63, "citationCount": 296, "influentialCitationCount": 36, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.240.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-31", "journal": {"pages": "2699-2712"}, "authors": [{"authorId": "143733211", "name": "Julian Salazar"}, {"authorId": "25130521", "name": "Davis Liang"}, {"authorId": "32125163", "name": "Toan Q. Nguyen"}, {"authorId": "1783839", "name": "K. Kirchhoff"}]}, {"paperId": "018e629e5c53794ea7dadad623bdfa2bcf2c9f92", "externalIds": {"DBLP": "journals/corr/abs-1911-04910", "ArXiv": "1911.04910", "ACL": "2020.acl-main.241", "MAG": "3034758281", "DOI": "10.18653/v1/2020.acl-main.241", "CorpusId": 207863292}, "corpusId": 207863292, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/018e629e5c53794ea7dadad623bdfa2bcf2c9f92", "title": "Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding", "abstract": "Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 36, "citationCount": 68, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.241.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-09", "journal": {"name": "ArXiv", "volume": "abs/1911.04910"}, "authors": [{"authorId": "48066266", "name": "Yun Tang"}, {"authorId": "30768523", "name": "Jing Huang"}, {"authorId": "2148300", "name": "Guangtao Wang"}, {"authorId": "144137069", "name": "Xiaodong He"}, {"authorId": "150048906", "name": "Bowen Zhou"}]}, {"paperId": "c6971f00a07715bbd9144f4ba62ceb2b29d4e618", "externalIds": {"MAG": "3022329361", "ArXiv": "2004.14500", "DBLP": "conf/acl/JungKCMS20", "ACL": "2020.acl-main.242", "DOI": "10.18653/v1/2020.acl-main.242", "CorpusId": 216867328}, "corpusId": 216867328, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c6971f00a07715bbd9144f4ba62ceb2b29d4e618", "title": "Posterior Calibrated Training on Sentence Classification Tasks", "abstract": "Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone. When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications. Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives. Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline. We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline. PosCal training can be easily extendable to any types of classification tasks as a form of regularization term. Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 14, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.242.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "2723-2730"}, "authors": [{"authorId": "97638739", "name": "Taehee Jung"}, {"authorId": "48493368", "name": "Dongyeop Kang"}, {"authorId": "2117908368", "name": "Hua Cheng"}, {"authorId": "3422652", "name": "L. Mentch"}, {"authorId": "145849024", "name": "Thomas Schaaf"}]}, {"paperId": "fd061aa7d44021e71186e7798ad332f19be1ce1f", "externalIds": {"MAG": "2986499612", "ArXiv": "2005.04560", "DBLP": "journals/corr/abs-2005-04560", "ACL": "2020.acl-main.243", "DOI": "10.18653/v1/2020.acl-main.243", "CorpusId": 209090345}, "corpusId": 209090345, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fd061aa7d44021e71186e7798ad332f19be1ce1f", "title": "Posterior Control of Blackbox Generation", "abstract": "Text generation often requires high-precision output that obeys task-specific rules. This fine-grained control is difficult to enforce with off-the-shelf deep learning models. In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach. Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model. This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models. Experiments consider applications of this approach for text generation. We find that this method improves over standard benchmarks, while also providing fine-grained control.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 50, "citationCount": 23, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.243.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-10", "journal": {"name": "ArXiv", "volume": "abs/2005.04560"}, "authors": [{"authorId": "32551341", "name": "Xiang Lisa Li"}, {"authorId": "2531268", "name": "Alexander M. Rush"}]}, {"paperId": "97f08c1ae8ca5ddf5948c66bfbbc0546ac154807", "externalIds": {"MAG": "3015377432", "DBLP": "journals/corr/abs-2004-06100", "ACL": "2020.acl-main.244", "ArXiv": "2004.06100", "DOI": "10.18653/v1/2020.acl-main.244", "CorpusId": 215745290}, "corpusId": 215745290, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/97f08c1ae8ca5ddf5948c66bfbbc0546ac154807", "title": "Pretrained Transformers Improve Out-of-Distribution Robustness", "abstract": "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers\u2019 performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 62, "citationCount": 318, "influentialCitationCount": 42, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.244.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-13", "journal": {"name": "ArXiv", "volume": "abs/2004.06100"}, "authors": [{"authorId": "3422872", "name": "Dan Hendrycks"}, {"authorId": "2118894900", "name": "Xiaoyuan Liu"}, {"authorId": "145217343", "name": "Eric Wallace"}, {"authorId": "7485473", "name": "Adam Dziedzic"}, {"authorId": "1630364521", "name": "R. Krishnan"}, {"authorId": "143711382", "name": "D. Song"}]}, {"paperId": "32bc789f96acb37361ac55f36940bb52b759c229", "externalIds": {"DBLP": "journals/corr/abs-2005-01229", "MAG": "3035441470", "ArXiv": "2005.01229", "ACL": "2020.acl-main.245", "DOI": "10.18653/v1/2020.acl-main.245", "CorpusId": 218487582}, "corpusId": 218487582, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/32bc789f96acb37361ac55f36940bb52b759c229", "title": "Robust Encodings: A Framework for Combating Adversarial Typos", "abstract": "Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT. In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture. The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings. Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity). We instantiate RobEn to defend against a large family of adversarial typos. Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 85, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.245.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"name": "ArXiv", "volume": "abs/2005.01229"}, "authors": [{"authorId": "145790135", "name": "Erik Jones"}, {"authorId": "3422908", "name": "Robin Jia"}, {"authorId": "2655157", "name": "Aditi Raghunathan"}, {"authorId": "145419642", "name": "Percy Liang"}]}, {"paperId": "d80b23150de329e6434a436edc6ea4378bd0b658", "externalIds": {"ArXiv": "2004.13705", "MAG": "3021041462", "DBLP": "journals/corr/abs-2004-13705", "ACL": "2020.acl-main.246", "DOI": "10.18653/V1/2020.ACL-MAIN.246", "CorpusId": 216562713}, "corpusId": 216562713, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d80b23150de329e6434a436edc6ea4378bd0b658", "title": "Showing Your Work Doesn\u2019t Always Work", "abstract": "In natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks. One exemplar publication, titled \u201cShow Your Work: Improved Reporting of Experimental Results\u201d (Dodge et al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational budget. In the present work, we critically examine this paper. As far as statistical generalizability is concerned, we find unspoken pitfalls and caveats with this approach. We analytically show that their estimator is biased and uses error-prone assumptions. We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals. We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation. Our codebase is at https://github.com/castorini/meanmax.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 18, "citationCount": 4, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.246.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-28", "journal": {"pages": "2766-2772"}, "authors": [{"authorId": "26917433", "name": "Raphael Tang"}, {"authorId": "2108395397", "name": "Jaejun Lee"}, {"authorId": "2059016789", "name": "Ji Xin"}, {"authorId": "2110789748", "name": "Xinyu Liu"}, {"authorId": "40508553", "name": "Yaoliang Yu"}, {"authorId": "145580839", "name": "Jimmy J. Lin"}]}, {"paperId": "6c8503803760c5c7790f72437d0f8b874334e6f0", "externalIds": {"DBLP": "journals/corr/abs-1909-04120", "MAG": "2972738865", "ArXiv": "1909.04120", "ACL": "2020.acl-main.247", "DOI": "10.18653/v1/2020.acl-main.247", "CorpusId": 202542881}, "corpusId": 202542881, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6c8503803760c5c7790f72437d0f8b874334e6f0", "title": "Span Selection Pre-training for Question Answering", "abstract": "BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA). BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction. In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding. Span Selection PreTraining (SSPT) poses cloze-like training instances, but rather than draw the answer from the model\u2019s parameters, it is selected from a relevant passage. We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension (MRC) datasets. Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction. We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system. Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 28, "citationCount": 54, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.247.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-09", "journal": {"pages": "2773-2782"}, "authors": [{"authorId": "143742133", "name": "Michael R. Glass"}, {"authorId": "1711133", "name": "A. Gliozzo"}, {"authorId": "38725355", "name": "Rishav Chakravarti"}, {"authorId": "1388016369", "name": "Anthony Ferritto"}, {"authorId": "2101328894", "name": "Lin Pan"}, {"authorId": "146774050", "name": "G P Shrivatsa Bhargav"}, {"authorId": "50252087", "name": "Dinesh Garg"}, {"authorId": "2707234", "name": "Avirup Sil"}]}, {"paperId": "9a9bc0d06449ce949a29470e10cc0437dc5df8e4", "externalIds": {"ACL": "2020.acl-main.248", "DBLP": "conf/acl/PrabhumoyeSB20", "ArXiv": "2005.00432", "MAG": "3034476261", "DOI": "10.18653/V1/2020.ACL-MAIN.248", "CorpusId": 215806653}, "corpusId": 215806653, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9a9bc0d06449ce949a29470e10cc0437dc5df8e4", "title": "Topological Sort for Sentence Ordering", "abstract": "Sentence ordering is the task of arranging the sentences of a given text in the correct order. Recent work using deep neural networks for this task has framed it as a sequence prediction problem. In this paper, we propose a new framing of this task as a constraint solving problem and introduce a new technique to solve it. Additionally, we propose a human evaluation for this task. The results on both automatic and human metrics across four different datasets show that this new technique is better at capturing coherence in documents.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 37, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.00432", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00432"}, "authors": [{"authorId": "9358910", "name": "Shrimai Prabhumoye"}, {"authorId": "145124475", "name": "R. Salakhutdinov"}, {"authorId": "1690706", "name": "A. Black"}]}, {"paperId": "0d360a1256ccdfca58cf98d12243df8407fd442d", "externalIds": {"MAG": "3035367371", "ACL": "2020.acl-main.249", "DBLP": "conf/acl/KuritaMN20", "ArXiv": "2004.06660", "DOI": "10.18653/v1/2020.acl-main.249", "CorpusId": 215754328}, "corpusId": 215754328, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0d360a1256ccdfca58cf98d12243df8407fd442d", "title": "Weight Poisoning Attacks on Pretrained Models", "abstract": "Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct \u201cweight poisoning\u201d attacks where pre-trained weights are injected with vulnerabilities that expose \u201cbackdoors\u201d after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 59, "citationCount": 244, "influentialCitationCount": 48, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.249.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-14", "journal": {"pages": "2793-2806"}, "authors": [{"authorId": "147225682", "name": "Keita Kurita"}, {"authorId": "144397625", "name": "Paul Michel"}, {"authorId": "1700325", "name": "Graham Neubig"}]}, {"paperId": "a81674f480dba239e12c80910528cae5d3a28e97", "externalIds": {"MAG": "3025946885", "DBLP": "conf/acl/KhetanK20", "ACL": "2020.acl-main.250", "ArXiv": "2005.06628", "DOI": "10.18653/v1/2020.acl-main.250", "CorpusId": 218629975}, "corpusId": 218629975, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a81674f480dba239e12c80910528cae5d3a28e97", "title": "schuBERT: Optimizing Elements of BERT", "abstract": "Transformers have gradually become a key component for many state-of-the-art natural language representation models. A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is computationally prohibitive and has a huge number of parameters. In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model. We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency. We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers. In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 26, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.250.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-09", "journal": {"pages": "2807-2818"}, "authors": [{"authorId": "3083159", "name": "A. Khetan"}, {"authorId": "3386660", "name": "Zohar S. Karnin"}]}, {"paperId": "f2c0b478a30e653157dcdfe879b3082c6bbb0913", "externalIds": {"MAG": "3034425996", "ArXiv": "2005.00850", "DBLP": "journals/corr/abs-2005-00850", "ACL": "2020.acl-main.251", "DOI": "10.18653/v1/2020.acl-main.251", "CorpusId": 218486908}, "corpusId": 218486908, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f2c0b478a30e653157dcdfe879b3082c6bbb0913", "title": "ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation", "abstract": "We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy. This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 40, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.00850", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00850"}, "authors": [{"authorId": "3376969", "name": "Lifu Tu"}, {"authorId": "46230016", "name": "Richard Yuanzhe Pang"}, {"authorId": "2844243", "name": "Sam Wiseman"}, {"authorId": "1700980", "name": "Kevin Gimpel"}]}, {"paperId": "5967a76ecb0f15de1da1a2cc869be581656411b1", "externalIds": {"DBLP": "conf/acl/SiddhantBCFCKAW20", "ArXiv": "2005.04816", "ACL": "2020.acl-main.252", "MAG": "3035144493", "DOI": "10.18653/V1/2020.ACL-MAIN.252", "CorpusId": 218580964}, "corpusId": 218580964, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5967a76ecb0f15de1da1a2cc869be581656411b1", "title": "Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation", "abstract": "Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 75, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.252.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-11", "journal": {"pages": "2827-2835"}, "authors": [{"authorId": "9356387", "name": "Aditya Siddhant"}, {"authorId": "12295226", "name": "Ankur Bapna"}, {"authorId": "145144022", "name": "Yuan Cao"}, {"authorId": "2345617", "name": "Orhan Firat"}, {"authorId": "48623026", "name": "M. Chen"}, {"authorId": "35871436", "name": "Sneha Kudugunta"}, {"authorId": "3365231", "name": "N. Arivazhagan"}, {"authorId": "48607963", "name": "Yonghui Wu"}]}, {"paperId": "8b5b8bd0942d5f39d01c1f4b89d174fb3fde99cc", "externalIds": {"DBLP": "conf/acl/EdunovORA20", "MAG": "2967600676", "ArXiv": "1908.05204", "ACL": "2020.acl-main.253", "DOI": "10.18653/v1/2020.acl-main.253", "CorpusId": 199577473}, "corpusId": 199577473, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8b5b8bd0942d5f39d01c1f4b89d174fb3fde99cc", "title": "On The Evaluation of Machine Translation SystemsTrained With Back-Translation", "abstract": "Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese. This is believed to be due to translationese inputs better matching the back-translated training data. In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 38, "citationCount": 76, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.253.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-08-14", "journal": {"name": "ArXiv", "volume": "abs/1908.05204"}, "authors": [{"authorId": "2068070", "name": "Sergey Edunov"}, {"authorId": "40511414", "name": "Myle Ott"}, {"authorId": "1706809", "name": "Marc'Aurelio Ranzato"}, {"authorId": "2325985", "name": "Michael Auli"}]}, {"paperId": "183a1a62719bd116da23db4cbf5b2bd2576e4ee8", "externalIds": {"MAG": "3035348852", "ArXiv": "2004.13169", "ACL": "2020.acl-main.254", "DBLP": "conf/acl/ZhengLZMLH20", "DOI": "10.18653/v1/2020.acl-main.254", "CorpusId": 216562811}, "corpusId": 216562811, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/183a1a62719bd116da23db4cbf5b2bd2576e4ee8", "title": "Simultaneous Translation Policies: From Fixed to Adaptive", "abstract": "Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 54, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.254.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-27", "journal": {"pages": "2847-2853"}, "authors": [{"authorId": "20712300", "name": "Baigong Zheng"}, {"authorId": "66057453", "name": "Kaibo Liu"}, {"authorId": "40223399", "name": "Renjie Zheng"}, {"authorId": "1847848", "name": "Mingbo Ma"}, {"authorId": "2110117273", "name": "Hairong Liu"}, {"authorId": "48545084", "name": "Liang Huang"}]}, {"paperId": "81393a94e6bd33f9a3623814f754f662082a5c11", "externalIds": {"ACL": "2020.acl-main.255", "MAG": "3034782826", "DBLP": "conf/acl/BevilacquaN20", "DOI": "10.18653/v1/2020.acl-main.255", "CorpusId": 220045824}, "corpusId": 220045824, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/81393a94e6bd33f9a3623814f754f662082a5c11", "title": "Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information", "abstract": "Neural architectures are the current state of the art in Word Sense Disambiguation (WSD). However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB). We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set. As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks. On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 122, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.255.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2854-2864"}, "authors": [{"authorId": "143802044", "name": "Michele Bevilacqua"}, {"authorId": "1733928", "name": "Roberto Navigli"}]}, {"paperId": "2f9cf50558792a9ff81c2cc70e6ccfb585c2f605", "externalIds": {"MAG": "3034674003", "ACL": "2020.acl-main.256", "DBLP": "conf/acl/ChenYL20", "DOI": "10.18653/v1/2020.acl-main.256", "CorpusId": 220048496}, "corpusId": 220048496, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2f9cf50558792a9ff81c2cc70e6ccfb585c2f605", "title": "Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs", "abstract": "Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus. We show that characters\u2019 written form, Glyphs, in ideographic languages could carry rich semantics. We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem. Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese NLP systems, especially for low-resource scenarios. Experiments across different applications show the significant effectiveness of our model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 10, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.256.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2865-2871"}, "authors": [{"authorId": "1390824389", "name": "Hong-You Chen"}, {"authorId": "1753913816", "name": "Sz-Han Yu"}, {"authorId": "2145196500", "name": "Shou-De Lin"}]}, {"paperId": "523d7fa653314d6e90c1dc44948948105b77b380", "externalIds": {"DBLP": "conf/acl/GerzVRRK20", "ArXiv": "2005.05264", "MAG": "3021315325", "ACL": "2020.acl-main.257", "DOI": "10.18653/v1/2020.acl-main.257", "CorpusId": 218581798}, "corpusId": 218581798, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/523d7fa653314d6e90c1dc44948948105b77b380", "title": "Multidirectional Associative Optimization of Function-Specific Word Representations", "abstract": "We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together. The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure. We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity. The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 67, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.257.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-11", "journal": {"pages": "2872-2882"}, "authors": [{"authorId": "51176335", "name": "D. Gerz"}, {"authorId": "1747849", "name": "Ivan Vulic"}, {"authorId": "145687301", "name": "Marek Rei"}, {"authorId": "1762757", "name": "Roi Reichart"}, {"authorId": "145762466", "name": "A. Korhonen"}]}, {"paperId": "54a01b0bcae0737430038926a5bac79b118be1ba", "externalIds": {"DBLP": "conf/acl/HattySDW20", "ACL": "2020.acl-main.258", "MAG": "3035223328", "DOI": "10.18653/v1/2020.acl-main.258", "CorpusId": 220046052}, "corpusId": 220046052, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/54a01b0bcae0737430038926a5bac79b118be1ba", "title": "Predicting Degrees of Technicality in Automatic Terminology Extraction", "abstract": "While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied. We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction. When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings. We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally. Both models outperform previous approaches, with the multi-channel model performing best.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 14, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.258.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2883-2889"}, "authors": [{"authorId": "3449046", "name": "Anna H\u00e4tty"}, {"authorId": "3449121", "name": "Dominik Schlechtweg"}, {"authorId": "2558045", "name": "Michael Dorna"}, {"authorId": "7965906", "name": "Sabine Schulte im Walde"}]}, {"paperId": "ea923aa464cca33c8374ae3799711aa7b1502f52", "externalIds": {"ACL": "2020.acl-main.259", "DBLP": "conf/acl/RohanianRTH20", "MAG": "3035263353", "DOI": "10.18653/v1/2020.acl-main.259", "CorpusId": 220048355}, "corpusId": 220048355, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ea923aa464cca33c8374ae3799711aa7b1502f52", "title": "Verbal Multiword Expressions for Identification of Metaphor", "abstract": "Metaphor is a linguistic device in which a concept is expressed by mentioning another. Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics. Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models. This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs. To the best of our knowledge, this is the first \u201cMWE-aware\u201d metaphor identification system paving the way for further experiments on the complex interactions of these phenomena. The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 15, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.259.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2890-2895"}, "authors": [{"authorId": "25119982", "name": "Omid Rohanian"}, {"authorId": "145687301", "name": "Marek Rei"}, {"authorId": "2555867", "name": "Shiva Taslimipoor"}, {"authorId": "34903252", "name": "L. Ha"}]}, {"paperId": "e3e9d2bdcc3fefab7c294196c8b2e149727376ed", "externalIds": {"ACL": "2020.acl-main.260", "DBLP": "journals/corr/abs-2005-00699", "ArXiv": "2005.00699", "MAG": "3023556715", "DOI": "10.18653/v1/2020.acl-main.260", "CorpusId": 218487087}, "corpusId": 218487087, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e3e9d2bdcc3fefab7c294196c8b2e149727376ed", "title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer", "abstract": "Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 69, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.260.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00699"}, "authors": [{"authorId": "33524946", "name": "Jieyu Zhao"}, {"authorId": "1777140", "name": "Subhabrata Mukherjee"}, {"authorId": "2195458", "name": "Saghar Hosseini"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}, {"authorId": "1977489", "name": "Ahmed Hassan Awadallah"}]}, {"paperId": "f1ae2104bb018618f84e5f8ec2c9e2d44f5a15c2", "externalIds": {"DBLP": "journals/corr/abs-2005-13213", "ACL": "2020.acl-main.261", "MAG": "3031424129", "ArXiv": "2005.13213", "DOI": "10.18653/v1/2020.acl-main.261", "CorpusId": 218900504}, "corpusId": 218900504, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f1ae2104bb018618f84e5f8ec2c9e2d44f5a15c2", "title": "Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?", "abstract": "As part of growing NLP capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research. We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with making such a decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.261.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-27", "journal": {"pages": "2908-2913"}, "authors": [{"authorId": "10836604", "name": "Kobi Leins"}, {"authorId": "1800564", "name": "Jey Han Lau"}, {"authorId": "145465286", "name": "Timothy Baldwin"}]}, {"paperId": "6b3e271e60df9825542374ee5e31e4a1ec2ee1bc", "externalIds": {"DBLP": "journals/corr/abs-2004-12332", "MAG": "3034422725", "ArXiv": "2004.12332", "ACL": "2020.acl-main.262", "DOI": "10.18653/v1/2020.acl-main.262", "CorpusId": 216553205}, "corpusId": 216553205, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6b3e271e60df9825542374ee5e31e4a1ec2ee1bc", "title": "Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds", "abstract": "Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias. In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. We provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias. In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. For example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences \u2013 to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 14, "citationCount": 17, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.262.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-26", "journal": {"pages": "2914-2919"}, "authors": [{"authorId": "10324691", "name": "Kawin Ethayarajh"}]}, {"paperId": "3dfa2d2d7e13eb8b29be6f3bc97f518e94be09e5", "externalIds": {"MAG": "3023553115", "DBLP": "journals/corr/abs-2005-04364", "ACL": "2020.acl-main.263", "ArXiv": "2005.04364", "DOI": "10.18653/v1/2020.acl-main.263", "CorpusId": 218581359}, "corpusId": 218581359, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3dfa2d2d7e13eb8b29be6f3bc97f518e94be09e5", "title": "It\u2019s Morphin\u2019 Time! Combating Linguistic Discrimination with Inflectional Perturbations", "abstract": "Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 54, "citationCount": 82, "influentialCitationCount": 12, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.263.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-09", "journal": {"name": "ArXiv", "volume": "abs/2005.04364"}, "authors": [{"authorId": "145814654", "name": "Samson Tan"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "37596605", "name": "Min-Yen Kan"}, {"authorId": "2166511", "name": "R. Socher"}]}, {"paperId": "0f4bcebc95548a7286106b67bf1115802f093469", "externalIds": {"DBLP": "conf/acl/JiaMZC20", "MAG": "3024274218", "ACL": "2020.acl-main.264", "ArXiv": "2005.06251", "DOI": "10.18653/v1/2020.acl-main.264", "CorpusId": 218613632}, "corpusId": 218613632, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0f4bcebc95548a7286106b67bf1115802f093469", "title": "Mitigating Gender Bias Amplification in Distribution by Posterior Regularization", "abstract": "Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models\u2019 top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 26, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.264.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-13", "journal": {"pages": "2936-2942"}, "authors": [{"authorId": "1693706507", "name": "Shengyu Jia"}, {"authorId": "2056088060", "name": "Tao Meng"}, {"authorId": "33524946", "name": "Jieyu Zhao"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}]}, {"paperId": "0ab2fb6c850bd1c5882deb4984d37b4ccbee580c", "externalIds": {"DBLP": "journals/corr/abs-1911-03642", "MAG": "3034778990", "ArXiv": "1911.03642", "DOI": "10.18653/v1/2020.acl-main.265", "CorpusId": 207852900}, "corpusId": 207852900, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0ab2fb6c850bd1c5882deb4984d37b4ccbee580c", "title": "Towards Understanding Gender Bias in Relation Extraction", "abstract": "Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction (AKBC). While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to our knowledge to evaluate social biases in NRE systems. We create WikiGenderBias, a distantly supervised dataset with a human annotated test set. WikiGenderBias has sentences specifically curated to analyze gender bias in relation extraction systems. We use WikiGenderBias to evaluate systems for bias and find that NRE systems exhibit gender biased predictions and lay groundwork for future evaluation of bias in NRE. We also analyze how name anonymization, hard debiasing for word embeddings, and counterfactual data augmentation affect gender bias in predictions and performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 58, "citationCount": 30, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.265.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-09", "journal": {"pages": "2943-2953"}, "authors": [{"authorId": "146072982", "name": "Andrew Gaut"}, {"authorId": "1516120843", "name": "Tony Sun"}, {"authorId": "148149462", "name": "Shirlyn Tang"}, {"authorId": "2154731574", "name": "Yuxin Huang"}, {"authorId": "144130537", "name": "Jing Qian"}, {"authorId": "2165346", "name": "Mai Elsherief"}, {"authorId": "2110117732", "name": "Jieyu Zhao"}, {"authorId": "1705929", "name": "Diba Mirza"}, {"authorId": "8208963", "name": "E. Belding"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}, {"authorId": "1682479", "name": "William Yang Wang"}]}, {"paperId": "f70577af5417a7ce95ebdbc9d5542621482e1b19", "externalIds": {"MAG": "3022380359", "ACL": "2020.acl-main.266", "ArXiv": "2005.01646", "DBLP": "journals/corr/abs-2005-01646", "DOI": "10.18653/v1/2020.acl-main.266", "CorpusId": 218486915}, "corpusId": 218486915, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f70577af5417a7ce95ebdbc9d5542621482e1b19", "title": "A Probabilistic Generative Model for Typographical Analysis of Early Modern Printing", "abstract": "We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents. We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance. Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with Early Modern printing. Critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures. We show that our approach outperforms rigid interpretable clustering baselines (c.f. Ocular) and overly-flexible deep generative models (VAE) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 18, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.01646", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"name": "ArXiv", "volume": "abs/2005.01646"}, "authors": [{"authorId": "37195917", "name": "Kartik Goyal"}, {"authorId": "1745899", "name": "Chris Dyer"}, {"authorId": "50355374", "name": "Christopher N. Warren"}, {"authorId": "1456636059", "name": "M. G'Sell"}, {"authorId": "1400419309", "name": "Taylor Berg-Kirkpatrick"}]}, {"paperId": "421beeabd2b981754c2871055a8d48e08cd2a5ad", "externalIds": {"ACL": "2020.acl-main.267", "MAG": "3035164270", "DBLP": "conf/acl/WuWQCH20", "DOI": "10.18653/v1/2020.acl-main.267", "CorpusId": 220045902}, "corpusId": 220045902, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/421beeabd2b981754c2871055a8d48e08cd2a5ad", "title": "Attentive Pooling with Learnable Norms for Text Representation", "abstract": "Pooling is an important technique for learning text representations in many neural NLP models. In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L\u221e norm of input features. However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks. In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited. In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation. Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks. In addition, we propose two methods to ensure the numerical stability of the model training. The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion. The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation. Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-25", "journal": {"pages": "2961-2970"}, "authors": [{"authorId": "15161448", "name": "Chuhan Wu"}, {"authorId": "2397264", "name": "Fangzhao Wu"}, {"authorId": "50329599", "name": "Tao Qi"}, {"authorId": "2100653779", "name": "Xiaohui Cui"}, {"authorId": "1731776", "name": "Yongfeng Huang"}]}, {"paperId": "aa95b13370d663ff805fcba1b4493aae07166ef3", "externalIds": {"MAG": "3035368332", "DBLP": "conf/acl/SchroderB20", "ACL": "2020.acl-main.268", "DOI": "10.18653/v1/2020.acl-main.268", "CorpusId": 218500939}, "corpusId": 218500939, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/aa95b13370d663ff805fcba1b4493aae07166ef3", "title": "Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks", "abstract": "Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach. We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups. Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel. Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work. We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance. We provide an efficient, open-source implementation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 58, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.268.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "2971-2985"}, "authors": [{"authorId": "51448921", "name": "Fynn Schr\u00f6der"}, {"authorId": "1829342", "name": "Christian Biemann"}]}, {"paperId": "812aed3f4032bd28a14d4cd3a40c77cb82066b9c", "externalIds": {"DBLP": "journals/corr/abs-2005-00979", "ACL": "2020.acl-main.269", "MAG": "3023333410", "ArXiv": "2005.00979", "DOI": "10.18653/v1/2020.acl-main.269", "CorpusId": 218487187}, "corpusId": 218487187, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/812aed3f4032bd28a14d4cd3a40c77cb82066b9c", "title": "How Does Selective Mechanism Improve Self-Attention Networks?", "abstract": "Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax. Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs. Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 45, "citationCount": 26, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.269.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"pages": "2986-2995"}, "authors": [{"authorId": "2075336379", "name": "Xinwei Geng"}, {"authorId": "1800190", "name": "Longyue Wang"}, {"authorId": "48631170", "name": "Xing Wang"}, {"authorId": "152277111", "name": "Bing Qin"}, {"authorId": "40282288", "name": "Ting Liu"}, {"authorId": "2909321", "name": "Zhaopeng Tu"}]}, {"paperId": "3ff8d265f4351e4b1fdac5b586466bee0b5d6fff", "externalIds": {"DBLP": "conf/acl/PressSL20", "ArXiv": "1911.03864", "MAG": "2988945824", "ACL": "2020.acl-main.270", "DOI": "10.18653/v1/2020.acl-main.270", "CorpusId": 207853045}, "corpusId": 207853045, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3ff8d265f4351e4b1fdac5b586466bee0b5d6fff", "title": "Improving Transformer Models by Reordering their Sublayers", "abstract": "Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 31, "citationCount": 64, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.270.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-10", "journal": {"name": "ArXiv", "volume": "abs/1911.03864"}, "authors": [{"authorId": "40170001", "name": "Ofir Press"}, {"authorId": "1685669", "name": "Noah A. Smith"}, {"authorId": "39455775", "name": "Omer Levy"}]}, {"paperId": "68e14fbe76f19710c26cc090298cfd3cbd2834e8", "externalIds": {"MAG": "3021738317", "DBLP": "conf/acl/KuwabaraSN20", "ArXiv": "2005.00879", "ACL": "2020.acl-main.271", "DOI": "10.18653/v1/2020.acl-main.271", "CorpusId": 218487700}, "corpusId": 218487700, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/68e14fbe76f19710c26cc090298cfd3cbd2834e8", "title": "Single Model Ensemble using Pseudo-Tags and Distinct Vectors", "abstract": "Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort. In this study, we propose a novel method that replicates the effects of a model ensemble with a single model. Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors. Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 3, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.271.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"pages": "3006-3013"}, "authors": [{"authorId": "2066167988", "name": "Ryosuke Kuwabara"}, {"authorId": "144042991", "name": "Jun Suzuki"}, {"authorId": "48731103", "name": "Hideki Nakayama"}]}, {"paperId": "bdd4f05c853bb66e8b01b70ac009aa4242c1c4a4", "externalIds": {"MAG": "3034608141", "DBLP": "conf/acl/YeGCCXZWZC20", "ACL": "2020.acl-main.272", "DOI": "10.18653/v1/2020.acl-main.272", "CorpusId": 220045392}, "corpusId": 220045392, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bdd4f05c853bb66e8b01b70ac009aa4242c1c4a4", "title": "Zero-shot Text Classification via Reinforced Self-training", "abstract": "Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity. In this situation, transferring from seen classes to unseen classes is extremely hard. To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data. Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets. We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection. Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 55, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.272.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3014-3024"}, "authors": [{"authorId": "2114132405", "name": "Zhiquan Ye"}, {"authorId": "152683218", "name": "Yuxia Geng"}, {"authorId": "1731892", "name": "Jiaoyan Chen"}, {"authorId": "1728711", "name": "Jingmin Chen"}, {"authorId": "2112435159", "name": "Xiaoxiao Xu"}, {"authorId": "2110644019", "name": "Suhang Zheng"}, {"authorId": "2145757199", "name": "Feng Wang"}, {"authorId": "2155661636", "name": "Jun Zhang"}, {"authorId": "1729778", "name": "Huajun Chen"}]}, {"paperId": "dae130ba6f363dc8f83e67cd09896af983f7b7d2", "externalIds": {"MAG": "3034871396", "ACL": "2020.acl-main.273", "DBLP": "conf/acl/YinMSZYZL20", "ArXiv": "2007.08742", "DOI": "10.18653/v1/2020.acl-main.273", "CorpusId": 220045417}, "corpusId": 220045417, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dae130ba6f363dc8f83e67cd09896af983f7b7d2", "title": "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation", "abstract": "Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 83, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2007.08742", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3025-3035"}, "authors": [{"authorId": "79701068", "name": "Yongjing Yin"}, {"authorId": "33427918", "name": "Fandong Meng"}, {"authorId": "34739384", "name": "Jinsong Su"}, {"authorId": "151481178", "name": "Chulun Zhou"}, {"authorId": "21518096", "name": "Zhengyuan Yang"}, {"authorId": "2108485135", "name": "Jie Zhou"}, {"authorId": "33642939", "name": "Jiebo Luo"}]}, {"paperId": "68b9d085a52d89196e3371e8102117ee6f9e9f6e", "externalIds": {"MAG": "3034623056", "DBLP": "journals/corr/abs-2010-07095", "ACL": "2020.acl-main.274", "ArXiv": "2010.07095", "DOI": "10.18653/v1/2020.acl-main.274", "CorpusId": 220047811}, "corpusId": 220047811, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/68b9d085a52d89196e3371e8102117ee6f9e9f6e", "title": "A Relaxed Matching Procedure for Unsupervised BLI", "abstract": "Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest. One of the crucial parts in methods for the BLI task is the matching procedure. Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings. Thus We propose a relaxed matching procedure to find a more precise matching between two languages. We also find that aligning source and target language embedding space bidirectionally will bring significant improvement. We follow the previous iterative framework to conduct experiments. Results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 9, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.274.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3036-3041"}, "authors": [{"authorId": "1410132011", "name": "Xu Zhao"}, {"authorId": "1390878075", "name": "Zihao Wang"}, {"authorId": "33603226", "name": "Yong Zhang"}, {"authorId": "1491949456", "name": "Hao Wu"}]}, {"paperId": "64adba957aadb96dfa202bd6d1fa179c6134c5df", "externalIds": {"ArXiv": "2005.06606", "MAG": "3034789084", "DBLP": "conf/acl/HeHN20", "ACL": "2020.acl-main.275", "DOI": "10.18653/v1/2020.acl-main.275", "CorpusId": 218629985}, "corpusId": 218629985, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/64adba957aadb96dfa202bd6d1fa179c6134c5df", "title": "Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation", "abstract": "This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 34, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.275.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"name": "ArXiv", "volume": "abs/2005.06606"}, "authors": [{"authorId": "2116986629", "name": "Xuanli He"}, {"authorId": "2561045", "name": "Gholamreza Haffari"}, {"authorId": "144739074", "name": "Mohammad Norouzi"}]}, {"paperId": "7f8328228991bca363e73d0885b98528ff3e32c4", "externalIds": {"MAG": "3017024875", "DBLP": "conf/acl/JawanpuriaMM20", "ArXiv": "2004.08243", "ACL": "2020.acl-main.276", "DOI": "10.18653/v1/2020.acl-main.276", "CorpusId": 215814124}, "corpusId": 215814124, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7f8328228991bca363e73d0885b98528ff3e32c4", "title": "Geometry-aware domain adaptation for unsupervised alignment of word embeddings", "abstract": "We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices. This viewpoint arises from the aim to align the second order information of the two language spaces. The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation. Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs. The performance improvement is more significant for distant language pairs.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 54, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.276.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-06", "journal": {"name": "ArXiv", "volume": "abs/2004.08243"}, "authors": [{"authorId": "1720974", "name": "Pratik Jawanpuria"}, {"authorId": "51887737", "name": "Mayank Meghwanshi"}, {"authorId": "37585320", "name": "Bamdev Mishra"}]}, {"paperId": "7a94d81ef57b093e6587b27f5a3363d049dc0133", "externalIds": {"MAG": "3034877014", "DBLP": "conf/acl/RanLLZ20", "ArXiv": "2006.05165", "ACL": "2020.acl-main.277", "DOI": "10.18653/v1/2020.acl-main.277", "CorpusId": 219559126}, "corpusId": 219559126, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7a94d81ef57b093e6587b27f5a3363d049dc0133", "title": "Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation", "abstract": "Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 33, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.277.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-01", "journal": {"pages": "3059-3069"}, "authors": [{"authorId": "9326177", "name": "Qiu Ran"}, {"authorId": "2427350", "name": "Yankai Lin"}, {"authorId": "2209965245", "name": "Peng Li"}, {"authorId": "49640256", "name": "Jie Zhou"}]}, {"paperId": "3ca35c7df229549997491787d93c01de29af206d", "externalIds": {"DBLP": "journals/corr/abs-2005-00963", "MAG": "3035072529", "ArXiv": "2005.00963", "ACL": "2020.acl-main.278", "DOI": "10.18653/v1/2020.acl-main.278", "CorpusId": 218487046}, "corpusId": 218487046, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3ca35c7df229549997491787d93c01de29af206d", "title": "On the Inference Calibration of Neural Machine Translation", "abstract": "Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 59, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.00963", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"name": "ArXiv", "volume": "abs/2005.00963"}, "authors": [{"authorId": "12782489", "name": "Shuo Wang"}, {"authorId": "2909321", "name": "Zhaopeng Tu"}, {"authorId": "34720053", "name": "Shuming Shi"}, {"authorId": "2152798100", "name": "Yang Liu"}]}, {"paperId": "759fa42c473d1bdf05e37e2e8d39394499a476ae", "externalIds": {"DBLP": "conf/acl/JiangGDKSZL20", "ACL": "2020.acl-main.279", "MAG": "3035256610", "DOI": "10.18653/v1/2020.acl-main.279", "CorpusId": 220045827}, "corpusId": 220045827, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/759fa42c473d1bdf05e37e2e8d39394499a476ae", "title": "Camouflaged Chinese Spam Content Detection with Semi-supervised Generative Active Learning", "abstract": "We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task. A \u201cself-diversity\u201d criterion is proposed for measuring the \u201cworthiness\u201d of a candidate for annotation. A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation. The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task. To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.279.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3080-3085"}, "authors": [{"authorId": "1695957219", "name": "Zhuoren Jiang"}, {"authorId": "2116491668", "name": "Zhe Gao"}, {"authorId": "2087104526", "name": "Yu Duan"}, {"authorId": "153818687", "name": "Yangyang Kang"}, {"authorId": "2060934", "name": "Changlong Sun"}, {"authorId": "2115485", "name": "Qiong Zhang"}, {"authorId": "1713802", "name": "Xiaozhong Liu"}]}, {"paperId": "77349a8b2b2466cd01d9ca14bf3ee7680fb46930", "externalIds": {"ACL": "2020.acl-main.280", "MAG": "3034892514", "DBLP": "conf/acl/XuWCPWZ20", "ArXiv": "2004.02557", "DOI": "10.18653/v1/2020.acl-main.280", "CorpusId": 214802587}, "corpusId": 214802587, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/77349a8b2b2466cd01d9ca14bf3ee7680fb46930", "title": "Distinguish Confusing Law Articles for Legal Judgment Prediction", "abstract": "Legal Judgement Prediction (LJP) is the task of automatically predicting a law case\u2019s judgment results given a text describing the case\u2019s facts, which has great prospects in judicial assistance systems and handy services for the public. In practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged. To address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems. In this paper, we present an end-to-end model, LADAN, to solve the task of LJP. To distinguish confusing charges, we propose a novel graph neural network, GDL, to automatically learn subtle differences between confusing law articles, and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions. Experiments conducted on real-world datasets demonstrate the superiority of our LADAN.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 90, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.02557", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.02557"}, "authors": [{"authorId": "1399889216", "name": "Nuo Xu"}, {"authorId": "2152206172", "name": "Pinghui Wang"}, {"authorId": "2118172159", "name": "Long Chen"}, {"authorId": "145656336", "name": "Li Pan"}, {"authorId": "2118776459", "name": "Xiaoyan Wang"}, {"authorId": "153199045", "name": "Junzhou Zhao"}]}, {"paperId": "96a3813b6f59331bf4aecaee475d5c1a349fa1cf", "externalIds": {"ACL": "2020.acl-main.281", "DBLP": "conf/acl/LiuLZCSH20", "MAG": "3034908190", "DOI": "10.18653/v1/2020.acl-main.281", "CorpusId": 220047864}, "corpusId": 220047864, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/96a3813b6f59331bf4aecaee475d5c1a349fa1cf", "title": "Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation", "abstract": "Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real-world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 25, "citationCount": 4, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3096-3104"}, "authors": [{"authorId": "7954660", "name": "Liting Liu"}, {"authorId": "2146651412", "name": "Jie Liu"}, {"authorId": null, "name": "Wenzheng Zhang"}, {"authorId": "83365410", "name": "Ziming Chi"}, {"authorId": "3194752", "name": "Wenxuan Shi"}, {"authorId": "9221107", "name": "Yalou Huang"}]}, {"paperId": "a9508e1a00d2897da37df243173dd5580bba91ac", "externalIds": {"MAG": "3034503829", "DBLP": "conf/acl/CaoCLZLC20", "ACL": "2020.acl-main.282", "DOI": "10.18653/v1/2020.acl-main.282", "CorpusId": 220047363}, "corpusId": 220047363, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a9508e1a00d2897da37df243173dd5580bba91ac", "title": "HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding", "abstract": "The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code. ICD coding aims to assign proper ICD codes to a medical record. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence. In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem. Specifically, we propose a hyperbolic representation method to leverage the code hierarchy. Moreover, we propose a graph convolutional network to utilize the code co-occurrence. Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 71, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.282.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3105-3114"}, "authors": [{"authorId": "49776272", "name": "Pengfei Cao"}, {"authorId": "1763402", "name": "Yubo Chen"}, {"authorId": "77397868", "name": "Kang Liu"}, {"authorId": "11447228", "name": "Jun Zhao"}, {"authorId": "2035396", "name": "Shengping Liu"}, {"authorId": "1509643951", "name": "Weifeng Chong"}]}, {"paperId": "f3db38044d64cd33b61a6af256f132131f012027", "externalIds": {"ACL": "2020.acl-main.283", "MAG": "3035113230", "DBLP": "conf/acl/ChenHXJ20", "DOI": "10.18653/v1/2020.acl-main.283", "CorpusId": 220045739}, "corpusId": 220045739, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f3db38044d64cd33b61a6af256f132131f012027", "title": "Hyperbolic Capsule Networks for Multi-Label Classification", "abstract": "Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling). Such operations limit the performance. For instance, a multi-label document may contain several concepts. In this case, one vector can not sufficiently capture its salient and discriminative content. Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits. First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents. Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks. To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing. Extensive experiments are conducted on four benchmark datasets. Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 16, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.283.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3115-3124"}, "authors": [{"authorId": "3058753", "name": "Boli Chen"}, {"authorId": "2152662361", "name": "Xin Huang"}, {"authorId": "2149552149", "name": "Lin Xiao"}, {"authorId": "144889532", "name": "L. Jing"}]}, {"paperId": "2b391921c88566cf6a841b22a5768df13aacd8f6", "externalIds": {"MAG": "3035218628", "ArXiv": "2005.11055", "DBLP": "journals/corr/abs-2005-11055", "ACL": "2020.acl-main.284", "DOI": "10.18653/v1/2020.acl-main.284", "CorpusId": 218863009}, "corpusId": 218863009, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2b391921c88566cf6a841b22a5768df13aacd8f6", "title": "Improving Segmentation for Technical Support Problems", "abstract": "Technical support problems are often long and complex. They typically contain user descriptions of the problem, the setup, and steps for attempted resolution. Often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces. These elements contain potentially crucial information for problem resolution. However, they cannot be correctly parsed by tools designed for natural language. In this paper, we address the problem of segmentation for technical support questions. We formulate the problem as a sequence labelling task, and study the performance of state of the art approaches. We compare this against an intuitive contextual sentence-level classification baseline, and a state of the art supervised text-segmentation approach. We also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources, which achieves a marked improvement over using embeddings from a single pre-trained language model. Finally, we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 0, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.284.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-22", "journal": {"name": "ArXiv", "volume": "abs/2005.11055"}, "authors": [{"authorId": "70083483", "name": "Kushal Chauhan"}, {"authorId": "30214918", "name": "Abhirut Gupta"}]}, {"paperId": "16df90a87d689519a2da665480b99a2f2243f42a", "externalIds": {"DBLP": "conf/acl/YuLXZWFLWHLLT20", "MAG": "3035721489", "ACL": "2020.acl-main.285", "DOI": "10.18653/v1/2020.acl-main.285", "CorpusId": 220046694}, "corpusId": 220046694, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/16df90a87d689519a2da665480b99a2f2243f42a", "title": "MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs", "abstract": "The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc. However, the publicly available datasets of MOOC are limited in size with few types of data, which hinders advanced models and novel attempts in related topics. Therefore, we present MOOCCube, a large-scale data repository of over 700 MOOC courses, 100k concepts, 8 million student behaviors with an external resource. Moreover, we conduct a prerequisite discovery task as an example application to show the potential of MOOCCube in facilitating relevant research. The data repository is now available at http://moocdata.cn/data/MOOCCube.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 59, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.285.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3135-3142"}, "authors": [{"authorId": "81962214", "name": "Jifan Yu"}, {"authorId": "1500655520", "name": "Gan Luo"}, {"authorId": "1612970962", "name": "Tong Xiao"}, {"authorId": "51211976", "name": "Qingyang Zhong"}, {"authorId": "2108034695", "name": "Yuquan Wang"}, {"authorId": "2114325306", "name": "Wenzheng Feng"}, {"authorId": "2141104718", "name": "Junyi Luo"}, {"authorId": "2109501922", "name": "Chenyu Wang"}, {"authorId": "145779862", "name": "Lei Hou"}, {"authorId": "8549842", "name": "Juan-Zi Li"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "46199760", "name": "Jie Tang"}]}, {"paperId": "0140588666895742e1e6be2c8e710112d0c16e5d", "externalIds": {"DBLP": "conf/acl/ChenDYLH20", "MAG": "3034360800", "ACL": "2020.acl-main.286", "DOI": "10.18653/v1/2020.acl-main.286", "CorpusId": 220048523}, "corpusId": 220048523, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0140588666895742e1e6be2c8e710112d0c16e5d", "title": "Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs", "abstract": "The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system. The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare. The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 24, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.286.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3143-3153"}, "authors": [{"authorId": null, "name": "Jun Chen"}, {"authorId": "117651839", "name": "Xiaoya Dai"}, {"authorId": "1642024996", "name": "Quan Yuan"}, {"authorId": "2110142014", "name": "Chao Lu"}, {"authorId": "30898812", "name": "Hai-ting Huang"}]}, {"paperId": "ac3e2a0ff52228fd7604d48b8f5602bf2a1e13e1", "externalIds": {"ACL": "2020.acl-main.287", "DBLP": "conf/acl/BaffWKS20", "MAG": "3034209844", "DOI": "10.18653/v1/2020.acl-main.287", "CorpusId": 220044846}, "corpusId": 220044846, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ac3e2a0ff52228fd7604d48b8f5602bf2a1e13e1", "title": "Analyzing the Persuasive Effect of Style in News Editorial Argumentation", "abstract": "News editorials argue about political issues in order to challenge or reinforce the stance of readers with different ideologies. Previous research has investigated such persuasive effects for argumentative content. In contrast, this paper studies how important the style of news editorials is to achieve persuasion. To this end, we first compare content- and style-oriented classifiers on editorials from the liberal NYTimes with ideology-specific effect annotations. We find that conservative readers are resistant to NYTimes style, but on liberals, style even has more impact than content. Focusing on liberals, we then cluster the leads, bodies, and endings of editorials, in order to learn about writing style patterns of effective argumentation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 34, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.287.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Sociology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3154-3160"}, "authors": [{"authorId": "51185829", "name": "Roxanne El Baff"}, {"authorId": "2626599", "name": "Henning Wachsmuth"}, {"authorId": "2248209", "name": "Khalid Al Khatib"}, {"authorId": "1405867539", "name": "Benno Stein"}]}, {"paperId": "e420768df546efee307c58886a29289b878623cf", "externalIds": {"DBLP": "conf/acl/DingXY20", "ACL": "2020.acl-main.288", "MAG": "3035563474", "DOI": "10.18653/v1/2020.acl-main.288", "CorpusId": 220046814}, "corpusId": 220046814, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e420768df546efee307c58886a29289b878623cf", "title": "ECPE-2D: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction", "abstract": "In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis. It aims at extracting the potential pairs of emotions and their corresponding causes in a document. To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes. However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step. To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme. A 2D transformer module and two variants, window-constrained and cross-road 2D transformers, are further proposed to model the interactions of different emotion-cause pairs. The 2D representation, interaction, and prediction are integrated into a joint framework. In addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28% to 68.89%.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 79, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.288.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3161-3170"}, "authors": [{"authorId": "3465740", "name": "Zixiang Ding"}, {"authorId": "1491639587", "name": "Rui Xia"}, {"authorId": "143621743", "name": "Jianfei Yu"}]}, {"paperId": "e52e996884b303b9f5d56fe7d7330f5a8094143a", "externalIds": {"ACL": "2020.acl-main.289", "DBLP": "conf/acl/WeiZM20", "MAG": "3035327313", "DOI": "10.18653/v1/2020.acl-main.289", "CorpusId": 220049701}, "corpusId": 220049701, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e52e996884b303b9f5d56fe7d7330f5a8094143a", "title": "Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction", "abstract": "Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document. Previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs. However, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well. In this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction. It models the interrelations between the clauses in a document to learn clause representations with graph attention, and enhances clause pair representations with kernel-based relative position embedding for effective ranking. Experimental results show that our approach significantly outperforms the current two-step systems, especially in the condition of extracting multiple pairs in one document.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 50, "citationCount": 79, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.289.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3171-3181"}, "authors": [{"authorId": "47093412", "name": "Penghui Wei"}, {"authorId": "2109946913", "name": "Jiahao Zhao"}, {"authorId": "2419472", "name": "W. Mao"}]}, {"paperId": "8d90d7ff23313745e5feb14187b5851f1e5aefe7", "externalIds": {"MAG": "3035611181", "DBLP": "journals/corr/abs-2004-13580", "ACL": "2020.acl-main.290", "ArXiv": "2004.13580", "DOI": "10.18653/v1/2020.acl-main.290", "CorpusId": 216562513}, "corpusId": 216562513, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8d90d7ff23313745e5feb14187b5851f1e5aefe7", "title": "Embarrassingly Simple Unsupervised Aspect Extraction", "abstract": "We present a simple but effective method for aspect identification in sentiment analysis. Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages. We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable. Previous work relied on syntactic features and complex neural models. We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed. The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 22, "citationCount": 37, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.290.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-28", "journal": {"pages": "3182-3187"}, "authors": [{"authorId": "3438882", "name": "St\u00e9phan Tulkens"}, {"authorId": "2791585", "name": "Andreas van Cranenburgh"}]}, {"paperId": "49c1811a66fac98c210634e0ac3f200d1d642965", "externalIds": {"MAG": "3035413677", "DBLP": "conf/acl/ZhangYLYXD20", "ACL": "2020.acl-main.291", "DOI": "10.18653/v1/2020.acl-main.291", "CorpusId": 220048371}, "corpusId": 220048371, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/49c1811a66fac98c210634e0ac3f200d1d642965", "title": "Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge", "abstract": "Stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target. Remarkable success has been achieved when sufficient labeled training data is available. However, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets. In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across different targets. Specifically, a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons, which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags. Then, the learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the bidirectional long short-term memory (BiLSTM) stance classifier by adding a novel knowledge-aware memory unit to the BiLSTM cell. Extensive experiments on a large real-world dataset demonstrate the superiority of SEKT against the state-of-the-art baseline methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 22, "citationCount": 64, "influentialCitationCount": 6, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3188-3197"}, "authors": [{"authorId": "2208118550", "name": "Bowen Zhang"}, {"authorId": "2110950699", "name": "Min Yang"}, {"authorId": "48569885", "name": "Xutao Li"}, {"authorId": "144782498", "name": "Yunming Ye"}, {"authorId": "1785762", "name": "Xiaofei Xu"}, {"authorId": "2054548034", "name": "Kuai Dai"}]}, {"paperId": "b39a6412fbfa9e6ee3036f394aae5c3ae2d13bc2", "externalIds": {"ACL": "2020.acl-main.292", "ArXiv": "2005.00791", "MAG": "3035157853", "DBLP": "journals/corr/abs-2005-00791", "DOI": "10.18653/v1/2020.acl-main.292", "CorpusId": 218487075}, "corpusId": 218487075, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b39a6412fbfa9e6ee3036f394aae5c3ae2d13bc2", "title": "KinGDOM: Knowledge-Guided DOMain Adaptation for Sentiment Analysis", "abstract": "Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis. In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts. These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner. Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 73, "citationCount": 50, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.292.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"pages": "3198-3210"}, "authors": [{"authorId": "32528506", "name": "Deepanway Ghosal"}, {"authorId": "8223433", "name": "Devamanyu Hazarika"}, {"authorId": "35122767", "name": "Navonil Majumder"}, {"authorId": "91007923", "name": "Abhinaba Roy"}, {"authorId": "1746416", "name": "Soujanya Poria"}, {"authorId": "2105984203", "name": "Rada Mihalcea"}]}, {"paperId": "6cae7e5a1611c765be0ca6a2dfd27858dc5d1d37", "externalIds": {"MAG": "3034206885", "DBLP": "conf/acl/PhanO20", "ACL": "2020.acl-main.293", "DOI": "10.18653/v1/2020.acl-main.293", "CorpusId": 220045430}, "corpusId": 220045430, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6cae7e5a1611c765be0ca6a2dfd27858dc5d1d37", "title": "Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis", "abstract": "The aspect-based sentiment analysis (ABSA) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification. Rather than considering the tasks separately, we build an end-to-end ABSA solution. Previous works in ABSA tasks did not fully leverage the importance of syntactical information. Hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms. On the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words. This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning. We combine part-of-speech embeddings, dependency-based embeddings and contextualized embeddings (e.g. BERT, RoBERTa) to enhance the performance of the aspect extractor. We also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words, having weak syntactic connection with the aspect terms. This increases the accuracy of the aspect sentiment classifier. Our solutions outperform the state-of-the-art models on SemEval-2014 dataset in both two subtasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 22, "citationCount": 105, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.293.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3211-3220"}, "authors": [{"authorId": "1754253385", "name": "M. Phan"}, {"authorId": "1719314", "name": "P. Ogunbona"}]}, {"paperId": "598fdb78e86415cea390195eeac605db92ac01c6", "externalIds": {"MAG": "3035125262", "DBLP": "conf/acl/ZhangGS20", "ACL": "2020.acl-main.294", "ArXiv": "2005.07522", "DOI": "10.18653/v1/2020.acl-main.294", "CorpusId": 218665411}, "corpusId": 218665411, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/598fdb78e86415cea390195eeac605db92ac01c6", "title": "Parallel Data Augmentation for Formality Style Transfer", "abstract": "The main barrier to progress in the task of Formality Style Transfer is the inadequacy of training data. In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems. Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 60, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.294.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-14", "journal": {"name": "ArXiv", "volume": "abs/2005.07522"}, "authors": [{"authorId": "46867473", "name": "Yi Zhang"}, {"authorId": "50251691", "name": "Tao Ge"}, {"authorId": "11774802", "name": "Xu Sun"}]}, {"paperId": "8e85df955707cb7959299d83b6b8c8a28b2b53dd", "externalIds": {"MAG": "3035529900", "ACL": "2020.acl-main.295", "DBLP": "journals/corr/abs-2004-12362", "ArXiv": "2004.12362", "DOI": "10.18653/v1/2020.acl-main.295", "CorpusId": 216553425}, "corpusId": 216553425, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8e85df955707cb7959299d83b6b8c8a28b2b53dd", "title": "Relational Graph Attention Network for Aspect-based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews. Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words. However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections. In this paper, we address this problem by means of effective encoding of syntax information. Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree. Then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction. Extensive experiments are conducted on the SemEval 2014 and Twitter datasets, and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach, and the performance of the graph attention network (GAT) is significantly improved as a consequence.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 323, "influentialCitationCount": 70, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.295.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-04-26", "journal": {"pages": "3229-3238"}, "authors": [{"authorId": "37833805", "name": "Kai Wang"}, {"authorId": "74132442", "name": "Weizhou Shen"}, {"authorId": "49307508", "name": "Yunyi Yang"}, {"authorId": "38472218", "name": "Xiaojun Quan"}, {"authorId": "145258523", "name": "Rui Wang"}]}, {"paperId": "7e3faf1c17a61de18f99daf978e9d1961c621d27", "externalIds": {"ACL": "2020.acl-main.296", "DBLP": "conf/acl/ZhaoHZLX20", "MAG": "3034884160", "DOI": "10.18653/v1/2020.acl-main.296", "CorpusId": 220047441}, "corpusId": 220047441, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7e3faf1c17a61de18f99daf978e9d1961c621d27", "title": "SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction", "abstract": "Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis (ABSA). The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems. However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms. Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs. To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction (PAOTE). Furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works. We propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries. Meanwhile, the pair-wise relations are jointly identified using the span representations. Extensive experiments show that our model consistently outperforms state-of-the-art methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 101, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.296.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3239-3248"}, "authors": [{"authorId": "2143592208", "name": "He Zhao"}, {"authorId": "7535611", "name": "Longtao Huang"}, {"authorId": "2119062118", "name": "Rong Zhang"}, {"authorId": "2117522225", "name": "Quan Lu"}, {"authorId": "1645209767", "name": "Hui Xue"}]}, {"paperId": "a08beb49d2107851dc9bf8ef267e9905f23c49ec", "externalIds": {"DBLP": "conf/acl/ZhangZWLZ20", "ACL": "2020.acl-main.297", "MAG": "3034438872", "DOI": "10.18653/v1/2020.acl-main.297", "CorpusId": 220047306}, "corpusId": 220047306, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a08beb49d2107851dc9bf8ef267e9905f23c49ec", "title": "Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks", "abstract": "Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer \u201cwho expressed what kind of sentiment towards what?\u201d. Due to the scarcity of labeled data, ORL remains challenging for data-driven methods. In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations. We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels. In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously. We verify our methods on the benchmark MPQA corpus. The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline. In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT). Our best model achieves 4.34 higher F1 score than the current state-ofthe-art.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 25, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.297.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3249-3258"}, "authors": [{"authorId": "1723442179", "name": "Bo Zhang"}, {"authorId": "2145913150", "name": "Yue Zhang"}, {"authorId": "2151036808", "name": "Rui Wang"}, {"authorId": "51003520", "name": "Zhenghua Li"}, {"authorId": "1390813134", "name": "Min Zhang"}]}, {"paperId": "e4a3808d0facde2184146529f9ac97f07ae1ce85", "externalIds": {"DBLP": "conf/acl/MorioOMKY20", "ACL": "2020.acl-main.298", "MAG": "3035152150", "DOI": "10.18653/v1/2020.acl-main.298", "CorpusId": 220047946}, "corpusId": 220047946, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e4a3808d0facde2184146529f9ac97f07ae1ce85", "title": "Towards Better Non-Tree Argument Mining: Proposition-Level Biaffine Parsing with Task-Specific Parameterization", "abstract": "State-of-the-art argument mining studies have advanced the techniques for predicting argument structures. However, the technology for capturing non-tree-structured arguments is still in its infancy. In this paper, we focus on non-tree argument mining with a neural model. We jointly predict proposition types and edges between propositions. Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges. Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 25, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3259-3266"}, "authors": [{"authorId": "29347584", "name": "Gaku Morio"}, {"authorId": "36904068", "name": "Hiroaki Ozaki"}, {"authorId": "1379579811", "name": "Terufumi Morishita"}, {"authorId": "2740047", "name": "Yuta Koreeda"}, {"authorId": "2272115", "name": "Kohsuke Yanai"}]}, {"paperId": "bc102a232413cffef62ae4f9bfbeb444fb02469d", "externalIds": {"DBLP": "journals/corr/abs-2004-14704", "ArXiv": "2004.14704", "ACL": "2020.acl-main.299", "MAG": "3034212072", "DOI": "10.18653/v1/2020.acl-main.299", "CorpusId": 216867188}, "corpusId": 216867188, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bc102a232413cffef62ae4f9bfbeb444fb02469d", "title": "A Span-based Linearization for Constituent Trees", "abstract": "We propose a novel linearization of a constituent tree, together with a new locally normalized model. For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them. Compared with global models, our model is fast and parallelizable. Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.299.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"name": "ArXiv", "volume": "abs/2004.14704"}, "authors": [{"authorId": "2112600055", "name": "Yang Wei"}, {"authorId": "3174675", "name": "Yuanbin Wu"}, {"authorId": "49284832", "name": "Man Lan"}]}, {"paperId": "0463b497e2600f55298af3b1eec2159f9cd56a37", "externalIds": {"MAG": "3016560063", "ACL": "2020.acl-main.300", "DBLP": "conf/acl/LiCCJT20", "DOI": "10.18653/V1/2020.ACL-MAIN.300", "CorpusId": 218784865}, "corpusId": 218784865, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0463b497e2600f55298af3b1eec2159f9cd56a37", "title": "An Empirical Comparison of Unsupervised Constituency Parsing Methods", "abstract": "Unsupervised constituency parsing aims to learn a constituency parser from a training corpus without parse tree annotations. While many methods have been proposed to tackle the problem, including statistical and neural methods, their experimental results are often not directly comparable due to discrepancies in datasets, data preprocessing, lexicalization, and evaluation metrics. In this paper, we first examine experimental settings used in previous work and propose to standardize the settings for better comparability between methods. We then empirically compare several existing methods, including decade-old and newly proposed ones, under the standardized settings on English and Japanese, two languages with different branching tendencies. We find that recent models do not show a clear advantage over decade-old models in our experiments. We hope our work can provide new insights into existing methods and facilitate future empirical evaluation of unsupervised constituency parsing.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.300.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3278-3283"}, "authors": [{"authorId": "2152754973", "name": "Jun Li"}, {"authorId": "150346752", "name": "Yifan Cao"}, {"authorId": "2115669960", "name": "Jiong Cai"}, {"authorId": "50262192", "name": "Yong Jiang"}, {"authorId": "40341553", "name": "Kewei Tu"}]}, {"paperId": "701d5c82360d4caa6b4a0741de19d2907138dc79", "externalIds": {"DBLP": "conf/acl/NguyenNJL20", "ACL": "2020.acl-main.301", "MAG": "3035400757", "ArXiv": "2006.13557", "DOI": "10.18653/v1/2020.acl-main.301", "CorpusId": 220041555}, "corpusId": 220041555, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/701d5c82360d4caa6b4a0741de19d2907138dc79", "title": "Efficient Constituency Parsing by Pointing", "abstract": "We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span. Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference. The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity. Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.301.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-24", "journal": {"name": "ArXiv", "volume": "abs/2006.13557"}, "authors": [{"authorId": "29403359", "name": "Thanh-Tung Nguyen"}, {"authorId": "1399659909", "name": "Xuan-Phi Nguyen"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}, {"authorId": "39952499", "name": "Xiaoli Li"}]}, {"paperId": "ce18780963b067a1295fc847e7ab33f2fcbfaca1", "externalIds": {"MAG": "3035058125", "ACL": "2020.acl-main.302", "DBLP": "conf/acl/ZhangLZ20", "ArXiv": "2005.00975", "DOI": "10.18653/v1/2020.acl-main.302", "CorpusId": 218487136}, "corpusId": 218487136, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ce18780963b067a1295fc847e7ab33f2fcbfaca1", "title": "Efficient Second-Order TreeCRF for Neural Dependency Parsing", "abstract": "In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation. As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss. This paper for the first time presents a second-order TreeCRF extension to the biaffine parser. For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF. To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation. Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data. We release our code at https://github.com/yzhangcs/crfpar.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 49, "citationCount": 83, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.302.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"name": "ArXiv", "volume": "abs/2005.00975"}, "authors": [{"authorId": "49890808", "name": "Yu Zhang"}, {"authorId": "51003520", "name": "Zhenghua Li"}, {"authorId": "1390813134", "name": "Min Zhang"}]}, {"paperId": "25316018268ed68c7096fe8b2d8fbd66f998a201", "externalIds": {"MAG": "3035295386", "DBLP": "conf/acl/LeporiLM20", "ArXiv": "2005.00019", "ACL": "2020.acl-main.303", "DOI": "10.18653/v1/2020.acl-main.303", "CorpusId": 218470219}, "corpusId": 218470219, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/25316018268ed68c7096fe8b2d8fbd66f998a201", "title": "Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs", "abstract": "Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. Such tree-based networks can be provided with a constituency parse, a dependency parse, or both. We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task. We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement. Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 11, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.303.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"pages": "3306-3316"}, "authors": [{"authorId": "91556719", "name": "Michael A. Lepori"}, {"authorId": "2467508", "name": "Tal Linzen"}, {"authorId": "145534175", "name": "R. Thomas McCoy"}]}, {"paperId": "82eb7ed876778df5114e92ba8156444ec83461f5", "externalIds": {"MAG": "3015374498", "ACL": "2020.acl-main.304", "DBLP": "conf/acl/WangJBWHT20", "ArXiv": "2004.03846", "DOI": "10.18653/v1/2020.acl-main.304", "CorpusId": 215416276}, "corpusId": 215416276, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/82eb7ed876778df5114e92ba8156444ec83461f5", "title": "Structure-Level Knowledge Distillation For Multilingual Sequence Labeling", "abstract": "Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages. Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages. However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations. In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student). We propose two novel KD methods based on structure-level information: (1) approximately minimizes the distance between the student\u2019s and the teachers\u2019 structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions. Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 56, "citationCount": 27, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.304.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-08", "journal": {"pages": "3317-3330"}, "authors": [{"authorId": "47120498", "name": "Xinyu Wang"}, {"authorId": "50262192", "name": "Yong Jiang"}, {"authorId": "144756231", "name": "Nguyen Bach"}, {"authorId": "1491094850", "name": "Tao Wang"}, {"authorId": "143857288", "name": "Fei Huang"}, {"authorId": "40341553", "name": "Kewei Tu"}]}, {"paperId": "d7de8ef789cb06b900a73c077c4a0aed13746b7b", "externalIds": {"ACL": "2020.acl-main.305", "MAG": "3035242086", "DBLP": "conf/acl/ZengLWMW20", "DOI": "10.18653/v1/2020.acl-main.305", "CorpusId": 219720103}, "corpusId": 219720103, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d7de8ef789cb06b900a73c077c4a0aed13746b7b", "title": "Dynamic Online Conversation Recommendation", "abstract": "Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner. Here we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests. While most prior work assumes static user interests, our model is able to capture the temporal aspects of user interests, and further handle future conversations that are unseen during training time. Concretely, we propose a neural architecture to exploit changes of user interactions and interests over time, to predict which discussions they are likely to enter. We conduct experiments on large-scale collections of Reddit conversations, and results on three subreddits show that our model significantly outperforms state-of-the-art models that make a static assumption of user interests. We further evaluate on handling \u201ccold start\u201d, and observe consistently better performance by our model when considering various degrees of sparsity of user\u2019s chatting history and conversation contexts. Lastly, analyses on our model outputs indicate user interest change, explaining the advantage and efficacy of our approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 11, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.305.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3331-3341"}, "authors": [{"authorId": "46180553", "name": "Xingshan Zeng"}, {"authorId": "46276037", "name": "Jun Yu Li"}, {"authorId": "2153515905", "name": "Lu Wang"}, {"authorId": "2054290226", "name": "Zhiming Mao"}, {"authorId": "1784988", "name": "Kam-Fai Wong"}]}, {"paperId": "1b4a45b3d2f7573451048d695fbf81cfcee9253e", "externalIds": {"ACL": "2020.acl-main.306", "DBLP": "conf/acl/YuJYX20", "MAG": "3035448883", "DOI": "10.18653/v1/2020.acl-main.306", "CorpusId": 220045906}, "corpusId": 220045906, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1b4a45b3d2f7573451048d695fbf81cfcee9253e", "title": "Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer", "abstract": "In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts. Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context. To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations. To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions. Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 97, "influentialCitationCount": 32, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.306.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3342-3352"}, "authors": [{"authorId": "143621743", "name": "Jianfei Yu"}, {"authorId": "144924128", "name": "Jing Jiang"}, {"authorId": null, "name": "Li Yang"}, {"authorId": "1491639587", "name": "Rui Xia"}]}, {"paperId": "465b79c0e2ce7d25bdf456ac5ea393fef33f1862", "externalIds": {"ACL": "2020.acl-main.307", "DBLP": "conf/acl/DuT20", "MAG": "3035389441", "DOI": "10.18653/v1/2020.acl-main.307", "CorpusId": 220046498}, "corpusId": 220046498, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/465b79c0e2ce7d25bdf456ac5ea393fef33f1862", "title": "Stock Embeddings Acquired from News Articles and Price History, and an Application to Portfolio Optimization", "abstract": "Previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements. The textual and price information were both encoded in the neural network, and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction. In contrast, this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding. The stock embedding is acquired with a deep learning framework using both news articles and price history. Because the embedding takes the operational form of a vector, it is applicable to other financial problems besides price prediction. As one example application, we show the results of portfolio optimization using Reuters & Bloomberg headlines, producing a capital gain 2.8 times larger than that obtained with a baseline method using only stock price data. This suggests that the proposed stock embedding can leverage textual financial semantics to solve financial prediction problems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 37, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.307.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3353-3363"}, "authors": [{"authorId": "145035467", "name": "Xin Du"}, {"authorId": "6830931", "name": "Kumiko Tanaka-Ishii"}]}, {"paperId": "e7a00d7bdc5f9e2d4aaa17a3d44ee1239f33fc30", "externalIds": {"MAG": "3021166088", "DBLP": "conf/acl/BalyKAKDAGN20", "ACL": "2020.acl-main.308", "ArXiv": "2005.04518", "DOI": "10.18653/v1/2020.acl-main.308", "CorpusId": 218581979}, "corpusId": 218581979, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e7a00d7bdc5f9e2d4aaa17a3d44ee1239f33fc30", "title": "What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context", "abstract": "Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically. Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content. This makes it possible to detect likely \u201cfake news\u201d the moment they are published, by simply checking the reliability of their source. From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium\u2019s audience on social media). We further study (iii) what was written about the target medium (in Wikipedia). The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 46, "citationCount": 48, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.308.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-09", "journal": {"name": "ArXiv", "volume": "abs/2005.04518"}, "authors": [{"authorId": "3490018", "name": "R. Baly"}, {"authorId": "48764610", "name": "Georgi Karadzhov"}, {"authorId": "40660541", "name": "Jisun An"}, {"authorId": "2592694", "name": "Haewoon Kwak"}, {"authorId": "1379925776", "name": "Yoan Dinkov"}, {"authorId": "2141768778", "name": "Ahmed Ali"}, {"authorId": "145898106", "name": "James R. Glass"}, {"authorId": "1683562", "name": "Preslav Nakov"}]}, {"paperId": "320df925e7f83412b6e6b947bc9c9227aa9be46e", "externalIds": {"DBLP": "conf/acl/NojiT20", "ACL": "2020.acl-main.309", "MAG": "3035597164", "ArXiv": "2004.02451", "DOI": "10.18653/v1/2020.acl-main.309", "CorpusId": 214802433}, "corpusId": 214802433, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/320df925e7f83412b6e6b947bc9c9227aa9be46e", "title": "An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models", "abstract": "We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks. Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model\u2019s robustness on them in English, with a negligible loss of perplexity. The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word. We then provide a detailed analysis of the trained models. One of our findings is the difficulty of object-relative clauses for RNNs. We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause. Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses. Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.309.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-06", "journal": {"pages": "3375-3385"}, "authors": [{"authorId": "3253887", "name": "Hiroshi Noji"}, {"authorId": "36514372", "name": "Hiroya Takamura"}]}, {"paperId": "57968d578b0cc09d8875fa476ea87e91da463ec2", "externalIds": {"ACL": "2020.acl-main.310", "DBLP": "conf/acl/YinLMC20", "ArXiv": "2005.05683", "MAG": "3025544213", "DOI": "10.18653/v1/2020.acl-main.310", "CorpusId": 218596217}, "corpusId": 218596217, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/57968d578b0cc09d8875fa476ea87e91da463ec2", "title": "On the Robustness of Language Encoders against Grammatical Errors", "abstract": "We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors. Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data. We use this approach to facilitate debugging models on downstream applications. Results confirm that the performance of all tested models is affected but the degree of impact varies. To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors. We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions. We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context. Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 24, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.310.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-12", "journal": {"name": "ArXiv", "volume": "abs/2005.05683"}, "authors": [{"authorId": "2065089223", "name": "Fan Yin"}, {"authorId": "1500525090", "name": "Quanyu Long"}, {"authorId": "2056088060", "name": "Tao Meng"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}]}, {"paperId": "5b6d03ed66473599ee31872b3cd5ad2ce282371f", "externalIds": {"MAG": "3034256339", "ACL": "2020.acl-main.311", "DBLP": "conf/acl/JoM20", "DOI": "10.18653/v1/2020.acl-main.311", "CorpusId": 220047270}, "corpusId": 220047270, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5b6d03ed66473599ee31872b3cd5ad2ce282371f", "title": "Roles and Utilization of Attention Heads in Transformer-based Neural Language Models", "abstract": "Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model. For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA). Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 21, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3404-3417"}, "authors": [{"authorId": "3012797", "name": "Jae-young Jo"}, {"authorId": "1754166", "name": "Sung-Hyon Myaeng"}]}, {"paperId": "94e586cd3342940422c0bd01ad7f252db9327394", "externalIds": {"MAG": "3034834827", "ACL": "2020.acl-main.312", "DBLP": "conf/acl/SunL20", "DOI": "10.18653/v1/2020.acl-main.312", "CorpusId": 220047423}, "corpusId": 220047423, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/94e586cd3342940422c0bd01ad7f252db9327394", "title": "Understanding Attention for Text Classification", "abstract": "Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the token\u2019s significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities can contribute towards model performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 25, "citationCount": 50, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.312.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3418-3428"}, "authors": [{"authorId": "1773435", "name": "Xiaobing Sun"}, {"authorId": "1750913147", "name": "Wei Lu"}]}, {"paperId": "f7ebec954266044ad6b366da872554ac61fe1a32", "externalIds": {"ArXiv": "1907.06080", "DBLP": "conf/acl/NguyenNP20", "MAG": "3035531911", "ACL": "2020.acl-main.313", "DOI": "10.18653/v1/2020.acl-main.313", "CorpusId": 214802539}, "corpusId": 214802539, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f7ebec954266044ad6b366da872554ac61fe1a32", "title": "A Relational Memory-based Embedding Model for Triple Classification and Search Personalization", "abstract": "Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems. To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples. R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism. Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector. Consequently, R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple. Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task, and on WN11 and FB13 for the triple classification task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 32, "citationCount": 18, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.313.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-07-13", "journal": {"pages": "3429-3435"}, "authors": [{"authorId": "38798269", "name": "D. Q. Nguyen"}, {"authorId": "3314511", "name": "T. Nguyen"}, {"authorId": "1749657", "name": "Dinh Q. Phung"}]}, {"paperId": "1ad8d99a3501dd34e0b14d49a77dcd534ef08d97", "externalIds": {"MAG": "3041135054", "DBLP": "journals/corr/abs-2007-06162", "ACL": "2020.acl-main.314", "ArXiv": "2007.06162", "DOI": "10.18653/v1/2020.acl-main.314", "CorpusId": 220048587}, "corpusId": 220048587, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1ad8d99a3501dd34e0b14d49a77dcd534ef08d97", "title": "Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods", "abstract": "It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data. In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem. In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 13, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.314.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"name": "ArXiv", "volume": "abs/2007.06162"}, "authors": [{"authorId": "1381583629", "name": "Ning Miao"}, {"authorId": "1610782647", "name": "Yuxuan Song"}, {"authorId": "2111824520", "name": "Hao Zhou"}, {"authorId": "143900005", "name": "Lei Li"}]}, {"paperId": "ff64b2ce3ae4a72726c198fd4d6a58e28b97d06e", "externalIds": {"ACL": "2020.acl-main.315", "MAG": "3035051781", "ArXiv": "1911.02821", "DBLP": "journals/corr/abs-1911-02821", "DOI": "10.18653/v1/2020.acl-main.315", "CorpusId": 207847868}, "corpusId": 207847868, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ff64b2ce3ae4a72726c198fd4d6a58e28b97d06e", "title": "Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention", "abstract": "Most Chinese pre-trained models take character as the basic unit and learn representation according to character\u2019s external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese. Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models. Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion. As a result, word and character information are explicitly integrated at the fine-tuning procedure. Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT, ERNIE and BERT-wwm.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 33, "citationCount": 22, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.315.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-07", "journal": {"name": "ArXiv", "volume": "abs/1911.02821"}, "authors": [{"authorId": "32732179", "name": "Yanzeng Li"}, {"authorId": "48613402", "name": "Yu Bowen"}, {"authorId": "145470644", "name": "Mengge Xue"}, {"authorId": "2079682", "name": "Tingwen Liu"}]}, {"paperId": "e5e024651b7bdf24f700f62bf98c8fe0f13583ae", "externalIds": {"MAG": "3017075622", "ArXiv": "2004.09189", "DBLP": "journals/corr/abs-2004-09189", "ACL": "2020.acl-main.316", "DOI": "10.18653/v1/2020.acl-main.316", "CorpusId": 215827464}, "corpusId": 215827464, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e5e024651b7bdf24f700f62bf98c8fe0f13583ae", "title": "On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond", "abstract": "Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling. By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold. We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them. To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching. We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy. Experiments on benchmark datasets (i.e., PTB, Yelp, and Yahoo) show consistently improved results in terms of probability estimation and richness of the latent space. We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 2, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.316.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.09189"}, "authors": [{"authorId": "4438547", "name": "Chen Wu"}, {"authorId": "2108230528", "name": "P. Wang"}, {"authorId": "152876475", "name": "W. Wang"}]}, {"paperId": "0a9c0e729dd95f5559e05f8bb4b7408f9409388e", "externalIds": {"DBLP": "conf/acl/YeGL20", "MAG": "3035164976", "ACL": "2020.acl-main.317", "ArXiv": "2005.14424", "DOI": "10.18653/v1/2020.acl-main.317", "CorpusId": 219124328}, "corpusId": 219124328, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0a9c0e729dd95f5559e05f8bb4b7408f9409388e", "title": "SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions", "abstract": "State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution. For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution. In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness. Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as BERT) and any types of models (world-level or subword-level). Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks. To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 63, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.317.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-29", "journal": {"pages": "3465-3475"}, "authors": [{"authorId": "144785541", "name": "Mao Ye"}, {"authorId": "29777869", "name": "Chengyue Gong"}, {"authorId": "47362268", "name": "Qiang Liu"}]}, {"paperId": "c7ee8572a1bdce2978b6ca3f6e28c96ead103de8", "externalIds": {"ACL": "2020.acl-main.318", "MAG": "3034913382", "DBLP": "conf/acl/RenLZM20", "DOI": "10.18653/v1/2020.acl-main.318", "CorpusId": 220048677}, "corpusId": 220048677, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c7ee8572a1bdce2978b6ca3f6e28c96ead103de8", "title": "A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction", "abstract": "Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages. Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages. However, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words. To deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way. We first build a graph for each language with its vertices representing different words. Then we extract word cliques from the graphs and map the cliques of two languages. Based on that, we induce the initial word translation solution with the central words of the aligned cliques. This coarse-to-fine approach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings. Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-25", "journal": {"pages": "3476-3485"}, "authors": [{"authorId": "50052368", "name": "Shuo Ren"}, {"authorId": "1803054", "name": "Shujie Liu"}, {"authorId": "143849609", "name": "M. Zhou"}, {"authorId": "47325834", "name": "Shuai Ma"}]}, {"paperId": "f257eeed9f8c4a126af44db25df3d8f831ab7832", "externalIds": {"MAG": "3034397670", "DBLP": "conf/acl/ZouHXDC20", "ACL": "2020.acl-main.319", "DOI": "10.18653/v1/2020.acl-main.319", "CorpusId": 263880262}, "corpusId": 263880262, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f257eeed9f8c4a126af44db25df3d8f831ab7832", "title": "A Reinforced Generation of Adversarial Examples for Neural Machine Translation", "abstract": "Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems\u2014fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance. Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning. Our paradigm could expose pitfalls for a given performance metric, e.g., BLEU, and could target any given neural machine translation architecture. We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search, and Transformer. The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples. We also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 34, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/1911.03677", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-09", "journal": {"pages": "3486-3497"}, "authors": [{"authorId": "9276071", "name": "Wei Zou"}, {"authorId": "2046010", "name": "Shujian Huang"}, {"authorId": "2257348031", "name": "Jun Xie"}, {"authorId": "3035069", "name": "Xinyu Dai"}, {"authorId": "1838162", "name": "Jiajun Chen"}]}, {"paperId": "c3f26879abb03e56d92d6066082c59183be44f59", "externalIds": {"DBLP": "conf/acl/RenWLZM20", "ACL": "2020.acl-main.320", "MAG": "3035376412", "DOI": "10.18653/v1/2020.acl-main.320", "CorpusId": 220045818}, "corpusId": 220045818, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c3f26879abb03e56d92d6066082c59183be44f59", "title": "A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation", "abstract": "The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models. We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-25", "journal": {"pages": "3498-3504"}, "authors": [{"authorId": "50052368", "name": "Shuo Ren"}, {"authorId": "1887625", "name": "Yuehua Wu"}, {"authorId": "1803054", "name": "Shujie Liu"}, {"authorId": "143849609", "name": "M. Zhou"}, {"authorId": "47325834", "name": "Shuai Ma"}]}, {"paperId": "46b66719849498742908d21b996b09d7eebbee5c", "externalIds": {"MAG": "3034351728", "DBLP": "conf/acl/MaZZ20", "ACL": "2020.acl-main.321", "DOI": "10.18653/v1/2020.acl-main.321", "CorpusId": 220046461}, "corpusId": 220046461, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/46b66719849498742908d21b996b09d7eebbee5c", "title": "A Simple and Effective Unified Encoder for Document-Level Machine Translation", "abstract": "Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 55, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.321.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-25", "journal": {"pages": "3505-3511"}, "authors": [{"authorId": "8093340", "name": "Shuming Ma"}, {"authorId": "40232931", "name": "Dongdong Zhang"}, {"authorId": "143849609", "name": "M. Zhou"}]}, {"paperId": "1a97f45c2b67053b9e763ba19e86edec2ec33216", "externalIds": {"MAG": "3023598618", "DBLP": "conf/acl/LiLWJXZLL20", "ACL": "2020.acl-main.322", "ArXiv": "2005.03393", "DOI": "10.18653/v1/2020.acl-main.322", "CorpusId": 218538467}, "corpusId": 218538467, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1a97f45c2b67053b9e763ba19e86edec2ec33216", "title": "Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation", "abstract": "In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in document-level neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 25, "citationCount": 61, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.322.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-07", "journal": {"pages": "3512-3518"}, "authors": [{"authorId": "49730090", "name": "Bei Li"}, {"authorId": "2152805250", "name": "Hui Liu"}, {"authorId": "152764184", "name": "Ziyang Wang"}, {"authorId": "145626389", "name": "Yufan Jiang"}, {"authorId": "1739326269", "name": "Tong Xiao"}, {"authorId": "1728004", "name": "Jingbo Zhu"}, {"authorId": "2111149004", "name": "Tongran Liu"}, {"authorId": "2348067", "name": "Changliang Li"}]}, {"paperId": "69d7e2244541d0cc4b4a1e250880b34716f6b346", "externalIds": {"MAG": "3023104759", "ACL": "2020.acl-main.323", "DBLP": "conf/acl/XuGXL20", "ArXiv": "2005.02008", "DOI": "10.18653/v1/2020.acl-main.323", "CorpusId": 218502425}, "corpusId": 218502425, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/69d7e2244541d0cc4b4a1e250880b34716f6b346", "title": "Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change", "abstract": "The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate, comparatively few papers concentrate on the effect of batch size. In this paper, we analyze how increasing batch size affects gradient direction, and propose to evaluate the stability of gradients with their angle change. Based on our observations, the angle change of gradient direction first tends to stabilize (i.e. gradually decrease) while accumulating mini-batches, and then starts to fluctuate. We propose to automatically and dynamically determine batch sizes by accumulating gradients of mini-batches and performing an optimization step at just the time when the direction of gradients starts to fluctuate. To improve the efficiency of our approach for large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size. Our approach dynamically determines proper and efficient batch sizes during training. In our experiments on the WMT 14 English to German and English to French tasks, our approach improves the Transformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.323.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"pages": "3519-3524"}, "authors": [{"authorId": "49507285", "name": "Hongfei Xu"}, {"authorId": "7519068", "name": "Josef van Genabith"}, {"authorId": "2694222", "name": "Deyi Xiong"}, {"authorId": "14147919", "name": "Qiuhui Liu"}]}, {"paperId": "1b856b7dd486d0db7565031720db4e051420ec3b", "externalIds": {"MAG": "3019378119", "ArXiv": "2004.10171", "ACL": "2020.acl-main.324", "DBLP": "journals/corr/abs-2004-10171", "DOI": "10.18653/v1/2020.acl-main.324", "CorpusId": 216036257}, "corpusId": 216036257, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1b856b7dd486d0db7565031720db4e051420ec3b", "title": "Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation", "abstract": "Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 37, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.324.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.10171"}, "authors": [{"authorId": "122309052", "name": "Haipeng Sun"}, {"authorId": "108085542", "name": "Rui Wang"}, {"authorId": "2849740", "name": "Kehai Chen"}, {"authorId": "1802277", "name": "M. Utiyama"}, {"authorId": "1698363", "name": "E. Sumita"}, {"authorId": "145382463", "name": "T. Zhao"}]}, {"paperId": "3835af1e41e0883edce96beb70e80c115bb513ae", "externalIds": {"MAG": "3017817427", "DBLP": "journals/corr/abs-2004-12681", "ACL": "2020.acl-main.325", "ArXiv": "2004.12681", "DOI": "10.18653/v1/2020.acl-main.325", "CorpusId": 216553002}, "corpusId": 216553002, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3835af1e41e0883edce96beb70e80c115bb513ae", "title": "Lexically Constrained Neural Machine Translation with Levenshtein Transformer", "abstract": "This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation. Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads. Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model (Gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed. Our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries. Experiments on English-German WMT datasets show that our approach improves an unconstrained baseline and previous approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 25, "citationCount": 72, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.325.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "3536-3543"}, "authors": [{"authorId": "32406168", "name": "Raymond Hendy Susanto"}, {"authorId": "3422793", "name": "Shamil Chollampatt"}, {"authorId": "40268710", "name": "Liling Tan"}]}, {"paperId": "0b15c6acfc9f7f92c52aa6a185a829f88975c743", "externalIds": {"MAG": "3022463379", "ArXiv": "2005.03642", "ACL": "2020.acl-main.326", "DBLP": "journals/corr/abs-2005-03642", "DOI": "10.18653/v1/2020.acl-main.326", "CorpusId": 218538004}, "corpusId": 218538004, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0b15c6acfc9f7f92c52aa6a185a829f88975c743", "title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation", "abstract": "The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 113, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.326.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-07", "journal": {"name": "ArXiv", "volume": "abs/2005.03642"}, "authors": [{"authorId": "2144522700", "name": "Chaojun Wang"}, {"authorId": "2082372", "name": "Rico Sennrich"}]}, {"paperId": "6a47df76943f849cd47be65ee6c56a62ca38c666", "externalIds": {"DBLP": "conf/acl/TakahashiSN20", "MAG": "3034489696", "ACL": "2020.acl-main.327", "DOI": "10.18653/v1/2020.acl-main.327", "CorpusId": 220046544}, "corpusId": 220046544, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6a47df76943f849cd47be65ee6c56a62ca38c666", "title": "Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model", "abstract": "We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method using Cross-lingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 18, "citationCount": 13, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3553-3558"}, "authors": [{"authorId": "2116095106", "name": "Kosuke Takahashi"}, {"authorId": "1790811", "name": "Katsuhito Sudoh"}, {"authorId": "145223960", "name": "Satoshi Nakamura"}]}, {"paperId": "9cf4bd0443d80aade14ef5ef88c9ca5374da8c17", "externalIds": {"ACL": "2020.acl-main.328", "MAG": "3034463275", "DBLP": "conf/acl/ShaoN20", "DOI": "10.18653/v1/2020.acl-main.328", "CorpusId": 218611161}, "corpusId": 218611161, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9cf4bd0443d80aade14ef5ef88c9ca5374da8c17", "title": "ChartDialogs: Plotting from Natural Language Instructions", "abstract": "This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61% plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 7, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3559-3574"}, "authors": [{"authorId": "30046004", "name": "Yutong Shao"}, {"authorId": "3115592", "name": "Ndapandula Nakashole"}]}, {"paperId": "00cd2650a89734105fa0c0aba3bf07935b318290", "externalIds": {"MAG": "3019003205", "DBLP": "conf/acl/KhanujaDSSC20", "ACL": "2020.acl-main.329", "ArXiv": "2004.12376", "DOI": "10.18653/v1/2020.acl-main.329", "CorpusId": 216553264}, "corpusId": 216553264, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/00cd2650a89734105fa0c0aba3bf07935b318290", "title": "GLUECoS: An Evaluation Benchmark for Code-Switched NLP", "abstract": "Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference. We present results on all these tasks using cross-lingual word embedding models and multilingual models. In addition, we fine-tune multilingual models on artificially generated code-switched data. Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 102, "influentialCitationCount": 17, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.329.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-26", "journal": {"pages": "3575-3585"}, "authors": [{"authorId": "1452678825", "name": "Simran Khanuja"}, {"authorId": "34725175", "name": "Sandipan Dandapat"}, {"authorId": "48180698", "name": "A. Srinivasan"}, {"authorId": "3010457", "name": "Sunayana Sitaram"}, {"authorId": "143990839", "name": "M. Choudhury"}]}, {"paperId": "725d5acdbdf0a11677f785a16e1722b92c55a47f", "externalIds": {"MAG": "3019858811", "DBLP": "conf/acl/XuPWLL20", "ACL": "2020.acl-main.330", "ArXiv": "2004.12302", "DOI": "10.18653/v1/2020.acl-main.330", "CorpusId": 216553243}, "corpusId": 216553243, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/725d5acdbdf0a11677f785a16e1722b92c55a47f", "title": "MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization", "abstract": "Recently, large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing. However, there is currently no cross-task dataset in NLP, which hinders the development of multi-task learning. We propose MATINF, the first jointly labeled large-scale dataset for classification, question answering and summarization. MATINF contains 1.07 million question-answer pairs with human-labeled categories and user-generated question descriptions. Based on such rich information, MATINF is applicable for three major NLP tasks, including classification, question answering, and summarization. We benchmark existing methods and a novel multi-task baseline over MATINF to inspire further research. Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the merits held by MATINF.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 67, "citationCount": 12, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.330.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "3586-3596"}, "authors": [{"authorId": "66247317", "name": "Canwen Xu"}, {"authorId": "51136528", "name": "Jiaxin Pei"}, {"authorId": "2120430896", "name": "Hongtao Wu"}, {"authorId": "2108151092", "name": "Yiyu Liu"}, {"authorId": "2829009", "name": "Chenliang Li"}]}, {"paperId": "dbdfd22ec8b71d48ff100819235eff56a4a374a3", "externalIds": {"ACL": "2020.acl-main.331", "MAG": "3034503922", "DBLP": "conf/acl/WuQCWQLLXGWZ20", "DOI": "10.18653/v1/2020.acl-main.331", "CorpusId": 220046458}, "corpusId": 220046458, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dbdfd22ec8b71d48ff100819235eff56a4a374a3", "title": "MIND: A Large-scale Dataset for News Recommendation", "abstract": "News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named MIND for news recommendation. Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body. We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. The MIND dataset will be available at https://msnews.github.io.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 280, "influentialCitationCount": 58, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.331.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3597-3606"}, "authors": [{"authorId": "2397264", "name": "Fangzhao Wu"}, {"authorId": "2056923187", "name": "Ying Qiao"}, {"authorId": "33926030", "name": "Jiun-Hung Chen"}, {"authorId": "15161448", "name": "Chuhan Wu"}, {"authorId": "50329599", "name": "Tao Qi"}, {"authorId": "2813328", "name": "Jianxun Lian"}, {"authorId": "2822590", "name": "Danyang Liu"}, {"authorId": "144076239", "name": "Xing Xie"}, {"authorId": "1800422", "name": "Jianfeng Gao"}, {"authorId": "2110027452", "name": "Winnie Wu"}, {"authorId": "143849609", "name": "M. Zhou"}]}, {"paperId": "20b2f18aaf10a9221c5edf3720d4cce7da672104", "externalIds": {"MAG": "3024647820", "DBLP": "conf/acl/ShaarBMN20", "ACL": "2020.acl-main.332", "ArXiv": "2005.06058", "DOI": "10.18653/v1/2020.acl-main.332", "CorpusId": 218613630}, "corpusId": 218613630, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/20b2f18aaf10a9221c5edf3720d4cce7da672104", "title": "That is a Known Lie: Detecting Previously Fact-Checked Claims", "abstract": "The recent proliferation of \u201dfake news\u201d has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 58, "citationCount": 113, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.332.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-12", "journal": {"pages": "3607-3618"}, "authors": [{"authorId": "65877664", "name": "Shaden Shaar"}, {"authorId": "34086979", "name": "Giovanni Da San Martino"}, {"authorId": "1693976811", "name": "Nikolay Babulkov"}, {"authorId": "1683562", "name": "Preslav Nakov"}]}, {"paperId": "5dd8e4b80f5033aef1db1fad974437bc9b8e8112", "externalIds": {"DBLP": "conf/acl/PangNHZLT20", "MAG": "2990563493", "ACL": "2020.acl-main.333", "DOI": "10.18653/V1/2020.ACL-MAIN.333", "CorpusId": 212800438}, "corpusId": 212800438, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5dd8e4b80f5033aef1db1fad974437bc9b8e8112", "title": "Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation", "abstract": "Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our metrics is demonstrated by strong correlations with human judgments. We open source the code and relevant materials.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 66, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.333.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3619-3629"}, "authors": [{"authorId": "2063096824", "name": "Bo Pang"}, {"authorId": "2043490", "name": "Erik Nijkamp"}, {"authorId": "144836032", "name": "Wenjuan Han"}, {"authorId": "2129002103", "name": "A. Zhou"}, {"authorId": "35283208", "name": "Yixian Liu"}]}, {"paperId": "5e11ee60242b07d4cdcff339f6c671564314f99a", "externalIds": {"MAG": "3034704198", "ACL": "2020.acl-main.334", "DBLP": "conf/acl/WangH20", "DOI": "10.18653/v1/2020.acl-main.334", "CorpusId": 220047076}, "corpusId": 220047076, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5e11ee60242b07d4cdcff339f6c671564314f99a", "title": "BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection", "abstract": "The hypernymy detection task has been addressed under various frameworks. Previously, the design of unsupervised hypernymy scores has been extensively studied. In contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from \u201clexical memorization\u201d. In this work, we revisit supervised distributional models for hypernymy detection. Rather than taking embeddings of two terms as classification inputs, we introduce a representation learning framework named Bidirectional Residual Relation Embeddings (BiRRE). In this model, a term pair is represented by a BiRRE vector as features for hypernymy classification, which models the possibility of a term being mapped to another in the embedding space by hypernymy relations. A Latent Projection Model with Negative Regularization (LPMNR) is proposed to simulate how hypernyms and hyponyms are generated by neural language models, and to generate BiRRE vectors based on bidirectional residuals of projections. Experiments verify BiRRE outperforms strong baselines over various evaluation frameworks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 52, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3630-3640"}, "authors": [{"authorId": "121899912", "name": "Chengyu Wang"}, {"authorId": "143644849", "name": "Xiaofeng He"}]}, {"paperId": "4f639d95debf61bd2f3b0b4e2320b6d592e49c6d", "externalIds": {"ACL": "2020.acl-main.335", "DBLP": "journals/corr/abs-2005-00239", "ArXiv": "2005.00239", "MAG": "3022486637", "DOI": "10.18653/v1/2020.acl-main.335", "CorpusId": 218470427}, "corpusId": 218470427, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4f639d95debf61bd2f3b0b4e2320b6d592e49c6d", "title": "Biomedical Entity Representations with Synonym Marginalization", "abstract": "Biomedical named entities often play important roles in many biomedical text mining tools. However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging. In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities. To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates. Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves. In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates. On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 83, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.335.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "3641-3650"}, "authors": [{"authorId": "147610425", "name": "Mujeen Sung"}, {"authorId": "120688457", "name": "Hwisang Jeon"}, {"authorId": "46664096", "name": "Jinhyuk Lee"}, {"authorId": "144323862", "name": "Jaewoo Kang"}]}, {"paperId": "20a5ece6e7e36d0641275401c9fc0d76e0db4c23", "externalIds": {"ACL": "2020.acl-main.336", "DBLP": "conf/acl/YuHZN20", "MAG": "3034588514", "DOI": "10.18653/v1/2020.acl-main.336", "CorpusId": 220047820}, "corpusId": 220047820, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/20a5ece6e7e36d0641275401c9fc0d76e0db4c23", "title": "Hypernymy Detection for Low-Resource Languages via Meta Learning", "abstract": "Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks. Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios. This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages. We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue. Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 18, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.336.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3651-3656"}, "authors": [{"authorId": "49778308", "name": "Changlong Yu"}, {"authorId": "3024833", "name": "Jialong Han"}, {"authorId": "2108558798", "name": "Haisong Zhang"}, {"authorId": "1695492", "name": "Wilfred Ng"}]}, {"paperId": "d90c251bc0cde1971ca701821d241dbac3b75bf3", "externalIds": {"ACL": "2020.acl-main.337", "DBLP": "conf/acl/SasanoK20", "MAG": "3034237830", "DOI": "10.18653/v1/2020.acl-main.337", "CorpusId": 220047273}, "corpusId": 220047273, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d90c251bc0cde1971ca701821d241dbac3b75bf3", "title": "Investigating Word-Class Distributions in Word Vector Spaces", "abstract": "This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space. To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model. Specifically, we considered two types of word classes \u2013 the semantic class of direct objects of a verb and the semantic class in a thesaurus \u2013 and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class. Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution. We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3657-3666"}, "authors": [{"authorId": "2293543", "name": "Ryohei Sasano"}, {"authorId": "145762466", "name": "A. Korhonen"}]}, {"paperId": "aadecc2d19b9a64c82de4e25448fb32e319693cc", "externalIds": {"MAG": "3034442148", "DBLP": "conf/acl/ChenSWLSZZ20", "ACL": "2020.acl-main.338", "DOI": "10.18653/v1/2020.acl-main.338", "CorpusId": 220045943}, "corpusId": 220045943, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/aadecc2d19b9a64c82de4e25448fb32e319693cc", "title": "Aspect Sentiment Classification with Document-level Sentiment Preference Modeling", "abstract": "In the literature, existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in ASC. In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency). On the basis, we propose a Cooperative Graph Attention Networks (CoGAN) approach for cooperatively learning the aspect-related sentence representation. Specifically, two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively, followed by an interactive mechanism to integrate the two-fold preference. Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the state-of-the-art baselines. This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 33, "influentialCitationCount": 7, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3667-3677"}, "authors": [{"authorId": "2109198511", "name": "Xiao Chen"}, {"authorId": "2060934", "name": "Changlong Sun"}, {"authorId": "2109839506", "name": "Jingjing Wang"}, {"authorId": "2109167274", "name": "Shoushan Li"}, {"authorId": "2059080424", "name": "Luo Si"}, {"authorId": "2156053331", "name": "Min Zhang"}, {"authorId": "143740945", "name": "Guodong Zhou"}]}, {"paperId": "fb42a43aef09c993b21d869f2dcfeb74ebdfb323", "externalIds": {"MAG": "3034802586", "DBLP": "conf/acl/WeiHZCY20", "ACL": "2020.acl-main.339", "DOI": "10.18653/v1/2020.acl-main.339", "CorpusId": 220045369}, "corpusId": 220045369, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fb42a43aef09c993b21d869f2dcfeb74ebdfb323", "title": "Don\u2019t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction", "abstract": "The current aspect extraction methods suffer from boundary errors. In general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth. However, they hurt the performance severely. In this paper, we propose to utilize a pointer network for repositioning the boundaries. Recycling mechanism is used, which enables the training data to be collected without manual intervention. We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant. Experimental results show that our method achieves substantial improvements over the baseline, and outperforms state-of-the-art methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 24, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.339.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3678-3684"}, "authors": [{"authorId": "1754238238", "name": "Zhenkai Wei"}, {"authorId": "144873792", "name": "Yu Hong"}, {"authorId": "3078054", "name": "Bowei Zou"}, {"authorId": "2072996485", "name": "M. Cheng"}, {"authorId": "2973770", "name": "Jianmin Yao"}]}, {"paperId": "b91e36e3f0db1dd2ed333e271cc3acea9380119f", "externalIds": {"DBLP": "conf/acl/ChenQ20", "ACL": "2020.acl-main.340", "MAG": "3034990686", "DOI": "10.18653/v1/2020.acl-main.340", "CorpusId": 220045831}, "corpusId": 220045831, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b91e36e3f0db1dd2ed333e271cc3acea9380119f", "title": "Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) involves three subtasks, i.e., aspect term extraction, opinion term extraction, and aspect-level sentiment classification. Most existing studies focused on one of these subtasks only. Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework. However, the interactive relations among three subtasks are still under-exploited. We argue that such relations encode collaborative signals between different subtasks. For example, when the opinion term is \u201cdelicious\u201d, the aspect term must be \u201cfood\u201d rather than \u201cplace\u201d. In order to fully exploit these relations, we propose a Relation-Aware Collaborative Learning (RACL) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network. Extensive experiments on three real-world datasets demonstrate that RACL significantly outperforms the state-of-the-art methods for the complete ABSA task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 48, "citationCount": 132, "influentialCitationCount": 17, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.340.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3685-3694"}, "authors": [{"authorId": null, "name": "Zhuang Chen"}, {"authorId": "34559283", "name": "T. Qian"}]}, {"paperId": "9e52a2f0ad93fc3a173bf061bde91c57fb46737d", "externalIds": {"DBLP": "conf/acl/YinMC20", "ACL": "2020.acl-main.341", "MAG": "3034854575", "ArXiv": "2005.04114", "DOI": "10.18653/v1/2020.acl-main.341", "CorpusId": 218570990}, "corpusId": 218570990, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9e52a2f0ad93fc3a173bf061bde91c57fb46737d", "title": "SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics", "abstract": "We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics. The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition. Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification. We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks. Moreover, we conduct ablation studies and design visualization methods to understand SentiBERT. We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 84, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.341.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-08", "journal": {"name": "ArXiv", "volume": "abs/2005.04114"}, "authors": [{"authorId": "144508458", "name": "Da Yin"}, {"authorId": "2056088060", "name": "Tao Meng"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}]}, {"paperId": "36fbd3030a2b6b4221328446cfd2cd2003168d7d", "externalIds": {"MAG": "3035101819", "ACL": "2020.acl-main.342", "DBLP": "conf/acl/FanYDGYX20", "DOI": "10.18653/v1/2020.acl-main.342", "CorpusId": 220047822}, "corpusId": 220047822, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/36fbd3030a2b6b4221328446cfd2cd2003168d7d", "title": "Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction", "abstract": "Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text. Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation. Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction. The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently. Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (p<0.01) in F1 measure.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 67, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.342.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3707-3717"}, "authors": [{"authorId": "20964758", "name": "Chuang Fan"}, {"authorId": "1720981237", "name": "Chaofa Yuan"}, {"authorId": "2186356", "name": "Jiachen Du"}, {"authorId": "145096580", "name": "Lin Gui"}, {"authorId": "2110950699", "name": "Min Yang"}, {"authorId": "1753529", "name": "Ruifeng Xu"}]}, {"paperId": "fc85469f9ff785f24212a50c58b497de563ae3da", "externalIds": {"ACL": "2020.acl-main.343", "DBLP": "conf/acl/YuXMZMWZY20", "MAG": "3034849760", "DOI": "10.18653/v1/2020.acl-main.343", "CorpusId": 220048228}, "corpusId": 220048228, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fc85469f9ff785f24212a50c58b497de563ae3da", "title": "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality", "abstract": "Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities. In this paper, we introduce a Chinese single- and multi-modal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.Furthermore, we propose a multi-task learning framework based on late fusion as the baseline. Extensive experiments on the CH-SIMS show that our methods achieve state-of-the-art performance and learn more distinctive unimodal representations. The full dataset and codes are available for use at https://github.com/thuiar/MMSA.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 31, "citationCount": 108, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.343.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3718-3727"}, "authors": [{"authorId": "1754430080", "name": "Wenmeng Yu"}, {"authorId": "2124947706", "name": "Hua Xu"}, {"authorId": "1774797", "name": "Fanyang Meng"}, {"authorId": "2144317597", "name": "Yilin Zhu"}, {"authorId": "95952564", "name": "Yixiao Ma"}, {"authorId": "1754148511", "name": "Jiele Wu"}, {"authorId": "1657286768", "name": "Jiyun Zou"}, {"authorId": "2118047966", "name": "Kaicheng Yang"}]}, {"paperId": "099524cb3765e02090e0c07369bb418a03095781", "externalIds": {"MAG": "3034571331", "ACL": "2020.acl-main.344", "DBLP": "conf/acl/WangWLZY20", "ArXiv": "2004.10093", "DOI": "10.18653/v1/2020.acl-main.344", "CorpusId": 216035773}, "corpusId": 216035773, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/099524cb3765e02090e0c07369bb418a03095781", "title": "Curriculum Pre-training for End-to-End Speech Translation", "abstract": "End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 81, "influentialCitationCount": 17, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.344.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-21", "journal": {"name": "ArXiv", "volume": "abs/2004.10093"}, "authors": [{"authorId": "8206308", "name": "Chengyi Wang"}, {"authorId": "2142240763", "name": "Yu Wu"}, {"authorId": "1803054", "name": "Shujie Liu"}, {"authorId": "92660691", "name": "Ming Zhou"}, {"authorId": "2881049", "name": "Zhenglu Yang"}]}, {"paperId": "83cd082177350cd21fdc0bc7bf0cd77dc96d4978", "externalIds": {"DBLP": "conf/acl/PrasadJ20", "ACL": "2020.acl-main.345", "MAG": "3034273309", "DOI": "10.18653/v1/2020.acl-main.345", "CorpusId": 220046813}, "corpusId": 220046813, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/83cd082177350cd21fdc0bc7bf0cd77dc96d4978", "title": "How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems", "abstract": "In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system. We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents. We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers. We find different accents exhibiting similar trends irrespective of the probing technique used. We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 22, "influentialCitationCount": 2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3739-3753"}, "authors": [{"authorId": "1677896557", "name": "Archiki Prasad"}, {"authorId": "144859542", "name": "P. Jyothi"}]}, {"paperId": "934daec63a822cfb142aab81787df4dca22000b3", "externalIds": {"MAG": "3034323214", "ArXiv": "2004.05323", "ACL": "2020.acl-main.346", "DBLP": "conf/acl/LouJ20", "DOI": "10.18653/v1/2020.acl-main.346", "CorpusId": 215745503}, "corpusId": 215745503, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/934daec63a822cfb142aab81787df4dca22000b3", "title": "Improving Disfluency Detection by Self-Training a Self-Attentive Model", "abstract": "Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts. Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant. However, we show that self-training \u2014 a semi-supervised technique for incorporating unlabeled data \u2014 sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations. We also show that ensembling self-trained parsers provides further gains for disfluency detection.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 35, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.346.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "3754-3763"}, "authors": [{"authorId": "38464604", "name": "Paria Jamshid Lou"}, {"authorId": "145177145", "name": "Mark Johnson"}]}, {"paperId": "b3eaf6ac2bc38d42585fcb20cfcb40ccf3ac4629", "externalIds": {"DBLP": "conf/acl/HuangC20", "ACL": "2020.acl-main.347", "ArXiv": "2007.02629", "MAG": "3035676545", "DOI": "10.18653/v1/2020.acl-main.347", "CorpusId": 218610511}, "corpusId": 218610511, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b3eaf6ac2bc38d42585fcb20cfcb40ccf3ac4629", "title": "Learning Spoken Language Representations with Neural Lattice Language Modeling", "abstract": "Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language. Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency. Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs. The code is available at https://github.com/MiuLab/Lattice-ELMo.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.347.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3764-3769"}, "authors": [{"authorId": "47396497", "name": "Chao-Wei Huang"}, {"authorId": "1755002726", "name": "Yun-Nung Vivian"}]}, {"paperId": "28f09e7b8d6d9acac916773a7d1443ad92e00ba4", "externalIds": {"ArXiv": "2004.14228", "MAG": "3035268246", "ACL": "2020.acl-main.348", "DBLP": "journals/corr/abs-2004-14228", "DOI": "10.18653/v1/2020.acl-main.348", "CorpusId": 216641835}, "corpusId": 216641835, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/28f09e7b8d6d9acac916773a7d1443ad92e00ba4", "title": "Meta-Transfer Learning for Code-Switched Speech Recognition", "abstract": "An increasing number of people in the world today speak a mixed-language as a result of being multilingual. However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data. We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets. Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data. Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 41, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.348.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "3770-3776"}, "authors": [{"authorId": "9162688", "name": "Genta Indra Winata"}, {"authorId": "66986482", "name": "Samuel Cahyawijaya"}, {"authorId": "100466830", "name": "Zhaojiang Lin"}, {"authorId": "152613855", "name": "Zihan Liu"}, {"authorId": "145011005", "name": "Peng Xu"}, {"authorId": "40539650", "name": "Pascale Fung"}]}, {"paperId": "0986fde9085339b4704546f799f2c496c6d6df56", "externalIds": {"ACL": "2020.acl-main.349", "DBLP": "conf/acl/XuZM20", "MAG": "3035565303", "DOI": "10.18653/v1/2020.acl-main.349", "CorpusId": 220046222}, "corpusId": 220046222, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0986fde9085339b4704546f799f2c496c6d6df56", "title": "Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association", "abstract": "Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means. With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms. In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions. Thus traditional text-based methods are insufficient to detect multimodal sarcasm. To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context. Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network (namely D&R Net). The decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context. Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 49, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.349.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3777-3786"}, "authors": [{"authorId": "144586903", "name": "Nan Xu"}, {"authorId": "1998946156", "name": "Zhixiong Zeng"}, {"authorId": "2419472", "name": "W. Mao"}]}, {"paperId": "f0dfe7f0528eded4096a741a751aea4b1f707e82", "externalIds": {"ACL": "2020.acl-main.350", "MAG": "3034586846", "DBLP": "conf/acl/RenLTZQZL20", "DOI": "10.18653/v1/2020.acl-main.350", "CorpusId": 220048083}, "corpusId": 220048083, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f0dfe7f0528eded4096a741a751aea4b1f707e82", "title": "SimulSpeech: End-to-End Simultaneous Speech to Text Translation", "abstract": "In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 57, "influentialCitationCount": 19, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.350.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3787-3796"}, "authors": [{"authorId": "1500435161", "name": "Yi Ren"}, {"authorId": "48211720", "name": "Jinglin Liu"}, {"authorId": "48391466", "name": "Xu Tan"}, {"authorId": "2111573042", "name": "Chen Zhang"}, {"authorId": "143826491", "name": "Tao Qin"}, {"authorId": "47122432", "name": "Zhou Zhao"}, {"authorId": "2110264337", "name": "Tie-Yan Liu"}]}, {"paperId": "8993e9c8cad6c98a4f4821f08bb9dc092979d644", "externalIds": {"ACL": "2020.acl-main.351", "DBLP": "conf/acl/SinglaCAN20", "MAG": "3034461631", "DOI": "10.18653/v1/2020.acl-main.351", "CorpusId": 220045476, "PubMed": "36751434"}, "corpusId": 220045476, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8993e9c8cad6c98a4f4821f08bb9dc092979d644", "title": "Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations", "abstract": "Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR). We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding. Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features. This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec. Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels. We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 11, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.351.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"name": "Proceedings of the conference. Association for Computational Linguistics. Meeting", "pages": "\n          3797-3803\n        ", "volume": "2020"}, "authors": [{"authorId": "3471353", "name": "Karan Singla"}, {"authorId": "27058423", "name": "Zhuohao Chen"}, {"authorId": "2863328", "name": "David C. Atkins"}, {"authorId": "145254843", "name": "Shrikanth S. Narayanan"}]}, {"paperId": "6db10ecd4415d9f52b36735a51553dfe741485c5", "externalIds": {"MAG": "3031764138", "DBLP": "journals/corr/abs-2005-13486", "ACL": "2020.acl-main.352", "ArXiv": "2005.13486", "DOI": "10.18653/V1/2020.ACL-MAIN.352", "CorpusId": 218900793}, "corpusId": 218900793, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6db10ecd4415d9f52b36735a51553dfe741485c5", "title": "Neural Temporal Opinion Modelling for Opinion Prediction on Twitter", "abstract": "Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context. In this paper, we model users\u2019 tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user\u2019s historical tweet sequence and tweets posted by their neighbours. We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context. Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.352.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-27", "journal": {"name": "ArXiv", "volume": "abs/2005.13486"}, "authors": [{"authorId": "2131133148", "name": "Lixing Zhu"}, {"authorId": "1704133", "name": "Yulan He"}, {"authorId": "1725992", "name": "Deyu Zhou"}]}, {"paperId": "45204e83165e0bb7ee081d1a077c3b7fb899c5e4", "externalIds": {"DBLP": "conf/acl/PeskovCEBDB20", "MAG": "3034545571", "ACL": "2020.acl-main.353", "DOI": "10.18653/v1/2020.acl-main.353", "CorpusId": 220047262}, "corpusId": 220047262, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/45204e83165e0bb7ee081d1a077c3b7fb899c5e4", "title": "It Takes Two to Lie: One to Lie, and One to Listen", "abstract": "Trust is implicit in many online text conversations\u2014striking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 67, "citationCount": 22, "influentialCitationCount": 5, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3811-3854"}, "authors": [{"authorId": "21317593", "name": "Denis Peskov"}, {"authorId": "2055923908", "name": "Benny Cheng"}, {"authorId": "143718836", "name": "Ahmed Elgohary"}, {"authorId": "40080808", "name": "Joe Barrow"}, {"authorId": "1388368997", "name": "Cristian Danescu-Niculescu-Mizil"}, {"authorId": "1389036863", "name": "Jordan L. Boyd-Graber"}]}, {"paperId": "87291216db08c61f6b02fe19e9adac2cf0ab88d5", "externalIds": {"ACL": "2020.acl-main.354", "DBLP": "journals/corr/abs-2005-03588", "MAG": "3022907967", "ArXiv": "2005.03588", "DOI": "10.18653/v1/2020.acl-main.354", "CorpusId": 218538329}, "corpusId": 218538329, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/87291216db08c61f6b02fe19e9adac2cf0ab88d5", "title": "Learning Implicit Text Generation via Feature Matching", "abstract": "Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks. In this paper, we present new GFMN formulations that are effective for sequential data. Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer. SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.354.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-07", "journal": {"name": "ArXiv", "volume": "abs/2005.03588"}, "authors": [{"authorId": "8350409", "name": "Inkit Padhi"}, {"authorId": "1839363", "name": "Pierre L. Dognin"}, {"authorId": "144402849", "name": "Ke Bai"}, {"authorId": "1790831", "name": "C. D. Santos"}, {"authorId": "1776280", "name": "Vijil Chenthamarakshan"}, {"authorId": "2211263", "name": "Youssef Mroueh"}, {"authorId": "1730372", "name": "Payel Das"}]}, {"paperId": "24cbc48bb0cb1c275fb88e50965dfe1af57fd784", "externalIds": {"ArXiv": "1909.10158", "MAG": "3035315091", "ACL": "2020.acl-main.355", "DBLP": "journals/corr/abs-1909-10158", "DOI": "10.18653/v1/2020.acl-main.355", "CorpusId": 202718912}, "corpusId": 202718912, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/24cbc48bb0cb1c275fb88e50965dfe1af57fd784", "title": "Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data", "abstract": "A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures. In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks. In this work, we show that this is also the case for text generation from structured and unstructured data. We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively. Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models. Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks. Code is available at https://github.com/h-shahidi/2birds-gen.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 43, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.355.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-23", "journal": {"pages": "3864-3870"}, "authors": [{"authorId": "1387982258", "name": "H. Shahidi"}, {"authorId": "2150652992", "name": "Ming Li"}, {"authorId": "145580839", "name": "Jimmy J. Lin"}]}, {"paperId": "b1e9c4a3bd04bfcaa53d367b9fb25c71423a3091", "externalIds": {"MAG": "3016783921", "DBLP": "journals/corr/abs-2004-07126", "ArXiv": "2004.07126", "ACL": "2020.acl-main.356", "DOI": "10.18653/v1/2020.acl-main.356", "CorpusId": 215768727}, "corpusId": 215768727, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b1e9c4a3bd04bfcaa53d367b9fb25c71423a3091", "title": "Bayesian Hierarchical Words Representation Learning", "abstract": "This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm. BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations. Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors. Finally, we further show that BHWR produces better representations for rare words.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 26, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.356.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-12", "journal": {"pages": "3871-3877"}, "authors": [{"authorId": "48797862", "name": "Oren Barkan"}, {"authorId": "27732655", "name": "Idan Rejwan"}, {"authorId": "27743758", "name": "Avi Caciularu"}, {"authorId": "1683070", "name": "Noam Koenigstein"}]}, {"paperId": "d6599d4dfaeb78bea1f975db683aa653e26b3987", "externalIds": {"ACL": "2020.acl-main.357", "ArXiv": "2004.14074", "MAG": "3020987135", "DBLP": "journals/corr/abs-2004-14074", "DOI": "10.18653/v1/2020.acl-main.357", "CorpusId": 216641845}, "corpusId": 216641845, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d6599d4dfaeb78bea1f975db683aa653e26b3987", "title": "Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning", "abstract": "Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 48, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.357.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "3878-3887"}, "authors": [{"authorId": "153521202", "name": "Alexandre Tamborrino"}, {"authorId": "144556993", "name": "Nicola Pellican\u00f2"}, {"authorId": "91723794", "name": "B. Pannier"}, {"authorId": "1661215648", "name": "Pascal Voitot"}, {"authorId": "1661214297", "name": "Louise Naudin"}]}, {"paperId": "7b13f5ed0ef0a50e243e8169c236b91af822bee5", "externalIds": {"DBLP": "conf/acl/XuZHSYL20", "MAG": "3035263097", "ArXiv": "2005.00856", "ACL": "2020.acl-main.358", "DOI": "10.18653/v1/2020.acl-main.358", "CorpusId": 218487188}, "corpusId": 218487188, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7b13f5ed0ef0a50e243e8169c236b91af822bee5", "title": "SEEK: Segmented Embedding of Knowledge Graphs", "abstract": "In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering. However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory. To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity. Our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations. It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases. Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework. Source codes and data can be found at https://github.com/Wentao-Xu/SEEK.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 23, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.358.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00856"}, "authors": [{"authorId": "50232080", "name": "W. Xu"}, {"authorId": "2111073463", "name": "Shun Zheng"}, {"authorId": "2109334724", "name": "Liang He"}, {"authorId": "2064567675", "name": "Bin Shao"}, {"authorId": "2111609908", "name": "Jian Yin"}, {"authorId": "2110264337", "name": "Tie-Yan Liu"}]}, {"paperId": "491bb6941cbdd3fa4f37e418a2e4b01f0f5b4849", "externalIds": {"DBLP": "journals/corr/abs-2005-00308", "MAG": "3035306625", "ACL": "2020.acl-main.359", "ArXiv": "2005.00308", "DOI": "10.18653/v1/2020.acl-main.359", "CorpusId": 218470116}, "corpusId": 218470116, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/491bb6941cbdd3fa4f37e418a2e4b01f0f5b4849", "title": "Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation", "abstract": "Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems. We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora. We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 50, "citationCount": 15, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.359.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "3898-3908"}, "authors": [{"authorId": "148377854", "name": "Xabier Soto"}, {"authorId": "2848410", "name": "D. Shterionov"}, {"authorId": "27731932", "name": "Alberto Poncelas"}, {"authorId": "144315616", "name": "Andy Way"}]}, {"paperId": "143b0bd73e7e765945e0b8620f84f52878617560", "externalIds": {"MAG": "3023182682", "ACL": "2020.acl-main.360", "ArXiv": "2005.03454", "DBLP": "conf/acl/BrixBN20", "DOI": "10.18653/v1/2020.acl-main.360", "CorpusId": 218538350}, "corpusId": 218538350, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/143b0bd73e7e765945e0b8620f84f52878617560", "title": "Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture", "abstract": "Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs. This is relevant both for time-critical and on-device computations using neural networks. The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model. On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity. Furthermore, we confirm that the parameter\u2019s initial sign and not its specific value is the primary factor for successful training, and show that magnitude pruning cannot be used to find winning lottery tickets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 34, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.360.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "3909-3915"}, "authors": [{"authorId": "51518834", "name": "Christopher Brix"}, {"authorId": "1872287", "name": "Parnia Bahar"}, {"authorId": "145322333", "name": "H. Ney"}]}, {"paperId": "a67efc92394bc632a257c2c62346030d7c46de39", "externalIds": {"MAG": "3023818073", "DBLP": "journals/corr/abs-2005-05189", "ArXiv": "2005.05189", "ACL": "2020.acl-main.361", "DOI": "10.18653/v1/2020.acl-main.361", "CorpusId": 218581822}, "corpusId": 218581822, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a67efc92394bc632a257c2c62346030d7c46de39", "title": "A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction", "abstract": "Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor. The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence. Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC. To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process. At each iteration, a base MRC model is trained with golden answers and noisy evidence labels. The trained model will predict pseudo evidence labels as extra supervision in the next iteration. We evaluate STM on seven datasets over three MRC tasks. Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 45, "citationCount": 27, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.361.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.05189"}, "authors": [{"authorId": "10680347", "name": "Yilin Niu"}, {"authorId": "1689176705", "name": "Fangkai Jiao"}, {"authorId": "4984454", "name": "Mantong Zhou"}, {"authorId": "48577275", "name": "Ting Yao"}, {"authorId": "2774294", "name": "Jingfang Xu"}, {"authorId": "1730108", "name": "Minlie Huang"}]}, {"paperId": "ac9173e4b8b34dc24c18980ee32be6bf15e7b38d", "externalIds": {"MAG": "3035498813", "DBLP": "conf/acl/ZhangWLBWSL20", "ACL": "2020.acl-main.362", "DOI": "10.18653/v1/2020.acl-main.362", "CorpusId": 220046465}, "corpusId": 220046465, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ac9173e4b8b34dc24c18980ee32be6bf15e7b38d", "title": "Graph-to-Tree Learning for Solving Math Word Problems", "abstract": "While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well. This results in poor quantity representations and incorrect solution expressions. In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions. Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs. We conduct extensive experiments on two available datasets. Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly. We also discuss case studies and empirically examine Graph2Tree\u2019s effectiveness in translating the MWP text into solution expressions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 133, "influentialCitationCount": 39, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.362.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3928-3937"}, "authors": [{"authorId": "50561049", "name": "Jipeng Zhang"}, {"authorId": "145131956", "name": "Lei Wang"}, {"authorId": "38656724", "name": "R. Lee"}, {"authorId": "2054618348", "name": "Yi Bin"}, {"authorId": "40566130", "name": "Yan Wang"}, {"authorId": "2111876759", "name": "Jie Shao"}, {"authorId": "1709901", "name": "Ee-Peng Lim"}]}, {"paperId": "652fad49a9582a5902ce23b9644ba0319791727e", "externalIds": {"MAG": "3033085300", "DBLP": "journals/corr/abs-2006-01245", "ArXiv": "2006.01245", "ACL": "2020.acl-main.363", "DOI": "10.18653/v1/2020.acl-main.363", "CorpusId": 219179431}, "corpusId": 219179431, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/652fad49a9582a5902ce23b9644ba0319791727e", "title": "An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results", "abstract": "In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as \u201cpositive\u201d, \u201cneutral\u201d, \u201cnegative\u201d in sentiment analysis. Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes). In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory. Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously. In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 29, "citationCount": 14, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.363.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-01", "journal": {"pages": "3938-3949"}, "authors": [{"authorId": "2066495445", "name": "Enrique Amig'o"}, {"authorId": "1681160", "name": "Julio Gonzalo"}, {"authorId": "1726978", "name": "Stefano Mizzaro"}, {"authorId": "1410230608", "name": "Jorge Carrillo-de-Albornoz"}]}, {"paperId": "49e5b35176c62f11e0b0720fa33858529697f0cd", "externalIds": {"ACL": "2020.acl-main.364", "MAG": "3035762680", "DBLP": "conf/acl/KimKL20", "DOI": "10.18653/v1/2020.acl-main.364", "CorpusId": 220046201}, "corpusId": 220046201, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/49e5b35176c62f11e0b0720fa33858529697f0cd", "title": "Adaptive Compression of Word Embeddings", "abstract": "Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks. However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices). In this paper, we propose a novel method to adaptively compress word embeddings. We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4). However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks. The proposed method works in two steps. First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks. After selecting the code length, each word learns discrete codes through a neural network with a binary constraint. To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks. Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy. Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 15, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3950-3959"}, "authors": [{"authorId": "2140096418", "name": "Yeachan Kim"}, {"authorId": "151476639", "name": "Kang-Min Kim"}, {"authorId": "66593705", "name": "SangKeun Lee"}]}, {"paperId": "328669ab500649ef14bc98ab59b78c483dbd3db4", "externalIds": {"DBLP": "journals/corr/abs-2004-14118", "ArXiv": "2004.14118", "MAG": "3021480884", "ACL": "2020.acl-main.365", "DOI": "10.18653/V1/2020.ACL-MAIN.365", "CorpusId": 216641736}, "corpusId": 216641736, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/328669ab500649ef14bc98ab59b78c483dbd3db4", "title": "Analysing Lexical Semantic Change with Contextualised Word Representations", "abstract": "This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 63, "citationCount": 123, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": {"url": "https://pure.uva.nl/ws/files/63250055/2020.acl_main.365.pdf", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-29", "journal": {"pages": "3960-3973"}, "authors": [{"authorId": "24068173", "name": "Mario Giulianelli"}, {"authorId": "3448493", "name": "Marco Del Tredici"}, {"authorId": "2147411696", "name": "Raquel Fern'andez"}]}, {"paperId": "e78b1c6cf0fbe935f12adf3a5ce3cde629252316", "externalIds": {"ACL": "2020.acl-main.366", "DBLP": "conf/acl/ChiuSTSM20", "MAG": "3034349864", "DOI": "10.18653/v1/2020.acl-main.366", "CorpusId": 220048374}, "corpusId": 220048374, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/e78b1c6cf0fbe935f12adf3a5ce3cde629252316", "title": "Autoencoding Keyword Correlation Graph for Document Clustering", "abstract": "Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global). Existing representation learning models do not fully capture these features. To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph. The graph is constructed with topical keywords as nodes and multiple local and global features as edges. A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them. Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes. Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 48, "citationCount": 14, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.366.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "3974-3981"}, "authors": [{"authorId": "50493332", "name": "Billy Chiu"}, {"authorId": "3422905", "name": "Sunil Kumar Sahu"}, {"authorId": "2111218448", "name": "Derek Thomas"}, {"authorId": "1880394", "name": "Neha Sengupta"}, {"authorId": "1754924481", "name": "Mohammady Mahdy"}]}, {"paperId": "eb4f4d4c4128907bb02c034a72347387e092f05d", "externalIds": {"ACL": "2020.acl-main.367", "ArXiv": "2005.02991", "MAG": "3035050194", "DBLP": "journals/corr/abs-2005-02991", "DOI": "10.18653/v1/2020.acl-main.367", "CorpusId": 218517046}, "corpusId": 218517046, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/eb4f4d4c4128907bb02c034a72347387e092f05d", "title": "Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics", "abstract": "Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector. However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge. In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference. This allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming BERT, a large pre-trained language model.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 94, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.02991", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-06", "journal": {"pages": "3982-3995"}, "authors": [{"authorId": "145406160", "name": "Guy Edward Toh Emerson"}]}, {"paperId": "9f4c37f154946e141a67ae2816c70b19241b3224", "externalIds": {"ArXiv": "1910.07181", "DBLP": "journals/corr/abs-1910-07181", "ACL": "2020.acl-main.368", "MAG": "2980697054", "DOI": "10.18653/v1/2020.acl-main.368", "CorpusId": 204734306}, "corpusId": 204734306, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9f4c37f154946e141a67ae2816c70b19241b3224", "title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "abstract": "Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Sch\u00fctze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 40, "citationCount": 36, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.368.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-16", "journal": {"name": "ArXiv", "volume": "abs/1910.07181"}, "authors": [{"authorId": "32246932", "name": "Timo Schick"}, {"authorId": "144418438", "name": "Hinrich Sch\u00fctze"}]}, {"paperId": "090b4bf41a900c13acbeea42d3310a826396b4e8", "externalIds": {"ACL": "2020.acl-main.369", "MAG": "3034569695", "DBLP": "conf/acl/PasiniSS20", "DOI": "10.18653/v1/2020.acl-main.369", "CorpusId": 220047261}, "corpusId": 220047261, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/090b4bf41a900c13acbeea42d3310a826396b4e8", "title": "CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages", "abstract": "Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly. However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary. To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences. Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches. When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models. Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks. We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 14, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.369.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4008-4018"}, "authors": [{"authorId": "40438851", "name": "Tommaso Pasini"}, {"authorId": "2070325", "name": "Federico Scozzafava"}, {"authorId": "150046993", "name": "Bianca Scarlini"}]}, {"paperId": "769ede14e65e6f99dd15e22e301e392191864ea2", "externalIds": {"ACL": "2020.acl-main.370", "MAG": "3035431747", "DBLP": "conf/acl/DuSWQL20", "DOI": "10.18653/v1/2020.acl-main.370", "CorpusId": 220045953}, "corpusId": 220045953, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/769ede14e65e6f99dd15e22e301e392191864ea2", "title": "Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis", "abstract": "Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data. It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation. Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge. To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task. The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way. Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features. Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin. The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 128, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.370.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4019-4028"}, "authors": [{"authorId": "1382567037", "name": "Chunning Du"}, {"authorId": "3211546", "name": "Haifeng Sun"}, {"authorId": "1519274136", "name": "Jingyu Wang"}, {"authorId": "40806187", "name": "Q. Qi"}, {"authorId": "145184961", "name": "J. Liao"}]}, {"paperId": "01f20660e578e781b94af1d31e36790f01a61aec", "externalIds": {"MAG": "3021632220", "ArXiv": "2005.01619", "DBLP": "journals/corr/abs-2005-01619", "ACL": "2020.acl-main.371", "DOI": "10.18653/v1/2020.acl-main.371", "CorpusId": 218487506}, "corpusId": 218487506, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/01f20660e578e781b94af1d31e36790f01a61aec", "title": "From Arguments to Key Points: Towards Automatic Argument Summarization", "abstract": "Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in advance. We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. We report empirical results for an extensive set of experiments with this dataset, showing promising performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 65, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.371.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"pages": "4029-4039"}, "authors": [{"authorId": "1693525", "name": "Roy Bar-Haim"}, {"authorId": "1668029117", "name": "Lilach Eden"}, {"authorId": "1404506446", "name": "Roni Friedman"}, {"authorId": "2965962", "name": "Yoav Kantor"}, {"authorId": "1396093833", "name": "Dan Lahav"}, {"authorId": "1766595", "name": "N. Slonim"}]}, {"paperId": "4450493ecb806cb889b341ae8e430886f2549a61", "externalIds": {"MAG": "3034323190", "DBLP": "journals/corr/abs-2005-00547", "ArXiv": "2005.00547", "ACL": "2020.acl-main.372", "DOI": "10.18653/v1/2020.acl-main.372", "CorpusId": 218486942}, "corpusId": 218486942, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4450493ecb806cb889b341ae8e430886f2549a61", "title": "GoEmotions: A Dataset of Fine-Grained Emotions", "abstract": "Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 345, "influentialCitationCount": 89, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.372.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "4040-4054"}, "authors": [{"authorId": "81551000", "name": "Dorottya Demszky"}, {"authorId": "3218950", "name": "Dana Movshovitz-Attias"}, {"authorId": "37614272", "name": "Jeongwoo Ko"}, {"authorId": "49531494", "name": "Alan S. Cowen"}, {"authorId": "2064494954", "name": "Gaurav Nemade"}, {"authorId": "120209444", "name": "Sujith Ravi"}]}, {"paperId": "7ab3fcf1001991adb29a7951da3d661df835eb3c", "externalIds": {"DBLP": "conf/acl/ChirilMBMOC20", "MAG": "3035400430", "ACL": "2020.acl-main.373", "DOI": "10.18653/v1/2020.acl-main.373", "CorpusId": 220047592}, "corpusId": 220047592, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/7ab3fcf1001991adb29a7951da3d661df835eb3c", "title": "He said \u201cwho\u2019s gonna take care of your children when you are at ACL?\u201d: Reported Sexist Acts are Not Sexist", "abstract": "In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet\u2019s vectorial representations (word embeddings, linguistic features, and various generalization strategies). Our results are encouraging and constitute a first step towards offensive content moderation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 66, "citationCount": 24, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.373.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4055-4066"}, "authors": [{"authorId": "73773451", "name": "Patricia Chiril"}, {"authorId": "2975481", "name": "V\u00e9ronique Moriceau"}, {"authorId": "3350632", "name": "F. Benamara"}, {"authorId": "2623411", "name": "A. Mari"}, {"authorId": "2129146", "name": "G. Origgi"}, {"authorId": "2065327064", "name": "Marl\u00e8ne Coulomb-Gully"}]}, {"paperId": "d7d5bb7c5424a725d3c2b7d352aa299f0f90a5e5", "externalIds": {"DBLP": "conf/acl/TianGXLHWWW20", "ACL": "2020.acl-main.374", "ArXiv": "2005.05635", "MAG": "3024994608", "DOI": "10.18653/v1/2020.acl-main.374", "CorpusId": 218595658}, "corpusId": 218595658, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d7d5bb7c5424a725d3c2b7d352aa299f0f90a5e5", "title": "SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis", "abstract": "Recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches. However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches. In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified sentiment representation for multiple sentiment analysis tasks. With the help of automatically-mined knowledge, SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation. In particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair. Experiments on three kinds of sentiment tasks show that SKEP significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets. We release our code at https://github.com/baidu/Senta.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 159, "influentialCitationCount": 17, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.374.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-12", "journal": {"name": "ArXiv", "volume": "abs/2005.05635"}, "authors": [{"authorId": "50007795", "name": "Hao Tian"}, {"authorId": "144528413", "name": "Can Gao"}, {"authorId": "2107521158", "name": "Xinyan Xiao"}, {"authorId": "2143855615", "name": "Hao Liu"}, {"authorId": "1384304596", "name": "Bolei He"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "144270731", "name": "Haifeng Wang"}, {"authorId": "2110920049", "name": "Feng Wu"}]}, {"paperId": "f2a9ef36b79c1b25d15bf10ff061db5eaad06798", "externalIds": {"MAG": "3035261420", "ArXiv": "2004.14096", "ACL": "2020.acl-main.375", "DBLP": "conf/acl/KulmizevRAN20", "DOI": "10.18653/v1/2020.acl-main.375", "CorpusId": 216641651}, "corpusId": 216641651, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f2a9ef36b79c1b25d15bf10ff061db5eaad06798", "title": "Do Neural Language Models Show Preferences for Syntactic Formalisms?", "abstract": "Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure. We find that both models exhibit a preference for UD over SUD \u2014 with interesting variations across languages and layers \u2014 and that the strength of this preference is correlated with differences in tree shape.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 31, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.375.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-29", "journal": {"name": "ArXiv", "volume": "abs/2004.14096"}, "authors": [{"authorId": "25061910", "name": "Artur Kulmizev"}, {"authorId": "24881798", "name": "Vinit Ravishankar"}, {"authorId": "30671790", "name": "Mostafa Abdou"}, {"authorId": "1720988", "name": "Joakim Nivre"}]}, {"paperId": "47a4d9486a9c5ed43f9f917dc8008dc0945e71ef", "externalIds": {"MAG": "3034904072", "DBLP": "conf/acl/Fernandez-Gonzalez20", "ArXiv": "2005.13334", "ACL": "2020.acl-main.376", "DOI": "10.18653/v1/2020.acl-main.376", "CorpusId": 218900930}, "corpusId": 218900930, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/47a4d9486a9c5ed43f9f917dc8008dc0945e71ef", "title": "Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing", "abstract": "Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. In this paper, we show that these results can be improved by using an in-order linearization instead. Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)\u2019s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers. Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 26, "citationCount": 8, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.376.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-27", "journal": {"pages": "4092-4099"}, "authors": [{"authorId": "1403069822", "name": "Daniel Fern\u00e1ndez-Gonz\u00e1lez"}, {"authorId": "2450508", "name": "Carlos G\u00f3mez-Rodr\u00edguez"}]}, {"paperId": "8c082ccbc26d48b9d46f7ed308afeb6213287437", "externalIds": {"MAG": "3035005343", "DBLP": "conf/acl/YeS20", "ACL": "2020.acl-main.377", "DOI": "10.18653/v1/2020.acl-main.377", "CorpusId": 220046481}, "corpusId": 220046481, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8c082ccbc26d48b9d46f7ed308afeb6213287437", "title": "Exact yet Efficient Graph Parsing, Bi-directional Locality and the Constructivist Hypothesis", "abstract": "A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar. We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars (HRGs). The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules. In particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a subclass of HRG, i.e. what we call Weakly Regular Graph Grammar, and 3) distributing argument-structures to both lexical and phrasal rules.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.377.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4100-4110"}, "authors": [{"authorId": "47107527", "name": "Yajie Ye"}, {"authorId": "144780931", "name": "Weiwei SUN"}]}, {"paperId": "af7a5099588b009b0955e15c79113025224f6078", "externalIds": {"MAG": "3035231850", "DBLP": "conf/acl/StanojevicS20", "ACL": "2020.acl-main.378", "DOI": "10.18653/v1/2020.acl-main.378", "CorpusId": 220047819}, "corpusId": 220047819, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/af7a5099588b009b0955e15c79113025224f6078", "title": "Max-Margin Incremental CCG Parsing", "abstract": "Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank. We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias. While known techniques for tackling these biases improve results, they still do not make the parser state of the art. Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation. The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 72, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.378.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-10", "journal": {"pages": "4111-4122"}, "authors": [{"authorId": "2741066", "name": "Milo\u0161 Stanojevi\u0107"}, {"authorId": "145332819", "name": "Mark Steedman"}]}, {"paperId": "d7b1c561711458e670c0281fc512badf5060fb32", "externalIds": {"DBLP": "conf/acl/DoR20", "ACL": "2020.acl-main.379", "MAG": "3035198952", "DOI": "10.18653/v1/2020.acl-main.379", "CorpusId": 220047365}, "corpusId": 220047365, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d7b1c561711458e670c0281fc512badf5060fb32", "title": "Neural Reranking for Dependency Parsing: An Evaluation", "abstract": "Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser. However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configurational word order and poor morphology. In the paper, we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich(er) languages, German and Czech. In addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (GCNs). We show that the GCN not only outperforms previous models on English but is the only model that is able to improve results over the baselines on German and Czech. We explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.379.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4123-4133"}, "authors": [{"authorId": "36984159", "name": "Bich-Ngoc Do"}, {"authorId": "3019312", "name": "Ines Rehbein"}]}, {"paperId": "35058bebfde0534b2e871e5d2cc3cd30b2d2e36b", "externalIds": {"MAG": "3034224415", "DBLP": "conf/acl/ZhangBZBZZ20", "ArXiv": "2004.14088", "ACL": "2020.acl-main.380", "DOI": "10.18653/v1/2020.acl-main.380", "CorpusId": 216641659}, "corpusId": 216641659, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/35058bebfde0534b2e871e5d2cc3cd30b2d2e36b", "title": "Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting", "abstract": "With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., \u201cgay\u201d, \u201cblack\u201d) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like \u201cShe makes me happy to be gay\u201d as abusive simply because of the word \u201cgay.\u201d In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models\u2019 generalization ability.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 50, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.380.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "4134-4145"}, "authors": [{"authorId": "46266569", "name": "Guanhua Zhang"}, {"authorId": "144490441", "name": "Bing Bai"}, {"authorId": "94236296", "name": "Junqi Zhang"}, {"authorId": "1380011779", "name": "Kun Bai"}, {"authorId": "2675365", "name": "Conghui Zhu"}, {"authorId": "145382463", "name": "T. Zhao"}]}, {"paperId": "92ee4ef1c648c8c18eaf1f399ea3495a2bff7634", "externalIds": {"DBLP": "conf/acl/ChrupalaHA20", "ACL": "2020.acl-main.381", "ArXiv": "2004.07070", "MAG": "3035750922", "DOI": "10.18653/v1/2020.acl-main.381", "CorpusId": 209175941}, "corpusId": 209175941, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/92ee4ef1c648c8c18eaf1f399ea3495a2bff7634", "title": "Analyzing analytical methods: The case of phonology in neural models of spoken language", "abstract": "Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method. As a step in this direction we study the case of representations of phonology in neural network models of spoken language. We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences. We manipulate two factors that can affect the outcome of analysis. First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models. Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance. We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.381.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-15", "journal": {"pages": "4146-4156"}, "authors": [{"authorId": "2756960", "name": "Grzegorz Chrupa\u0142a"}, {"authorId": "8564290", "name": "Bertrand Higy"}, {"authorId": "103538973", "name": "A. Alishahi"}]}, {"paperId": "4338266891240f968b3968cf7727fed394bae3ce", "externalIds": {"MAG": "2979532866", "ArXiv": "1910.03065", "ACL": "2020.acl-main.382", "DBLP": "conf/acl/CamburuSMLB20", "DOI": "10.18653/v1/2020.acl-main.382", "CorpusId": 203905467}, "corpusId": 203905467, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4338266891240f968b3968cf7727fed394bae3ce", "title": "Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations", "abstract": "To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as \u201dBecause there is a dog in the image.\u201d and \u201dBecause there is no dog in the [same] image.\u201d, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 38, "citationCount": 68, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.382.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-07", "journal": {"pages": "4157-4165"}, "authors": [{"authorId": "3317152", "name": "Oana-Maria Camburu"}, {"authorId": "3144580", "name": "Brendan Shillingford"}, {"authorId": "3051815", "name": "Pasquale Minervini"}, {"authorId": "1690572", "name": "Thomas Lukasiewicz"}, {"authorId": "1685771", "name": "Phil Blunsom"}]}, {"paperId": "3aaa8aaad5ef36550a6b47d6ee000f0b346a5a1f", "externalIds": {"DBLP": "conf/acl/WuCKL20", "ArXiv": "2004.14786", "ACL": "2020.acl-main.383", "MAG": "3034503989", "DOI": "10.18653/v1/2020.acl-main.383", "CorpusId": 216914626}, "corpusId": 216914626, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3aaa8aaad5ef36550a6b47d6ee000f0b346a5a1f", "title": "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT", "abstract": "By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 57, "citationCount": 127, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.383.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"pages": "4166-4176"}, "authors": [{"authorId": "150358371", "name": "Zhiyong Wu"}, {"authorId": "47558643", "name": "Yun Chen"}, {"authorId": "145868757", "name": "B. Kao"}, {"authorId": "1688015", "name": "Qun Liu"}]}, {"paperId": "bb429a17280c2df86ac34789df880a4f728009ae", "externalIds": {"DBLP": "conf/acl/SorodocGB20", "ACL": "2020.acl-main.384", "MAG": "3034685497", "DOI": "10.18653/v1/2020.acl-main.384", "CorpusId": 220047937}, "corpusId": 220047937, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bb429a17280c2df86ac34789df880a4f728009ae", "title": "Probing for Referential Information in Language Models", "abstract": "Language models keep track of complex information about the preceding context \u2013 including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. However, we find traces of the latter aspect, too.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 25, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4177-4189"}, "authors": [{"authorId": "1947259", "name": "Ionut-Teodor Sorodoc"}, {"authorId": "2807281", "name": "Kristina Gulordava"}, {"authorId": "1807810", "name": "Gemma Boleda"}]}, {"paperId": "76a9f336481b39515d6cea2920696f11fb686451", "externalIds": {"MAG": "3022265721", "ACL": "2020.acl-main.385", "ArXiv": "2005.00928", "DBLP": "conf/acl/AbnarZ20", "DOI": "10.18653/v1/2020.acl-main.385", "CorpusId": 218487351}, "corpusId": 218487351, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/76a9f336481b39515d6cea2920696f11fb686451", "title": "Quantifying Attention Flow in Transformers", "abstract": "In the Transformer model, \u201cself-attention\u201d combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 412, "influentialCitationCount": 45, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.385.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "4190-4197"}, "authors": [{"authorId": "2786352", "name": "Samira Abnar"}, {"authorId": "83390207", "name": "W. Zuidema"}]}, {"paperId": "579476d19566efc842929ea6bdd18ab760c8cfa2", "externalIds": {"MAG": "3015575765", "DBLP": "journals/corr/abs-2004-03685", "ACL": "2020.acl-main.386", "ArXiv": "2004.03685", "DOI": "10.18653/v1/2020.acl-main.386", "CorpusId": 215416110}, "corpusId": 215416110, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/579476d19566efc842929ea6bdd18ab760c8cfa2", "title": "Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?", "abstract": "With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is \u201cdefined\u201d by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 363, "influentialCitationCount": 38, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.386.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-04-07", "journal": {"pages": "4198-4205"}, "authors": [{"authorId": "41016275", "name": "Alon Jacovi"}, {"authorId": "79775260", "name": "Yoav Goldberg"}]}, {"paperId": "3d61a34611c6171f203286119f76ec52f8016580", "externalIds": {"ACL": "2020.acl-main.387", "MAG": "3021385998", "ArXiv": "2004.14243", "DBLP": "conf/acl/MohankumarNNKSR20", "DOI": "10.18653/v1/2020.acl-main.387", "CorpusId": 216641945}, "corpusId": 216641945, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3d61a34611c6171f203286119f76ec52f8016580", "title": "Towards Transparent and Explainable Attention Models", "abstract": "Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model\u2019s predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model\u2019s prediction. They can be considered a plausible explanation if they provide a human-understandable justification for the model\u2019s predictions. In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model\u2019s predictions. We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model\u2019s predictions. Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model\u2019s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions. To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model\u2019s predictions (iii) correlate better with gradient-based attribution methods. Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model\u2019s predictions. Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 37, "citationCount": 74, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.387.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-29", "journal": {"pages": "4206-4216"}, "authors": [{"authorId": "1389549528", "name": "Akash Kumar Mohankumar"}, {"authorId": "9192775", "name": "Preksha Nema"}, {"authorId": "1516227958", "name": "Sharan Narasimhan"}, {"authorId": "2361078", "name": "Mitesh M. Khapra"}, {"authorId": "2881425", "name": "Balaji Vasan Srinivasan"}, {"authorId": "1723632", "name": "Balaraman Ravindran"}]}, {"paperId": "71b554589d131880170f1c96c68bf3b69e014eb1", "externalIds": {"ACL": "2020.acl-main.388", "MAG": "3035596036", "DBLP": "conf/acl/MaoYLD20", "DOI": "10.18653/v1/2020.acl-main.388", "CorpusId": 220047321}, "corpusId": 220047321, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/71b554589d131880170f1c96c68bf3b69e014eb1", "title": "Tchebycheff Procedure for Multi-task Text Classification", "abstract": "Multi-task Learning methods have achieved great progress in text classification. However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications. To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption. The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 16, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.388.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4217-4226"}, "authors": [{"authorId": "1753922779", "name": "Yuren Mao"}, {"authorId": "2072547504", "name": "Shuang Yun"}, {"authorId": "2155130238", "name": "Weiwei Liu"}, {"authorId": "2106680651", "name": "Bo Du"}]}, {"paperId": "1e0a14db59c5a1a18c83dfbeb17eb5be9e67623d", "externalIds": {"DBLP": "conf/acl/MarcoF20", "ACL": "2020.acl-main.389", "MAG": "3035669818", "DOI": "10.18653/v1/2020.acl-main.389", "CorpusId": 218595134}, "corpusId": 218595134, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1e0a14db59c5a1a18c83dfbeb17eb5be9e67623d", "title": "Modeling Word Formation in English\u2013German Neural Machine Translation", "abstract": "This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology. Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation. The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 20, "citationCount": 10, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4227-4232"}, "authors": [{"authorId": "26388316", "name": "Marion Weller-Di Marco"}, {"authorId": "2277248", "name": "Alexander M. Fraser"}]}, {"paperId": "5333cf58dc9c4e43725e49ae90433842f9ffb53c", "externalIds": {"MAG": "3035433211", "DBLP": "journals/corr/abs-2005-04470", "ArXiv": "2005.04470", "ACL": "2020.acl-main.390", "DOI": "10.18653/v1/2020.acl-main.390", "CorpusId": 218581653}, "corpusId": 218581653, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5333cf58dc9c4e43725e49ae90433842f9ffb53c", "title": "Empowering Active Learning to Jointly Optimize System and User Demands", "abstract": "Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances). We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills. We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 6, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.390.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-09", "journal": {"name": "ArXiv", "volume": "abs/2005.04470"}, "authors": [{"authorId": "150151160", "name": "Ji-Ung Lee"}, {"authorId": "145575695", "name": "Christian M. Meyer"}, {"authorId": "1730400", "name": "Iryna Gurevych"}]}, {"paperId": "5714c48293bf77e399fd1ba8ad62e74e252fdb9e", "externalIds": {"ArXiv": "2005.00987", "MAG": "3113146152", "DBLP": "journals/corr/abs-2005-00987", "ACL": "2020.acl-main.391", "DOI": "10.18653/v1/2020.acl-main.391", "CorpusId": 218487230}, "corpusId": 218487230, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5714c48293bf77e399fd1ba8ad62e74e252fdb9e", "title": "Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction", "abstract": "This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC. For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods. Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 108, "influentialCitationCount": 14, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"name": "ArXiv", "volume": "abs/2005.00987"}, "authors": [{"authorId": "143655216", "name": "Masahiro Kaneko"}, {"authorId": "35643168", "name": "Masato Mita"}, {"authorId": "32140786", "name": "Shun Kiyono"}, {"authorId": "144042991", "name": "Jun Suzuki"}, {"authorId": "3040648", "name": "Kentaro Inui"}]}, {"paperId": "1eed0659d561354d5c471a723cf3381430561d04", "externalIds": {"MAG": "3034414418", "ACL": "2020.acl-main.392", "DBLP": "conf/acl/HuXLYSDXZ20", "DOI": "10.18653/v1/2020.acl-main.392", "CorpusId": 220050047}, "corpusId": 220050047, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1eed0659d561354d5c471a723cf3381430561d04", "title": "Graph Neural News Recommendation with Unsupervised Preference Disentanglement", "abstract": "With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents. Most existing methods usually learn the representations of users and news from news contents for recommendation. However, they seldom consider high-order connectivity underlying the user-news interactions. Moreover, existing methods failed to disentangle a user\u2019s latent preference factors which cause her clicks on different news. In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD. Our model can encode high-order relationships into user and news representations by information propagation along the graph. Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability. A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations. Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 97, "influentialCitationCount": 5, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-25", "journal": {"pages": "4255-4264"}, "authors": [{"authorId": "1771202", "name": "Linmei Hu"}, {"authorId": "74839268", "name": "Siyong Xu"}, {"authorId": "2116520703", "name": "Chen Li"}, {"authorId": "3443627", "name": "Cheng Yang"}, {"authorId": "144123161", "name": "C. Shi"}, {"authorId": "46429989", "name": "Nan Duan"}, {"authorId": "144076239", "name": "Xing Xie"}, {"authorId": "143849609", "name": "M. Zhou"}]}, {"paperId": "fdbde15bd93395a166ad6cb914105dc47817ad91", "externalIds": {"ACL": "2020.acl-main.393", "DBLP": "conf/acl/HuLC20", "MAG": "3035159419", "DOI": "10.18653/v1/2020.acl-main.393", "CorpusId": 220045868}, "corpusId": 220045868, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fdbde15bd93395a166ad6cb914105dc47817ad91", "title": "Identifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description", "abstract": "In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case. We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story. We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework. Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants\u2019 impacts in a complex case.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 21, "citationCount": 1, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4265-4269"}, "authors": [{"authorId": "1797487", "name": "Yakun Hu"}, {"authorId": "2730687", "name": "Zhunchen Luo"}, {"authorId": "7277241", "name": "Wen-Han Chao"}]}, {"paperId": "ad21aa6e4e0d70ac093ff9071d6fb832507c8f47", "externalIds": {"MAG": "3035685359", "DBLP": "conf/acl/RajamanickamMYS20", "ACL": "2020.acl-main.394", "ArXiv": "2005.14028", "DOI": "10.18653/v1/2020.acl-main.394", "CorpusId": 218971633}, "corpusId": 218971633, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ad21aa6e4e0d70ac093ff9071d6fb832507c8f47", "title": "Joint Modelling of Emotion and Abusive Language Detection", "abstract": "The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 44, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.394.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-03", "journal": {"name": "ArXiv", "volume": "abs/2005.14028"}, "authors": [{"authorId": "1723418777", "name": "S. Rajamanickam"}, {"authorId": "3047561", "name": "Pushkar Mishra"}, {"authorId": "2169553", "name": "H. Yannakoudakis"}, {"authorId": "2362276", "name": "Ekaterina Shutova"}]}, {"paperId": "89fdb0555f6643b18c0094bcce66eaea701bef85", "externalIds": {"DBLP": "conf/acl/WeigeltSHT20", "ACL": "2020.acl-main.395", "MAG": "3035183089", "DOI": "10.18653/v1/2020.acl-main.395", "CorpusId": 220048457}, "corpusId": 220048457, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/89fdb0555f6643b18c0094bcce66eaea701bef85", "title": "Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding", "abstract": "The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 49, "citationCount": 7, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.395.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "117-118"}, "authors": [{"authorId": "2057575768", "name": "Sebastian Weigelt"}, {"authorId": "40914900", "name": "Vanessa Steurer"}, {"authorId": "47507501", "name": "Tobias Hey"}, {"authorId": "1679754", "name": "W. Tichy"}]}, {"paperId": "8c72389d79157324271b9bd13d61e7dcad22e382", "externalIds": {"MAG": "3032574806", "ArXiv": "2006.00998", "DBLP": "journals/corr/abs-2006-00998", "ACL": "2020.acl-main.396", "DOI": "10.18653/v1/2020.acl-main.396", "CorpusId": 219176615}, "corpusId": 219176615, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8c72389d79157324271b9bd13d61e7dcad22e382", "title": "Toxicity Detection: Does Context Really Matter?", "abstract": "Moderation is crucial to promoting healthy online discussions. Although several \u2018toxicity\u2019 detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently. We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title. We find that context can both amplify or mitigate the perceived toxicity of posts. Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context. Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware. This points to the need for larger datasets of comments annotated in context. We make our code and data publicly available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 38, "citationCount": 94, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.396.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-01", "journal": {"pages": "4296-4305"}, "authors": [{"authorId": "2332587", "name": "John Pavlopoulos"}, {"authorId": "144431938", "name": "Jeffrey Scott Sorensen"}, {"authorId": "2065639113", "name": "Lucas Dixon"}, {"authorId": "2665391", "name": "Nithum Thain"}, {"authorId": "1752430", "name": "Ion Androutsopoulos"}]}, {"paperId": "2f455f19260d1243cd03afd641450f5fae31066d", "externalIds": {"MAG": "3035169973", "DBLP": "conf/acl/ZhouZJT20", "ACL": "2020.acl-main.397", "DOI": "10.18653/v1/2020.acl-main.397", "CorpusId": 220048573}, "corpusId": 220048573, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2f455f19260d1243cd03afd641450f5fae31066d", "title": "AMR Parsing with Latent Structural Information", "abstract": "Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences. We investigate parsing AMR with explicit dependency structures and interpretable latent structures. We generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks. The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 (77.5% Smatch F1 on LDC2017T10) and AMR 1.0 ((71.8% Smatch F1 on LDC2014T12).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 61, "citationCount": 24, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.397.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4306-4319"}, "authors": [{"authorId": "92758499", "name": "Qiji Zhou"}, {"authorId": "1591125925", "name": "Yue Zhang"}, {"authorId": "1719916", "name": "D. Ji"}, {"authorId": "2112389014", "name": "Hao Tang"}]}, {"paperId": "52cb05d721688cb766c6e282e9d55c3b8e3dc0cf", "externalIds": {"DBLP": "journals/corr/abs-2004-02349", "ArXiv": "2004.02349", "MAG": "3014265582", "ACL": "2020.acl-main.398", "DOI": "10.18653/v1/2020.acl-main.398", "CorpusId": 214802901}, "corpusId": 214802901, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/52cb05d721688cb766c6e282e9d55c3b8e3dc0cf", "title": "TaPas: Weakly Supervised Table Parsing via Pre-training", "abstract": "Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT\u2019s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 47, "citationCount": 378, "influentialCitationCount": 102, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.398.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-05", "journal": {"pages": "4320-4333"}, "authors": [{"authorId": "47426264", "name": "Jonathan Herzig"}, {"authorId": "5274550", "name": "Pawel Krzysztof Nowak"}, {"authorId": "40150608", "name": "Thomas M\u00fcller"}, {"authorId": "2174596", "name": "Francesco Piccinno"}, {"authorId": "117595858", "name": "Julian Martin Eisenschlos"}]}, {"paperId": "356466a042a763de6cff0fbaa3ceaa6ac65b3d80", "externalIds": {"MAG": "3022758713", "DBLP": "conf/acl/AlshomarySPW20", "ACL": "2020.acl-main.399", "DOI": "10.18653/V1/2020.ACL-MAIN.399", "CorpusId": 219065800}, "corpusId": 219065800, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/356466a042a763de6cff0fbaa3ceaa6ac65b3d80", "title": "Target Inference in Argument Conclusion Generation", "abstract": "In argumentation, people state premises to reason towards a conclusion. The conclusion conveys a stance towards some target, such as a concept or statement. Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons. However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation. We thus study the question to what extent an argument\u2019s conclusion can be reconstructed from its premises. In particular, we argue here that a decisive step is to infer a conclusion\u2019s target, and we hypothesize that this target is related to the premises\u2019 targets. We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network. Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines. According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 42, "citationCount": 21, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.399.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4334-4345"}, "authors": [{"authorId": "2300829", "name": "Milad Alshomary"}, {"authorId": "18417916", "name": "S. Syed"}, {"authorId": "3046200", "name": "Martin Potthast"}, {"authorId": "2626599", "name": "Henning Wachsmuth"}]}, {"paperId": "64a0c241095faffdba00cbab90ce385b6db41b06", "externalIds": {"MAG": "3034773362", "ACL": "2020.acl-main.400", "DBLP": "conf/acl/YaoW20", "DOI": "10.18653/v1/2020.acl-main.400", "CorpusId": 220045418}, "corpusId": 220045418, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/64a0c241095faffdba00cbab90ce385b6db41b06", "title": "Multimodal Transformer for Multimodal Machine Translation", "abstract": "Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 17, "citationCount": 94, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.400.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4346-4350"}, "authors": [{"authorId": "47188971", "name": "Shaowei Yao"}, {"authorId": "145078589", "name": "Xiaojun Wan"}]}, {"paperId": "571c9d6d3404a59042ee08c85fd07f41eb213913", "externalIds": {"DBLP": "conf/acl/ChauhanREB20", "ACL": "2020.acl-main.401", "MAG": "3035177206", "DOI": "10.18653/v1/2020.acl-main.401", "CorpusId": 220044878}, "corpusId": 220044878, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/571c9d6d3404a59042ee08c85fd07f41eb213913", "title": "Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis", "abstract": "In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario. We, at first, manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes, both implicit and explicit. For multi-tasking, we propose two attention mechanisms, viz. Inter-segment Inter-modal Attention (Ie-Attention) and Intra-segment Inter-modal Attention (Ia-Attention). The main motivation of Ie-Attention is to learn the relationship between the different segments of the sentence across the modalities. In contrast, Ia-Attention focuses within the same segment of the sentence across the modalities. Finally, representations from both the attentions are concatenated and shared across the five classes (i.e., sarcasm, implicit sentiment, explicit sentiment, implicit emotion, explicit emotion) for multi-tasking. Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems. The evaluation also shows that the proposed multi-task framework yields better performance for the primary task, i.e., sarcasm detection, with the help of two secondary tasks, emotion and sentiment analysis.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 12, "citationCount": 82, "influentialCitationCount": 9, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4351-4360"}, "authors": [{"authorId": "145036777", "name": "Dushyant Singh Chauhan"}, {"authorId": "1754231407", "name": "R. DhanushS."}, {"authorId": "1734904", "name": "Asif Ekbal"}, {"authorId": "145532184", "name": "P. Bhattacharyya"}]}, {"paperId": "bff9b22a96a49a35edd10f6a1cec04694cfdc79e", "externalIds": {"ACL": "2020.acl-main.402", "MAG": "3035473672", "DBLP": "conf/acl/SahaPSB20", "DOI": "10.18653/v1/2020.acl-main.402", "CorpusId": 220047267}, "corpusId": 220047267, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/bff9b22a96a49a35edd10f6a1cec04694cfdc79e", "title": "Towards Emotion-aided Multi-modal Dialogue Act Classification", "abstract": "The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of both multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 56, "citationCount": 43, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.402.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4361-4372"}, "authors": [{"authorId": "52219377", "name": "Tulika Saha"}, {"authorId": "46561390", "name": "Aditya Patra"}, {"authorId": "145470045", "name": "S. Saha"}, {"authorId": "145532184", "name": "P. Bhattacharyya"}]}, {"paperId": "dd07c26973bb7b2d16ce47e1aa4ad2da55136cd3", "externalIds": {"ACL": "2020.acl-main.403", "MAG": "3034761782", "DBLP": "journals/corr/abs-2004-13878", "ArXiv": "2004.13878", "DOI": "10.18653/v1/2020.acl-main.403", "CorpusId": 216641617}, "corpusId": 216641617, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dd07c26973bb7b2d16ce47e1aa4ad2da55136cd3", "title": "Analyzing Political Parody in Social Media", "abstract": "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 68, "citationCount": 16, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.403.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Political Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.13878"}, "authors": [{"authorId": "1661214231", "name": "Antonis Maronikolakis"}, {"authorId": "1661213189", "name": "Danae S\u00e1nchez Villegas"}, {"authorId": "1398830377", "name": "Daniel Preotiuc-Pietro"}, {"authorId": "3238627", "name": "Nikolaos Aletras"}]}, {"paperId": "756e84448d2b21e0aee58840dc2872fd359a5c7d", "externalIds": {"ACL": "2020.acl-main.404", "DBLP": "conf/acl/DayanikP20", "MAG": "3035658430", "DOI": "10.18653/v1/2020.acl-main.404", "CorpusId": 220047861}, "corpusId": 220047861, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/756e84448d2b21e0aee58840dc2872fd359a5c7d", "title": "Masking Actor Information Leads to Fairer Political Claims Detection", "abstract": "A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties. We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better. We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias. We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 28, "citationCount": 18, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.404.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4385-4391"}, "authors": [{"authorId": "46185703", "name": "Erenay Dayanik"}, {"authorId": "1708581", "name": "Sebastian Pad\u00f3"}]}, {"paperId": "176420f394014d9e67d5cff1e1d430541fa0f55f", "externalIds": {"DBLP": "journals/corr/abs-2004-12043", "MAG": "3034183707", "ArXiv": "2004.12043", "ACL": "2020.acl-main.405", "DOI": "10.18653/v1/2020.acl-main.405", "CorpusId": 216552915}, "corpusId": 216552915, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/176420f394014d9e67d5cff1e1d430541fa0f55f", "title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "abstract": "Social biases are encoded in word embeddings. This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 55, "citationCount": 23, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.405.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-04-25", "journal": {"pages": "4392-4415"}, "authors": [{"authorId": "3306556", "name": "K. Joseph"}, {"authorId": "40395376", "name": "Jonathan H. Morgan"}]}, {"paperId": "85b49a68f2471022c2e17c7c57637c1b34b8f163", "externalIds": {"MAG": "3035487250", "ACL": "2020.acl-main.406", "DBLP": "conf/acl/ZhangIR20", "DOI": "10.18653/v1/2020.acl-main.406", "CorpusId": 220045832}, "corpusId": 220045832, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/85b49a68f2471022c2e17c7c57637c1b34b8f163", "title": "\u201cWho said it, and Why?\u201d Provenance for Natural Language Claims", "abstract": "In an era where generating content and publishing it is so easy, we are bombarded with information and are exposed to all kinds of claims, some of which do not always rank high on the truth scale. This paper suggests that the key to a longer-term, holistic, and systematic approach to navigating this information pollution is capturing the provenance of claims. To do that, we develop a formal definition of provenance graph for a given natural language claim, aiming to understand where the claim may come from and how it has evolved. To construct the graph, we model provenance inference, formulated mainly as an information extraction task and addressed via a textual entailment model. We evaluate our approach using two benchmark datasets, showing initial success in capturing the notion of provenance and its effectiveness on the application of claim verification.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 11, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4416-4426"}, "authors": [{"authorId": "46867002", "name": "Yi Zhang"}, {"authorId": "1804315", "name": "Z. Ives"}, {"authorId": "144590225", "name": "D. Roth"}]}, {"paperId": "d70af4990cba2574c41b1235030f7a5b702e2d70", "externalIds": {"MAG": "3034622111", "ArXiv": "2004.09124", "DBLP": "conf/acl/ChaabouniKBDB20", "ACL": "2020.acl-main.407", "DOI": "10.18653/v1/2020.acl-main.407", "CorpusId": 215827766}, "corpusId": 215827766, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d70af4990cba2574c41b1235030f7a5b702e2d70", "title": "Compositionality and Generalization In Emergent Languages", "abstract": "Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 92, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.09124", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Education", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "4427-4442"}, "authors": [{"authorId": "1706980", "name": "Rahma Chaabouni"}, {"authorId": "144875326", "name": "E. Kharitonov"}, {"authorId": "3365029", "name": "Diane Bouchacourt"}, {"authorId": "2202008", "name": "Emmanuel Dupoux"}, {"authorId": "145283199", "name": "Marco Baroni"}]}, {"paperId": "087dd95e13efd47aef2a6582e6801b39fc0f83d8", "externalIds": {"DBLP": "journals/corr/abs-1911-03429", "ArXiv": "1911.03429", "MAG": "3035503910", "ACL": "2020.acl-main.408", "DOI": "10.18653/v1/2020.acl-main.408", "CorpusId": 207847663}, "corpusId": 207847663, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/087dd95e13efd47aef2a6582e6801b39fc0f83d8", "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models", "abstract": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the \u2018reasoning\u2019 behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of \u201crationales\u201d (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 76, "citationCount": 434, "influentialCitationCount": 123, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.408.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-08", "journal": {"pages": "4443-4458"}, "authors": [{"authorId": "48727916", "name": "Jay DeYoung"}, {"authorId": "49837811", "name": "Sarthak Jain"}, {"authorId": "8937909", "name": "Nazneen Rajani"}, {"authorId": "51172373", "name": "Eric P. Lehman"}, {"authorId": "2228109", "name": "Caiming Xiong"}, {"authorId": "2166511", "name": "R. Socher"}, {"authorId": "1912476", "name": "Byron C. Wallace"}]}, {"paperId": "922e6e3bafe38a712597c05d3a907bd10763b427", "externalIds": {"DBLP": "conf/acl/JainWPW20", "ACL": "2020.acl-main.409", "ArXiv": "2005.00115", "MAG": "3023638010", "DOI": "10.18653/v1/2020.acl-main.409", "CorpusId": 218470359}, "corpusId": 218470359, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/922e6e3bafe38a712597c05d3a907bd10763b427", "title": "Learning to Faithfully Rationalize by Construction", "abstract": "In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text \u2018responsible for\u2019 corresponding model output; when such a snippet comprises tokens that indeed informed the model\u2019s prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to \u2018end-to-end\u2019 approaches, while being more general and easier to train. Code is available at https://github.com/successar/FRESH.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 48, "citationCount": 132, "influentialCitationCount": 35, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.00115", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"pages": "4459-4473"}, "authors": [{"authorId": "49837811", "name": "Sarthak Jain"}, {"authorId": "35823986", "name": "Sarah Wiegreffe"}, {"authorId": "1826312", "name": "Yuval Pinter"}, {"authorId": "1912476", "name": "Byron C. Wallace"}]}, {"paperId": "ec51c9be66fef4321e45a4904c7368287ec0321c", "externalIds": {"ACL": "2020.acl-main.410", "MAG": "3035129496", "DBLP": "conf/acl/YueGS20", "ArXiv": "2005.00574", "DOI": "10.18653/v1/2020.acl-main.410", "CorpusId": 218486765}, "corpusId": 218486765, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ec51c9be66fef4321e45a4904c7368287ec0321c", "title": "Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset", "abstract": "Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. Recently, Pampari et al. (EMNLP\u201918) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes. In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC) task. From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge. From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert\u2019s performance, and (v) BERT models do not beat the best performing base model. Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts. We argue that both should be considered when creating future datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 40, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.410.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00574"}, "authors": [{"authorId": "145548079", "name": "Xiang Yue"}, {"authorId": "1666169546", "name": "Bernal Jimenez Gutierrez"}, {"authorId": "1515546612", "name": "Huan Sun"}]}, {"paperId": "851de6751aef4128d7feb7c6ca36b180a0e0835e", "externalIds": {"MAG": "3022934659", "ACL": "2020.acl-main.411", "DBLP": "conf/acl/CaoTBB20", "ArXiv": "2005.00697", "DOI": "10.18653/v1/2020.acl-main.411", "CorpusId": 218487151}, "corpusId": 218487151, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/851de6751aef4128d7feb7c6ca36b180a0e0835e", "title": "DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering", "abstract": "Transformer-based QA models use input-wide self-attention \u2013 i.e. across both the question and the input passage \u2013 at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset. We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. We open source the code at https://github.com/StonyBrookNLP/deformer.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 52, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.411.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Physics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00697"}, "authors": [{"authorId": "31961604", "name": "Qingqing Cao"}, {"authorId": "6365809", "name": "H. Trivedi"}, {"authorId": "143859272", "name": "A. Balasubramanian"}, {"authorId": "35217367", "name": "Niranjan Balasubramanian"}]}, {"paperId": "0c06bd20403a204bcd95dae1f176c10894fe7139", "externalIds": {"ACL": "2020.acl-main.412", "DBLP": "conf/acl/SaxenaTT20", "MAG": "3034862985", "DOI": "10.18653/v1/2020.acl-main.412", "CorpusId": 220047862}, "corpusId": 220047862, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0c06bd20403a204bcd95dae1f176c10894fe7139", "title": "Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings", "abstract": "Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn\u2019t always readily available. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far. We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA\u2019s effectiveness over other state-of-the-art baselines.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 293, "influentialCitationCount": 51, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.412.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4498-4507"}, "authors": [{"authorId": "46961776", "name": "Apoorv Saxena"}, {"authorId": "19206251", "name": "Aditay Tripathi"}, {"authorId": "2408872", "name": "P. Talukdar"}]}, {"paperId": "192a952ccde4f1c7bb348161a124f772d0e6a867", "externalIds": {"MAG": "3034879633", "DBLP": "conf/acl/FabbriNWNX20", "ArXiv": "2004.11892", "ACL": "2020.acl-main.413", "DOI": "10.18653/v1/2020.acl-main.413", "CorpusId": 216144850}, "corpusId": 216144850, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/192a952ccde4f1c7bb348161a124f772d0e6a867", "title": "Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering", "abstract": "Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows. A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset. This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data. We propose an unsupervised approach to training QA models with generated pseudo-training data. We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships. Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving state-of-the-art performance on SQuAD for unsupervised QA.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 14, "citationCount": 62, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.413.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-24", "journal": {"pages": "4508-4513"}, "authors": [{"authorId": "22281632", "name": "A. R. Fabbri"}, {"authorId": "145878390", "name": "Patrick Ng"}, {"authorId": "40296541", "name": "Zhiguo Wang"}, {"authorId": "1701451", "name": "Ramesh Nallapati"}, {"authorId": "144028698", "name": "Bing Xiang"}]}, {"paperId": "dcb0249ba85140849a07e3cbae358ec3e9b89ac5", "externalIds": {"MAG": "3021762271", "ACL": "2020.acl-main.414", "ArXiv": "2005.01218", "DBLP": "journals/corr/abs-2005-01218", "DOI": "10.18653/v1/2020.acl-main.414", "CorpusId": 218487313}, "corpusId": 218487313, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/dcb0249ba85140849a07e3cbae358ec3e9b89ac5", "title": "Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering", "abstract": "Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 33, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.414.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.01218"}, "authors": [{"authorId": "143618944", "name": "Vikas Yadav"}, {"authorId": "2105138", "name": "Steven Bethard"}, {"authorId": "1760868", "name": "M. Surdeanu"}]}, {"paperId": "257146b025a7caa537e548e46a259bd3b08fd9a0", "externalIds": {"MAG": "3035051527", "ACL": "2020.acl-main.415", "ArXiv": "2005.13962", "DBLP": "journals/corr/abs-2005-13962", "DOI": "10.18653/v1/2020.acl-main.415", "CorpusId": 218971641}, "corpusId": 218971641, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/257146b025a7caa537e548e46a259bd3b08fd9a0", "title": "A Corpus for Large-Scale Phonetic Typology", "abstract": "A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available. We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings. Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 65, "citationCount": 17, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.415.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-28", "journal": {"pages": "4526-4546"}, "authors": [{"authorId": "3448427", "name": "Elizabeth Salesky"}, {"authorId": "3448883", "name": "Eleanor Chodroff"}, {"authorId": "1388571351", "name": "Tiago Pimentel"}, {"authorId": "1500652334", "name": "Matthew Wiesner"}, {"authorId": "1750769", "name": "Ryan Cotterell"}, {"authorId": "1690706", "name": "A. Black"}, {"authorId": "145043214", "name": "Jason Eisner"}]}, {"paperId": "cf1f69b38962f085a0628b46b44021dc86315fdc", "externalIds": {"ACL": "2020.acl-main.416", "MAG": "3034793162", "DBLP": "conf/acl/LiuCL20", "DOI": "10.18653/v1/2020.acl-main.416", "CorpusId": 219963636}, "corpusId": 219963636, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cf1f69b38962f085a0628b46b44021dc86315fdc", "title": "Dscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing", "abstract": "Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length. Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance. DRSs are typically visualized as boxes which are not straightforward to process automatically. Counter transforms DRSs to clauses and measures clause overlap by searching for variable mappings between two DRSs. However, this metric is computationally costly (with respect to memory and CPU time) and does not scale with longer texts. We introduce Dscorer, an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams. Experiments show that Dscorer computes accuracy scores that are correlated with Counter at a fraction of the time.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 24, "citationCount": 4, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4547-4554"}, "authors": [{"authorId": "2262374", "name": "Jiangming Liu"}, {"authorId": "40146204", "name": "Shay B. Cohen"}, {"authorId": "1747893", "name": "Mirella Lapata"}]}, {"paperId": "14fc61fdc8f2205ff96ff6dc9c4c881e4063db8c", "externalIds": {"DBLP": "conf/acl/BanonCHHHEFKKKO20", "MAG": "3035016936", "ACL": "2020.acl-main.417", "DOI": "10.18653/v1/2020.acl-main.417", "CorpusId": 219165306}, "corpusId": 219165306, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/14fc61fdc8f2205ff96ff6dc9c4c881e4063db8c", "title": "ParaCrawl: Web-Scale Acquisition of Parallel Corpora", "abstract": "We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 70, "citationCount": 151, "influentialCitationCount": 8, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.417.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4555-4567"}, "authors": [{"authorId": "1410107692", "name": "Marta Ba\u00f1\u00f3n"}, {"authorId": "143616669", "name": "Pinzhen Chen"}, {"authorId": "2259100", "name": "B. Haddow"}, {"authorId": "1702066", "name": "Kenneth Heafield"}, {"authorId": "143620134", "name": "Hieu D. Hoang"}, {"authorId": "1755370253", "name": "Miquel Espl\u00e0-GomisF"}, {"authorId": "1754834785", "name": "Mikel ForcadaF"}, {"authorId": "35132887", "name": "Amir Kamran"}, {"authorId": "1388337832", "name": "Faheem Kirefu"}, {"authorId": "49604675", "name": "Philipp Koehn"}, {"authorId": "1403862873", "name": "Sergio Ortiz-Rojas"}, {"authorId": "1754834841", "name": "Leopoldo PlaF"}, {"authorId": "1405306535", "name": "Gema Ram\u00edrez-S\u00e1nchez"}, {"authorId": "1755380287", "name": "Elsa Sarr\u0131\u0301asF"}, {"authorId": "1411425788", "name": "Marek St\u0159elec"}, {"authorId": "143992775", "name": "Brian Thompson"}, {"authorId": "145770915", "name": "W. Waites"}, {"authorId": "1755307026", "name": "Dion WigginsN"}, {"authorId": "1754834963", "name": "Jaume Zaragoza"}]}, {"paperId": "40a6e8d8f253882c585f163b7333842d60ed6f14", "externalIds": {"MAG": "3034515982", "ArXiv": "1910.13913", "DBLP": "journals/corr/abs-1910-13913", "ACL": "2020.acl-main.418", "DOI": "10.18653/v1/2020.acl-main.418", "CorpusId": 204961553}, "corpusId": 204961553, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/40a6e8d8f253882c585f163b7333842d60ed6f14", "title": "Toward Gender-Inclusive Coreference Resolution", "abstract": "Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 281, "citationCount": 100, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.418.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-30", "journal": {"name": "ArXiv", "volume": "abs/1910.13913"}, "authors": [{"authorId": "48696491", "name": "Yang Trista Cao"}, {"authorId": "1722360", "name": "Hal Daum\u00e9"}]}, {"paperId": "167b7dbbb1257e56c5b07c43ba57c44f93bdbcf1", "externalIds": {"ACL": "2020.acl-main.419", "DBLP": "conf/acl/SenHYKR20", "MAG": "3034444624", "DOI": "10.18653/v1/2020.acl-main.419", "CorpusId": 220046827}, "corpusId": 220046827, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/167b7dbbb1257e56c5b07c43ba57c44f93bdbcf1", "title": "Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?", "abstract": "Motivated by human attention, computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data. While attention mechanisms are claimed to achieve interpretability, little is known about the actual relationships between machine and human attention. In this work, we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task. To achieve this, we design and conduct a large-scale crowd-sourcing study to collect human attention maps that encode the parts of a text that humans focus on when conducting text classification. Based on this new resource of human attention dataset for text classification, YELP-HAT, collected on the publicly available YELP dataset, we perform a quantitative comparative analysis of machine attention maps created by deep learning models and human attention maps. Our analysis offers insights into the relationships between human versus machine attention maps along three dimensions: overlap in word selections, distribution over lexical categories, and context-dependency of sentiment polarity. Our findings open promising future research opportunities ranging from supervised attention to the design of human-centric attention-based explanations.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 38, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.419.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4596-4608"}, "authors": [{"authorId": "69520752", "name": "Cansu Sen"}, {"authorId": "32452740", "name": "Thomas Hartvigsen"}, {"authorId": "39250075", "name": "Biao Yin"}, {"authorId": "1833914", "name": "Xiangnan Kong"}, {"authorId": "66044002", "name": "E. Rundensteiner"}]}, {"paperId": "738c6d664aa6c3854e1aa894957bd595f621fc42", "externalIds": {"DBLP": "journals/corr/abs-2004-03061", "MAG": "3015449890", "ACL": "2020.acl-main.420", "ArXiv": "2004.03061", "DOI": "10.18653/v1/2020.acl-main.420", "CorpusId": 215238965}, "corpusId": 215238965, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/738c6d664aa6c3854e1aa894957bd595f621fc42", "title": "Information-Theoretic Probing for Linguistic Structure", "abstract": "The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually \u201cknow\u201d about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network\u2019s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research\u2014plus English\u2014totalling eleven languages. Our implementation is available in https://github.com/rycolab/info-theoretic-probing.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 157, "influentialCitationCount": 23, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.420.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-07", "journal": {"name": "ArXiv", "volume": "abs/2004.03061"}, "authors": [{"authorId": "1388571351", "name": "Tiago Pimentel"}, {"authorId": "51130686", "name": "Josef Valvoda"}, {"authorId": "1388061045", "name": "R. Maudslay"}, {"authorId": "51044403", "name": "Ran Zmigrod"}, {"authorId": "81840293", "name": "Adina Williams"}, {"authorId": "1750769", "name": "Ryan Cotterell"}]}, {"paperId": "9e9d919c1de684ca42c8b581ec62c7aa685f431e", "externalIds": {"MAG": "2982180741", "ACL": "2020.acl-main.421", "ArXiv": "1910.11856", "DBLP": "journals/corr/abs-1910-11856", "DOI": "10.18653/v1/2020.acl-main.421", "CorpusId": 204901567}, "corpusId": 204901567, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9e9d919c1de684ca42c8b581ec62c7aa685f431e", "title": "On the Cross-lingual Transferability of Monolingual Representations", "abstract": "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 52, "citationCount": 532, "influentialCitationCount": 111, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.421.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-25", "journal": {"pages": "4623-4637"}, "authors": [{"authorId": "2347956", "name": "Mikel Artetxe"}, {"authorId": "2884561", "name": "Sebastian Ruder"}, {"authorId": "1755465", "name": "Dani Yogatama"}]}, {"paperId": "ca4ecf116a9b97ce525a01f3f51117877688ddf5", "externalIds": {"MAG": "3034487470", "ArXiv": "2005.01172", "DBLP": "journals/corr/abs-2005-01172", "ACL": "2020.acl-main.422", "DOI": "10.18653/v1/2020.acl-main.422", "CorpusId": 218487587}, "corpusId": 218487587, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ca4ecf116a9b97ce525a01f3f51117877688ddf5", "title": "Similarity Analysis of Contextual Word Representation Models", "abstract": "This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 63, "citationCount": 47, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.422.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"name": "ArXiv", "volume": "abs/2005.01172"}, "authors": [{"authorId": "2111156163", "name": "John M. Wu"}, {"authorId": "2083259", "name": "Yonatan Belinkov"}, {"authorId": "145775792", "name": "Hassan Sajjad"}, {"authorId": "145938140", "name": "Nadir Durrani"}, {"authorId": "6415321", "name": "Fahim Dalvi"}, {"authorId": "145898106", "name": "James R. Glass"}]}, {"paperId": "772717eb2e369cd68c11b7da7aa779450dced9d0", "externalIds": {"ACL": "2020.acl-main.423", "DBLP": "conf/acl/LevineLDRPSSSS20", "MAG": "2968289784", "ArXiv": "1908.05646", "DOI": "10.18653/v1/2020.acl-main.423", "CorpusId": 199668663}, "corpusId": 199668663, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/772717eb2e369cd68c11b7da7aa779450dced9d0", "title": "SenseBERT: Driving Some Sense into BERT", "abstract": "The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content. This paper proposes a method to employ weak-supervision directly at the word sense level. Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses. Accordingly, we attain a lexical-semantic level language model, without the use of human annotation. SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the \u2018Word in Context\u2019 task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 43, "citationCount": 162, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.423.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-08-15", "journal": {"name": "ArXiv", "volume": "abs/1908.05646"}, "authors": [{"authorId": "152754428", "name": "Yoav Levine"}, {"authorId": "1412384990", "name": "Barak Lenz"}, {"authorId": "1751526108", "name": "O. Dagan"}, {"authorId": "73775461", "name": "Ori Ram"}, {"authorId": "102589705", "name": "Dan Padnos"}, {"authorId": "3074811", "name": "Or Sharir"}, {"authorId": "1389955537", "name": "S. Shalev-Shwartz"}, {"authorId": "3140335", "name": "A. Shashua"}, {"authorId": "1701353", "name": "Y. Shoham"}]}, {"paperId": "fe951b02cc6e04705923c1c91c5634578eb99546", "externalIds": {"ACL": "2020.acl-main.424", "ArXiv": "2005.00481", "MAG": "3034639488", "DBLP": "journals/corr/abs-2005-00481", "DOI": "10.18653/v1/2020.acl-main.424", "CorpusId": 218470237}, "corpusId": 218470237, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fe951b02cc6e04705923c1c91c5634578eb99546", "title": "ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations", "abstract": "In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 52, "citationCount": 94, "influentialCitationCount": 27, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.424.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "4668-4679"}, "authors": [{"authorId": "69930782", "name": "Fernando Alva-Manchego"}, {"authorId": "143792623", "name": "Louis Martin"}, {"authorId": "1713934", "name": "Antoine Bordes"}, {"authorId": "2797847", "name": "Carolina Scarton"}, {"authorId": "68990982", "name": "Beno\u00eet Sagot"}, {"authorId": "1702974", "name": "Lucia Specia"}]}, {"paperId": "36d7e8c618bbc5e59ba3d13d7cce7e94d829eea5", "externalIds": {"MAG": "3035391111", "DBLP": "conf/acl/CalabreseBN20", "ACL": "2020.acl-main.425", "DOI": "10.18653/v1/2020.acl-main.425", "CorpusId": 220045472}, "corpusId": 220045472, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/36d7e8c618bbc5e59ba3d13d7cce7e94d829eea5", "title": "Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts", "abstract": "Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet, multimodal language-vision research is in full bloom. However, events, feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets. Nevertheless, we would expect a wide-coverage language understanding system to be able to classify images depicting recess and remorse, not just cats, dogs and bridges. We fill this gap by presenting BabelPic, a hand-labeled dataset built by cleaning the image-synset association found within the BabelNet Lexical Knowledge Base (LKB). BabelPic explicitly targets non-concrete concepts, thus providing refreshing new data for the community. We also show that pre-trained language-vision systems can be used to further expand the resource by exploiting natural language knowledge available in the LKB. BabelPic is available for download at http://babelpic.org.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 12, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4680-4686"}, "authors": [{"authorId": "1490966079", "name": "Agostina Calabrese"}, {"authorId": "143802044", "name": "Michele Bevilacqua"}, {"authorId": "1733928", "name": "Roberto Navigli"}]}, {"paperId": "109c26ca53210f832459a86246d0885934856456", "externalIds": {"MAG": "3035048090", "ArXiv": "2006.05489", "DBLP": "conf/acl/GaonkarKBBC20", "ACL": "2020.acl-main.426", "DOI": "10.18653/v1/2020.acl-main.426", "CorpusId": 219559224}, "corpusId": 219559224, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/109c26ca53210f832459a86246d0885934856456", "title": "Modeling Label Semantics for Predicting Emotional Reactions", "abstract": "Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict. They ignore information that may be conveyed by the emotion labels themselves. We propose that the semantics of emotion labels can guide a model\u2019s attention when representing the input story. Further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness. In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference. We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data. Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-the-art on an emotion inference task.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 12, "citationCount": 23, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.426.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-09", "journal": {"name": "ArXiv", "volume": "abs/2006.05489"}, "authors": [{"authorId": "117962087", "name": "Radhika Gaonkar"}, {"authorId": "2050205", "name": "Heeyoung Kwon"}, {"authorId": "31924015", "name": "Mohaddeseh Bastan"}, {"authorId": "35217367", "name": "Niranjan Balasubramanian"}, {"authorId": "1729918", "name": "Nathanael Chambers"}]}, {"paperId": "908ccb603d61291f09e6826c54f179f468c31f00", "externalIds": {"ACL": "2020.acl-main.427", "MAG": "3034696093", "DBLP": "conf/acl/SrinetJGS20", "DOI": "10.18653/v1/2020.acl-main.427", "CorpusId": 220045830}, "corpusId": 220045830, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/908ccb603d61291f09e6826c54f179f468c31f00", "title": "CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant", "abstract": "We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game Minecraft. The dataset consists of 7K human utterances and their corresponding parses. Given proper world state, the parses can be interpreted and executed in game. We report the performance of baseline models, and analyze their successes and failures.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 47, "citationCount": 10, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.427.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4693-4714"}, "authors": [{"authorId": "27693639", "name": "Kavya Srinet"}, {"authorId": "2262249", "name": "Yacine Jernite"}, {"authorId": "2114735703", "name": "Jonathan Gray"}, {"authorId": "3149531", "name": "Arthur Szlam"}]}, {"paperId": "ec55f76812cacc12d29ec632d924377524a13022", "externalIds": {"DBLP": "conf/acl/LiRKWBCW20", "ArXiv": "1911.03860", "ACL": "2020.acl-main.428", "MAG": "2988615798", "DOI": "10.18653/v1/2020.acl-main.428", "CorpusId": 207853191}, "corpusId": 207853191, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ec55f76812cacc12d29ec632d924377524a13022", "title": "Don\u2019t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training", "abstract": "Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws.In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases. We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues. For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability. We demonstrate the efficacy of our approach across several dialogue tasks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 29, "citationCount": 140, "influentialCitationCount": 20, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.428.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-10", "journal": {"name": "ArXiv", "volume": "abs/1911.03860"}, "authors": [{"authorId": "6649233", "name": "Margaret Li"}, {"authorId": "3849208", "name": "Stephen Roller"}, {"authorId": "38729182", "name": "Ilia Kulikov"}, {"authorId": "2129663", "name": "S. Welleck"}, {"authorId": "90841478", "name": "Y-Lan Boureau"}, {"authorId": "1979489", "name": "Kyunghyun Cho"}, {"authorId": "145183709", "name": "J. Weston"}]}, {"paperId": "868349fe969bc7c6b14b5f35e118a26075b7b1f2", "externalIds": {"MAG": "3034779619", "DBLP": "conf/acl/ZhaoB20", "ACL": "2020.acl-main.429", "DOI": "10.18653/v1/2020.acl-main.429", "CorpusId": 220045413}, "corpusId": 220045413, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/868349fe969bc7c6b14b5f35e118a26075b7b1f2", "title": "How does BERT\u2019s attention change when you fine-tune? An analysis methodology and a case study in negation scope", "abstract": "Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 25, "citationCount": 30, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4729-4747"}, "authors": [{"authorId": "71078011", "name": "Yiyun Zhao"}, {"authorId": "2105138", "name": "Steven Bethard"}]}, {"paperId": "0339e72c61296d6ba505774c06e32788dd58de57", "externalIds": {"MAG": "3023756949", "DBLP": "journals/corr/abs-2005-01190", "ArXiv": "2005.01190", "ACL": "2020.acl-main.430", "DOI": "10.18653/v1/2020.acl-main.430", "CorpusId": 218487836}, "corpusId": 218487836, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0339e72c61296d6ba505774c06e32788dd58de57", "title": "Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models", "abstract": "LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks. Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English. Lacking this understanding, the generality of LSTM performance on this task and their suitability for related tasks remains uncertain. Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults. We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network. The approach refines the notion of influence (the subject\u2019s grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths. The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors). We exemplify the methodology on a widely-studied multi-layer LSTM language model, demonstrating its accounting for subject-verb number agreement. The results offer both a finer and a more complete view of an LSTM\u2019s handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 8, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.430.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"name": "ArXiv", "volume": "abs/2005.01190"}, "authors": [{"authorId": "51153248", "name": "Kaiji Lu"}, {"authorId": "3251561", "name": "Piotr (Peter) Mardziel"}, {"authorId": "35802340", "name": "Klas Leino"}, {"authorId": "2623167", "name": "Matt Fredrikson"}, {"authorId": "33374965", "name": "Anupam Datta"}]}, {"paperId": "d34580c522c79d5cde620331dd9ffb18643a8090", "externalIds": {"DBLP": "conf/acl/BommasaniDC20", "MAG": "3035102548", "ACL": "2020.acl-main.431", "DOI": "10.18653/v1/2020.acl-main.431", "CorpusId": 220046499}, "corpusId": 220046499, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d34580c522c79d5cde620331dd9ffb18643a8090", "title": "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings", "abstract": "Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings \u2014 while more diverse and mature than those available for their dynamic counterparts \u2014 are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 85, "citationCount": 112, "influentialCitationCount": 33, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.431.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4758-4781"}, "authors": [{"authorId": "150272855", "name": "Rishi Bommasani"}, {"authorId": "2111798539", "name": "Kelly Davis"}, {"authorId": "1748501", "name": "Claire Cardie"}]}, {"paperId": "cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a", "externalIds": {"ArXiv": "1909.07913", "MAG": "3035563045", "DBLP": "conf/acl/PruthiGDNL20", "ACL": "2020.acl-main.432", "DOI": "10.18653/v1/2020.acl-main.432", "CorpusId": 202583616}, "corpusId": 202583616, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a", "title": "Learning to Deceive with Attention-Based Explanations", "abstract": "Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention\u2019s reliability as a tool for auditing algorithms in the context of fairness and accountability.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 25, "citationCount": 162, "influentialCitationCount": 13, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.432.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-17", "journal": {"pages": "4782-4793"}, "authors": [{"authorId": "7880098", "name": "Danish Pruthi"}, {"authorId": "46722266", "name": "Mansi Gupta"}, {"authorId": "34994191", "name": "Bhuwan Dhingra"}, {"authorId": "1700325", "name": "Graham Neubig"}, {"authorId": "32219137", "name": "Zachary Chase Lipton"}]}, {"paperId": "d313e0d83267e4c8af0d7a4169e041c9abed9975", "externalIds": {"MAG": "3021244780", "DBLP": "journals/corr/abs-2005-00110", "ACL": "2020.acl-main.433", "ArXiv": "2005.00110", "DOI": "10.18653/v1/2020.acl-main.433", "CorpusId": 218470100}, "corpusId": 218470100, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d313e0d83267e4c8af0d7a4169e041c9abed9975", "title": "On the Spontaneous Emergence of Discrete and Compositional Signals", "abstract": "We propose a general framework to study language emergence through signaling games with neural agents. Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge. We explore whether categorical perception effects follow and show that the messages are not compositional.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 5, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.433.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-30", "journal": {"pages": "4794-4800"}, "authors": [{"authorId": "104450036", "name": "N. Lan"}, {"authorId": "2107195", "name": "E. Chemla"}, {"authorId": "1403904282", "name": "Shane Steinert-Threlkeld"}]}, {"paperId": "cae24695391e7ef8e7d351a8c922b4016fbfbd02", "externalIds": {"MAG": "3023806338", "ArXiv": "2005.01810", "ACL": "2020.acl-main.434", "DBLP": "conf/acl/KlafkaE20", "DOI": "10.18653/v1/2020.acl-main.434", "CorpusId": 218502143}, "corpusId": 218502143, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cae24695391e7ef8e7d351a8c922b4016fbfbd02", "title": "Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words", "abstract": "Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability\u2014but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance. We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 27, "citationCount": 33, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.434.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "4801-4811"}, "authors": [{"authorId": "1397147728", "name": "Josef Klafka"}, {"authorId": "37907837", "name": "Allyson Ettinger"}]}, {"paperId": "9a87c1f04bae4ab507ed0e03bfd10d870d733367", "externalIds": {"MAG": "3035167603", "ArXiv": "2005.06409", "ACL": "2020.acl-main.435", "DBLP": "journals/corr/abs-2005-06409", "DOI": "10.18653/v1/2020.acl-main.435", "CorpusId": 218614041}, "corpusId": 218614041, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9a87c1f04bae4ab507ed0e03bfd10d870d733367", "title": "Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA", "abstract": "Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 41, "citationCount": 28, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.435.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-13", "journal": {"pages": "4812-4822"}, "authors": [{"authorId": "51270689", "name": "Hyounghun Kim"}, {"authorId": "151270642", "name": "Zineng Tang"}, {"authorId": "143977268", "name": "Mohit Bansal"}]}, {"paperId": "17c0df95e608f91c2385e42c629a65f095988c10", "externalIds": {"MAG": "3035184646", "DBLP": "conf/acl/MuLG20", "ACL": "2020.acl-main.436", "ArXiv": "1911.02683", "DOI": "10.18653/V1/2020.ACL-MAIN.436", "CorpusId": 207847274}, "corpusId": 207847274, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/17c0df95e608f91c2385e42c629a65f095988c10", "title": "Shaping Visual Representations with Language for Few-Shot Classification", "abstract": "By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 43, "citationCount": 38, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.436.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-06", "journal": {"name": "ArXiv", "volume": "abs/1911.02683"}, "authors": [{"authorId": "24835910", "name": "Jesse Mu"}, {"authorId": "145419642", "name": "Percy Liang"}, {"authorId": "144002017", "name": "Noah D. Goodman"}]}, {"paperId": "9d276d08d68be880eb29f8d650cafa12371651c4", "externalIds": {"MAG": "3034299899", "DBLP": "conf/acl/JinWSL20", "ArXiv": "2006.06226", "ACL": "2020.acl-main.437", "DOI": "10.18653/v1/2020.acl-main.437", "CorpusId": 218914066}, "corpusId": 218914066, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9d276d08d68be880eb29f8d650cafa12371651c4", "title": "Discrete Latent Variable Representations for Low-Resource Text Classification", "abstract": "While much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient. We consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable. We compare the performance of the learned representations as features for low-resource document and sentence classification. Our best models outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations. Interestingly, we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 14, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.437.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-11", "journal": {"name": "ArXiv", "volume": "abs/2006.06226"}, "authors": [{"authorId": "46193777", "name": "Shuning Jin"}, {"authorId": "2844243", "name": "Sam Wiseman"}, {"authorId": "1714215", "name": "K. Stratos"}, {"authorId": "2924113", "name": "Karen Livescu"}]}, {"paperId": "0ba05a24b090435d0edd3865abdd70a9168290b4", "externalIds": {"MAG": "3035690155", "ACL": "2020.acl-main.438", "ArXiv": "2006.01209", "DBLP": "journals/corr/abs-2006-01209", "DOI": "10.18653/v1/2020.acl-main.438", "CorpusId": 219179670}, "corpusId": 219179670, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0ba05a24b090435d0edd3865abdd70a9168290b4", "title": "Learning Constraints for Structured Prediction Using Rectifier Networks", "abstract": "Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 6, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.438.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-23", "journal": {"name": "ArXiv", "volume": "abs/2006.01209"}, "authors": [{"authorId": "3265780", "name": "Xingyuan Pan"}, {"authorId": "41016174", "name": "Maitrey Mehta"}, {"authorId": "3052879", "name": "Vivek Srikumar"}]}, {"paperId": "a8c48ecd6aac3130f300345cb451c6ed68d2cc50", "externalIds": {"MAG": "3035027743", "DBLP": "journals/corr/abs-2005-10389", "ACL": "2020.acl-main.439", "ArXiv": "2005.10389", "DOI": "10.18653/v1/2020.acl-main.439", "CorpusId": 218763303}, "corpusId": 218763303, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a8c48ecd6aac3130f300345cb451c6ed68d2cc50", "title": "Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models", "abstract": "Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. We propose Conpono, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences. Given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus. On the discourse representation benchmark DiscoEval, our model improves over the previous state-of-the-art by up to 13% and on average 4% absolute across 7 tasks. Our model is the same size as BERT-Base, but outperforms the much larger BERT-Large model and other more recent approaches that incorporate discourse. We also show that Conpono yields gains of 2%-6% absolute even for tasks that do not explicitly evaluate discourse: textual entailment (RTE), common sense reasoning (COPA) and reading comprehension (ReCoRD).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 23, "citationCount": 63, "influentialCitationCount": 7, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.439.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-20", "journal": {"pages": "4859-4870"}, "authors": [{"authorId": "3310951", "name": "Dan Iter"}, {"authorId": "2091768", "name": "Kelvin Guu"}, {"authorId": "117867510", "name": "L. Lansing"}, {"authorId": "1746807", "name": "Dan Jurafsky"}]}, {"paperId": "96c4c061426fe19a021e2e68690d35f19aa40aa2", "externalIds": {"ACL": "2020.acl-main.440", "MAG": "3034728660", "DBLP": "conf/acl/LinRCNBDD20", "ArXiv": "2005.09606", "DOI": "10.18653/v1/2020.acl-main.440", "CorpusId": 218684494}, "corpusId": 218684494, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/96c4c061426fe19a021e2e68690d35f19aa40aa2", "title": "A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks", "abstract": "Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools. In the cooking domain, the web offers many, partially-overlapping, text and video recipes (i.e. procedures) that describe how to make the same dish (i.e. high-level task). Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world procedures are structured. Learning to align these different instruction sets is challenging because: a) different recipes vary in their order of instructions and use of ingredients; and b) video instructions can be noisy and tend to contain far more information than text instructions. To address these challenges, we use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish. We then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish. We release the Microsoft Research Multimodal Aligned Recipe Corpus containing ~150K pairwise alignments between recipes across 4262 dishes with rich commonsense information.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 16, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.09606", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-19", "journal": {"name": "ArXiv", "volume": "abs/2005.09606"}, "authors": [{"authorId": "145800141", "name": "Angela S. Lin"}, {"authorId": "1845230025", "name": "Sudha Rao"}, {"authorId": "1709797", "name": "Asli Celikyilmaz"}, {"authorId": "2074997", "name": "E. Nouri"}, {"authorId": "3125776", "name": "Chris Brockett"}, {"authorId": "1780951", "name": "Debadeepta Dey"}, {"authorId": "66648221", "name": "Bill Dolan"}]}, {"paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "externalIds": {"MAG": "3034850762", "ACL": "2020.acl-main.441", "DBLP": "conf/acl/NieWDBWK20", "ArXiv": "1910.14599", "DOI": "10.18653/v1/2020.acl-main.441", "CorpusId": 207756753}, "corpusId": 207756753, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "abstract": "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 68, "citationCount": 661, "influentialCitationCount": 140, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.441.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-31", "journal": {"name": "ArXiv", "volume": "abs/1910.14599"}, "authors": [{"authorId": "40383658", "name": "Yixin Nie"}, {"authorId": "81840293", "name": "Adina Williams"}, {"authorId": "31461304", "name": "Emily Dinan"}, {"authorId": "143977268", "name": "Mohit Bansal"}, {"authorId": "145183709", "name": "J. Weston"}, {"authorId": "1743722", "name": "Douwe Kiela"}]}, {"paperId": "33ec7eb2168e37e3007d1059aa96b9a63254b4da", "externalIds": {"ArXiv": "2005.04118", "MAG": "3035507081", "DBLP": "conf/acl/RibeiroWGS20", "ACL": "2020.acl-main.442", "DOI": "10.18653/v1/2020.acl-main.442", "CorpusId": 218551201}, "corpusId": 218551201, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/33ec7eb2168e37e3007d1059aa96b9a63254b4da", "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList", "abstract": "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 786, "influentialCitationCount": 122, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.442.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-08", "journal": {"name": "ArXiv", "volume": "abs/2005.04118"}, "authors": [{"authorId": "78846919", "name": "Marco Tulio Ribeiro"}, {"authorId": "35232494", "name": "Tongshuang Sherry Wu"}, {"authorId": "1730156", "name": "Carlos Guestrin"}, {"authorId": "34650964", "name": "Sameer Singh"}]}, {"paperId": "3a2b241ef190d433e24ea2e643de2a9e8336863f", "externalIds": {"MAG": "3034503357", "DBLP": "conf/acl/TabassumMXR20", "ACL": "2020.acl-main.443", "ArXiv": "2005.01634", "DOI": "10.18653/v1/2020.acl-main.443", "CorpusId": 218487168}, "corpusId": 218487168, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3a2b241ef190d433e24ea2e643de2a9e8336863f", "title": "Code and Named Entity Recognition in StackOverflow", "abstract": "There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users. Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences. In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types. We trained in-domain BERT representations (BERTOverflow) on 152 million sentences from StackOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F-1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERT-based tagging model. Our code and data are available at: https://github.com/jeniyat/StackOverflowNER/", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 53, "citationCount": 73, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.443.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"pages": "4913-4926"}, "authors": [{"authorId": "2504467", "name": "Jeniya Tabassum"}, {"authorId": "80032861", "name": "Mounica Maddela"}, {"authorId": "144174114", "name": "Wei Xu"}, {"authorId": "1863425", "name": "Alan Ritter"}]}, {"paperId": "2614f6ee7c450bb21249abb98c231cfcddbb204c", "externalIds": {"ACL": "2020.acl-main.444", "MAG": "3035566559", "DBLP": "journals/corr/abs-2004-08056", "ArXiv": "2004.08056", "DOI": "10.18653/v1/2020.acl-main.444", "CorpusId": 215814290}, "corpusId": 215814290, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2614f6ee7c450bb21249abb98c231cfcddbb204c", "title": "Dialogue-Based Relation Extraction", "abstract": "We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https://dataset.org/dialogre/.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 74, "citationCount": 95, "influentialCitationCount": 27, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.444.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-17", "journal": {"name": "ArXiv", "volume": "abs/2004.08056"}, "authors": [{"authorId": "41190054", "name": "Dian Yu"}, {"authorId": "49871029", "name": "Kai Sun"}, {"authorId": "1748501", "name": "Claire Cardie"}, {"authorId": "144580027", "name": "Dong Yu"}]}, {"paperId": "4079cfe12460f3b77291e3f14a49198e0b322413", "externalIds": {"MAG": "3034266472", "DBLP": "conf/acl/MaoLZRH20", "ACL": "2020.acl-main.445", "ArXiv": "1908.10383", "DOI": "10.18653/v1/2020.acl-main.445", "CorpusId": 220046460}, "corpusId": 220046460, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/4079cfe12460f3b77291e3f14a49198e0b322413", "title": "Facet-Aware Evaluation for Extractive Summarization", "abstract": "Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level. In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries. Specifically, we treat each sentence in the reference summary as a facet, identify the sentences in the document that express the semantics of each facet as support sentences of the facet, and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and support sentences of all the facets in the reference summary. To facilitate this new evaluation setup, we construct an extractive version of the CNN/Daily Mail dataset and perform a thorough quantitative investigation, through which we demonstrate that facet-aware evaluation manifests better correlation with human judgment than ROUGE, enables fine-grained evaluation as well as comparative analysis, and reveals valuable insights of state-of-the-art summarization methods. Data can be found at https://github.com/morningmoni/FAR.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 54, "citationCount": 19, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.445.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-08-27", "journal": {"pages": "4941-4957"}, "authors": [{"authorId": "3375249", "name": "Yuning Mao"}, {"authorId": "46458310", "name": "Liyuan Liu"}, {"authorId": "2152206948", "name": "Qi Zhu"}, {"authorId": "145201124", "name": "Xiang Ren"}, {"authorId": "153034701", "name": "Jiawei Han"}]}, {"paperId": "50d9da7d6384f20d9c505573ae829055db9855e7", "externalIds": {"DBLP": "conf/acl/StasaskiYH20", "MAG": "3034599929", "ACL": "2020.acl-main.446", "DOI": "10.18653/v1/2020.acl-main.446", "CorpusId": 219707158}, "corpusId": 219707158, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/50d9da7d6384f20d9c505573ae829055db9855e7", "title": "More Diverse Dialogue Datasets via Diversity-Informed Data Collection", "abstract": "Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpus-level statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpus-level metrics.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 10, "influentialCitationCount": 2, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "4958-4968"}, "authors": [{"authorId": "25157093", "name": "Katherine Stasaski"}, {"authorId": "10237116", "name": "G. Yang"}, {"authorId": "1716902", "name": "Marti A. Hearst"}]}, {"paperId": "5c5751d45e298cea054f32b392c12c61027d2fe7", "externalIds": {"MAG": "3015453090", "DBLP": "conf/acl/LoWNKW20", "ACL": "2020.acl-main.447", "DOI": "10.18653/V1/2020.ACL-MAIN.447", "CorpusId": 215416146}, "corpusId": 215416146, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5c5751d45e298cea054f32b392c12c61027d2fe7", "title": "S2ORC: The Semantic Scholar Open Research Corpus", "abstract": "We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 59, "citationCount": 345, "influentialCitationCount": 85, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.447.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "journal": {"pages": "4969-4983"}, "authors": [{"authorId": "46258841", "name": "Kyle Lo"}, {"authorId": "31860505", "name": "Lucy Lu Wang"}, {"authorId": "2060376981", "name": "Mark Neumann"}, {"authorId": "143967880", "name": "Rodney Michael Kinney"}, {"authorId": "1780531", "name": "Daniel S. Weld"}]}, {"paperId": "868207797e69df5055f5c3fd4aa78a33e5a7ca45", "externalIds": {"MAG": "3035408261", "ArXiv": "2006.06264", "DBLP": "journals/corr/abs-2006-06264", "ACL": "2020.acl-main.448", "DOI": "10.18653/v1/2020.acl-main.448", "CorpusId": 219573654}, "corpusId": 219573654, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/868207797e69df5055f5c3fd4aa78a33e5a7ca45", "title": "Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics", "abstract": "Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric\u2019s efficacy. Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected. Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 170, "influentialCitationCount": 24, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.448.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-11", "journal": {"name": "ArXiv", "volume": "abs/2006.06264"}, "authors": [{"authorId": "2615454", "name": "Nitika Mathur"}, {"authorId": "123917295", "name": "Tim Baldwin"}, {"authorId": "143620680", "name": "Trevor Cohn"}]}, {"paperId": "805a6d1df9f460abfcea3d51d181cf1e80680be4", "externalIds": {"DBLP": "conf/acl/AhmadCRC20", "MAG": "3022580834", "ACL": "2020.acl-main.449", "ArXiv": "2005.00653", "DOI": "10.18653/v1/2020.acl-main.449", "CorpusId": 218486987}, "corpusId": 218486987, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/805a6d1df9f460abfcea3d51d181cf1e80680be4", "title": "A Transformer-based Approach for Source Code Summarization", "abstract": "Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens\u2019 position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 34, "citationCount": 259, "influentialCitationCount": 64, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.449.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00653"}, "authors": [{"authorId": "38123220", "name": "Wasi Uddin Ahmad"}, {"authorId": "47570053", "name": "Saikat Chakraborty"}, {"authorId": "31631000", "name": "Baishakhi Ray"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}]}, {"paperId": "d36e39aedd802aea4be1ea303c70dc56e97dbc3c", "externalIds": {"ArXiv": "2004.04228", "DBLP": "journals/corr/abs-2004-04228", "ACL": "2020.acl-main.450", "MAG": "3034188538", "DOI": "10.18653/v1/2020.acl-main.450", "CorpusId": 215548661}, "corpusId": 215548661, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d36e39aedd802aea4be1ea303c70dc56e97dbc3c", "title": "Asking and Answering Questions to Evaluate the Factual Consistency of Summaries", "abstract": "Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS (pronounced \u201ckags\u201d), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why. We believe QAGS is a promising tool in automatically generating usable and factually consistent text. Code for QAGS will be available at https://github.com/W4ngatang/qags.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 45, "citationCount": 298, "influentialCitationCount": 64, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.450.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-08", "journal": {"name": "ArXiv", "volume": "abs/2004.04228"}, "authors": [{"authorId": "144906624", "name": "Alex Wang"}, {"authorId": "1979489", "name": "Kyunghyun Cho"}, {"authorId": "35084211", "name": "M. Lewis"}]}, {"paperId": "76037594f29a663fbd2799de2e5c7463c02a8a1d", "externalIds": {"ArXiv": "1910.14142", "MAG": "3019713789", "DBLP": "conf/acl/XuGCL20", "ACL": "2020.acl-main.451", "DOI": "10.18653/v1/2020.acl-main.451", "CorpusId": 219036690}, "corpusId": 219036690, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/76037594f29a663fbd2799de2e5c7463c02a8a1d", "title": "Discourse-Aware Neural Extractive Text Summarization", "abstract": "Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 53, "citationCount": 194, "influentialCitationCount": 22, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.451.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-30", "journal": {"pages": "5021-5031"}, "authors": [{"authorId": "34837371", "name": "Jiacheng Xu"}, {"authorId": "144702900", "name": "Zhe Gan"}, {"authorId": "145215470", "name": "Yu Cheng"}, {"authorId": "46700348", "name": "Jingjing Liu"}]}, {"paperId": "547335ea173cc4cdc0133ce24d10172780d9fc52", "externalIds": {"MAG": "3034539042", "ArXiv": "2005.01791", "ACL": "2020.acl-main.452", "DBLP": "conf/acl/SchumannMLVM20", "DOI": "10.18653/v1/2020.acl-main.452", "CorpusId": 218502538}, "corpusId": 218502538, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/547335ea173cc4cdc0133ce24d10172780d9fc52", "title": "Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction", "abstract": "Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length. Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 52, "citationCount": 31, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.452.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"pages": "5032-5042"}, "authors": [{"authorId": "47947548", "name": "Raphael Schumann"}, {"authorId": "38956216", "name": "Lili Mou"}, {"authorId": "2143370890", "name": "Yao Lu"}, {"authorId": "1712417", "name": "Olga Vechtomova"}, {"authorId": "1686341", "name": "K. Markert"}]}, {"paperId": "8643ff4f59d4eb8916c60e003acebc7ce946ad27", "externalIds": {"MAG": "3035753785", "DBLP": "conf/acl/LadhakLAM20", "ACL": "2020.acl-main.453", "ArXiv": "2005.01840", "DOI": "10.18653/v1/2020.acl-main.453", "CorpusId": 218502513}, "corpusId": 218502513, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/8643ff4f59d4eb8916c60e003acebc7ce946ad27", "title": "Exploring Content Selection in Summarization of Novel Chapters", "abstract": "We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides. This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries. We focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries. We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods. Our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 32, "citationCount": 24, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.453.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-04", "journal": {"name": "ArXiv", "volume": "abs/2005.01840"}, "authors": [{"authorId": "8759332", "name": "Faisal Ladhak"}, {"authorId": "82630293", "name": "Bryan Li"}, {"authorId": "1403907739", "name": "Yaser Al-Onaizan"}, {"authorId": "145590324", "name": "K. McKeown"}]}, {"paperId": "11b6d1fee0f47a8f9f892ab0d86f370c449097aa", "externalIds": {"MAG": "3099766584", "DBLP": "journals/corr/abs-2005-03754", "ACL": "2020.acl-main.454", "ArXiv": "2005.03754", "DOI": "10.18653/V1/2020.ACL-MAIN.454", "CorpusId": 218571335}, "corpusId": 218571335, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/11b6d1fee0f47a8f9f892ab0d86f370c449097aa", "title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization", "abstract": "Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 56, "citationCount": 263, "influentialCitationCount": 54, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.454.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-07", "journal": {"name": "ArXiv", "volume": "abs/2005.03754"}, "authors": [{"authorId": "41152329", "name": "Esin Durmus"}, {"authorId": "144533687", "name": "He He"}, {"authorId": "1700007", "name": "Mona T. Diab"}]}, {"paperId": "f13aaaeffc63490395ddce0861375d1440721a5e", "externalIds": {"DBLP": "conf/acl/XuDLRK20", "MAG": "3034535994", "ACL": "2020.acl-main.455", "DOI": "10.18653/v1/2020.acl-main.455", "CorpusId": 220047754}, "corpusId": 220047754, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f13aaaeffc63490395ddce0861375d1440721a5e", "title": "Fact-based Content Weighting for Evaluating Abstractive Summarisation", "abstract": "Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient. We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary. We fol- low the assumption that a good summary will reflect all relevant facts, i.e. the ones present in the ground truth (human-generated refer- ence summary). We confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of Hardy et al. (2019).", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 30, "citationCount": 15, "influentialCitationCount": 0, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5071-5081"}, "authors": [{"authorId": "31501619", "name": "Xinnuo Xu"}, {"authorId": "2544049", "name": "Ondrej Dusek"}, {"authorId": "2116479109", "name": "Jingyi Li"}, {"authorId": "1681799", "name": "Verena Rieser"}, {"authorId": "2621022", "name": "Ioannis Konstas"}]}, {"paperId": "051b1049e0e0cf1e0620065e7c69f908e98e9ece", "externalIds": {"ArXiv": "2004.01980", "MAG": "3035068386", "DBLP": "journals/corr/abs-2004-01980", "ACL": "2020.acl-main.456", "DOI": "10.18653/v1/2020.acl-main.456", "CorpusId": 214802410}, "corpusId": 214802410, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/051b1049e0e0cf1e0620065e7c69f908e98e9ece", "title": "Hooks in the Headline: Learning to Generate Headlines with Controlled Styles", "abstract": "Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles. We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers. With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework. We also introduced a novel parameter sharing scheme to further disentangle the style from text. Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait. The attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68%, even outperforming human-written references.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 55, "citationCount": 52, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.456.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-04", "journal": {"pages": "5082-5093"}, "authors": [{"authorId": "2068347799", "name": "Di Jin"}, {"authorId": "8752221", "name": "Zhijing Jin"}, {"authorId": "10638646", "name": "Joey Tianyi Zhou"}, {"authorId": "2065295357", "name": "Lisa Orii"}, {"authorId": "1679873", "name": "Peter Szolovits"}]}, {"paperId": "fa83f4f369af53e5d4fbdc7cce1808520e8e50a2", "externalIds": {"ACL": "2020.acl-main.457", "MAG": "3035643691", "DBLP": "conf/acl/HuangWW20", "ArXiv": "2005.01159", "DOI": "10.18653/v1/2020.acl-main.457", "CorpusId": 218487279}, "corpusId": 218487279, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/fa83f4f369af53e5d4fbdc7cce1808520e8e50a2", "title": "Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward", "abstract": "Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders\u2014a sequential document encoder and a graph-structured encoder\u2014to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 50, "citationCount": 123, "influentialCitationCount": 10, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.457.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"name": "ArXiv", "volume": "abs/2005.01159"}, "authors": [{"authorId": "80518559", "name": "Luyang huang"}, {"authorId": "3008832", "name": "Lingfei Wu"}, {"authorId": "2153516659", "name": "Lu Wang"}]}, {"paperId": "93dc7870d37ea8aad5dc282d255dacf4fef33821", "externalIds": {"ArXiv": "1911.02541", "DBLP": "journals/corr/abs-1911-02541", "MAG": "3034863243", "ACL": "2020.acl-main.458", "DOI": "10.18653/v1/2020.acl-main.458", "CorpusId": 207794190}, "corpusId": 207794190, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/93dc7870d37ea8aad5dc282d255dacf4fef33821", "title": "Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports", "abstract": "Neural abstractive summarization models are able to generate summaries which have high overlap with human references. However, existing models are not optimized for factual correctness, a critical metric in real-world applications. In this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module. We further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning. We apply the proposed method to the summarization of radiology reports, where factual correctness is a key requirement. On two separate datasets collected from hospitals, we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of human-authored ones.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 44, "citationCount": 134, "influentialCitationCount": 24, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.458.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-06", "journal": {"pages": "5108-5120"}, "authors": [{"authorId": "49889487", "name": "Yuhao Zhang"}, {"authorId": "2976167", "name": "Derek Merck"}, {"authorId": "36355051", "name": "E. Tsai"}, {"authorId": "144783904", "name": "Christopher D. Manning"}, {"authorId": "2356307", "name": "C. Langlotz"}]}, {"paperId": "0f6b71d88a06d0e16872476773b3cad3eb0cfbc7", "externalIds": {"DBLP": "conf/acl/RameshkumarB20", "ACL": "2020.acl-main.459", "MAG": "3035259249", "DOI": "10.18653/v1/2020.acl-main.459", "CorpusId": 220047255}, "corpusId": 220047255, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0f6b71d88a06d0e16872476773b3cad3eb0cfbc7", "title": "Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset", "abstract": "This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses. Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game. The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction. For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues. In addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural ML approaches, and we provide an abstractive summarization benchmark and evaluation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 46, "citationCount": 39, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.459.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-14", "journal": {"pages": "5121-5134"}, "authors": [{"authorId": "2470510", "name": "Revanth Rameshkumar"}, {"authorId": "144753756", "name": "P. Bailey"}]}, {"paperId": "ed32242d76f0f52e3eace19b0314868f1d868b89", "externalIds": {"DBLP": "journals/corr/abs-2105-05361", "MAG": "3034991617", "ArXiv": "2105.05361", "ACL": "2020.acl-main.460", "DOI": "10.18653/v1/2020.acl-main.460", "CorpusId": 219323550}, "corpusId": 219323550, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/ed32242d76f0f52e3eace19b0314868f1d868b89", "title": "The Summary Loop: Learning to Write Abstractive Summaries Without Examples", "abstract": "This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries. When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods. Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 47, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.460.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5135-5150"}, "authors": [{"authorId": "46180754", "name": "Philippe Laban"}, {"authorId": "1754922339", "name": "Andrew Hsi Bloomberg"}, {"authorId": "1729041", "name": "J. Canny"}, {"authorId": "1716902", "name": "Marti A. Hearst"}]}, {"paperId": "c8169538d8909ce345d4c63b7f01460362fb3569", "externalIds": {"ArXiv": "1911.02247", "ACL": "2020.acl-main.461", "DBLP": "conf/acl/BrazinskasLT20", "MAG": "3016671332", "DOI": "10.18653/v1/2020.acl-main.461", "CorpusId": 218883429}, "corpusId": 218883429, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c8169538d8909ce345d4c63b7f01460362fb3569", "title": "Unsupervised Opinion Summarization as Copycat-Review Generation", "abstract": "Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews. While the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries. Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs. Since such training data is expensive to acquire, we instead consider the unsupervised setting, in other words, we do not use any summaries in training. We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the \u201camount of novelty\u201d going into the new review or, equivalently, vary the extent to which it deviates from the input. At test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions. We capture this intuition by defining a hierarchical variational autoencoder model. Both individual reviews and the products they correspond to are associated with stochastic latent codes, and the review generator (\u201cdecoder\u201d) has direct access to the text of input reviews through the pointer-generator mechanism. Experiments on Amazon and Yelp datasets, show that setting at test time the review\u2019s latent code to its mean, allows the model to produce fluent and coherent summaries reflecting common opinions.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 47, "citationCount": 85, "influentialCitationCount": 32, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.461.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2019-11-06", "journal": {"pages": "5151-5169"}, "authors": [{"authorId": "30152660", "name": "Arthur Brazinskas"}, {"authorId": "1747893", "name": "Mirella Lapata"}, {"authorId": "144889265", "name": "Ivan Titov"}]}, {"paperId": "c0e83bcdebd5fea326cac80a3aa5215a5ef221d8", "externalIds": {"ACL": "2020.acl-main.462", "MAG": "3027460876", "DBLP": "conf/acl/TrottTCS20", "ArXiv": "2005.09099", "DOI": "10.18653/v1/2020.acl-main.462", "CorpusId": 218684951}, "corpusId": 218684951, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c0e83bcdebd5fea326cac80a3aa5215a5ef221d8", "title": "(Re)construing Meaning in NLP", "abstract": "Human speakers have an extensive toolkit of ways to express themselves. In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding\u2014namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed. We first define this phenomenon more precisely, drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics. We then survey some dimensions of construed meaning and show how insights from construal could inform theoretical and practical work in NLP.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 136, "citationCount": 27, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.462.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Sociology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-05-18", "journal": {"pages": "5170-5184"}, "authors": [{"authorId": "3393311", "name": "Sean Trott"}, {"authorId": "46308692", "name": "Tiago Timponi Torrent"}, {"authorId": "145375772", "name": "Nancy Chang"}, {"authorId": "145254207", "name": "Nathan Schneider"}]}, {"paperId": "02eaaf87f9cae34cca398fed146079e6eeb1f868", "externalIds": {"MAG": "3034723486", "ACL": "2020.acl-main.463", "DBLP": "conf/acl/BenderK20", "DOI": "10.18653/v1/2020.acl-main.463", "CorpusId": 211029226}, "corpusId": 211029226, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/02eaaf87f9cae34cca398fed146079e6eeb1f868", "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data", "abstract": "The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as \u201cunderstanding\u201d language or capturing \u201cmeaning\u201d. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of \u201cTaking Stock of Where We\u2019ve Been and Where We\u2019re Going\u201d, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 75, "citationCount": 542, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.463.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5185-5198"}, "authors": [{"authorId": "2471699", "name": "Emily M. Bender"}, {"authorId": "145542037", "name": "Alexander Koller"}]}, {"paperId": "3701afb6e94f8ee9fccb2c95e0984e57db38f3d8", "externalIds": {"MAG": "3034211723", "ACL": "2020.acl-main.464", "DBLP": "journals/corr/abs-2005-00912", "ArXiv": "2005.00912", "DOI": "10.18653/v1/2020.acl-main.464", "CorpusId": 218487486}, "corpusId": 218487486, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3701afb6e94f8ee9fccb2c95e0984e57db38f3d8", "title": "Examining Citations of Natural Language Processing Literature", "abstract": "We extracted information from the ACL Anthology (AA) and Google Scholar (GS) to examine trends in citations of NLP papers. We explore questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers from different areas of within NLP? etc. Notably, we show that only about 56% of the papers in AA are cited ten or more times. CL Journal has the most cited papers, but its citation dominance has lessened in recent years. On average, long papers get almost three times as many citations as short papers; and papers on sentiment classification, anaphora resolution, and entity recognition have the highest median citations. The analyses presented here, and the associated dataset of NLP papers mapped to citations, have a number of uses including: understanding how the field is growing and quantifying the impact of different types of papers.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 22, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.464.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "History"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "History", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"pages": "5199-5209"}, "authors": [{"authorId": "143880621", "name": "Saif M. Mohammad"}]}, {"paperId": "d0cda85c030711aaa5383c80d5928a4d22f8d3bf", "externalIds": {"DBLP": "conf/acl/Linzen20", "ArXiv": "2005.00955", "MAG": "3022579746", "ACL": "2020.acl-main.465", "DOI": "10.18653/v1/2020.acl-main.465", "CorpusId": 218487293}, "corpusId": 218487293, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d0cda85c030711aaa5383c80d5928a4d22f8d3bf", "title": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?", "abstract": "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 48, "citationCount": 150, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.465.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"pages": "5210-5217"}, "authors": [{"authorId": "2467508", "name": "Tal Linzen"}]}, {"paperId": "9b529fe170823f95509585d5aa39fa01a43558fd", "externalIds": {"ACL": "2020.acl-main.466", "ArXiv": "2004.12158", "DBLP": "journals/corr/abs-2004-12158", "MAG": "3035668167", "DOI": "10.18653/v1/2020.acl-main.466", "CorpusId": 216552897}, "corpusId": 216552897, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9b529fe170823f95509585d5aa39fa01a43558fd", "title": "How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence", "abstract": "Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 117, "citationCount": 153, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.466.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Law", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-25", "journal": {"pages": "5218-5230"}, "authors": [{"authorId": "51125639", "name": "Haoxiang Zhong"}, {"authorId": "51131083", "name": "Chaojun Xiao"}, {"authorId": "2652217", "name": "Cunchao Tu"}, {"authorId": "145086110", "name": "T. Zhang"}, {"authorId": "49293587", "name": "Zhiyuan Liu"}, {"authorId": "1753344", "name": "Maosong Sun"}]}, {"paperId": "673e970fd835c7dd1bb1e071c5a37e9df99b7c8e", "externalIds": {"DBLP": "journals/corr/abs-2005-00628", "MAG": "3023419341", "ArXiv": "2005.00628", "ACL": "2020.acl-main.467", "DOI": "10.18653/v1/2020.acl-main.467", "CorpusId": 220045835}, "corpusId": 220045835, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/673e970fd835c7dd1bb1e071c5a37e9df99b7c8e", "title": "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?", "abstract": "While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 62, "citationCount": 157, "influentialCitationCount": 16, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.467.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "5231-5247"}, "authors": [{"authorId": "100984698", "name": "Yada Pruksachatkun"}, {"authorId": "80842917", "name": "Jason Phang"}, {"authorId": "48447436", "name": "Haokun Liu"}, {"authorId": "41022736", "name": "Phu Mon Htut"}, {"authorId": "46446933", "name": "Xiaoyi Zhang"}, {"authorId": "46230016", "name": "Richard Yuanzhe Pang"}, {"authorId": "3054462", "name": "Clara Vania"}, {"authorId": "3422953", "name": "Katharina Kann"}, {"authorId": "3644767", "name": "Samuel R. Bowman"}]}, {"paperId": "eef4df3a5232c7ce70123aaebb326ff9169a3c8c", "externalIds": {"ACL": "2020.acl-main.468", "MAG": "2998334385", "DBLP": "journals/corr/abs-1912-11078", "ArXiv": "1912.11078", "DOI": "10.18653/v1/2020.acl-main.468", "CorpusId": 209461005}, "corpusId": 209461005, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/eef4df3a5232c7ce70123aaebb326ff9169a3c8c", "title": "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview", "abstract": "An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 97, "citationCount": 184, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.468.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2019-11-09", "journal": {"pages": "5248-5264"}, "authors": [{"authorId": "1994268", "name": "Deven Santosh Shah"}, {"authorId": "145035129", "name": "H. A. Schwartz"}, {"authorId": "2022288", "name": "Dirk Hovy"}]}, {"paperId": "cdcdc7ab1f5b6e86146b5c0224cba7d8cd35142c", "externalIds": {"DBLP": "conf/acl/LiYYHC20", "MAG": "3034837210", "ACL": "2020.acl-main.469", "DOI": "10.18653/v1/2020.acl-main.469", "CorpusId": 218610661}, "corpusId": 218610661, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cdcdc7ab1f5b6e86146b5c0224cba7d8cd35142c", "title": "What Does BERT with Vision Look At?", "abstract": "Pre-trained visually grounded language models such as ViLBERT, LXMERT, and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear. In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions. Specifically, some heads can map entities to image regions, performing the task known as entity grounding. Some heads can even detect the syntactic relations between non-entity words and image regions, tracking, for example, associations between verbs and regions corresponding to their arguments. We denote this ability as syntactic grounding. We verify grounding both quantitatively and qualitatively, using Flickr30K Entities as a testbed.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 50, "citationCount": 97, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.469.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5265-5275"}, "authors": [{"authorId": "32562635", "name": "Liunian Harold Li"}, {"authorId": "2064210", "name": "Mark Yatskar"}, {"authorId": "144508458", "name": "Da Yin"}, {"authorId": "1793529", "name": "Cho-Jui Hsieh"}, {"authorId": "2782886", "name": "Kai-Wei Chang"}]}, {"paperId": "524812824cb165f718f015a2ccfe55cfc80c1945", "externalIds": {"MAG": "3021756211", "ArXiv": "2005.04245", "ACL": "2020.acl-main.470", "DBLP": "journals/corr/abs-2005-04245", "DOI": "10.18653/v1/2020.acl-main.470", "CorpusId": 218501192}, "corpusId": 218501192, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/524812824cb165f718f015a2ccfe55cfc80c1945", "title": "Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards", "abstract": "Throughout a conversation, participants make choices that can orient the flow of the interaction. Such choices are particularly salient in the consequential domain of crisis counseling, where a difficulty for counselors is balancing between two key objectives: advancing the conversation towards a resolution, and empathetically addressing the crisis situation. In this work, we develop an unsupervised methodology to quantify how counselors manage this balance. Our main intuition is that if an utterance can only receive a narrow range of appropriate replies, then its likely aim is to advance the conversation forwards, towards a target within that range. Likewise, an utterance that can only appropriately follow a narrow range of possible utterances is likely aimed backwards at addressing a specific situation within that range. By applying this intuition, we can map each utterance to a continuous orientation axis that captures the degree to which it is intended to direct the flow of the conversation forwards or backwards. This unsupervised method allows us to characterize counselor behaviors in a large dataset of crisis counseling conversations, where we show that known counseling strategies intuitively align with this axis. We also illustrate how our measure can be indicative of a conversation\u2019s progress, as well as its effectiveness.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 34, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.470.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-08", "journal": {"name": "ArXiv", "volume": "abs/2005.04245"}, "authors": [{"authorId": "3051097", "name": "Justine Zhang"}, {"authorId": "1388368997", "name": "Cristian Danescu-Niculescu-Mizil"}]}, {"paperId": "0b65ddb999572a8a3c3076830beedb822acead55", "externalIds": {"MAG": "3021922361", "DBLP": "conf/acl/DesaiCL20", "ArXiv": "2004.14299", "ACL": "2020.acl-main.471", "DOI": "10.18653/v1/2020.acl-main.471", "CorpusId": 216642016}, "corpusId": 216642016, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0b65ddb999572a8a3c3076830beedb822acead55", "title": "Detecting Perceived Emotions in Hurricane Disasters", "abstract": "Natural disasters (e.g., hurricanes) affect millions of people each year, causing widespread destruction in their wake. People have recently taken to social media websites (e.g., Twitter) to share their sentiments and feelings with the larger community. Consequently, these platforms have become instrumental in understanding and perceiving emotions at scale. In this paper, we introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria. We present a comprehensive study of fine-grained emotions and propose classification tasks to discriminate between coarse-grained emotion groups. Our best BERT model, even after task-guided pre-training which leverages unlabeled Twitter data, achieves only 68% accuracy (averaged across all groups). HurricaneEmo serves not only as a challenging benchmark for models but also as a valuable resource for analyzing emotions in disaster-centric domains.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 65, "citationCount": 24, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.14299", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"name": "ArXiv", "volume": "abs/2004.14299"}, "authors": [{"authorId": "120777041", "name": "Shrey Desai"}, {"authorId": "1690656", "name": "Cornelia Caragea"}, {"authorId": "22319255", "name": "Junyi Jessy Li"}]}, {"paperId": "6093c2b5ce85df6ddce7998b75028cda4ef19e47", "externalIds": {"MAG": "3035156228", "DBLP": "conf/acl/LynnBS20", "ACL": "2020.acl-main.472", "DOI": "10.18653/v1/2020.acl-main.472", "CorpusId": 220048497}, "corpusId": 220048497, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/6093c2b5ce85df6ddce7998b75028cda4ef19e47", "title": "Hierarchical Modeling for User Personality Prediction: The Role of Message-Level Attention", "abstract": "Not all documents are equally important. Language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals, but most approaches neglect to consider whether all documents of an individual are equally informative. In this paper, we present a novel model that uses message-level attention to learn the relative weight of users\u2019 social media posts for assessing their five factor personality traits. We demonstrate that models with message-level attention outperform those with word-level attention, and ultimately yield state-of-the-art accuracies for all five traits by using both word and message attention in combination with past approaches (an average increase in Pearson r of 2.5%). In addition, examination of the high-signal posts identified by our model provides insight into the relationship between language and personality, helping to inform future work.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 40, "citationCount": 44, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.472.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5306-5316"}, "authors": [{"authorId": "5536113", "name": "Veronica E. Lynn"}, {"authorId": "35217367", "name": "Niranjan Balasubramanian"}, {"authorId": "145035129", "name": "H. A. Schwartz"}]}, {"paperId": "a7e592d598986640395522e733b93ee09991182c", "externalIds": {"MAG": "3035442306", "DBLP": "journals/corr/abs-2006-07425", "ArXiv": "2006.07425", "ACL": "2020.acl-main.473", "DOI": "10.18653/v1/2020.acl-main.473", "CorpusId": 219686752}, "corpusId": 219686752, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a7e592d598986640395522e733b93ee09991182c", "title": "Measuring Forecasting Skill from Text", "abstract": "People vary in their ability to make accurate predictions about the future. Prior studies have shown that some individuals can predict the outcome of future events with consistently better accuracy. This leads to a natural question: what makes some forecasters better than others? In this paper we explore connections between the language people use to describe their predictions and their forecasting skill. Datasets from two different forecasting domains are explored: (1) geopolitical forecasts from Good Judgment Open, an online prediction forum and (2) a corpus of company earnings forecasts made by financial analysts. We present a number of linguistic metrics which are computed over text associated with people\u2019s predictions about the future including: uncertainty, readability, and emotion. By studying linguistic factors associated with predictions, we are able to shed some light on the approach taken by skilled forecasters. Furthermore, we demonstrate that it is possible to accurately predict forecasting skill using a model that is based solely on language. This could potentially be useful for identifying accurate predictions or potentially skilled forecasters earlier.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 59, "citationCount": 14, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.473.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-12", "journal": {"name": "ArXiv", "volume": "abs/2006.07425"}, "authors": [{"authorId": "49392769", "name": "Shi Zong"}, {"authorId": "1863425", "name": "Alan Ritter"}, {"authorId": "144547315", "name": "E. Hovy"}]}, {"paperId": "79c5a02c387c90202e66d7a36942a94e0dba8c70", "externalIds": {"MAG": "3035083705", "ACL": "2020.acl-main.474", "DBLP": "journals/corr/abs-2005-00649", "ArXiv": "2005.00649", "DOI": "10.18653/v1/2020.acl-main.474", "CorpusId": 218487374}, "corpusId": 218487374, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/79c5a02c387c90202e66d7a36942a94e0dba8c70", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "abstract": "Many applications of computational social science aim to infer causal conclusions from non-experimental data. Such observational data often contains confounders, variables that influence both potential causes and potential effects. Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text. For example, an individual\u2019s entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders.Yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent.This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions. Despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 157, "citationCount": 83, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.474.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.00649"}, "authors": [{"authorId": "145137850", "name": "Katherine A. Keith"}, {"authorId": "2057064256", "name": "David D. Jensen"}, {"authorId": "1401020033", "name": "Brendan T. O'Connor"}]}, {"paperId": "49f9d8505e8d27041cc57cd3d06c2741f21120c1", "externalIds": {"MAG": "3035318823", "DBLP": "journals/corr/abs-2005-04232", "ArXiv": "2005.04232", "ACL": "2020.acl-main.475", "DOI": "10.18653/v1/2020.acl-main.475", "CorpusId": 218581891}, "corpusId": 218581891, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/49f9d8505e8d27041cc57cd3d06c2741f21120c1", "title": "Text-Based Ideal Points", "abstract": "Ideal point models analyze lawmakers\u2019 votes to quantify their political positions, or ideal points. But votes are not the only way to express a political position. Lawmakers also give speeches, release press statements, and post tweets. In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors. We demonstrate the TBIP with two types of politicized text data: U.S. Senate speeches and senator tweets. Though the model does not analyze their votes or political affiliations, the TBIP separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points. One benefit of analyzing texts, as opposed to votes, is that the TBIP can estimate ideal points of anyone who authors political texts, including non-voting actors. To this end, we use it to study tweets from the 2020 Democratic presidential candidates. Using only the texts of their tweets, it identifies them along an interpretable progressive-to-moderate spectrum.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 63, "citationCount": 17, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.475.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-08", "journal": {"name": "ArXiv", "volume": "abs/2005.04232"}, "authors": [{"authorId": "70025184", "name": "Keyon Vafa"}, {"authorId": "47824203", "name": "S. Naidu"}, {"authorId": "1796335", "name": "D. Blei"}]}, {"paperId": "0cdbc49b8e0f98af5cdcb5bc9dae5156c332f255", "externalIds": {"ACL": "2020.acl-main.476", "MAG": "3034936923", "DBLP": "conf/acl/DavoodiWG20", "DOI": "10.18653/v1/2020.acl-main.476", "CorpusId": 220048147}, "corpusId": 220048147, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0cdbc49b8e0f98af5cdcb5bc9dae5156c332f255", "title": "Understanding the Language of Political Agreement and Disagreement in Legislative Texts", "abstract": "While national politics often receive the spotlight, the overwhelming majority of legislation proposed, discussed, and enacted is done at the state level. Despite this fact, there is little awareness of the dynamics that lead to adopting these policies. In this paper, we take the first step towards a better understanding of these processes and the underlying dynamics that shape them, using data-driven methods. We build a new large-scale dataset, from multiple data sources, connecting state bills and legislator information, geographical information about their districts, and donations and donors\u2019 information. We suggest a novel task, predicting the legislative body\u2019s vote breakdown for a given bill, according to different criteria of interest, such as gender, rural-urban and ideological splits. Finally, we suggest a shared relational embedding model, representing the interactions between the text of the bill and the legislative context in which it is presented. Our experiments show that providing this context helps improve the prediction over strong text-based models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 9, "influentialCitationCount": 1, "isOpenAccess": false, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Political Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5358-5368"}, "authors": [{"authorId": "31671167", "name": "Maryam Davoodi"}, {"authorId": "114654431", "name": "Eric N. Waltenburg"}, {"authorId": "2877164", "name": "Dan Goldwasser"}]}, {"paperId": "1e26ff02f330fcf198bea8db0510d295c07ff908", "externalIds": {"MAG": "3034418628", "ACL": "2020.acl-main.477", "DBLP": "conf/acl/TayOFCCLP20", "DOI": "10.18653/v1/2020.acl-main.477", "CorpusId": 220044874}, "corpusId": 220044874, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1e26ff02f330fcf198bea8db0510d295c07ff908", "title": "Would you Rather? A New Benchmark for Learning Machine Alignment with Cultural Values and Social Preferences", "abstract": "Understanding human preferences, along with cultural and social nuances, lives at the heart of natural language understanding. Concretely, we present a new task and corpus for learning alignments between machine and human preferences. Our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations. Our problem is framed as a natural language inference task with crowd-sourced preference votes by human players, obtained from a gamified voting platform. We benchmark several state-of-the-art neural models, along with BERT and friends on this task. Our experimental results show that current state-of-the-art NLP models still leave much room for improvement.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 19, "citationCount": 13, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.477.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5369-5373"}, "authors": [{"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "1754231907", "name": "D. Ong"}, {"authorId": "2089772402", "name": "Jie Fu"}, {"authorId": "143962358", "name": "Alvin Chan"}, {"authorId": "2118768820", "name": "Nancy Chen"}, {"authorId": "1755919", "name": "A. Luu"}, {"authorId": "1972076", "name": "C. Pal"}]}, {"paperId": "2941b843d309576971106d520fd4b848a113504f", "externalIds": {"DBLP": "conf/acl/ChoubeyLHW20", "ACL": "2020.acl-main.478", "MAG": "3035185704", "DOI": "10.18653/v1/2020.acl-main.478", "CorpusId": 218515665}, "corpusId": 218515665, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2941b843d309576971106d520fd4b848a113504f", "title": "Discourse as a Function of Event: Profiling Discourse Structure in News Articles around the Main Event", "abstract": "Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event. To enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources. Next, we propose several document-level neural-network models to automatically construct news content structures. Finally, we demonstrate that incorporating system predicted news structures yields new state-of-the-art performance for event coreference resolution. The news documents we annotated are openly available and the annotations are publicly released for future research.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 63, "citationCount": 44, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.478.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Sociology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5374-5386"}, "authors": [{"authorId": "3466801", "name": "Prafulla Kumar Choubey"}, {"authorId": "2116043027", "name": "A. Lee"}, {"authorId": "40372969", "name": "Ruihong Huang"}, {"authorId": "2153518220", "name": "Lu Wang"}]}, {"paperId": "0b5bde098ad150585c01cf999d48a99d8a1ea527", "externalIds": {"ArXiv": "1910.14254", "MAG": "3018878311", "ACL": "2020.acl-main.479", "DBLP": "conf/acl/SchusterCD20", "DOI": "10.18653/v1/2020.acl-main.479", "CorpusId": 216080977}, "corpusId": 216080977, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0b5bde098ad150585c01cf999d48a99d8a1ea527", "title": "Harnessing the linguistic signal to predict scalar inferences", "abstract": "Pragmatic inferences often subtly depend on the presence or absence of linguistic features. For example, the presence of a partitive construction (of the) increases the strength of a so-called scalar inference: listeners perceive the inference that Chris did not eat all of the cookies to be stronger after hearing \u201cChris ate some of the cookies\u201d than after hearing the same utterance without a partitive, \u201cChris ate some cookies\u201d. In this work, we explore to what extent neural network sentence encoders can learn to predict the strength of scalar inferences. We first show that an LSTM-based sentence encoder trained on an English dataset of human inference strength ratings is able to predict ratings with high accuracy (r = 0.78). We then probe the model\u2019s behavior using manually constructed minimal sentence pairs and corpus data. We first that the model inferred previously established associations between linguistic features and inference strength, suggesting that the model learns to use linguistic features to predict pragmatic inferences.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 66, "citationCount": 25, "influentialCitationCount": 1, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.479.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-31", "journal": {"pages": "5387-5403"}, "authors": [{"authorId": "145157639", "name": "Sebastian Schuster"}, {"authorId": "2109167672", "name": "Yuxing Chen"}, {"authorId": "38548904", "name": "Judith Degen"}]}, {"paperId": "54cdc2fa88e45a34d954d9d3b322d62400e9220d", "externalIds": {"MAG": "3034870693", "DBLP": "conf/acl/KimFGL20", "ACL": "2020.acl-main.480", "DOI": "10.18653/v1/2020.acl-main.480", "CorpusId": 220045825}, "corpusId": 220045825, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/54cdc2fa88e45a34d954d9d3b322d62400e9220d", "title": "Implicit Discourse Relation Classification: We Need to Talk about Evaluation", "abstract": "Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations. However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature. In this work, we highlight these inconsistencies and propose an improved evaluation protocol. Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0. Furthermore, this work is the first to explore fine-grained relation classification on PDTB 3.0. We expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 30, "influentialCitationCount": 11, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5404-5414"}, "authors": [{"authorId": "8756748", "name": "Najoung Kim"}, {"authorId": "2113510699", "name": "Song Feng"}, {"authorId": "144543562", "name": "R. Chulaka Gunasekara"}, {"authorId": "8390140", "name": "L. Lastras"}]}, {"paperId": "9a01df50d24ce3e384e6dbef921c81f93ac49c64", "externalIds": {"DBLP": "conf/acl/ToshniwalEGL20", "ArXiv": "2005.02990", "ACL": "2020.acl-main.481", "MAG": "3021715649", "DOI": "10.18653/v1/2020.acl-main.481", "CorpusId": 218514717}, "corpusId": 218514717, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9a01df50d24ce3e384e6dbef921c81f93ac49c64", "title": "PeTra: A Sparsely Supervised Memory Model for People Tracking", "abstract": "We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots. PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture. We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance. To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach. PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 39, "citationCount": 5, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.481.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-06", "journal": {"pages": "5415-5428"}, "authors": [{"authorId": "2634203", "name": "Shubham Toshniwal"}, {"authorId": "37907837", "name": "Allyson Ettinger"}, {"authorId": "1700980", "name": "Kevin Gimpel"}, {"authorId": "2924113", "name": "Karen Livescu"}]}, {"paperId": "0cf6c5c0950053506706cb0d815665818a5aad32", "externalIds": {"MAG": "3035736543", "ACL": "2020.acl-main.482", "DBLP": "conf/acl/SongXZCY20", "DOI": "10.18653/v1/2020.acl-main.482", "CorpusId": 220045820}, "corpusId": 220045820, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0cf6c5c0950053506706cb0d815665818a5aad32", "title": "ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT", "abstract": "Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively. We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately. For zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees. Experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 17, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.482.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5429-5434"}, "authors": [{"authorId": "1748796", "name": "Linfeng Song"}, {"authorId": "151485141", "name": "Kun Xu"}, {"authorId": "1591125925", "name": "Yue Zhang"}, {"authorId": "1720246", "name": "Jianshu Chen"}, {"authorId": "2111505433", "name": "Dong Yu"}]}, {"paperId": "5d8314f9988129b347b18880965a4a431c59a4a6", "externalIds": {"MAG": "3034282334", "ACL": "2020.acl-main.483", "DBLP": "journals/corr/abs-2005-02439", "ArXiv": "2005.02439", "DOI": "10.18653/v1/2020.acl-main.483", "CorpusId": 218517088}, "corpusId": 218517088, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/5d8314f9988129b347b18880965a4a431c59a4a6", "title": "Contextualizing Hate Speech Classifiers with Post-hoc Explanation", "abstract": "Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like \u201cgay\u201d or \u201cblack\u201d are used in offensive or prejudiced ways. Such biases manifest in false positives when these identifiers are present, due to models\u2019 inability to learn the contexts which constitute a hateful usage of identifiers. We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms. Then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves. Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 44, "citationCount": 107, "influentialCitationCount": 15, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.483.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"name": "ArXiv", "volume": "abs/2005.02439"}, "authors": [{"authorId": "145304843", "name": "Brendan Kennedy"}, {"authorId": "51134926", "name": "Xisen Jin"}, {"authorId": "119603124", "name": "Aida Mostafazadeh Davani"}, {"authorId": "2111849697", "name": "Morteza Dehghani"}, {"authorId": "1384550891", "name": "Xiang Ren"}]}, {"paperId": "b58abb51a5dae7fb753be4535e468a6f1f07f873", "externalIds": {"MAG": "2995214047", "ArXiv": "2005.00965", "DBLP": "journals/corr/abs-2005-00965", "ACL": "2020.acl-main.484", "DOI": "10.18653/v1/2020.acl-main.484", "CorpusId": 204770514}, "corpusId": 204770514, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/b58abb51a5dae7fb753be4535e468a6f1f07f873", "title": "Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation", "abstract": "Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 41, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.484.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-03", "journal": {"pages": "5443-5453"}, "authors": [{"authorId": "1785372925", "name": "Tianlu Wang"}, {"authorId": "143724481", "name": "Xi Victoria Lin"}, {"authorId": "8937909", "name": "Nazneen Rajani"}, {"authorId": "2004053", "name": "Vicente Ordonez"}, {"authorId": "1576585374", "name": "Caimng Xiong"}]}, {"paperId": "d47a682723f710395454687319bb55635e653105", "externalIds": {"DBLP": "journals/corr/abs-2005-14050", "MAG": "3032388710", "ArXiv": "2005.14050", "ACL": "2020.acl-main.485", "DOI": "10.18653/v1/2020.acl-main.485", "CorpusId": 218971825}, "corpusId": 218971825, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105", "title": "Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP", "abstract": "We survey 146 papers analyzing \u201cbias\u201d in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 253, "citationCount": 733, "influentialCitationCount": 79, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.485.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2020-05-28", "journal": {"name": "ArXiv", "volume": "abs/2005.14050"}, "authors": [{"authorId": "3422038", "name": "Su Lin Blodgett"}, {"authorId": "2881033", "name": "Solon Barocas"}, {"authorId": "2065041692", "name": "Hal Daum'e"}, {"authorId": "1831395", "name": "Hanna M. Wallach"}]}, {"paperId": "2ea64b7c7617f6cc1768373124ca0243d772a90f", "externalIds": {"ACL": "2020.acl-main.486", "MAG": "2984662393", "ArXiv": "1911.03891", "DBLP": "journals/corr/abs-1911-03891", "DOI": "10.18653/v1/2020.acl-main.486", "CorpusId": 207853290}, "corpusId": 207853290, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/2ea64b7c7617f6cc1768373124ca0243d772a90f", "title": "Social Bias Frames: Reasoning about Social and Power Implications of Language", "abstract": "Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. For example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 89, "citationCount": 313, "influentialCitationCount": 63, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.486.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-10", "journal": {"name": "ArXiv", "volume": "abs/1911.03891"}, "authors": [{"authorId": "2729164", "name": "Maarten Sap"}, {"authorId": "119902504", "name": "Saadia Gabriel"}, {"authorId": "3444092", "name": "Lianhui Qin"}, {"authorId": "1746807", "name": "Dan Jurafsky"}, {"authorId": "144365876", "name": "Noah A. Smith"}, {"authorId": "1699545", "name": "Yejin Choi"}]}, {"paperId": "3cc2f69951cd24fe61be4cf32d62afbac297bc2b", "externalIds": {"MAG": "3020971214", "ArXiv": "2005.00813", "DBLP": "conf/acl/HutchinsonPDWZD20", "ACL": "2020.acl-main.487", "DOI": "10.18653/v1/2020.acl-main.487", "CorpusId": 218487466}, "corpusId": 218487466, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b", "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities", "abstract": "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 52, "citationCount": 191, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.487.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"name": "ArXiv", "volume": "abs/2005.00813"}, "authors": [{"authorId": "2083807", "name": "B. Hutchinson"}, {"authorId": "3331141", "name": "Vinodkumar Prabhakaran"}, {"authorId": "40081727", "name": "Emily L. Denton"}, {"authorId": "20825661", "name": "Kellie Webster"}, {"authorId": "2112887022", "name": "Yu Zhong"}, {"authorId": "1667883461", "name": "Stephen Denuyl"}]}, {"paperId": "0d965ed237a3b4592ecefdb618c29f63adedff76", "externalIds": {"ArXiv": "2007.08100", "ACL": "2020.acl-main.488", "DBLP": "conf/acl/LiangLZLSM20", "MAG": "3042791954", "DOI": "10.18653/v1/2020.acl-main.488", "CorpusId": 207996257}, "corpusId": 207996257, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0d965ed237a3b4592ecefdb618c29f63adedff76", "title": "Towards Debiasing Sentence Representations", "abstract": "As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 51, "citationCount": 139, "influentialCitationCount": 26, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.488.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5502-5515"}, "authors": [{"authorId": "28130078", "name": "P. Liang"}, {"authorId": "47841931", "name": "Irene Z Li"}, {"authorId": "2064345025", "name": "Emily Zheng"}, {"authorId": "144529448", "name": "Y. Lim"}, {"authorId": "145124475", "name": "R. Salakhutdinov"}, {"authorId": "49933077", "name": "Louis-Philippe Morency"}]}, {"paperId": "1fb3fa2a6a8c0d7a58b1d5dee8b676104d1a5da6", "externalIds": {"MAG": "3034374701", "DBLP": "conf/acl/SunVSTY20", "ArXiv": "1911.03903", "ACL": "2020.acl-main.489", "DOI": "10.18653/v1/2020.acl-main.489", "CorpusId": 207852450}, "corpusId": 207852450, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1fb3fa2a6a8c0d7a58b1d5dee8b676104d1a5da6", "title": "A Re-evaluation of Knowledge Graph Completion Methods", "abstract": "Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 31, "citationCount": 135, "influentialCitationCount": 21, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.489.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-10", "journal": {"pages": "5516-5522"}, "authors": [{"authorId": "48064856", "name": "Zhiqing Sun"}, {"authorId": "3404827", "name": "Shikhar Vashishth"}, {"authorId": "3313909", "name": "Soumya Sanyal"}, {"authorId": "2408872", "name": "P. Talukdar"}, {"authorId": "35729970", "name": "Yiming Yang"}]}, {"paperId": "da9d57ca205a1ce040c7a38319cde7be7c27da21", "externalIds": {"DBLP": "conf/acl/MuellerNPTL20", "ACL": "2020.acl-main.490", "ArXiv": "2005.00187", "MAG": "3035064549", "DOI": "10.18653/v1/2020.acl-main.490", "CorpusId": 218470403}, "corpusId": 218470403, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/da9d57ca205a1ce040c7a38319cde7be7c27da21", "title": "Cross-Linguistic Syntactic Evaluation of Word Prediction Models", "abstract": "A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how these models\u2019 ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models. CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop. We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT. Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses. On other constructions, agreement accuracy was generally higher in languages with richer morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 54, "citationCount": 38, "influentialCitationCount": 6, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.490.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "5523-5539"}, "authors": [{"authorId": "49355602", "name": "Aaron Mueller"}, {"authorId": "40156252", "name": "Garrett Nicolai"}, {"authorId": "1432608177", "name": "Panayiota Petrou-Zeniou"}, {"authorId": "147714277", "name": "N. Talmina"}, {"authorId": "2467508", "name": "Tal Linzen"}]}, {"paperId": "cffd8f947ba03644f62baea31c64c8920b06288e", "externalIds": {"MAG": "3035371891", "ArXiv": "2005.01831", "DBLP": "conf/acl/HaseB20", "ACL": "2020.acl-main.491", "DOI": "10.18653/v1/2020.acl-main.491", "CorpusId": 218502350}, "corpusId": 218502350, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/cffd8f947ba03644f62baea31c64c8920b06288e", "title": "Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?", "abstract": "Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 194, "influentialCitationCount": 18, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.491.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"pages": "5540-5552"}, "authors": [{"authorId": "144625004", "name": "Peter Hase"}, {"authorId": "143977268", "name": "Mohit Bansal"}]}, {"paperId": "0696ad8beb0d765973aa5cdbc6e118889d3583b0", "externalIds": {"ACL": "2020.acl-main.492", "DBLP": "conf/acl/HanWT20", "MAG": "3034475796", "ArXiv": "2005.06676", "DOI": "10.18653/v1/2020.acl-main.492", "CorpusId": 218628619}, "corpusId": 218628619, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0696ad8beb0d765973aa5cdbc6e118889d3583b0", "title": "Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions", "abstract": "Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which \u2018saliency maps\u2019 may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 42, "citationCount": 123, "influentialCitationCount": 9, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.06676", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-14", "journal": {"name": "ArXiv", "volume": "abs/2005.06676"}, "authors": [{"authorId": "40500540", "name": "Xiaochuang Han"}, {"authorId": "1912476", "name": "Byron C. Wallace"}, {"authorId": "145317727", "name": "Yulia Tsvetkov"}]}, {"paperId": "1376a8e1b06b7a7b7cacd45f52268e427c3b0135", "externalIds": {"MAG": "3022323543", "DBLP": "conf/acl/ChiHM20", "ACL": "2020.acl-main.493", "ArXiv": "2005.04511", "DOI": "10.18653/v1/2020.acl-main.493", "CorpusId": 218580945}, "corpusId": 218580945, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1376a8e1b06b7a7b7cacd45f52268e427c3b0135", "title": "Finding Universal Grammatical Relations in Multilingual BERT", "abstract": "Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks\u2019 internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 33, "citationCount": 114, "influentialCitationCount": 20, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.493.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-09", "journal": {"name": "ArXiv", "volume": "abs/2005.04511"}, "authors": [{"authorId": "48132426", "name": "Ethan A. Chi"}, {"authorId": "145430120", "name": "John Hewitt"}, {"authorId": "144783904", "name": "Christopher D. Manning"}]}, {"paperId": "1657220981714a6c312b364dbb51d604521f894e", "externalIds": {"DBLP": "journals/corr/abs-2004-02015", "MAG": "3015038845", "ACL": "2020.acl-main.494", "ArXiv": "2004.02015", "DOI": "10.18653/v1/2020.acl-main.494", "CorpusId": 214802514}, "corpusId": 214802514, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/1657220981714a6c312b364dbb51d604521f894e", "title": "Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection", "abstract": "Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them. It poses challenges for humans to interpret an explanation and connect it to model prediction. In this work, we build hierarchical explanations by detecting feature interactions. Such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models. The proposed method is evaluated with three neural text classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic and human evaluations. Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 47, "citationCount": 62, "influentialCitationCount": 13, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.494.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-04", "journal": {"pages": "5578-5593"}, "authors": [{"authorId": "7315244", "name": "Hanjie Chen"}, {"authorId": "3383040", "name": "Guangtao Zheng"}, {"authorId": "40608686", "name": "Yangfeng Ji"}]}, {"paperId": "0416fda32c39fc9531e87bab6a8a1a552bf9ada0", "externalIds": {"DBLP": "conf/acl/SubramanianBGWS20", "MAG": "3034873522", "ArXiv": "2005.00724", "ACL": "2020.acl-main.495", "DOI": "10.18653/v1/2020.acl-main.495", "CorpusId": 218487535}, "corpusId": 218487535, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/0416fda32c39fc9531e87bab6a8a1a552bf9ada0", "title": "Obtaining Faithful Interpretations from Compositional Neural Networks", "abstract": "Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model\u2019s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 38, "influentialCitationCount": 2, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.495.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-02", "journal": {"pages": "5594-5608"}, "authors": [{"authorId": "17097887", "name": "Sanjay Subramanian"}, {"authorId": "50757607", "name": "Ben Bogin"}, {"authorId": "2285178", "name": "Nitish Gupta"}, {"authorId": "51174907", "name": "Tomer Wolfson"}, {"authorId": "34650964", "name": "Sameer Singh"}, {"authorId": "1750652", "name": "Jonathan Berant"}, {"authorId": "40642935", "name": "Matt Gardner"}]}, {"paperId": "3425b03efbd69eb42a045786b3f973374f39440b", "externalIds": {"MAG": "3030885703", "DBLP": "journals/corr/abs-2005-13111", "ArXiv": "2005.13111", "ACL": "2020.acl-main.496", "DOI": "10.18653/v1/2020.acl-main.496", "CorpusId": 218900707}, "corpusId": 218900707, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/3425b03efbd69eb42a045786b3f973374f39440b", "title": "Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport", "abstract": "Selecting input features of top relevance has become a popular method for building self-explaining models. In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction. Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs. However, directly applying OT often produces dense and therefore uninterpretable alignments. To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity. Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations. We evaluate our model on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 67, "citationCount": 30, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.13111", "status": "GREEN"}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "journal": {"name": "ArXiv", "volume": "abs/2005.13111"}, "authors": [{"authorId": "2053382533", "name": "Kyle Swanson"}, {"authorId": "49297123", "name": "L. Yu"}, {"authorId": "49986267", "name": "Tao Lei"}]}, {"paperId": "f4874bd968b785cb9fceeccf26c333567a2b8dca", "externalIds": {"MAG": "3035631598", "ACL": "2020.acl-main.497", "DBLP": "conf/acl/DuaSG20", "DOI": "10.18653/v1/2020.acl-main.497", "CorpusId": 220045477}, "corpusId": 220045477, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/f4874bd968b785cb9fceeccf26c333567a2b8dca", "title": "Benefits of Intermediate Annotations in Reading Comprehension", "abstract": "Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer. A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task. In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection. We find that these intermediate annotations can provide two-fold benefits. First, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: DROP and Quoref. Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 25, "citationCount": 31, "influentialCitationCount": 0, "isOpenAccess": true, "openAccessPdf": null, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5627-5634"}, "authors": [{"authorId": "33546336", "name": "Dheeru Dua"}, {"authorId": "34650964", "name": "Sameer Singh"}, {"authorId": "40642935", "name": "Matt Gardner"}]}, {"paperId": "9780eea5e9702e30364a993ed535de49fce4f3a0", "externalIds": {"DBLP": "conf/acl/YuWZTDJ20", "MAG": "3023365376", "ArXiv": "2005.02557", "ACL": "2020.acl-main.498", "DOI": "10.18653/v1/2020.acl-main.498", "CorpusId": 218516730}, "corpusId": 218516730, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/9780eea5e9702e30364a993ed535de49fce4f3a0", "title": "Crossing Variational Autoencoders for Answer Retrieval", "abstract": "Answer retrieval is to find the most aligned answer from a large set of candidates given a question. Learning vector representations of questions/answers is the key factor. Question-answer alignment and question/answer semantics are two important signals for learning the representations. Existing methods learned semantic representations with dual encoders or dual variational auto-encoders. The semantic information was learned from language models or question-to-question (answer-to-answer) generative processes. However, the alignment and semantics were too separate to capture the aligned semantics between question and answer. In this work, we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions. Experiments show that our method outperforms the state-of-the-art answer retrieval method on SQuAD.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 15, "influentialCitationCount": 3, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.498.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-06", "journal": {"name": "ArXiv", "volume": "abs/2005.02557"}, "authors": [{"authorId": "38767143", "name": "W. Yu"}, {"authorId": "3008832", "name": "Lingfei Wu"}, {"authorId": "1694209", "name": "Qingkai Zeng"}, {"authorId": "152710186", "name": "Yu Deng"}, {"authorId": "145049359", "name": "S. Tao"}, {"authorId": "1470716407", "name": "Meng Jiang"}]}, {"paperId": "a390f26af171e784c15329847dd4a5e9806e15fa", "externalIds": {"ArXiv": "2004.10157", "DBLP": "journals/corr/abs-2004-10157", "MAG": "3035438620", "ACL": "2020.acl-main.499", "DOI": "10.18653/v1/2020.acl-main.499", "CorpusId": 216035859}, "corpusId": 216035859, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/a390f26af171e784c15329847dd4a5e9806e15fa", "title": "Logic-Guided Data Augmentation and Regularization for Consistent Question Answering", "abstract": "Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models. Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model. Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension. In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets. We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA. We further demonstrate that our approach can learn effectively from limited data.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 87, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.499.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "journal": {"pages": "5642-5650"}, "authors": [{"authorId": "35584853", "name": "Akari Asai"}, {"authorId": "2548384", "name": "Hannaneh Hajishirzi"}]}, {"paperId": "c830b1d07becf502c1cfd85a68afbf3d188a9311", "externalIds": {"DBLP": "conf/acl/SultanCAC20", "MAG": "3035621030", "ACL": "2020.acl-main.500", "DOI": "10.18653/v1/2020.acl-main.500", "CorpusId": 220047812}, "corpusId": 220047812, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/c830b1d07becf502c1cfd85a68afbf3d188a9311", "title": "On the Importance of Diversity in Question Generation for QA", "abstract": "Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA). In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search. We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 20, "citationCount": 50, "influentialCitationCount": 5, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.500.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "journal": {"pages": "5651-5656"}, "authors": [{"authorId": "2937809", "name": "Md Arafat Sultan"}, {"authorId": "145536500", "name": "Shubham Chandel"}, {"authorId": "3394760", "name": "Ram\u00f3n Fern\u00e1ndez Astudillo"}, {"authorId": "2879453", "name": "Vittorio Castelli"}]}, {"paperId": "74abf8638a3dde78f20047dc72413780f2c28fb7", "externalIds": {"MAG": "3023790734", "ArXiv": "2005.01898", "DBLP": "journals/corr/abs-2005-01898", "ACL": "2020.acl-main.501", "DOI": "10.18653/v1/2020.acl-main.501", "CorpusId": 218502386}, "corpusId": 218502386, "publicationVenue": {"id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44", "name": "Annual Meeting of the Association for Computational Linguistics", "type": "conference", "alternate_names": ["Annu Meet Assoc Comput Linguistics", "Meeting of the Association for Computational Linguistics", "ACL", "Meet Assoc Comput Linguistics"], "url": "https://www.aclweb.org/anthology/venues/acl/"}, "url": "https://www.semanticscholar.org/paper/74abf8638a3dde78f20047dc72413780f2c28fb7", "title": "Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering", "abstract": "We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings. We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans). We show that these assumptions interact, and that different configurations provide complementary benefits. We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation. Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 26, "citationCount": 21, "influentialCitationCount": 4, "isOpenAccess": true, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.501.pdf", "status": "HYBRID"}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-05", "journal": {"pages": "5657-5667"}, "authors": [{"authorId": "47413820", "name": "Hao Cheng"}, {"authorId": "2142348146", "name": "Ming-Wei Chang"}, {"authorId": "2544107", "name": "Kenton Lee"}, {"authorId": "3259253", "name": "Kristina Toutanova"}]}]