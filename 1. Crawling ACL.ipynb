{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a235f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import time # for time sleep\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aef1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url=\"https://dblp.org/db/conf/acl/index.html\"\n",
    "#Conference List Each Year\n",
    "links_response = requests.get(base_url)\n",
    "links=[]\n",
    "if links_response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(links_response.text, 'html.parser')\n",
    "    for link in soup.select('ul > li > cite > a.toc-link'):\n",
    "        links.append(link.get('href'))\n",
    "    soup.decompose()\n",
    "    \n",
    "#Crawling Papers in Each Conference\n",
    "paper_links=[]\n",
    "for link in links:\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        temp_list=soup.select('ul > li.inproceedings > nav > ul > li > div > ul > li.ee > a')\n",
    "        temp_list_len=len(temp_list)\n",
    "        for i in range(0,temp_list_len):\n",
    "            if temp_list[i].get('href').startswith('https://doi.org/'):\n",
    "                paper_links.append(temp_list[i].get('href'))\n",
    "        soup.decompose()\n",
    "    elif link_response.status_code == 404:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"THERE IS AN ERROR!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semanticscholar\n",
    "from semanticscholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper doi만 추출\n",
    "paper_DOI=[]\n",
    "for paper_link in paper_links:\n",
    "    paper_DOI.append(paper_link.split('https://doi.org/')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c45eec",
   "metadata": {},
   "source": [
    "## 약 10000개 paper 500개씩 Chunking & API CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88698cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the list of paper doi\n",
    "listOfPaper=[]\n",
    "before=0\n",
    "for i in range(500,len(paper_links),500):\n",
    "    listOfPaper.append(paper_DOI[before:i])\n",
    "    before=i\n",
    "listOfPaper.append(paper_DOI[10000:len(paper_links)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc268c",
   "metadata": {},
   "source": [
    "## 안에 있는 500개 중에 ERROR가 있어서 되는 chunk 먼저 API CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = SemanticScholar()\n",
    "\n",
    "## There is an error with API, so it needs to be modified a little bit\n",
    "## 7~20 index first\n",
    "temp7to9=[]\n",
    "for i in range(7,10):\n",
    "    paper=sch.get_papers(listOfPaper[i])\n",
    "    temp7to9+=list(paper)\n",
    "    \n",
    "## 10,11,12,13,14 is ERROR...............................ㅠㅠㅠㅠ\n",
    "temp15to17=[]  ## typo!! 16까지임\n",
    "for i in range(15,17):\n",
    "    paper=sch.get_papers(listOfPaper[i])\n",
    "    temp15to17+=list(paper)\n",
    "    \n",
    "#17,18,19 ERROR\n",
    "temp20=[]\n",
    "paper=sch.get_papers(listOfPaper[20])\n",
    "temp20+=list(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "884e2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_FIELDS = [\n",
    "        'abstract',\n",
    "        'authors',\n",
    "        'citationCount',\n",
    "        'corpusId',\n",
    "        'externalIds',\n",
    "        'fieldsOfStudy',\n",
    "        'influentialCitationCount',\n",
    "        'isOpenAccess',\n",
    "        'journal',\n",
    "        'openAccessPdf',\n",
    "        'paperId',\n",
    "        'publicationDate',\n",
    "        'publicationTypes',\n",
    "        'publicationVenue',\n",
    "        'referenceCount',\n",
    "        's2FieldsOfStudy',\n",
    "        'title',\n",
    "        'url',\n",
    "        'venue',\n",
    "        'year'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074412d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list11to13=[] #11~13번 chunk\n",
    "for i in [11,12,13]:\n",
    "    ls=[]\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        doi=listOfPaper[i][j].split(\"10.18653/v1/\")[1].upper()\n",
    "        ls.append(\"10.18653/v1/\"+doi)\n",
    "    list11to13.append(ls)    \n",
    "\n",
    "temp11to13=[]\n",
    "for i in range(0,3):\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(list11to13[i][j],SEARCH_FIELDS)\n",
    "            temp11to13.append(paper)\n",
    "        except:\n",
    "            print(i,j,list11to13[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d2d06",
   "metadata": {},
   "source": [
    "## 10.18653 이 아니라 10.3115인 구간으로 인해 (14,473)부터 다시 request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0755c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list14=[] \n",
    "for j in tqdm(range(0,473),desc=str(14)):\n",
    "        doi=listOfPaper[14][j].split(\"10.18653/v1/\")[1].upper()\n",
    "        list14.append(\"10.18653/v1/\"+doi)\n",
    "for j in tqdm(range(473,500),desc=str(14)):\n",
    "    doi=listOfPaper[14][j].split(\"10.3115/v1/\")[1].upper()\n",
    "    list14.append(\"10.3115/v1/\"+doi)\n",
    "\n",
    "temp14=[]\n",
    "for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(list14[j],SEARCH_FIELDS)\n",
    "            temp14.append(paper)\n",
    "        except:\n",
    "            print(j,list14[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16accc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list17to19=[]\n",
    "for i in [17,18]:\n",
    "    ls=[]\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        doi=listOfPaper[i][j].split(\"10.3115/\")[1].upper()\n",
    "        ls.append(\"10.3115/\"+doi)\n",
    "    list17to19.append(ls) \n",
    "    \n",
    "ls=[]\n",
    "for j in tqdm(range(0,260),desc=str(19)):\n",
    "    doi=listOfPaper[i][j].split(\"10.3115/\")[1].upper()\n",
    "    ls.append(\"10.3115/\"+doi)\n",
    "k=23\n",
    "for i in range(260,282):\n",
    "    ls.append(\"10.1007/3-540-55801-2_\"+str(k))\n",
    "    k+=1\n",
    "for j in tqdm(range(282,500),desc=str(19)):\n",
    "    doi=listOfPaper[19][j].split(\"10.3115/\")[1].upper()\n",
    "    ls.append(\"10.3115/\"+doi)\n",
    "list17to19.append(ls)\n",
    "\n",
    "\n",
    "temp17to19=[]\n",
    "for i in range(0,3):\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(list17to19[i][j],SEARCH_FIELDS)\n",
    "            temp17to19.append(paper)\n",
    "        except:\n",
    "            print(i,j,list17to19[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd1f2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j in tqdm(range(385,500),desc=str(10)):\n",
    "        doi=listOfPaper[10][j].split(\"10.18653/v1/\")[1].upper() \n",
    "        listOfPaper[10][j]=\"10.18653/v1/\"+doi\n",
    "\n",
    "temp10=[]\n",
    "\n",
    "for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(listOfPaper[10][j],SEARCH_FIELDS)\n",
    "            temp10.append(paper)\n",
    "        except:\n",
    "            print(i,j,listOfPaper[10][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://api.crossref.org/works/\"+listOfPaper[0][0]\n",
    "paper_response=requests.get(url)\n",
    "paper=paper_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddef34f",
   "metadata": {},
   "source": [
    "### DOI 중에 arkiv로 되어있는 DOI로 찾아야하는 경우가 많음. 따라서 semantic scholar에서 논문 제목을 통해 paperId 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd5ebe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paperIdList=[]\n",
    "for i in [0,1,2,3,4,5,6]:\n",
    "    ls=[]\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "            if j%10==0 and not (i==0 and j==0):\n",
    "                time.sleep(20)\n",
    "            url=\"https://api.crossref.org/works/\"+listOfPaper[i][j]\n",
    "            paper_response=requests.get(url)\n",
    "            if paper_response.status_code==200:\n",
    "                    paper=paper_response.json()\n",
    "                    title=paper['message']['title'][0]\n",
    "                    try:\n",
    "                        temp=sch.search_paper(title)\n",
    "                        if len(temp[0].paperId)==0:\n",
    "                            ls.append(listOfPaper[i][j])\n",
    "                            print(title,listOfPaper[i][j],i,j)\n",
    "                        else:\n",
    "                            ls.append(temp[0].paperId)\n",
    "                            \n",
    "                    except:\n",
    "                        ls.append(listOfPaper[i][j])\n",
    "                        print(title,listOfPaper[i][j],i,j)\n",
    "            else:\n",
    "                print(\"no\")\n",
    "    paperIdList.append(ls)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e74f1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp0to6=[]\n",
    "for i in range(0,7):\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(paperIdList[i][j],SEARCH_FIELDS)\n",
    "            temp0to6.append(paper)\n",
    "        except:\n",
    "            print(i,j,paperIdList[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "id": "39d68c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_papers=[]\n",
    "\n",
    "for temp in [temp0to6,temp7to9,temp10,temp11to13,temp14,temp15to17,temp17to19,temp20]:\n",
    "    for i in range(0,len(temp)):\n",
    "        temp[i]=dict(temp[i])\n",
    "    acl_papers+=temp\n",
    "\n",
    "datapath=\"./ACL_PAPERS.json\"\n",
    "with open(datapath,'w') as f:\n",
    "    json.dump(acl_papers,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4954e60",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa8b41",
   "metadata": {},
   "source": [
    "## 300개 missing data 중 55개 제외 살림 (abstract data 없는게 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6467bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "absls=[]\n",
    "for i in range(0,len(acl_papers)):\n",
    "        if acl_papers[i]['abstract']==None:\n",
    "            print(i, 'none')\n",
    "            absls.append(i)\n",
    "            c+=1\n",
    "        elif len(acl_papers[i]['abstract'])<2:\n",
    "            print(i,'dot')\n",
    "            absls.append(i)\n",
    "            c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in absls:\n",
    "    try:\n",
    "        doi=acl_papers[i]['externalIds']['DOI'].split(\"10.18653/v1/\")[1]\n",
    "        link=\"https://aclanthology.org/\"+doi\n",
    "        link_response = requests.get(link)\n",
    "    except:\n",
    "        print(i,\"no site\")\n",
    "        break\n",
    "    if link_response.status_code == 200:\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        abstract=soup.select('#main >div>div> div >div> span')\n",
    "        try:\n",
    "            if len(abstract[0].text)!=0:\n",
    "                acl_papers[i]['abstract']=abstract[0].text\n",
    "        except:\n",
    "            print(i,\"no abstract\")\n",
    "            break\n",
    "        soup.decompose()\n",
    "    elif link_response.status_code == 404:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e2fca",
   "metadata": {},
   "source": [
    "# 약 79~99 - ACM library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a98dd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in absls:\n",
    "    try:\n",
    "        doi=acl_papers[i]['externalIds']['DOI']\n",
    "        link=\"https://dl.acm.org/doi/abs/\"+doi\n",
    "        link_response = requests.get(link)\n",
    "    except:\n",
    "        print(i,\"no site\")\n",
    "        break\n",
    "    if link_response.status_code == 200:\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        abstract=soup.select('div.abstractSection > p')\n",
    "        try:\n",
    "            if len(abstract[0].text)!=0:\n",
    "                acl_papers[i]['abstract']=abstract[0].text\n",
    "        except:\n",
    "            print(i,\"no abstract\")\n",
    "            break\n",
    "        soup.decompose()\n",
    "    elif link_response.status_code == 404:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(\"!!\",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b2dd8",
   "metadata": {},
   "source": [
    "## 없는 데이터가 있다 .당연히 DOI가 모든 paper에 있을 줄 알고 doi.org/~ 된 것만 추가함..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawling Papers in Each Conference\n",
    "missed_links=[]\n",
    "for link in tqdm(links):\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        temp_list=soup.select('ul > li.inproceedings > nav > ul > li > div > ul > li.ee > a')\n",
    "        temp_list_len=len(temp_list)\n",
    "        alreadyIn=0\n",
    "        for i in range(0,temp_list_len):\n",
    "            if temp_list[i].get('href').startswith('https://doi.org/'):\n",
    "                alreadyIn=1\n",
    "                break\n",
    "        if alreadyIn==0:\n",
    "            for i in range(0,temp_list_len):\n",
    "                if temp_list[i].get('href').startswith('https://aclanthology.org/'):\n",
    "                    missed_links.append(temp_list[i].get('href'))\n",
    "        soup.decompose()\n",
    "    else:\n",
    "        print(link)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc492d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in tqdm(links[136:]):\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        temp_list=soup.select('ul > li.inproceedings > nav > ul > li > div > ul > li.ee > a')\n",
    "        temp_list_len=len(temp_list)\n",
    "        alreadyIn=0\n",
    "        for i in range(0,temp_list_len):\n",
    "            if temp_list[i].get('href').startswith('https://doi.org/'):\n",
    "                alreadyIn=1\n",
    "                break\n",
    "        if alreadyIn==0:\n",
    "            for i in range(0,temp_list_len):\n",
    "                if temp_list[i].get('href').startswith('https://aclanthology.org/'):\n",
    "                    missed_links.append(temp_list[i].get('href'))\n",
    "        soup.decompose()\n",
    "    else:\n",
    "        print(link)\n",
    "\n",
    "link_response = requests.get(\"https://aclanthology.org/W97-1005/\")\n",
    "if link_response.status_code == 200:\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        title=soup.select('#title > a')[0].text\n",
    "        temp=soup.select('p.lead > a')\n",
    "        authors=[{'name':i.text} for i in temp]\n",
    "        year=soup.select('div.order-2 > dl > dd')[3].text\n",
    "\n",
    "#for abstraction\n",
    "def paperId(title):\n",
    "    temp=sch.search_paper(title)\n",
    "    if temp[0]['title']==title:\n",
    "        return temp[0]\n",
    "    else :\n",
    "        print(title)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1099a77b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missed_papers=[]\n",
    "for link in tqdm(missed_links[2438:]):\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        title=soup.select('#title > a')[0].text\n",
    "        paper_info=paperId(title)\n",
    "        if paper_info is None:\n",
    "            continue\n",
    "        else:\n",
    "            missed_papers.append(paper_info)\n",
    "            x=random.randint(10, 15)\n",
    "            time.sleep(x)\n",
    "        soup.decompose()\n",
    "    else:\n",
    "        print(1,link)\n",
    "\n",
    "c=0\n",
    "mls=[]\n",
    "for i in range(0,len(missed_papers)):\n",
    "        if missed_papers[i]['abstract']==None:\n",
    "            print(i, 'none')\n",
    "            mls.append(i)\n",
    "            c+=1\n",
    "        elif len(missed_papers[i]['abstract'])<2:\n",
    "            print(i,'dot')\n",
    "            mls.append(i)\n",
    "            c+=1\n",
    "print(c)\n",
    "acl_papers+=missed_papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
