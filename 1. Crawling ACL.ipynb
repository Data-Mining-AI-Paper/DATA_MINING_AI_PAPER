{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a235f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import time # for time sleep\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aef1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url=\"https://dblp.org/db/conf/acl/index.html\"\n",
    "#Conference List Each Year\n",
    "links_response = requests.get(base_url)\n",
    "links=[]\n",
    "if links_response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(links_response.text, 'html.parser')\n",
    "    for link in soup.select('ul > li > cite > a.toc-link'):\n",
    "        links.append(link.get('href'))\n",
    "    soup.decompose()\n",
    "    \n",
    "#Crawling Papers in Each Conference\n",
    "paper_links=[]\n",
    "for link in links:\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        temp_list=soup.select('ul > li.inproceedings > nav > ul > li > div > ul > li.ee > a')\n",
    "        temp_list_len=len(temp_list)\n",
    "        for i in range(0,temp_list_len):\n",
    "            if temp_list[i].get('href').startswith('https://doi.org/'):\n",
    "                paper_links.append(temp_list[i].get('href'))\n",
    "        soup.decompose()\n",
    "    elif link_response.status_code == 404:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"THERE IS AN ERROR!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semanticscholar\n",
    "from semanticscholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper doi만 추출\n",
    "paper_DOI=[]\n",
    "for paper_link in paper_links:\n",
    "    paper_DOI.append(paper_link.split('https://doi.org/')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c45eec",
   "metadata": {},
   "source": [
    "## 약 10000개 paper 500개씩 Chunking & API CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88698cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the list of paper doi\n",
    "listOfPaper=[]\n",
    "before=0\n",
    "for i in range(500,len(paper_links),500):\n",
    "    listOfPaper.append(paper_DOI[before:i])\n",
    "    before=i\n",
    "listOfPaper.append(paper_DOI[10000:len(paper_links)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc268c",
   "metadata": {},
   "source": [
    "## 안에 있는 500개 중에 ERROR가 있어서 되는 chunk 먼저 API CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = SemanticScholar()\n",
    "\n",
    "## There is an error with API, so it needs to be modified a little bit\n",
    "## 7~20 index first\n",
    "temp7to9=[]\n",
    "for i in range(7,10):\n",
    "    paper=sch.get_papers(listOfPaper[i])\n",
    "    temp7to9+=list(paper)\n",
    "    \n",
    "## 10,11,12,13,14 is ERROR...............................ㅠㅠㅠㅠ\n",
    "temp15to17=[]  ## typo!! 16까지임\n",
    "for i in range(15,17):\n",
    "    paper=sch.get_papers(listOfPaper[i])\n",
    "    temp15to17+=list(paper)\n",
    "    \n",
    "#17,18,19 ERROR\n",
    "temp20=[]\n",
    "paper=sch.get_papers(listOfPaper[20])\n",
    "temp20+=list(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "884e2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_FIELDS = [\n",
    "        'abstract',\n",
    "        'authors',\n",
    "        'citationCount',\n",
    "        'corpusId',\n",
    "        'externalIds',\n",
    "        'fieldsOfStudy',\n",
    "        'influentialCitationCount',\n",
    "        'isOpenAccess',\n",
    "        'journal',\n",
    "        'openAccessPdf',\n",
    "        'paperId',\n",
    "        'publicationDate',\n",
    "        'publicationTypes',\n",
    "        'publicationVenue',\n",
    "        'referenceCount',\n",
    "        's2FieldsOfStudy',\n",
    "        'title',\n",
    "        'url',\n",
    "        'venue',\n",
    "        'year'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074412d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list11to13=[] #11~13번 chunk\n",
    "for i in [11,12,13]:\n",
    "    ls=[]\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        doi=listOfPaper[i][j].split(\"10.18653/v1/\")[1].upper()\n",
    "        ls.append(\"10.18653/v1/\"+doi)\n",
    "    list11to13.append(ls)    \n",
    "\n",
    "temp11to13=[]\n",
    "for i in range(0,3):\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(list11to13[i][j],SEARCH_FIELDS)\n",
    "            temp11to13.append(paper)\n",
    "        except:\n",
    "            print(i,j,list11to13[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d2d06",
   "metadata": {},
   "source": [
    "## 10.18653 이 아니라 10.3115인 구간으로 인해 (14,473)부터 다시 request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "38c0755c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  26%|████████████████████▋                                                            | 128/500 [04:53<04:38,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 10.18653/v1/P16-1146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  45%|████████████████████████████████████▎                                            | 224/500 [08:35<10:51,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223 10.18653/v1/P16-2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  71%|█████████████████████████████████████████████████████████▏                       | 353/500 [13:24<07:59,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 10.18653/v1/P16-3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2: 100%|█████████████████████████████████████████████████████████████████████████████████| 500/500 [18:47<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "list14=[] #14번 청크\n",
    "for j in tqdm(range(0,473),desc=str(14)):\n",
    "        doi=listOfPaper[14][j].split(\"10.18653/v1/\")[1].upper()\n",
    "        list14.append(\"10.18653/v1/\"+doi)\n",
    "for j in tqdm(range(473,500),desc=str(14)):\n",
    "    doi=listOfPaper[14][j].split(\"10.3115/v1/\")[1].upper()\n",
    "    list14.append(\"10.3115/v1/\"+doi)\n",
    "\n",
    "temp14=[]\n",
    "for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(list14[j],SEARCH_FIELDS)\n",
    "            temp14.append(paper)\n",
    "        except:\n",
    "            print(j,list14[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16accc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  34%|███████████████████████████▍                                                     | 169/500 [06:19<03:09,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 168 10.3115/1218955.1219038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  60%|████████████████████████████████████████████████▎                                | 298/500 [11:10<02:27,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 297 10.3115/1073083.1073091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  65%|████████████████████████████████████████████████████▊                            | 326/500 [12:26<12:13,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 325 10.3115/1073083.1073126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  73%|███████████████████████████████████████████████████████████▍                     | 367/500 [13:57<02:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 365 10.3115/1118149.1118157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "0:  74%|███████████████████████████████████████████████████████████▌                     | 368/500 [13:57<01:30,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 367 10.3115/1118149.1118159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  75%|████████████████████████████████████████████████████████████▍                    | 373/500 [14:19<06:53,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 372 10.3115/1118656.1118659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  75%|████████████████████████████████████████████████████████████▊                    | 375/500 [14:19<03:35,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 374 10.3115/1118656.1118661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  76%|█████████████████████████████████████████████████████████████▌                   | 380/500 [14:20<00:55,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 378 10.3115/1118656.1118665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  77%|██████████████████████████████████████████████████████████████▎                  | 385/500 [14:41<03:14,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 383 10.3115/1118656.1118670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 100%|█████████████████████████████████████████████████████████████████████████████████| 500/500 [18:51<00:00,  2.26s/it]\n",
      "1:  44%|███████████████████████████████████▊                                             | 221/500 [08:48<43:39,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 220 10.3115/980845.980951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1: 100%|█████████████████████████████████████████████████████████████████████████████████| 500/500 [18:56<00:00,  2.27s/it]\n",
      "2:  53%|██████████████████████████████████████████▉                                      | 265/500 [10:09<06:36,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 263 10.1007/3-540-55801-2_26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  64%|███████████████████████████████████████████████████▋                             | 319/500 [12:07<04:53,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 318 10.3115/981823.981860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:  90%|████████████████████████████████████████████████████████████████████████▉        | 450/500 [17:02<00:24,  2.08it/s]"
     ]
    }
   ],
   "source": [
    "list17to19=[]\n",
    "for i in [17,18]:\n",
    "    ls=[]\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        doi=listOfPaper[i][j].split(\"10.3115/\")[1].upper()\n",
    "        ls.append(\"10.3115/\"+doi)\n",
    "    list17to19.append(ls) \n",
    "    \n",
    "ls=[]\n",
    "for j in tqdm(range(0,260),desc=str(19)):\n",
    "    doi=listOfPaper[i][j].split(\"10.3115/\")[1].upper()\n",
    "    ls.append(\"10.3115/\"+doi)\n",
    "k=23\n",
    "for i in range(260,282):\n",
    "    ls.append(\"10.1007/3-540-55801-2_\"+str(k))\n",
    "    k+=1\n",
    "for j in tqdm(range(282,500),desc=str(19)):\n",
    "    doi=listOfPaper[19][j].split(\"10.3115/\")[1].upper()\n",
    "    ls.append(\"10.3115/\"+doi)\n",
    "list17to19.append(ls)\n",
    "\n",
    "\n",
    "temp17to19=[]\n",
    "for i in range(0,3):\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(list17to19[i][j],SEARCH_FIELDS)\n",
    "            temp17to19.append(paper)\n",
    "        except:\n",
    "            print(i,j,list17to19[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "53bd1f2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:   4%|██▌                                                                   | 18/500 [00:54<29:49,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 17 10.18653/v1/2020.acl-main.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:  74%|██████████████████████████████████████████████████▉                  | 369/500 [14:03<01:15,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 367 10.18653/v1/2020.acl-tutorials.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:  92%|███████████████████████████████████████████████████████████████▌     | 461/500 [17:46<04:09,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 460 10.18653/v1/P19-1076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [18:55<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "for j in tqdm(range(385,500),desc=str(10)):\n",
    "        doi=listOfPaper[10][j].split(\"10.18653/v1/\")[1].upper() ## 대문자로 변경해야 됨\n",
    "        listOfPaper[10][j]=\"10.18653/v1/\"+doi\n",
    "\n",
    "temp10=[]\n",
    "\n",
    "for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(listOfPaper[10][j],SEARCH_FIELDS)\n",
    "            temp10.append(paper)\n",
    "        except:\n",
    "            print(i,j,listOfPaper[10][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://api.crossref.org/works/\"+listOfPaper[0][0]\n",
    "paper_response=requests.get(url)\n",
    "paper=paper_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddef34f",
   "metadata": {},
   "source": [
    "### DOI 중에 arkiv로 되어있는 DOI로 찾아야하는 경우가 많음. 따라서 semantic scholar에서 논문 제목을 통해 paperId 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "cdd5ebe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  53%|████████████████████████████████████▌                                | 265/500 [33:14<20:42,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Ordinal Latent Variable Model of Conflict Intensity 10.18653/v1/2023.acl-long.265 0 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  53%|████████████████████████████████████▊                                | 267/500 [33:25<22:33,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training to Learn in Context 10.18653/v1/2023.acl-long.267 0 266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  81%|███████████████████████████████████████████████████████▊             | 404/500 [46:59<21:03, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peeking inside the black box: A Commonsense-aware Generative Framework for Explainable Complaint Detection 10.18653/v1/2023.acl-long.404 0 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  83%|█████████████████████████████████████████████████████████▏           | 414/500 [48:37<17:17, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic representations for fewer-shot relation extraction across domains 10.18653/v1/2023.acl-long.414 0 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  89%|█████████████████████████████████████████████████████████████▌       | 446/500 [50:11<04:14,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Document-Level Event Argument Extraction 10.18653/v1/2023.acl-long.446 0 445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [53:29<00:00,  6.42s/it]\n",
      "1:  13%|█████████▏                                                            | 66/500 [03:51<34:48,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just Like a Human Would, Direct Access to Sarcasm Augmented with Potential Result and Reaction 10.18653/v1/2023.acl-long.566 1 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [52:57<00:00,  6.36s/it]\n",
      "2: 100%|███████████████████████████████████████████████████████████████████| 500/500 [1:17:52<00:00,  9.35s/it]\n",
      "3:  70%|██████████████████████████████████████████████▋                    | 348/500 [1:56:05<20:33,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models 10.18653/v1/2023.findings-acl.741 3 347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  87%|████████████████████████████████████████████████████████▌        | 435/500 [2:16:58<1:35:38, 88.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffuSum: Generation Enhanced Extractive Summarization with Diffusion 10.18653/v1/2023.findings-acl.828 3 434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  89%|█████████████████████████████████████████████████████████▌       | 443/500 [2:26:58<1:34:42, 99.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract then Play: A Skill-centric Reinforcement Learning Framework for Text-based Games 10.18653/v1/2023.findings-acl.836 3 442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  90%|█████████████████████████████████████████████████████████▌      | 450/500 [2:43:10<2:02:32, 147.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don’t Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness 10.18653/v1/2023.findings-acl.843 3 449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  91%|██████████████████████████████████████████████████████████▎     | 456/500 [2:50:10<1:22:32, 112.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiaASQ: A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis 10.18653/v1/2023.findings-acl.849 3 455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  93%|███████████████████████████████████████████████████████████▌    | 465/500 [3:02:13<1:15:59, 130.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring the Effectiveness of Prompt Engineering for Legal Reasoning Tasks 10.18653/v1/2023.findings-acl.858 3 464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:  97%|████████████████████████████████████████████████████████████████▊  | 484/500 [3:21:16<24:50, 93.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection and Mitigation of the Negative Impact of Dataset Extractivity on Abstractive Summarization 10.18653/v1/2023.findings-acl.877 3 483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3: 100%|███████████████████████████████████████████████████████████████████| 500/500 [3:29:20<00:00, 25.12s/it]\n",
      "4:  78%|█████████████████████████████████████████████████████▉               | 391/500 [20:04<13:13,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating Failures of Automatic Translationin the Case of Unambiguous Gender 10.18653/v1/2022.acl-long.243 4 390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:  92%|███████████████████████████████████████████████████████████████▍     | 460/500 [23:07<00:36,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models 10.18653/v1/2022.acl-long.312 4 459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [24:57<00:00,  2.99s/it]\n",
      "5:  31%|█████████████████████▌                                               | 156/500 [07:39<26:38,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpness-Aware Minimization Improves Language Model Generalization 10.18653/v1/2022.acl-long.508 5 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [26:35<00:00,  3.19s/it]\n",
      "6: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [36:58<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "paperIdList=[]\n",
    "for i in [0,1,2,3,4,5,6]:\n",
    "    ls=[]\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "            if j%10==0 and not (i==0 and j==0):\n",
    "                time.sleep(20)\n",
    "            url=\"https://api.crossref.org/works/\"+listOfPaper[i][j]\n",
    "            paper_response=requests.get(url)\n",
    "            if paper_response.status_code==200:\n",
    "                    paper=paper_response.json()\n",
    "                    title=paper['message']['title'][0]\n",
    "                    try:\n",
    "                        temp=sch.search_paper(title)\n",
    "                        if len(temp[0].paperId)==0:\n",
    "                            ls.append(listOfPaper[i][j])\n",
    "                            print(title,listOfPaper[i][j],i,j)\n",
    "                        else:\n",
    "                            ls.append(temp[0].paperId)\n",
    "                            \n",
    "                    except:\n",
    "                        ls.append(listOfPaper[i][j])\n",
    "                        print(title,listOfPaper[i][j],i,j)\n",
    "            else:\n",
    "                print(\"no\")\n",
    "    paperIdList.append(ls)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e74f1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  39%|███████████████████████████▏                                         | 197/500 [08:06<20:48,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 196 ade39c39048d66465c7288e4a7f8258a1bce9e60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  79%|██████████████████████████████████████████████████████▊              | 397/500 [16:00<02:17,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 396 0232715f9089e3a2fc002cff6737bb9939805b8d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:  93%|███████████████████████████████████████████████████████████████▉     | 463/500 [18:53<03:54,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 462 52f3b181f6361cd85914798273e497264b2bc32d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [20:01<00:00,  2.40s/it]\n",
      "1: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [18:55<00:00,  2.27s/it]\n",
      "2:  29%|████████████████████▎                                                | 147/500 [05:44<09:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 146 ef5f7cd21b5d34797636239a7b9c8ba6af440aab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [19:09<00:00,  2.30s/it]\n",
      "6:   4%|██▊                                                                   | 20/500 [00:45<04:07,  1.94it/s]"
     ]
    }
   ],
   "source": [
    "temp0to6=[]\n",
    "for i in range(0,7):\n",
    "    for j in tqdm(range(0,500),desc=str(i)):\n",
    "        if j %10==0:\n",
    "            time.sleep(20)\n",
    "        try:\n",
    "            paper=sch.get_paper(paperIdList[i][j],SEARCH_FIELDS)\n",
    "            temp0to6.append(paper)\n",
    "        except:\n",
    "            print(i,j,paperIdList[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "id": "39d68c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_papers=[]\n",
    "\n",
    "for temp in [temp0to6,temp7to9,temp10,temp11to13,temp14,temp15to17,temp17to19,temp20]:\n",
    "    for i in range(0,len(temp)):\n",
    "        temp[i]=dict(temp[i])\n",
    "    acl_papers+=temp\n",
    "\n",
    "datapath=\"./ACL_PAPERS.json\"\n",
    "with open(datapath,'w') as f:\n",
    "    json.dump(acl_papers,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4954e60",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa8b41",
   "metadata": {},
   "source": [
    "## 300개 missing data 중 55개 제외 살림 (abstract data 없는게 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "id": "d6467bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9684 none\n",
      "9760 none\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "absls=[]\n",
    "for i in range(0,len(acl_papers)):\n",
    "        if acl_papers[i]['abstract']==None:\n",
    "            print(i, 'none')\n",
    "            absls.append(i)\n",
    "            c+=1\n",
    "        elif len(acl_papers[i]['abstract'])<2:\n",
    "            print(i,'dot')\n",
    "            absls.append(i)\n",
    "            c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "id": "f552f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4028 dd\n"
     ]
    }
   ],
   "source": [
    "for i in absls:\n",
    "    try:\n",
    "        doi=acl_papers[i]['externalIds']['DOI'].split(\"10.18653/v1/\")[1]\n",
    "        link=\"https://aclanthology.org/\"+doi\n",
    "        link_response = requests.get(link)\n",
    "    except:\n",
    "        print(i,\"no site\")\n",
    "        break\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        abstract=soup.select('#main >div>div> div >div> span')\n",
    "        try:\n",
    "            if len(abstract[0].text)!=0:\n",
    "                acl_papers[i]['abstract']=abstract[0].text\n",
    "        except:\n",
    "            print(i,\"no abstract\")\n",
    "            break\n",
    "        soup.decompose()\n",
    "    elif link_response.status_code == 404:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e2fca",
   "metadata": {},
   "source": [
    "# 약 79~99 - ACM library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "id": "a4a98dd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9760\n",
      "9761\n",
      "9762\n",
      "9764\n",
      "9765\n",
      "9766\n",
      "9767\n",
      "9768\n",
      "9769\n",
      "9770\n",
      "9771\n",
      "9772\n",
      "9773\n",
      "9774\n",
      "9775\n",
      "9776\n",
      "9777\n",
      "9778\n",
      "9779\n",
      "9780\n",
      "9781\n"
     ]
    }
   ],
   "source": [
    "for i in absls:\n",
    "    try:\n",
    "        doi=acl_papers[i]['externalIds']['DOI']\n",
    "        link=\"https://dl.acm.org/doi/abs/\"+doi\n",
    "        link_response = requests.get(link)\n",
    "    except:\n",
    "        print(i,\"no site\")\n",
    "        break\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        abstract=soup.select('div.abstractSection > p')\n",
    "        try:\n",
    "            if len(abstract[0].text)!=0:\n",
    "                acl_papers[i]['abstract']=abstract[0].text\n",
    "        except:\n",
    "            print(i,\"no abstract\")\n",
    "            break\n",
    "        soup.decompose()\n",
    "    elif link_response.status_code == 404:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(\"!!\",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b2dd8",
   "metadata": {},
   "source": [
    "## 없는 데이터가 있다 .당연히 DOI가 모든 paper에 있을 줄 알고 doi.org/~ 된 것만 추가함..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "id": "bd13561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████▊              | 135/168 [05:52<01:26,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THERE IS AN ERROR!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Crawling Papers in Each Conference\n",
    "missed_links=[]\n",
    "for link in tqdm(links):\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        temp_list=soup.select('ul > li.inproceedings > nav > ul > li > div > ul > li.ee > a')\n",
    "        temp_list_len=len(temp_list)\n",
    "        alreadyIn=0\n",
    "        for i in range(0,temp_list_len):\n",
    "            if temp_list[i].get('href').startswith('https://doi.org/'):\n",
    "                alreadyIn=1\n",
    "                break\n",
    "        if alreadyIn==0:\n",
    "            for i in range(0,temp_list_len):\n",
    "                if temp_list[i].get('href').startswith('https://aclanthology.org/'):\n",
    "                    missed_links.append(temp_list[i].get('href'))\n",
    "        soup.decompose()\n",
    "    else:\n",
    "        print(link)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc492d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in tqdm(links[136:]):\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        temp_list=soup.select('ul > li.inproceedings > nav > ul > li > div > ul > li.ee > a')\n",
    "        temp_list_len=len(temp_list)\n",
    "        alreadyIn=0\n",
    "        for i in range(0,temp_list_len):\n",
    "            if temp_list[i].get('href').startswith('https://doi.org/'):\n",
    "                alreadyIn=1\n",
    "                break\n",
    "        if alreadyIn==0:\n",
    "            for i in range(0,temp_list_len):\n",
    "                if temp_list[i].get('href').startswith('https://aclanthology.org/'):\n",
    "                    missed_links.append(temp_list[i].get('href'))\n",
    "        soup.decompose()\n",
    "    else:\n",
    "        print(link)\n",
    "\n",
    "link_response = requests.get(\"https://aclanthology.org/W97-1005/\")\n",
    "if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        title=soup.select('#title > a')[0].text\n",
    "        temp=soup.select('p.lead > a')\n",
    "        authors=[{'name':i.text} for i in temp]\n",
    "        year=soup.select('div.order-2 > dl > dd')[3].text\n",
    "\n",
    "#for abstraction\n",
    "def paperId(title):\n",
    "    temp=sch.search_paper(title)\n",
    "    if temp[0]['title']==title:\n",
    "        return temp[0]\n",
    "    else :\n",
    "        print(title)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "id": "1099a77b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2177 none\n",
      "2347 none\n",
      "2391 none\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "missed_papers=[]\n",
    "for link in tqdm(missed_links[2438:]):\n",
    "    link_response = requests.get(link)\n",
    "    if link_response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "        title=soup.select('#title > a')[0].text\n",
    "        paper_info=paperId(title)\n",
    "        if paper_info is None:\n",
    "            continue\n",
    "        else:\n",
    "            missed_papers.append(paper_info)\n",
    "            x=random.randint(10, 15)\n",
    "            time.sleep(x)\n",
    "        soup.decompose()\n",
    "    else:\n",
    "        print(1,link)\n",
    "\n",
    "c=0\n",
    "mls=[]\n",
    "for i in range(0,len(missed_papers)):\n",
    "        if missed_papers[i]['abstract']==None:\n",
    "            print(i, 'none')\n",
    "            mls.append(i)\n",
    "            c+=1\n",
    "        elif len(missed_papers[i]['abstract'])<2:\n",
    "            print(i,'dot')\n",
    "            mls.append(i)\n",
    "            c+=1\n",
    "print(c)\n",
    "acl_papers+=missed_papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
